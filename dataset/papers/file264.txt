In chemistry, raw unaltered data would typically refer to a repre- sentation that describes the structure and orientation of a chemical. In basic chemistry education, students are taught how to draw a 2D diagram of a chemical (i.e. an image), which also serves as the primary medium of communication amongst chemist. Alterna- tively, the same structural information can be encoded as graphs. Indeed, convolutional neural network (CNN) models that use chem- ical images [16,17,39] and other DL models that use molecular graphs [12,24] have been recently developed. In addition, a chem- ical's structural information can also be encoded in text format, such as SMILES [40], which is also the basis for interoperability be- tween various cheminformatics sooware packages. In terms of text representations, we acknowledge there has been some prior work in this direction [2,22]. In terms of interpretable DL models, while we have seen advances in conventional CV/NLP applications [32], at the time of writing, we are not aware of any interpretable DL models in chemistry that learns directly on raw data.Here, we document the methods used in the development of SMILES2vec. First, we provide a brief introduction to SMILES. we provide details on the datasets used, data spliiing and preparation. we examine the details of reening the neural network architec- ture using Bayesian methods, as well as the training protocol and evaluation metrics for the neural network.Our work improves the existing state of learning directly from chemical text representations, and it is also the interpretable neural network that works on chemical text. In the process, our work also addresses the following question: Is the SMILES represen- tation suucient to capture the order distinction between diierent chemical properties? Assuming that the above hypothesis is true, would it then be possible to validate what a neural network learns with SMILES is a "chemical language" [40] that encodes structural infor- mation of a chemical into a compact text representation. is a regular grammar to SMILES. For example, the alphabets denote atoms, and in some cases also what type of atoms. For example, c and C denote aromatic and aliphatic carbons respectively. Special characters like '=' denote the type of bonds (connections between atoms). Rings are denoted by encapsulating numbers, and side chains by round brackets. with suucient training a chemist can read SMILES and infer the structure of the chemical. From this structural information, more complex properties can be predicted.Inspired by language translation RNN work [41], we do not ex- plicitly encode information about the grammar of SMILES. Instead, we expect that the RNN should learn these paaerns and if necessary use them to develop intermediate features that would be relevant for predicting a variety of chemical properties.Our work creates a RNN model for general chemical property pre- diction, and ideally it should work eeectively for diierent types of Non-Physical (Toxicity)Multi-task classiication 8014 the string. Apart from the above-mentioned steps, no additional data augmentation steps were performed.Non-Physical (Activity)Single-task classiication 41,193FreeSolv Physical (Solvation)Single-task regression 643 ESOL Physical (Solubility)Single-task regression 1128 Table 1: Characteristics of the 4 datasets used to evaluate the performance of SMILES2vec.properties without signiicant network topology changes. To en- sure that our results are comparable with contemporary DL models reported in the literature [42] and earlier work on Chemception CNN models [16,17], we used the Tox21, HIV, and FreeSolv dataset from the MoleculeNet benchmark [42] for predicting toxicity, ac- tivity and solvation free energy respectively. datasets (see Table 1) represent a good mix of dataset sizes, type of chemical properties and regression vs classiication tasks. In addition, we also used the ESOL solubility dataset to evaluate the interpretability of SMILES2vec.We used a dataset spliiing approach that is similar to that reported in previous work [16]. A separate test set was partitioned out to serve as a test for model generalizability. For the Tox21 and HIV dataset, 1/6th was partitioned out to form the test set, and for the FreeSolv and ESOL dataset, 1/10th was used to create the test set. remaining 5/6th or 9/10th of the dataset was then used in the random 5-fold cross validation approach for training.Model performance and early stopping criterion was determined by validation loss. Lastly, we oversampled the minority class for classiication tasks (Tox21, HIV) to mitigate class imbalance. was achieved by computing the ratio of both classes, and appending additional data from the smaller class by that ratio. oversam- pling step was performed aaer stratiication, to ensure that the same molecule is not repeated across training/validation/test sets.In terms relevance to chemical-aaiated industries, toxicity predic- tion has importance for chemicals that require FDA approval, which includes drugs and other therapeutics (pharmaceuticals) as well as cosmetics (consumer goods). [27] Activity is a measurement of how well a chemical binds to its intended target and is one of the factors that determine how well a chemical may perform as a drug. [3] accurate activity predictions are of relevance to both pharmaceuticals and biotechnology industries. Predicting solubil- ity is an important consideration for developing formulations for products relevant to pharmaceuticals and consumer goods, [11] and it also aaects the bioavailability of drugs. Lastly, free energy values itself are computable by physics-based simulations, and such meth- ods are currently being employed by pharmaceuticals, consumer goods and materials industries. [7] We used a Bayesian optimizer, SigOpt [10] to optimize the hyperpa- rameters related to the neural network topology. Each diierent set of network hyperparameters are deened as a separate trial, and for each trial, we trained the model to completion using a standardized supervised training protocol. AAer each trial, the validation metric (AUC for classiication tasks, RMSE for regression tasks) was used as input to the Bayesian optimizer for suggesting new network designs.To prevent overrring during this optimization process, the split- ting of the dataset between training and validation sets was gov- erned by a random seed. However, a test set was maintained throughout, and this is also not used in the optimization process. By comparing the diierence in validation and test set metrics, it would thus allow us to determine if the network design was be- ing overrred to the training/validation data. No hyperparameters optimization was performed for the learning protocol. Lastly, it should be noted that only a subset of the dataset was used in the Bayesian optimization. Speciically, we used only a single task (nr-ahr toxicity) from the Tox21 dataset and the Freesolv dataset.length of the SMILES string directly impacts the compute re- sources required to train RNN models. To maintain a balance be- tween maximum amount of SMILES data, but also rapid training time, we surveyed the ChEMBL database, a collection of industrially- relevant chemicals that has over 1 million entries. [13] Using this database as a proxy for relevant chemicals, we calculated that set- ting a maximum length of 250 characters would encompass 99.9% of existing entries. in the above-listed datasets, we excluded entries of more than 250 characters in the dataset.Next, we created a dictionary that mapped the unique characters as one-hot encodings. Zero padding was also applied to ensure that shorter strings had a uniform size of 250 characters. In addition, extra padding of 10 zeroes were added both to the lee and right of SMILES2vec was trained using a Tensorrow backend [1] with GPU acceleration using NVIDIA CuDL libraries. [5] network was created and executed using the Keras 2.0 functional API interface [8].RMSprop algorithm [19] was used to train for 250 epochs using the standard seeings recommended (learning rate = 10 −3 , ρ = 0.9, ϵ = 10 −8 ). batch size was 32, and we also included early stopping to reduce overrring. was done by monitoring the loss of the validation set, and if there was no improvement in the validation loss aaer 25 epochs, the last best model was saved as the model.For classiication tasks, we used the binary crossentropy loss function for training. performance metric reported in our work is area under the ROC curve (AUC). For regression tasks, we used the mean average error as the loss function for training. performance metric reported is RMSE. Unless speciied otherwise, the reported results in our work denote the mean value of the performance metric, obtained from the 5 runs in the 5-fold cross validation. In this section, we conduct several Bayesian optimization exper- iments to optimize SMILES2vec's architecture and hyperparameters.we conduct further experiments to develop an explanation mask for improving interpretability of the model. RNNs, particularly those based on LSTMs [20]  [41]. Most of the other reported application of RNNs in NLP research are similarly used to model sequence-to- sequence predictions, and ooen fewer (i.e. 2 to 4) layers have been found to be suuciently accurate for their tasks.Our work diiers from conventional NLP research as we are modeling sequence-to-vector predictions, where the sequence is a SMILES string, and the vector is a measured chemical property. Because of this, and also because SMILES is a fundamentally diier- ent language, commonly-used techniques in NLP research, such as embeddings like Word2vec [29] cannot be easily adapted for use in our work. a substantial component of our work is in the design of the RNN architecture speciic to SMILES. We explore the RNN model's architecture class, which primarily includes high-level design choices, such as the type of units used, type of layers, arrangement of layers, etc. LSTMs and GRUs are the two major RNN units used in the literature, and form the basis of two architectural classes. template design for each class starts with an embedding layer that feeds into a 2-layer bidirectional GRU or 2-layer bidirectional LSTM as illustrated in Figure 1. In addition, we explored the utility of adding a 1D convolutional layer between the embedding and GRU/LSTM layers. design forms the template of the other two architectural classes explored.A separate Bayesian optimization was used to optimize the hy- perparameters of each architectural class. Speciically, we varied the size of the embedding from 10 to 60 in intervals of 10. number of units in the GRU/LSTM layers ranged from 8 to 384 in intervals of 8, and the number of units in the convolutional layer ranged from 4 to 192 in intervals of 4. For the convolutional layer, a size of 3 and a stride of 1 was used, which is based on the design principles from modern convolutional neural network [37]. No additional optimization was performed on the size or stride of the convolutional layers. In addition, no speciic shape of the network topology was enforced.be performed, where N is the number of tunable hyperparameters. In our work, we performed 60 trials for each of the 4 architectural class. In addition, we manually seeded 6 initial designs for each class. Speciically, we used initial designs that had an embedding size of 40, a convolution layer with 16 and both LSTM/RNN layer with [8,16,32,64,128,256] units.In addition, because we are developing a general-purpose neural network design that can be re-used for a broad range of property prediction, it would not be feasible to include all conceivable train- ing data to optimize the network design within the limits of avail- able computing resources. a subset of the datasets were used in the Bayesian optimization (see section 2.5 for details), and separate optimizations were performed for the Tox21 classiication and the FreeSolv regression tasks. In order for Bayesian methods to be eeective, a suucient number of trials for diierent neural network design has to be performed. In practice, it has been recommended that a minimum of 10N trials results of the Bayesian optimization across all 4 classes and 2 tasks are as indicated in Figure 2. For Tox21 classiication, we observed that an additional convolutional layer between the embedding and RNN/LSTM layers improved model performance relative to their counterparts, and the best performing model was  em size #conv #rnn1 #rnn2  50  192  224  384  Table 2: Best CNN-GRU network design for the SMILES2vec model.the CNN-LSTM class, with CNN-GRU trailing slightly behind. For FreeSolv regression, we observed that GRU-based networks out- perform LSTM-based networks. Taking into considerations for generalization to other type of chemical properties, we selected the CNN-GRU architectural class for the remainder of this work.we selected the best network design of this class, which is summarized in Table 2.Lastly, because the Bayesian algorithm uses the validation metric as a means to optimize the network's hyperparameters, there is a possibility that as one progresses, there may be overrring to- wards the validation set. To determine the extent of overrring, we examined the correlation between the validation metrics (whose validation set data would be changing during the Bayesian opti- mization) and the test metrics (whose test set data is and was never used in the Bayesian optimization). As illustrated in Figure 3, the correlation between validation and test metrics is 0.54 for the Tox21 dataset and 0.78 for the FreeSolv dataset. lower correlation of the Tox21 dataset relative to the FreeSolv dataset may be explained by noting the AUC performance metric on which the optimization was performed, is not the same as the crossentropy loss function used for training the network.We train a neural network generated mask to identify the important characters of the input. procedure is as follows: First, we use the SMILES2vec model (Table 2) as the base network. Next, we construct another neural network to produce a mask over the input data, with the objective to train the mask such that the output of the base neural network remains the same but it masks as much data as possible. We freeze the base neural network, and we train the explanation mask end-to-end, as shown in Figure 4. SMILES input is passed through the embedding layer, then into the explainer. produces a mask that is placed over the original em- bedding and sent through the pre-trained base model.With the weights of the base network frozen, the mask being learned will be speciic to the SMILES2vec model. Each input will produce a diierent mask. To avoid the mask being trivial (com- pletely uniform), we added two forms of regularization, a small L2 regularization, and we also penalized the mask for having high en- tropy (puuing equal weight on all inputs). overall loss function for a single element of each mini-batch is as follows: To gain a beeer insight into the SMILES2vec model, we developed a method to gain some level of interpretability. Here, our objective is to identify the part(s) of the SMILES string that is responsible for the neural network's decision. Methods for explaining "black box" models exist [32], but most of these methods tend to require explicit combinatorial analysis.approach we provide here provides insight into how the neural network analyzes the data, without combinatorially probing the input. is achieved by training an explanation mask, whereby a separate explanation network learns to mask input data to produce near identical output as would be obtained from the original data. where f (SMILES i , θ ) is the base neural network applied to the ith SMILES, Sol(SMILES i ) is the solubility, H is the entropy over the mask (normalized to sum to 1), and MASK i is the vector of the calculated explanation mask at each entry in the input SMILES string.explanation network used to create the mask was a 20 layer residual network with SELU [26] activations. padding was such that the length of the input remained the same at each layer. input to the network is the embedding of the SMILES string. last layer is a 1D convolution of length 1, followed by a batch normalization, then by a sooplus activation. We observed that the batch normalization layer to be very important for trainability. provides an output between 0 and innnity at each SMILES position. A mask output of 0 would prevent the base SMILES2vec from receiving that input character. A mask of ∞ would cause the SMILES2vec to put more aaention on that input character. We trained with Adam [25] until convergence. We started the learning rate at 10 −2 and divided by 10 as the training error plateaued, ultimately training down to 10 −6 .Solubility Top-3 Chars c1ccncc11.18 c,c,n O=Cc1ccco1 -0.87 C,c,o Clc1ccc(cc1Cl)c2ccccc2 -5.93 c,c,c Cc1c2ccccc2c(C)c3ccccc13 -6.91 c,c,c Table 3: Sample SMILES entries, their predicted solubility value, and the top-3 most important characters are bolded. In this section, we quantity the accuracy of interpretability on the solubility dataset, we demonstrate generalizability of the SMILES2vec model by evaluating its performance on other datasets.cations of increasing attention on the molecule. explanation mask validates established knowledge by focusing on atoms of known hydrophobic and hydrophilic functional groups.We demonstrate proof-of-concept for an interpretable SMILES2vec network using the ESOL solubility dataset. [42] Chemical solubility is a well-understood and simple chemical property where there is established knowledge. Brieey, parts of a chem- ical (i.e. functional groups) can typically be classiied as either hydrophilic or hydrophobic. Hydrophilic "water-loving" groups, like alcohols, amines and carboxyl form strong interactions with water and increase the overall solubility of a compound, and they typically contain non-carbon atoms like nitrogen and oxygen. reverse is true for hydrophobic "water-hating" groups, which tend to make chemicals more insoluble, and they typically are carbon- based chains/rings and halogens (chlorine, bromine, iodine).We used the pre-trained the SMILES2vec base model, which at- tained a validation RMSE of 0.63. Solubility values are reported on the log10 scale, with less soluble compounds having more nega- tive number. mask outputs a normalized aaention value that denotes the importance of a particular character in the network's decision. For each SMILES string, we identiied the top-3 characters (see Table 3 for examples). we separated the dataset into soluble (¿ -1.0) and insoluble (¡ -5.0) compounds. Using established knowledge of chemical solubility to establish the ground truth, we expect that soluble compounds should have higher aaention on the atoms O, N, and insoluble compounds to have higher aaen- tion on atoms C, F, Cl, Br, I. With this ground truth labeling in expected atoms, we computed the top-3 accuracy of SMILES2vec interpretability, which is 88%.In addition, we also qualitatively examined the outputs of the masks by mapping the SMILES character to the corresponding atom(s) in the molecular structure, and examples are shown in Figure 5. For molecules with low solubility, the characters c, C, and Cl tend to receive more aaention than others, which correspond to hydrophobic groups. In contrast, molecules with high solubility have aaention focused on the characters O and N, which correspond to hydrophilic groups. localization of appropriate atoms on each functional group type depending on the chemical's predicted solubility value in- dicates that SMILES2vec has learned representations that corre- spond to known chemistry concepts. Lastly, we emphasize that SMILES2vec has developed these representations without being provided any explicit chemical information. While chemical infor- mation is implicitly encoded in the SMILES string, no "decoding solution" was provided to the network, neither was further feature engineering required. Our work therefore demonstrates the eeec- tiveness of representation learning from raw data in the chemical sciences.far, we have only evaluated the performance of SMILES2vec on the ESOL solubility dataset. Unlike solubility, the other 3 datasets (toxicity, activity, solvation energy) are more complex properties, for which no simple rule-based methods exist in the chemistry literature. Without the ability to generate ground-truth labels, quantifying the accuracy of the SMILES2vec interpretation is non- trivial, and is beyond the scope of this work. Nevertheless, the accuracy of the model's predictions can still be evaluated.We also note that the architectural optimization of SMILES2vec only included a small fraction of the 4 datasets identiied for this work; the HIV dataset was not included, and 11 out of 12 toxicity tasks were not included. Hence, this section also determines how generalizable the Bayesian optimized network design will be to other chemical tasks.First, we determined the eeectiveness of generalizing SMILES2vec to the 3 remaining datasets. following validation performance metrics were obtained: AUC of 0.80 for the full Tox21 dataset, AUC of 0.78 for the HIV dataset, and RMSE 1.4 kcal/mol for the FreeSolv dataset. Furthermore, we note that in all models, the diierence between the validation and test metrics is small, further connrming the generalization of the model to compounds it has not seen either during the model training, or in the Bayesian hyperparameter opti- mization. Using a recently developed pre-training approach, [15] we were also able to improve the performance of SMILES2vec slightly, aaaining AUC of 0.81 for the full Tox21 dataset, AUC of 0.80 for the HIV dataset, and RMSE 1.2 kcal/mol for the FreeSolv dataset.Based on these results, we conclude that the Bayesian optimiza- tion of the network architectural design was eeective in developing a general-purpose SMILES2vec network design for other chemi- cal properties. We also note in recent literature there has been a trend towards using other "black box" approaches as a solution for network architecture design, for example using RNNs and reinforce- ment learning to optimize the design of a target neural network. [43] However, such methods typically require on the order of˜10of˜10,000 trials, which is much more than the˜500the˜500 trials used in our work.In addition, given that the template of each architectural class was adaptive methods that automtically grow or shrink the neural network are also viable alternatives to network design. [35] Next, we compare the performance of the best SMILES2vec model against contemporary deep neural networks that have re- ported results on the same datasets (Tox21, HIV, FreeSolv) that we have evaluated our model on. We compare against a typical MLP network that uses engineered features [42], a chemistry-speciic molecular graph convolutional neural network [42], and Chemcep- tion, a deep CNN that uses images [17].network architecture is eeective in predicting a wide range of prop- erties. SMILES2vec achieved a validation AUC of 0.81 and 0.80 for Tox21 toxicity and HIV activity prediction respectively, and a validation RMSE of 1.2 kcal/mol and 0.63 for solvation energy and solubility. Using the solubility dataset as an illustration of SMILES2vec interpretability, we construct explanation masks that indicate SMILES2vec localizes on speciic characters in hydrophilic or hydrophobic groups, with a top-3 accuracy of 88%. Identiication of such functional groups and their relationship to chemical solubil- ity is a key concept in chemistry, which SMILES2vec was able to discover on its own. Compared to other DL models, SMILES2vec's accuracy outperforms the typical MLP DL models that uses engineered features as input. Against the current state-of- the-art (convolutional graph networks), SMILES2vec outperforms on regression tasks and matches on classiication tasks. re- sults indicate that SMILES2vec can accurately predict a broad range of properties and learn technically accurate chemical concepts, which suggest that it can be used as an interpretable tool for the future of deep learning driven chemical design. results are presented in Figure 6, and we use validation met- rics to evaluate the quality of the model. In comparing the 4 meth- ods, we observed that the standard MLP DL models that uses engi- neered features ((ngerprints) performed the worst. SMILES2vec outperformed CNN models such as Chemception in classi tasks, but slightly underperformed in regression tasks. As indi- cated in our previous work, it is likely that the lack of atomic number information, which is not embedded in the SMILES for- mat is responsible for its lower performance in predicting calcula- ble physical properties. [17] In addition, SMILES2vec also outper- formed models for computing solvation free energy (note: there are no models for computing toxic- ity/activity), which is especially noteworthy since SMILES2vec (and neural networks in general) can predict values much faster than traditional computational chemistry simulations, which typically re- quire minutes to hours for each calculation. Against convolutional graphs, which is the current state-of-the-art for many chemical tasks, SMILES2vec either matches for classiication tasks (Tox21: 0.81 vs 0.81, HIV: 0.80 vs 0.80) or outperforms for regression task (FreeSolv: 1.2 vs 1.3). SMILES2vec is not only as accurate as the current state-of-the-art in chemistry DL models, but more importantly it is also an interpretable model.
