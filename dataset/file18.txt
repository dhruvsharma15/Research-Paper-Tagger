Gesture recognition is one of the core components in the thriving research field of human- computer interaction. The recognition of distinct hand and arm motions is becoming in- creasingly important, as it enables smart interactions with electronic devices. Furthermore, gesture identification in video can be seen as a first step towards sign language recognition, where even subtle differences in motion can play an important role. Some examples that complicate the identification of gestures are changes in background and lighting due to the varying environment, variations in the performance and speed of the gestures, different clothes worn by the performers and different positioning relative to the camera. Moreover, regular hand motion or out-of-vocabulary gestures should not to be confused with one of the target gestures.Convolutional neural networks (CNNs) ( LeCun et al., 1998) are the de facto standard approach in computer vision. CNNs have the ability to learn complex hierarchies with increasing levels of abstraction while being end-to-end trainable. Their success has had a huge impact on vision based applications like image classification ( Krizhevsky et al., 2012), object detection (Sermanet et al., 2013), human pose estimation (Toshev &amp; Szegedy, 2014) and many more. A video can be seen as an ordered collection of images. Classifying a video frame by frame with a CNN is bound to ignore motion characteristics, as there is no integration of temporal information. Depending on the task at hand, aggregating the spatial features produced by the CNN with temporal pooling can be a viable strategy ( Karpathy et al., 2014;Ng et al., 2015). As we'll show in this paper, however, this method is of limited use for gesture recognition.Apart from a collection of frames, a video can also be seen as a time series. Some of the most successful models for time series classification are recurrent neural networks (RNNs) with either standard cells or long short-term memory (LSTM) cells (Hochreiter &amp; Schmidhuber, 1997). Their ability to learn dynamic temporal dependencies has allowed researchers to achieve breakthrough results in e.g. speech recognition ( Graves et al., 2013), machine translation ( Sutskever et al., 2014) and image captioning ( Vinyals et al., 2015). Before feeding video to recurrent models, we need to incorporate some form of spatial or spatiotemporal feature ex- traction. This motivates the concept of combining CNNs with RNNs. CNNs have unparalleled spatial (and spatiotemporal with added temporal convolutions) feature extraction capabilities, while adding recurrence ensures the modeling of feature evolution over time.For general video classification datasets like UCF-101 ( Soomro et al., 2012), Sports-1M ( Karpathy et al., 2014) or HMDB-51 ( Kuehne et al., 2011), the temporal aspect is of less importance compared to a gesture recognition dataset. For example, the appearance of a violin almost certainly suggests the target class is "playing violin", as no other class involves a violin. The model has no need to capture motion information for this particular example. That being said, there are some categories where modeling motion in some way or another is always beneficial. In the case of gesture recognition, however, motion plays a more critical role. Many gestures are not only defined by their spatial hand and/or arm placement, but also by their motion pattern.In this work, we explore a variety of end-to-end trainable deep networks for video classification applied to frame-wise gesture recognition with the Montalbano dataset that was introduced in the ChaLearn LAP 2014Challenge (Escalera et al., 2014). We study two ways of capturing the temporal structure of these videos. The first method involves temporal convolutions to enable the learning of motion features. The second method introduces recurrence to our networks, which allows the modeling of temporal dynamics, which plays an essential role in gesture recognition.An extensive evaluation of CNNs on general video classification is provided by Karpathy et al. (2014) using the Sports-1M dataset. They compare different frame fusion methods to a baseline single-frame architecture and conclude that their best fusion strategy only modestly improves the accuracy of the baseline. Their work is extended by Ng et al. (2015), who show that LSTMs achieve no improvements over a temporal feature pooling scheme on the UCF-101 dataset for human action classification and only marginal improvements on the Sports-1M dataset. For this reason, the single-frame and the temporal pooling architectures are important baseline models.Another way to capture motion is to convert a video stream to a dense optical flow. This is a way to represent motion spatially by estimating displacement vectors of each pixel. It is a core component in the two-stream architecture described by Simonyan &amp; Zisserman (2014) and is used for human pose estimation ( Jain et al., 2014), for global video descriptor learning ( Ng et al., 2015) and for video captioning ). We have not experimented with optical flow, because (i) it has a greater computational preprocessing complexity and (ii) our models should implicitly learn to infer motion features in an end-to-end fashion, so we chose not to engineer them. Neverova et al. (2014) present an extended overview of their winning solution for the ChaLearn LAP 2014 gesture recognition challenge and achieve a state-of-the-art score on the Montalbano dataset. They propose a multi-modal 'ModDrop' network operating at three temporal scales and use an ensemble method to merge the features at different scales. They also developed a new training strategy, ModDrop, that makes the network's predictions robust to missing or corrupted channels.Most of the constituent parts in our architectures have been used before in other work for different purposes. Learning motion features with three-dimensional convolution layers has been studied by Ji et al. (2013) and Taylor et al. (2010) to classify short clips of human actions on the KTH dataset. Baccouche et al. (2011) proposed including a two-step scheme to model the temporal evolution of learned features with an LSTM. Finally, the combination of a CNN with an RNN has been used for speech recognition (Hannun et al., 2014), image captioning ( Vinyals et al., 2015) and video narration ( Donahue et al., 2015).In this section, we briefly describe the different architectures we investigate for gesture recognition in video. An overview of the models is depicted in Figure 1. Note that we pay close attention to the comparability of the network structures. The number of units in the fully connected layers and the number of cells in the recurrent models are optimized based on validation results for each network individually. All other hyper-parameters mentioned in this section and in Section 4.2 are optimized for the temporal pooling architecture. As a result, improvements over our baseline models are caused by architectural differences rather than better optimization, other hyper-parameters or preprocessing.Single-Frame The single-frame architecture (Figure 1a) worked well for general video classification ( Karpathy et al., 2014), but is not a very fitting solution for our frame-wise gesture recognition setting. Nevertheless, this will give us an indication on how much static images contribute to the recognition. It has 3×3 convolution kernels in every layer. Two convolutional layers are stacked before performing max-pooling on non-overlapping 2×2 spatial regions. The shorthand notation of the full architecture is as follows:where C(n c ) denotes a convolutional layer with n c feature maps, P a max-pooling layer, D(n d ) a fully connected layer with n d units and S a softmax classifier. We deploy leaky rectified linear units (leaky ReLUs) in every layer. Their activation function is defined as a :  where α = 0.3. Leaky ReLUs seemed to work better than conventional ReLUs and showed promising results in other work ( Maas et al., 2013;Graham, 2014;Dieleman et al., 2015;Xu et al., 2015).Temporal Feature Pooling The second baseline model exploits a temporal feature pooling strategy. As suggested by Ng et al. (2015), we position the temporal pooling layer right before the first fully connected layer as illustrated in Figure 1b. This layer performs either mean-pooling or max-pooling across all video frames. The structure of the CNN-component is identical to the single-frame model. This network is able to collect all the spatial features in a given time window. However, the order of the temporal events is lost due to the nature of pooling across frames.The core idea of RNNs is to create internal memory to learn the temporal dynamics in sequential data. An issue (in our case) with conventional recurrent networks is that their states are built up from previous time steps. A gesture, however, generally becomes recognizable only after a few time steps, while the frame-wise nature of the problem requires predictions from the very first frame. This is why we use bidirectional recurrence, which enables us to process sequences in both temporal directions.Describing the proposed model (Figure 1c) formally, we start with the CNN (identical to the single-frame model) transforming an input frame x t to a more compact vector representation v t :A bidirectional RNN computes two hidden sequences: the forward hidden sequence h (f ) and the backward hidden sequence h (b) :where H represents a recurrent layer and depends on the type of memory cell. There are two different cell types in widespread use: standard cells and LSTM cells (Hochreiter &amp; Schmidhuber, 1997) (we use the modern LSTM cell structure with peephole connections ( Gers et al., 2003)). Both cell types will be compared in this work. Finally, the output predictions y t are computed with a softmax classifier which takes the sum of the forward and backward hidden states as input:Our final set of architectures extends the CNN layers with temporal convolutions (convolutions over time). This enables the extraction of hierarchies of motion features and thus the capturing of temporal information from the first layer, instead of depending on higher layers to form spatiotemporal features. Performing three-dimensional convolutions is one approach to achieve this. However, this leads to a significant increase in the number of parameters in every layer, making this method more prone to overfitting. Therefore, we decide to factorize this operation into two-dimensional spatial convolutions and one-dimensional temporal convolutions. This leads to fewer parameters and optionally more nonlinearity if one decides to activate both operations. We opt to not include a bias or another nonlinearity in the spatial convolution step to maintain the comparability between architectures.First, we compute spatial feature maps s t for every frame x t . A pixel at position (i, j) of the k-th feature map is determined as follows:where N is the number of input channels and W spat are trainable parameters. Finally, we convolve across the time dimension for every position (i, j), add the bias b (k) and apply the activation function a:m=1 t where the variables W temp and b are trainable parameters and M is the number of spatial feature maps.Two different architectures are proposed using this new layer. In the first model ( Figure  1d), we replace the convolutional layers of the single-frame CNN with the spatiotemporal layer defined above. Furthermore, we apply three-dimensional max-pooling to reduce spatial as well as temporal dimensions while introducing slight translational invariance in time.Note that this architecture implies a sliding window approach for frame-wise classification, which is computationally intensive. In the second model, illustrated in Figure 1e, the time dimensionality is retained throughout the network. That means we only carry out spatial max- pooling. To this end, we are able to stack a bidirectional RNN with LSTM cells, responding to high-level temporal dependencies. It also incidentally resolves the need for a sliding window approach to implement frame-wise video classification.The To speed up the training, we crop part of the images containing the user and rescale them to 64 by 64 pixels using the skeleton information (other than that, we do not use any pose data). However, we show in Section 4.3 that we even achieve good results when we do not crop the images and leave out depth information.We train our models from scratch in an end-to-end fashion, backpropagating through time (BTT) for our recurrent architectures. The network parameters are optimized by minimizing the cross-entropy loss function using mini-batch gradient descent with the Adam update rule (Kingma &amp; Ba, 2015). We found that Adam works great in practice, especially when experimenting with very different layer types in the same model. All our models are trained the same way with early stopping, a mini-batch size of 32, a learning rate of 10 −3 and an exponential learning rate decay. Before training, we initialize the weights with a random orthogonal initialization method ( Saxe et al., 2013). Non-Recurrent Networks The single-frame CNN is trained frame by frame and all other non-recurrent networks are trained with the number of frames optimized for their specific architecture. The best number of frames to mean-pool across is 32, determined by validation scores with tested values in [8,16,32,64]. In the case of max-pooling, we find that pooling over 16 frames gives better outcomes. Also, pretraining the CNNs frame-by-frame and fine-tuning with temporal max-pooling gave slightly improved results. We observed no improvements, however, using this technique with temporal mean-pooling. The architecture with added temporal convolutions and three-dimensional max-pooling showed optimal results by considering 32 surrounding frames. The targets for all the non-recurrent networks are the labels associated with the centermost frame of the input video fragment. We evaluate these models using a sliding window with single-frame steps.Regularization and Data-Augmentation We employed many different methods to regu- larize the deep networks. Data augmentation has a significant impact on generalization. From each of these inter- vals, we sample a random value for each video fragment and apply the transformations online using the CPU. Dropout with p = 0.5 is used on the inputs of every fully connected layer. Fur- thermore, using leaky ReLUs instead of conventional ReLUs and factorizing three-dimensional convolutions into spatial and temporal convolutions also reduce overfitting.Jaccard Index Precision Recall Error Rate* Single-Frame CNN (Figure 1a) 0.465 67.86% 57.57% 20.68% Temp Max-Pooling (Figure 1b) 0.748 85.03% 82.92% 8.66% Temp Mean-Pooling (Figure 1b) 0.775 85.93% 85.80% 8.55% Temp Conv (Figure 1d) 0.842 89.36% 90.15% 4.67% RNN, Standard Cells (Figure 1c) 0.885 92.77% 93.56% 3.58% RNN, LSTM Cells (Figure 1c) 0.888 93.75% 93.28% 3.55% Temp Conv + LSTM (Figure 1e) 0.906 94.49% 94.57% 2.77% Table 1: A comparison of the results for our different architectures on the Montalbano gesture recognition dataset. The Jaccard index indicates the mean overlap between the binary predictions and the binary ground truth across gesture categories. We also compute precision and recall scores for each gesture class and report the mean score across classes. *The error rate is based on majority voted frame-wise predictions from isolated gesture fragments.We follow the ChaLearn LAP 2014 Challenge score to measure the performance of our architectures. This way, we can compare with previous work on the Montalbano dataset. The competition score is based on the Jaccard index, which is defined as follows:The binary ground truth for gesture category n in sequence s is denoted as the binary vector A s,n , whereas B s,n denotes the binary predictions. The Jaccard index J s,n can be seen as the overlap rate between A s,n and B s,n . To compute the final score, the mean Jaccard index among all categories and sequences is computed:where N = 20 is the number of categories and S the number of sequences in the test set.An overview of the results for our different architectures is shown in Table 1. The predictions of the single-frame baseline achieve a Jaccard index below 0.5. This is to be expected as no motion features are extracted. We observe a significant improvement with temporal feature pooling (a Jaccard index of 0.775 vs. 0.465). Furthermore, mean-pooling performs better than max-pooling. Adding temporal convolutions and three-dimensional max-pooling improves the Jaccard index to 0.842.The three last entries in Table 1 use recurrent networks. Surprisingly, the RNNs are only acting on high-level spatial features, yet are surpassing a CNN learning hierarchies of motion features (a Jaccard index of 0.842 vs. 0.888). The difference in performance for the two types of cells is very small and they can be considered equally capable for this type of problem where temporal dependencies are not too long-ranged. Finally, combining the temporal convolution architecture with an RNN using LSTM cells improves the score even more (0.906). ThisCrop Depth Pose Jaccard IndexChang (2014)   In Table 2, we compare our results with previous work. Our best model outperforms the method of Neverova et al. (2014) when we only consider RGB-D pixels as input features (0.906 vs. 0.836). When we remove depth information and perform no preprocessing other than rescaling the images, we still achieve better results (0.842). Moreover, we even achieve better results without the need for depth images or pose information (0.876 vs. 0.870).To illustrate the differences in output predictions of the different architectures, we show them for a randomly selected sequence in Figure 2. We see that the single-frame CNN has trouble classifying the gestures, while the temporal pooling is significantly more accurate. However, the latter still has difficulties with boundaries. Adding temporal convolutions shows improved results, but the output contains more jagged predictions. This seems to disappear by introducing recurrence. The output of the bidirectional RNN matches the target labels strikingly well.In Figure 3, we show that adding temporal convolutions enables neural networks to capture motion information. When the user is standing still, the units of the feature map are inactive, while the feature map from the network without temporal convolutions has a lot of active units. When the user is moving, the feature map shows strong activations at the movement locations. This suggests that the model has learned to extract motion features.We showed in this paper that adding bidirectional recurrence and temporal convolutions improves frame-wise gesture recognition in video significantly. We observed that RNNs responding to high-level spatial features perform much better than single-frame and temporal pooling architectures, without the need to take into account the temporal aspect in the lower layers of the network. However, adding temporal convolutions in all layers of the architecture has a notable impact on the performance, as they are able to learn hierarchies of motion features, unlike RNNs. Standard cells and LSTM cells appear to be equally strong for this problem. Furthermore, we observed that RNNs outperform non-recurrent networks and are able to predict the beginning and ending frames of gestures with great accuracy, whereas other models show uncertainty at these boundaries.In the future, we would like to build upon this work for research in the domain of sign language recognition. This is even more challenging than gesture recognition. The vocabulary is larger, the differences in finger positions and hand movements are more subtle and signs are context dependent, as they are part of a language. Sign language is not related to written or spoken language, which complicates annotation and translation. Moreover, signers communicate simultaneously with facial, manual (both hands are separate communication channels) and body expressions. This means that sign language video cannot be translated the way speech recognition can transcribe audio to written sentences. 
