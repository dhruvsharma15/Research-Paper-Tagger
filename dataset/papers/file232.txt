Neural network models are now widely embraced as cutting- edge solutions in computer vision ( Krizhevsky et al., 2012;He et al., 2016), speech recognition ( Graves et al., 2013), and text analysis (Collobert &amp; Weston, 2008). However, despite the near, and sometimes beyond, human-like results they achieve on key benchmarks ( He et al., 2015;Taigman et al., 2014), we still do not understand the true reliability and robustness of this performance. A prominent issue in this context is the existence of so-called adversarial exam- ples ( Szegedy et al., 2013;Goodfellow et al., 2014), i.e., inputs that are almost indistinguishable from natural data to the human eye, yet cause state-of-the-art classifiers to make incorrect predictions with high confidence. This raises con- cerns about the use of neural networks in contexts where reliability, dependability and security matter.In this work, we address this question by studying two basic image transformations: translations and rotations. While these transformations appear natural to a human, we show that they alone (i.e., without any additional perturbation added) cause a significant drop in the model's performance. This remains to be the case even when the model has been trained using appropriate data augmentation and no visual information is lost due to these transformations (e.g. due to cropping). See Figure 1 for an illustration.Natural Adversarial "revolver" "mousetrap"There is a long line of work ( Szegedy et al., 2013 "vulture" "orangutan"Figure 1: Examples of adversarial transformations and their predictions in the standard and "black canvas" setting.We start with standard models for the MNIST ( LeCun et al., 1998), CIFAR10 (Krizhevsky &amp; Hinton, 2009), and Ima- geNet ( Russakovsky et al., 2015) datasets. These models achieve close to state-of-the-art performance on the respec- tive benchmarks. We demonstrate however that even small rotations and translations alone can cause these models to have a significant drop in classification accuracy. This drop ranges from 34% to as high as 90%.• A simple attack based purely on rotations and trans- lations is very effective against state-of-the-art neural network models. This remains to be the case even when the model has been trained with appropriate data augmentation and no image information is lost during the transformation.• These attacks are fairly easy to execute. They require only a few black-box queries to the model.Additionally, we show that one does not even need direct access to the model (or its surrogate) to find such misclas- sifying transformations. In fact, choosing the worst out of 10 random transformations is sufficient to reduce the accuracy of these models by 26% on MNIST, 72% on CI- FAR10, and 28% on ImageNet (Top 1). This demonstrates that the existence of adversarial examples and the robust- ness of our neural network-based models should also be a concern outside of safety/security contexts. We thus need to properly examine and mitigate it in settings that are natural and benign.• One can take steps to increase the model's robustness to rotations and translations at the cost of increased train- ing and/or inference runtime. However these methods are still not sufficient to fully solve the problem.• Robustness to ∞ -bounded perturbations does not seem to affect spatial robustness. In fact, these two notions seem to be orthogonal to each other.To this end, we examine possible ways to alleviate these vulnerabilities. A natural first step here is to augment the training procedure with rotations and translations. While this does (largely) mitigate the problem on MNIST, the models trained on CIFAR10 and ImageNet are still far from being robust. We thus propose two natural methods for further increasing the robustness of these models. These methods are based on robust optimization and aggregation of random input transformations. They end up offering sig- nificant improvement, but at the expense of a considerable computational overhead. Even then, they are still not suffi- cient to completely mitigate the vulnerability. This suggests that obtaining models robust to spatial transformations of their inputs remains a challenge.Finally, we examine the interplay between this variant of adversarial perturbations and the widely used ∞ -based variant. We observe that these two variants seem orthogonal to each other. In particular, pixel-based robustness does not imply spatial robustness, while combining spatial and ∞ -bounded transformations seems to have a cumulative effect. This emphasizes the need for our current, pixel- wise notions of similarity used in the context of adversarial examples to be significantly broadened and include other natural spatial transformations, such as the rotations and translations considered here.Recall that, in the context of vision models, an adversarial example for a given image x and a classifier C is an image x that, on one hand, causes the classifier C to output a different label on x than on x, i.e., to have C(x) = C(x ), but, on the other hand, is "visually similar" to x. Clearly, the notion of visual similarity is not precisely defined here. In fact, providing such a precise and rigorous definition is ex- traordinarily difficult, as it would implicitly require formally capturing the notion of human perception. Consequently, previous work settled on assuming that x and x are close if and only if − x p ≤ ε for some p ∈ [0, ∞] and ε small enough. Arguably, when two images are close in some p norm they are visually similar. However the converse is not necessarily true. In this work, we aim to expand the range of similarity measures considered to include for natural ones. We will focus on rotations and translations which, while changing most images significantly in all p norms, tend to not affect human perception.Our initial goal is to develop sufficiently strong methods for generating adversarial examples of this type. So far, in the context of pixel-wise perturbations, the most successful ap- proach for constructing adversarial examples has been to use optimization methods on a suitable loss function ( Szegedy et al., 2013;Goodfellow et al., 2014;Carlini &amp; Wagner, 2016). Following this approach, we parametrize our attack method with a set of tunable parameters and then optimize over these parameters. We perform this optimization in three distinct ways:We perform extensive experiments that provide a fine- grained understanding of the vulnerabilities described so far on a wide spectrum of datasets and training regimes. In summary, we show that:• First-Order Method (FO): Starting from a random choice of parameters, we iteratively take steps in the direction of the loss function's gradient. This is the di- rection that locally maximizes the loss of the classifier (as a surrogate for misclassification probability). Note that unlike the p -norm case, we are not optimizing in the pixel space but in the latent space of rotation and translation parameters.maximization problem, there are no formal guarantees for the global optimality of our solution.• Grid Search: We discretize the parameter space and exhaustively examine every possible parametrization of the attack to find one that causes the classifier to give a wrong prediction (if such a parametrization exists).Since our parameter space is small enough, this method is computationally feasible (in contrast to a grid search for p -based adversaries).A natural idea to design models robust to rotations and translations is to augment the training with such random transformation. In order to further improve the model ro- bustness, we propose two novel defense mechanism that modify the training or inference method of the model.• Worst-of-k: We randomly sample k different attack parameter choices and choose the one on which the model performs worse. As we increase k, this attack interpolates between random choice and grid search.Observe that while a first-order attack requires full knowl- edge of the model in order to compute the gradient of the loss with respect to the input, the other two attacks do not. They simply require evaluating a chosen input which can be done even when there is only query access to the target model.We now need to define the exact range of attacks we want to optimize over. For the case of rotation and translation attacks, we wish to find parameters (δu, δv, θ) such that rotating the original image θ degrees around the center and then translating it by (δu, δv) pixels causes the classifier to make a wrong prediction. Formally, the pixel at position (u, v) is moved to the following position (assuming the point (0, 0) is the center of the image):Robust Optimization As a first step, instead of perform- ing standard empirical risk minimization to train the model, we will utilize robust optimization methods. Robust opti- mization has a rich history (Ben-Tal et al., 2009) and has recently been applied successfully in the context of defend- ing neural networks against adversarial examples (Madry et al., 2017;Sinha et al., 2017;Raghunathan et al., 2018;Kolter &amp; Wong, 2017). The main barrier to applying ro- bust optimization to improve spatial invariance is the lack of an efficient procedure to compute the worst-case pertur- bation of a given example. Performing a grid search (as described in Section 2), is prohibitive as this would increase the training time by a factor close to the grid size. More- over, the non-convexity of the landscape prevents first-order methods from discovering such (approximately) worst-case transformations efficiently (see Section 4 for details). uWe implement this transformation in a differentiable man- ner using the spatial transformer blocks of ( Jaderberg et al., 2015). In order to deal with pixels mapped to non-integer coordinates, these transformer units include a differentiable bilinear interpolation routine. Since our loss function is dif- ferentiable with respect to the input and the transformation is in turn differentiable with respect to its parameters, we can apply a first-order optimization method to our problem.Given that we cannot fully optimize over the space of trans- formations, we use a coarse approximation provided by the worst-of-10 adversary (described in Section 2). That is, first we sample 10 random transformations of each training ex- ample uniformly from the space of allowed transformations.We then evaluate the model on each of these transforma- tions and choose to train on the one with highest loss. This corresponds to minimizing a min-max formulation of the problem similar to that in ( Madry et al., 2017). Training against such an adversary increases the training by a factor of roughly six 2 .By defining the spatial transformation for some x as T (x; δu, δv, θ), we construct an adversarial perturbation for x by solving the problemwhere L is the loss function of the neural network 1 , and y is the correct label for example x. Since this is a non-concave 1 The loss L of the classifier is a function from images to real numbers that expresses the performance of the network on the particular example x (e.g., the cross-entropy between predicted and correct distributions).Aggregating Random Transformations For every ex- ample, most of the transformations don't result in misclassi- fication, as we will verify in Section 4. This motivates the following inference procedure: compute a (small) number of random transformations of the given image and output the label that is predicted the most over these transformations (majority vote). We constrain these random transformations to be within 5% of the input image size in each translation direction and up to 15 degrees of rotations. Observe that if an adversary rotates an image by 30• (a valid attack in our threat model), we might end up evaluating the image on ro- tations of up to 45• . The training procedure and model can remain unchanged while the inference time gets increased by a small factor (equal to the number of transformations we evaluate on).parameter range. When rotating and translating the images, we fill the empty space with zeros (black pixels).Data Augmentation We consider five variants of training for our models.Combining these methods These two methods are or- thogonal and in some sense complementary. We can there- fore combine robust training (using a worst-of-k adversary) and majority inference to further increase the robustness of our models.• Standard natural training: out-of-the-box training pro- cedure. For CIFAR10 this includes random left-right flipping, random translations of ±4, and per image standardization. For ImageNet, it includes random left-right flipping, random cropping similar to that in ( Szegedy et al., 2015), and various color space trans- formations. We refer to this as the Standard model.We will consider standard architectures for the MNIST ( LeCun et al., 1998), CIFAR10 (Krizhevsky &amp; Hinton, 2009) and ImageNet ( Russakovsky et al., 2015) datasets. In order to determine the extent to which misclassification is caused by non-sufficient data augmentation during training, we will examine various data augmentation methods.• Standard ∞ -bounded adversarial training: The clas- sifier is trained using ∞ -bounded adversarial exam- ples computed using Projected Gradient Descent in the pixel space at each step. For ImageNet there is not yet a classifier that can be efficiently and reliably trained against such an adversary. We refer to this model as ∞ -Adversarially Trained.Model Architecture For the MNIST dataset we use a simple convolutional neural network derived from the Ten- sorFlow Tutorial (tft). In order to obtain a fully convolu- tional version of the network, we replace the fully-connected layer by two convolutional layers with 128 and 256 filters each, followed by a global average pooling. For CIFAR10, we consider a standard ResNet ( He et al., 2016) model with 4 groups of residual layers with filter sizes [16,16,32,64] and 5 residual units each. We use naturally and ∞ -adversarially trained models similar to these studied by Madry et al. (Madry et al., 2017) 3,4 . For ImageNet, we use a ResNet-50 ( He et al., 2016) architecture using the code from the tensorpack repository (ten). We did not modify the model architectures or training procedures. We will make our full experimental code available soon.• No random cropping: In order to examine the effects of standard data augmentation during training we train the CIFAR10 and ImageNet models without random cropping. We still apply random left-right flips and the rest of the transformations. We refer to this model as No Crop.• Random rotations and translations: We perform a uni- formly random perturbation of training examples in the attack space described above at each training step. That is, on top of standard data augmentation, we translate and rotate the image choosing parameters uniformly at random from the allowed space. We refer to this model as Augmentation 30.Attack Space In order to maintain the visual similarity of images to the natural ones we restrict the space of allowed perturbations to be relatively small. We consider rotations of at most 30 degrees and translations of at most (roughly) 10% percent of the image size in each direction. • Random rotations and translations from larger inter- vals: Same as Augmentation 30, but we instead select random perturbations from a superset of the attack space. We sample random rotations of at most 40 de- grees and translations of at most 4 pixels for MNIST and CIFAR10, and 32 pixels for ImageNet. We refer to this model as Augmentation 40.We evaluate all models against random and grid search adversaries (rotations and translations considered both sepa- rately and together) and report the results in the classifiers' accuracy on the test set. For the standard models, accuracy drops from 99% to 26% on MNIST, 93% to 3% on CIFAR10, and 76% to 31% on ImageNet (Top 1 accuracy).The addition of random rotations and translations during training greatly improves both the random and adversarial accuracy of the classifier for MNIST and CIFAR10, but less so for ImageNet. Note that the Augmentation 40 model has been trained on transformations of larger magnitude than those that appear during the evaluation.We perform a fine-grained investigation of our findings:problem is a significant barrier. We investigate this issue by visualizing the loss landscape. For a few random examples we plot the cross-entropy loss of the examples as a function of translation and rotation. In Figure 3 we show one exam- ple for each dataset. Additional examples are visualized in Figure 9 of the Appendix. We clearly observe that the loss landscape is highly non-concave and is full of low-value local maxima. It appears that the low-dimensionality of the problem makes non-concavity a crucial obstacle. Even for the case of MNIST where we observe fewer local maxima, the large flat regions would prevent first-order methods from finding transformations of high loss.• For each dataset, we examine how many examples can be fooled by rotations only, translations only, neither, or both in Figure 2.• We visualize the set of fooling angles for a random sam- ple of the rotations-only grid in Figure 7 of Appendix A and observe that they form a highly non-convex set.• To investigate how brittle these networks are, we ana- lyze the percentage of grid points fooling each example in Figure 8 in the Appendix A. While the majority of perturbations are benign, a significant fraction of them fool each classifier.The performance of the worst-of-10 adversary is impressive given the limited inter- action with the model. The adversary performs 10 random, non-adaptive queries to the model and is able to find ad- versarial examples for a large fraction of inputs. Recently, there has been significant interest (Papernot et al., 2017;Chen et al., 2017;Bhagoji et al., 2017;Ilyas et al., 2017) in designing black-box attacks for neural network models. These attacks rely only on black-box queries to the model, without any additional information and aim to minimize the number of such queries required. Our results propose a very simple baseline strategy that produces adversarial examples for a significant fraction of inputs with very few queries.Black Canvas Experiments One might wonder if the re- duced accuracy of the models is due to the cropping applied during the transformation. We verify that this is not the case by padding each image in the CIFAR10 and ImageNet datasets with zeros. This creates a "black canvas" version of the dataset, ensuring that no information from the original image is lost during translation and rotation. We show a random set of adversarial examples in this case in Figure 10. The results from a full evaluation are shown in Table 4.Combining Spatial and ∞ -bounded perturbations In Table 1 we observe that models trained to be robust to ∞ attacks do not achieve higher robustness to spatial attacks. This provides evidence that the two families of perturba- tion are orthogonal to each other. We further investigate this possibility by considering a combined adversary that utilizes ∞ bounded perturbations on top of rotations and translations. The results are shown in Figure 11. We in- deed observe that, when combined, these attack reduce the classification accuracy in an additive manner.In Table 2 we compare different attack methods across mod- els and datasets. We observe that worst-of-10 is a very powerful adversary, despite the limited interaction it has with the model. The first-order adversary performs signifi- cantly worse, and despite being better than random choice fails to approximate the ground-truth accuracy of the models.In Table 1 we observe that training with a worst-of-10 adver- sary greatly increases the spatial robustness of the model. In fact, it performs significantly better that simply augmenting training with random transformations. We believe that using more reliable methods to compute the worst-case transfor- mations will further improve these results.The fact that first-order methods fail to reliably find adver- sarial examples is in sharp contrast to previous work (Carlini &amp; Wagner, 2016;Madry et al., 2017). They find that, for the case of pixel-based perturbations, the optimization landscape is well-behaved allowing first-order methods to consistently find maxima of good value. In the case of spa- tial perturbations, we observe that the non-concavity of the Our results for majority-based inference are presented in Table 3. Again, we observe a significant increase in the model's performance in the natural and adversarial setting.By combining these two defense, we manage to improve the worst-case performance of the models from 26% to 98% on MNIST, from 3% to 82% on CIFAR10, and from 31% to 56% on ImageNet (Top 1).  We compute how many examples can be fooled with either translations or rotations ("any"), how many can be fooled only by one of these, and how many require a combination to be fooled ("both").  Tr an sl at io n LR Tr an sl at io n LR Translations and rotations are restricted to 10% of the image pixels and 30 deg respectively. We observe that the landscape is significantly non-concave, making rendering FO methods for adversarial example generation powerless. Additional examples are visualized in Figure 9 of the Appendix. the original image that is constrained to be globally smooth and then optimized for misclassification probability. In (Tramèr &amp; Boneh, 2017), one considers an ∞ -bounded pixel-wise perturbation of a version of the original image that has been slightly rotated and in which a few random pixels have been flipped. Both these methods require direct access to the attacked model (or its surrogate) to compute (or at least estimate) the gradient of the loss function with respect to the model's input. In contrast, our attacks can be implemented using just a small number of random, non- adaptive transformations of the input. We examined the robustness of state-of-the-art vision clas- sifiers to translations and rotations. We observed that even a small number of randomly chosen perturbations of the input are sufficient to considerably degrade the model per- formance. The fact that small rotations and translation can fool neural network-based classifiers on MNIST and CIFAR10 was first observed in (Fawzi &amp; Frossard, 2015). The main difference from our work is that they focus on finding the minimum transformation required to fool a classifier. They use this as a measure for qualitative comparison of different architectures and training procedures. In contrast, we fix a range of small transformations, and we study the robustness of a model within this space.The vulnerability of state-of-the-art neural networks to such simple and naturally occurring spatial perturbations indi- cates that adversarial robustness should be a concern not only in the truly adversarial setting. The finding that such misclassified perturbations can be constructed by trying out a small number of randomly chosen input transformations emphasizes that such classifiers are indeed quite brittle. Ad- ditional techniques need to be incorporated in the architec- ture and training procedures of modern classifiers to achieve worst-case translational and rotational robustness. Also, these results underline the need for considering broader notions of similarity than just pixel-wise distances when studying adversarial misclassification attacks. In particular, we view combining the pixel-wise distances with rotations and translations as a next step towards capturing the "right" notion of similarity in the context of images.Additionally, we find that classifiers trained to be robust against ∞ -bounded pixel-based perturbations do not ex- hibit any additional spatial robustness, suggesting that these are orthogonal concepts. Combining ∞ -bounded attacks and spatial transformations seems to have a cumulative ef- fect on the model. Recently, ( Xiao et al., 2018) and (Tramèr &amp; Boneh, 2017) observed independently that it is possible to use various spatial transformations to construct adversarial examples for naturally and adversarially trained models. The main difference from our work is that we show even very simple transformations (translations and rotations) are sufficient to break a variety of classifiers, while the transformations employed in ( Xiao et al., 2018) and (Tramèr &amp; Boneh, 2017) are more involved. The transformation in ( Xiao et al., 2018     Tr an sl at io n LR Tr an sl at io n LR Tr an sl at io n LR Tr an sl at io n LR Tr an sl at io n LR Tr an sl at io n LR Tr an sl at io n LR   For each value of ε, we perform PGD to find the most adversarial ∞ -bounded perturbation. Additionally, we combine PGD with random rotations and translations and with a grid search over rotations and translations in order to find the transformation that combines with PGD in the most adversarial way.
