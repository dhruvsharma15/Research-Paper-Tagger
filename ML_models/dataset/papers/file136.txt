Inspired by the aforementioned observations ( Song et al., 2005;Martinez &amp; Alonso, 2003;Cheong et al., 2013), we per- form an exploratory study on different uncorrelated and cor- related probabilistic generative models for synaptic strength formation in deep neural networks and the potential influence of different distributions on modelling performance particularly for the scenario associated with small data sets.Here we model the synaptic strength distribution of the deep neural network as P(W ) where W is the set of synaptic strengths W = w i n 1 and n is the number of synapses. InWe have recently witnessed an explosive growth in machine learning research focused on modelling and real-world infer- ence problems. Notably, deep learning models such as deep neural networks (DNN) are a particularly powerful and bio- logically inspired class of learning algorithms that have con- sistently demonstrated state-of-the-art performance on tasks such as object recognition, image classification, image seg- mentation, and speech recognition. A particular type of DNN that has proven to be very effective in recent year are convo- lutional neural networks (CNNs) (see (Hubel &amp; Wiesel, 1968)) which are architecturally made up of layers of neurons mod- elled after simple and complex cells in the visual cortex.In order to train a DNN for a task such as classification, the synaptic strengths of the network are optimized based on training data. Optimizing a large-scale artificial neural archi- tecture such as a CNN for classification in a generalizable manner, however, requires on a large number of input im- age samples. This may be prohibitive in many practical sce- narios where labeled data is limited. To ameliorate this de- pendence, we explore whether it is possible to sidestep the training of a large portion of learnable parameters-synaptic strengths-in a neural network. More particularly, we are mo- tivated by (Eliasmith et al., 2012) where strong modelling and inference performance was exhibited when random synaptic strengths are leveraged in modelling of functional brain com- putationally. This suggests that the inherent structure of deep neural networks may itself be enough to elicit a powerful mod- elling and inference performance even when the formation of synaptic strengths are random.In particular, we draw inspiration from a number of stud- ies that investigated the distribution of synaptic strengths in the biological brain. For example, it has been observed that the synaptic strengths of certain synapses such as the excita- tory synapses can be well modelled as random variables fol- lowing well-known distributions such as truncated Gaussians (Barbour, Brunel, Hakim, &amp; Nadal, 2007). Furthermore, Song et al. (Song, SjöströmSj¨SjöstrSjöstr¨Sjöström, Reigl, Nelson, &amp; Chklovskii, 2005) found that the underlying synaptic strengths follows a log- normal distributions. Other studies (Martinez &amp; Alonso, 2003;Cheong, Tailby, Solomon, &amp; Martin, 2013) suggested a corre- lated relationship between synaptic strengths in earlier layers of the visual cortex, specifically circular concentric receptive fields modelled after Lateral Geniculate (LGN) cells.order to explore the effect of different probabilistic generative models for synaptic formation on modelling and inference per- formance in a focused manner, in this study we restrict the net- work architecture to be a convolutional neural network (CNN) architecture. More specifically, the synaptic strengths in the convolutional layers are synthesized based on P(W ) and are not fine-tuned, whereas the synaptic strengths of fully con- nected layers are synthesized and then trained to reach to their complete modelling capabilities. This setup allows us to localize the effect of P(W ) on synaptic strengths and fairly compare the modelling and inference performance of different synaptic formation drawn from various underlying biologically- inspired probability distributions. Furthermore, each random variable corresponding to a synaptic strength denoted as w i are drawn from a probabilistic generative model P(W ). In this study, we explore three different distribution models based on past biological studies:III Correlated center-surround:This approach to synapse strength formation can enable a drastic reduction in the number of parameters that need to be trained, which is an important factor in scenarios with small number of training data.Followed by biological observations, the effect of three dif- ferent P(W ) are examined on a same convolutional neural 1 Σ i is the covariance matrix at synapse i, where the non-zero off- diagonal elements characterize the correlation between neighboring synapses. Table 1: Impact of different probabilistic generative models for synaptic strength generation on modelling performance for 3 small datasets (see text on how datasets were generated). The synaptic strengths of the convolutional layers were generated from distributions describing synaptic strengths in the visual cortex. The convolutional layer synapses are frozen and not trained, whereas the fully connected layers of the CNN are trained over. In order to experiment the effect of different synaptic strength distributions on modelling performance, a CNN is uti- lized consisting of a convolutional layer comprising of 64 ker- nels with receptive fields of size 5 × 5, a max-pooling layer with stride 2, and a rectified non-linear unit, as well as two fully connected layers inspired by LeNet's fully connected layer ar- chitecture (LeCun, Bottou, Bengio, &amp; Haffner, 1998) and have be outperformed by random convolutional synaptic strengths. This result is a powerful first step towards designing deep neu- ral networks that do not require many data samples to learn, and can sidestep / reduce the burden of current training pro- cedures while maintaining or boosting classification and mod- elling performance. In future work, we are excited to explore this same effect on deeper networks with more synapses, and to investigate how and whether these synaptic strength dis- tributions may be used to design more efficient architectures and training algorithms.In this exploratory study, we examined three standard and publicly available object classification datasets including MNIST hand-written digits ( LeCun et al., 1998) for each dataset) were randomly selected from the available training data in each dataset to form a small dataset. How- ever to compute the test accuracy, the models are tested with all available testing samples. The reported results (mean and std) are computed based on three runs. Table 1 summarizes the results of our experiments. We also report the classification performance of the same CNN ar- chitecture on these datasets where the CNN is completely trained, and all synaptic strengths are fine-tuned. As ex- pected, the small number of training samples (i.e., 38 per class) results in the CNN's relatively poor classification per- formance, as is evident from the right-most column of Table 1 named "Fully Trained".Interestingly, sampling the convolutional synaptic strengths from a normal Gaussian distribution ("Normal" column) yields a classification performance comparable to that of "Fully Trained" for CIFAR-10 and SVHN. The most surprising of the preliminary results can be seen in the "Log-Normal" and "Center-Surround" columns. One possibility that these results suggest is that sampling the synaptic strengths of a CNN from well-known distributions that model synaptic strengths in the visual cortex can result in a classification system that potentially outperforms carefully fine-tuned CNNs on small datasets. This may suggest that in the scenario with very little data, learning a generalizable classification system may not be worth the effort put into training as the performance may
