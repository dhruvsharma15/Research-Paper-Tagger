There are a wide variety of dynamic patterns in video sequences, including dynamic textures [2] or textured mo- tions [24] that exhibit statistical stationarity or stochastic repetitiveness in the temporal dimension, and action patterns that are non-stationary in either spatial or temporal domain. Synthesizing and analyzing such dynamic patterns has been an interesting problem. In this paper, we focus on the task of synthesizing dynamic patterns using a generative version of the convolutional neural network (ConvNet or CNN).The ConvNet [14,12] has proven to be an immensely successful discriminative learning machine. The convolution operation in the ConvNet is particularly suited for signals such as images, videos and sounds that exhibit translation in- variance either in the spatial domain or the temporal domain or both. Recently, researchers have become increasingly interested in the generative aspects of ConvNet, for the pur- pose of visualizing the knowledge learned by the ConvNet, or synthesizing realistic signals, or developing generative models that can be used for unsupervised learning.In terms of synthesis, various approaches based on the ConvNet have been proposed to synthesize realistic static images [3,7,1,13,16]. However, there has not been much work in the literature on synthesizing dynamic patterns based on the ConvNet, and this is the focus of the present paper.Specifically, we propose to synthesize dynamic patterns by generalizing the generative ConvNet model recently pro- posed by [29]. The generative ConvNet can be derived from the discriminative ConvNet. It is a random field model or an energy-based model [15,20] that is in the form of exponen- tial tilting of a reference distribution such as the Gaussian white noise distribution or the uniform distribution. The exponential tilting is parametrized by a ConvNet that in- volves multiple layers of linear filters and rectified linear units (ReLU) [12], which seek to capture features or patterns at different scales.The generative ConvNet can be sampled by the Langevin dynamics. The model can be learned by the stochastic gradi- ent algorithm [31]. It is an "analysis by synthesis" scheme that seeks to match the synthesized signals generated by the Langevin dynamics to the observed training signals. Specifi- cally, the learning algorithm iterates the following two steps after initializing the parameters and the synthesized signals.Step 1 updates the synthesized signals by the Langevin dy- namics that samples from the currently learned model. Step 2 then updates the parameters based on the difference be- tween the synthesized data and the observed data in order to shift the density of the model from the synthesized data towards the observed data. It is shown by [29] that the learn- ing algorithm can synthesize realistic spatial image patterns such as textures and objects.In this article, we generalize the spatial generative Con- vNet by adding the temporal dimension, so that the resulting ConvNet consists of multiple layers of spatial-temporal fil- ters that seek to capture spatial-temporal patterns at various scales. We show that the learning algorithm for training the spatial-temporal generative ConvNet can synthesize realistic dynamic patterns. We also show that it is possible to learn the model from incomplete video sequences with either oc- cluded pixels or missing frames, so that model learning and pattern completion can be accomplished simultaneously.where l ∈ {1, 2, ..., L} indexes the layers. {F (l) 2. Related work k , k = 1, ..., N l } are the filters at layer l, and {FOur work is a generalization of the generative ConvNet model of [29] by adding the temporal dimension. [29] did not work on dynamic patterns such as those in the video sequences. The spatial-temporal discriminative ConvNet was used by [11] for analyzing video data. The connection between discriminative ConvNet and generative ConvNet was studied by [29].Dynamic textures or textured motions have been stud- ied by [2,24,25,9]. For instance, [2] proposed a vector auto-regressive model coupled with frame-wise dimension reduction by single value decomposition. It is a linear model with Gaussian innovations. [24] proposed a dynamic model based on sparse linear representation of frames. See [30] for a recent review of dynamic textures. The spatial-temporal generative ConvNet is a non-linear and non-Gaussian model and is expected to be more flexible in capturing complex spatial-temporal patterns in dynamic textures with multiple layers of non-linear spatial-temporal filters.Recently [23] generalized the generative adversarial net- works [6] to model dynamic patterns. Our model is an energy-based model and it also has an adversarial interpreta- tion. See section 3.4 for details.For temporal data, a popular model is the recurrent neural network [27,10]. It is a causal model and it requires a start- ing frame. In contrast, our model is non-causal, and does not require a starting frame. Compared to the recurrent net- work, our model is more convenient and direct in capturing temporal patterns at multiple time scales., i = 1, ..., N l−1 } are the filters at layer l − 1. k and i are used to index filters at layers l and l − 1 respectively, and N l and N l−1 are the numbers of filters at layers l and l − 1 respectively. The filters are locally supported, so the range of (y, s) is within a local support S l (such as a 7 × 7 × 3 box of image sequence). The weight parameters (w (l,k) i,y,s , (y, s) ∈ S l , i = 1, ..., N l−1 ) define a linear filter that operates on (F (l−1) i * I, i = 1, ..., N l−1 ). The linear fil- tering operation is followed by ReLU h(r) = max(0, r)., where k ∈ {R, G, B} indexes the three color channels. Sub- sampling may be implemented so that in [FThe spatial-temporal filters at multiple layers are expected to capture the spatial-temporal patterns at multiple scales. It is possible that the top-layer filters are fully connected in the spatial domain as well as the temporal domain (e.g., the feature maps are 1 × 1 in the spatial domain) if the dynamic pattern does not exhibit spatial or temporal stationarity.The spatial-temporal generative ConvNet is an energy- based model or a random field model defined on the image sequence I = (I(x, t), x ∈ D, t ∈ T ). It is in the form of exponential tilting of a reference distribution q(I):3. Spatial-temporal generative ConvNet where the scoring function f (I; w) is 3.1. Spatial-temporal filtersTo fix notation, let I(x, t) be an image sequence of a video defined on the square (or rectangular) image domain D and the time domain T , where x = (x 1 , x 2 ) ∈ D indexes the coordinates of pixels, and t ∈ T indexes the frames in the video sequence. We can treat I(x, t) as a three dimensional function defined on D × T . For a spatial-temporal filter F , we let F * I denote the filtered image sequence or feature map, and let [F * I](x, t) denote the filter response or feature at pixel x and time t.The spatial-temporal ConvNet is a composition of mul- tiple layers of linear filtering and ReLU non-linearity, as expressed by the following recursive formula:where w consists of all the weight and bias terms that define the filters (Fat layer L, and q is the Gaussian white noise model, i.e.,where |D × T | counts the number of pixels in the domain D × T . Without loss of generality, we shall assume σ 2 = 1. The scoring function f (I; w) in (3) tilts the Gaussian reference distribution into a non-Gaussian model. In fact, the purpose of f (I; w) is to identify the non-Gaussian spatial- temporal features or patterns. In the definition of f (I; w) in (3), we sum over the filter responses at the top layer L over all the filters, positions and times. The spatial and temporal pooling reflects the fact that we assume the model is stationary in spatial and temporal domains. If the dynamic texture is non-stationary in the spatial or temporal domain, then the top layer filters FA simple but consequential property of the ReLU non- linearity is that h(r) = max(0, r) = 1(r &gt; 0)r, where 1() is the indicator function, so that 1(r &gt; 0) = 1 if r &gt; 0 and 0 otherwise. As a result, the scoring func- tion f (I; w) is piecewise linear [17], and each linear piece is defined by the multiple layers of binary activation vari- ables δ One can sample from p(I; w) of model (2) by the Langevin dynamics:, which tells us whether a local spatial-temporal pattern represented by the k-th filter at layer l, F where τ indexes the time steps, is the step size, and Z τ ∼ N(0, 1). The dynamics is driven by the reconstruction error I − B w,δ(I;w) . The finiteness of the step size can be corrected by a Metropolis-Hastings acceptance-rejection step. The Langevin dynamics can be extended to Hamilto- nian Monte Carlo [18] or more sophisticated versions [5].The learning of w from training image sequenceswhere both a and B are defined by δ(I; w) and w. In fact, B = ∂f (I; w)/∂I, and can be computed by back- propagation, with h (r) = 1(r &gt; 0). The back-propagation process defines a top-down deconvolution process [32], where the filters at multiple layers become the basis functions at those layers, and the activation variables at different layers in δ(I; w) become the coefficients of the basis functions in the top-down deconvolution.p(I; w) in (2) is an energy-based model [15,20], whose energy function is a combination of the 2 norm 2 that comes from the reference distribution q(I) and the piecewise linear scoring function f (I; w), i.e.,The expectation can be approximated by the Monte Carlo samples [31] produced by the Langevin dynamics. See Al- gorithm 1 for a description of the learning and sampling al- gorithm. The algorithm keeps synthesizing image sequences from the current model, and updating the model parameters in order to match the synthesized image sequences to the observed image sequences. The learning algorithm keeps shifting the probability density or low energy regions of the model from the synthesized data towards the observed data.In the learning algorithm, the Langevin sampling step involves the computation of ∂f (I; w)/∂I, and the parame- ter updating step involves the computation of ∂f (I; w)/∂w. Because of the ConvNet structure of f (I; w), both gradi- ents can be computed efficiently by back-propagation, and the two gradients share most of their chain rule computa- tions in back-propagation. In term of MCMC sampling, the Langevin dynamics samples from an evolving distribu- tion because w (t) keeps changing. Thus the learning and sampling algorithm runs non-stationary chains.where const = −a w,δ(I;w) − w,δ(I;w) 2 /2, which is con- stant on the piece of image space with fixed δ(I; w).Since E(I; w) is a piecewise quadratic function, p(I; w) is piecewise Gaussian. On the piece of image space {I : δ(I; w) = δ}, where δ is a fixed value of δ(I; w), p(I; w) is N(B w,δ , 1) truncated to {I : δ(I; w) = δ}, where we use 1 to denote the identity matrix. If the mean of this Gaussian piece, B w,δ , is within {I : δ(I; w) = δ}, then B w,δ is also a local mode, and this local mode I satisfies a hierarchical auto- encoder, with a bottom-up encoding process δ = δ(I; w), and a top-down decoding process I = B w,δ . In general, for an image sequence I, B w,δ(I;w) can be considered a reconstruction of I, and this reconstruction is exact if I is a local mode of E(I; w).Our model is an energy-based modelThe update of w is based on L (w) which can be approxi- mated bywhere { ˜ I m , m = 1, ..., ˜ M } are the synthesized image se- quences that are generated by the Langevin dynamics. At For each m, run l steps of Langevin dynamics to update˜Iupdate˜update˜I m , i.e., starting from the current˜Icurrent˜current˜I m , each step follows equation (7).Calculate∂w f (I m ; w (t) )/M , andLet t ← t + 1 8: until t = T the zero temperature limit, the Langevin dynamics becomes gradient descent:Consider the value function V ( ˜ I m , m = 1, ..., ˜ M ; w): The updating of w is to increase V by shifting the low energy regions from the synthesized image sequences { ˜ I m } to the observed image sequences {I m }, whereas the updating of { ˜ I m , m = 1, ..., ˜ M } is to decrease V by moving the syn- thesized image sequences towards the low energy regions. This is an adversarial interpretation of the learning and sam- pling algorithm. It can also be considered a generalization of the herding method [26] from exponential family models to general energy-based models.In our work, we let −E(I; w) = f (I; w) − 2 /2σ 2 . We can also let −E(I; w) = f (I; w) by assuming a uni- form reference distribution q(I). Our experiments show that the model with the uniform q can also synthesize realistic dynamic patterns.The generative adversarial learning [6,23] has a generator network. Unlike our model which is based on a bottom-up ConvNet f (I; w), the generator network generates I by a top-down ConvNet I = g(X; ˜ w) where X is a latent vec- tor that follows a known prior distribution, and˜wand˜ and˜w collectsWe learn the spatial-temporal generative ConvNet from video clips collected from DynTex++ dataset of [4] and the Internet. The code in the experiments is based on the MatConvNet of [22] and MexConv3D of [21].We show the synthesis results by displaying the frames in the video sequences. We have posted the synthesis results on the project page http://www.stat.ucla.edu/ ~jxie/STGConvNet/STGConvNet.html, so that the reader can watch the videos.We first learn the model from dynamic textures that are stationary in both spatial and temporal domains. We use spatial-temporal filters that are convolutional in both spatial and temporal domains. The first layer has 120 15 × 15 × 15 filters with sub-sampling size of 7 pixels and frames. The (b) fountain (c) burning fire heating a pot category from one observed video that is prepared to be of the size 224 × 224 × 50 or 70. The range of intensi- ties is [0,255]. Mean subtraction is used as pre-processing. We use˜Muse˜ use˜M = 3 chain for Langevin sampling. The num- ber of Langevin iterations between every two consecutive updates of parameters, l = 20. The number of learning iterations T = 1200, where we add one more layer every 400 iterations. We use layer-specific learning rates, where the learning rate at the higher layer is less than that at the lower layer, in order to obtain stable convergence.with only temporal stationarity second layer has 40 7×7×7 filters with sub-sampling size of 3. The third layer has 20 3 × 3 × 2 filters with sub-sampling size of 2 × 2 × 1. Figure 1 displays 2 results. For each category, the first row displays 7 frames of the observed sequence, while the second and third rows show the corre- sponding frames of two synthesized sequences generated by the learning algorithm.We use the layer-by-layer learning scheme. Starting from the first layer, we sequentially add the layers one by one. Each time we learn the model and generate the synthesized image sequence using Algorithm 1. While learning the new layer of filters, we refine the lower layers of filters with back-propagation.We learn a spatial-temporal generative ConvNet for each Many dynamic textures have structured background and objects that are not stationary in the spatial domain. In this case, the network used in Experiment 1 may fail. However, we can modify the network in Experiment 1 by using filters that are fully connected in the spatial domain at the second layer. Specifically, the first layer has 120 7 × 7 × 7 filters with sub-sampling size of 3 pixels and frames. The second layer is a spatially fully connected layer, which contains 30 filters that are fully connected in the spatial domain but convolutional in the temporal domain. The temporal size of the filters is 4 frames with sub-sampling size of 2 frames in the temporal dimension. Due to the spatial full connectivity at the second layer, the spatial domain of the feature maps at the third layer is reduced to 1 × 1. The third layer has 5 1 × 1 × 2 filters with sub-sampling size of 1 in the temporal dimension.We use end-to-end learning scheme to learn the above 3-layer spatial-temporal generative ConvNet for dynamic textures. At each iteration, the 3 layers of filters are updated with 3 different layer-specific learning rates. The learning rate at the higher layer is much less than that at the lower layer to avoid the issue of large gradients.We learn a spatial-temporal generative ConvNet for each category from one training video. We synthesize˜Msynthesize˜ synthesize˜M = 3 (b) running tigers videos using the Langevin dynamics. Figure 2 displays the results. For each category, the first row shows 6 frames of the observed sequence (224 × 224 × 70), and the second row shows the corresponding frames of a synthesized sequence generated by the learning algorithm. We use the same set of parameters for all the categories without tuning. Figure 3 compares our method to that of [2], which is a linear dynamic system model. The image sequence generated by this model appears more blurred than the sequence generated by our method.The learning of our model can be scaled up. We learn the fire pattern from 30 training videos, with mini-batch implementation. The size of each mini-batch is 10 videos. Each video contains 30 frames (100 × 100 pixels). For each mini-batch, ˜ M = 13 parallel chains for Langevin sampling is used. For this experiment, we slightly modify the network by using 120 11 × 11 × 9 filters with sub-sampling size of 5 pixels and 4 frames at the first layer, and 30 spatially fully connected filters with temporal size of 5 frames and sub-sampling size of 2 at the second layer, while keeping the setting of the third layer unchanged. The number of learning iterations T = 1300. Figure 4 shows one frame for each of 30 observed sequences and the corresponding frame of the synthesized sequences. Two examples of synthesized sequences are also displayed. Experiments 1 and 2 show that the generative spatial- temporal ConvNet can learn from sequences without align- ment. We can also specialize it to learning roughly aligned video sequences of action patterns, which are non-stationary in either spatial or temporal domain, by using a single top- layer filter that covers the whole video sequence. We learn a 2-layer spatial-temporal generative ConvNet from video sequences of aligned actions. The first layer has 200 7×7×7 filters with sub-sampling size of 3 pixels and frames. The second layer is a fully connected layer with a single filter that covers the whole sequence. The observed sequences are of the size 100 × 200 × 70. Figure 5 displays two results of modeling and synthesiz- ing actions from roughly aligned video sequences. We learn a model for each category, where the number of training sequences is 5 for the running cow example, and 2 for the running tiger example. The videos are collected from the Internet and each has 70 frames. For each example, Figure 5 displays segments of 2 observed sequences, and segments of 2 synthesized action sequences generated by the learning al- gorithm. We ruñ M = 8 paralleled chains for the experiment of running cows, and 4 paralleled chains for the experiment of running tigers. The experiments show that our model can capture non-stationary action patterns.One limitation of our model is that it does not involve explicit tracking of the objects and their parts.Our model can learn from video sequences with occluded pixels. The task is inspired by the fact that most of the videos contain occluded objects. Our learning method can be adapted to this task with minimal modification. The modification involves, for each iteration, running k steps of Langevin dynamics to recover the occluded regions of the observed sequences. At each iteration, we use the completed observed sequences and the synthesized sequences to com- pute the gradient of the log-likelihood and update the model parameters. Our method simultaneously accomplishes the following tasks: (1) recover the occluded pixels of the train- ing video sequences, (2) synthesize new video sequences from the learned model, (3) learn the model by updating the model parameters using the recovered sequences and the synthesized sequences. See Algorithm 2 for the description of the learning, sampling, and recovery algorithm.  For each m, run k steps of Langevin dynamics to recover the occluded region of I m , i.e., starting from the current I m , each step follows equation (7) For each m, run l steps of Langevin dynamics to update˜Iupdate˜update˜I m , i.e., starting from the current˜Icurrent˜current˜I m , each step follows equation (7 We design 3 types of occlusions: (1) Type 1: salt and pepper occlusion, where we randomly place 7 × 7 masks on the 150 × 150 image domain to cover 50% of the pixels of the videos. (2) Type 2: single region mask occlusion, where we randomly place a 60 × 60 mask on the 150 × 150 image domain. (3) Type 3: missing frames, where we randomly block 50% of the image frames from each video. Figure 6 displays one example of the recovery result for each type of occlusion. Each video has 70 frames.To quantitatively evaluate the qualities of the recovered videos, we test our method on 7 video sequences, which are collected from DynTex++ dataset of [4], with 3 types of occlusions. We use the same model structure as the one used in Experiment 3. The number of Langevin steps for recovering is set to be equal to the number of Langevin steps for synthesizing, which is 20. For each experiment, we report the recovery errors measured by the average per pixel difference between the original image sequence and the recovered image sequence on the occluded pixels. The range of pixel intensities is [0,255]. We compare our results (c) 50% missing frames with the results obtained by a generic Markov random field model defined on the video sequence. The model is a 3D (spatial-temporal) Markov random field, whose potentials are pairwise 1 or 2 differences between nearest neighbor pixels, where the nearest neighbors are defined in both the spatial and temporal domains. The image sequences are recovered by sampling the intensities of the occluded pixels conditional on the observed pixels using the Gibbs sampler. Table 1 shows the comparison results for 3 types of occlusions. We can see that our model can recover the incomplete data, while learning from them. If a moving object in the video is occluded in each frame, it turns out that the recovery algorithm will become an algo- rithm for background inpainting of videos, where the goal is to remove the undesired moving object from the video. We use the same model as the one in Experiment 2 for Fig- ure 2. Figure 7 shows two examples of removals of (a) a moving boat and (b) a walking person respectively. The videos are collected from [19]. For each example, the first column displays 2 frames of the original video. The second column shows the corresponding frames with masks occlud- In this paper, we propose a spatial-temporal generative ConvNet model for synthesizing dynamic patterns, such as dynamic textures and action patterns. Our experiments show that the model can synthesize realistic dynamic pat- terns. Moreover, it is possible to learn the model from video sequences with occluded pixels or missing frames.Other experiments, not included in this paper, show that our method can also generate sound patterns.The MCMC sampling of the model can be sped up by learning and sampling the models at multiple scales, or by re- cruiting the generator network to reconstruct and regenerate the synthesized examples as in cooperative training [28].
