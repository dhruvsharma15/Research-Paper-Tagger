True text understanding is one of the key goals in Natural Language Processing and requires capa- bilities beyond the lexical semantics of individual words or phrases. Natural language descriptions are typically driven by an inter-sentential coher- ent structure, exhibiting specific discourse proper- ties, which in turn contribute significantly to the global meaning of a text. Automatically detecting how meaning units are organized benefits practi- cal downstream applications, such as question an- swering ( Sun and Chai, 2007), recognizing tex- tual entailment (Hickl, 2008), sentiment analysis ( Trivedi and Eisenstein, 2013), or text summariza- tion ( Hirao et al., 2013).Various formalisms in terms of semantic co- herence frameworks have been proposed to ac- count for these contextual assumptions ( Mann and Thompson, 1988;Lascarides and Asher, 1993;Webber, 2004). The annotation schemata of the Penn Discourse Treebank ( Prasad et al., 2008, PDTB) and the Chinese Discourse Treebank ( Zhou and Xue, 2012, CDTB), for instance, define Arg1: 会谈就一些原则和具体问题进行了 深入讨论，达成了一些谅解 In the talks, they discussed some principles and specific questions in depth, and reached some understandings Arg2: 双 方 一 致 认 为 会 谈 具 有 积 极 成 果 Both sides agree that the talks have positive re- sults Motivation: Previous work on implicit sense la- beling is heavily feature-rich and requires domain- specific, semantic lexicons ( Pitler et al., 2009;Feng and Hirst, 2012;Huang and Chen, 2011). Only recently, resource-lean architectures have been proposed. These promising neural meth- ods attempt to infer latent representations appro- priate for implicit relation classification ( Zhang et al., 2015;Ji et al., 2016;Chen et al., 2016). So far, unfortunately, these models have been eval- uated only on four top-level senses-sometimes even with inconsistent evaluation setups. 2 Further- more, most systems have initially been designed for the English PDTB and involve complex, task-specific architectures ( Liu and Li, 2016), while discourse modeling techniques for Chinese have received very little attention in the literature and are still seriously underrepresented in terms of publicly available systems. What is more, over 80% of all words in Chinese discourse relations are implicit-compared to only 52% in English ( Zhou and Xue, 2012).Recently, in the context of the CoNLL 2016 shared task ( ), a first independent evaluation platform beyond class level has been established. Surprisingly, the best performing neu- ral architectures to date are standard feedforward networks, cf. Wang and Lan (2016); Schenk et al. (2016); Qin et al. (2016). Even though these spe- cific models completely ignore word order within arguments, such feedforward architectures have been claimed by  to gen- erally outperform any thoroughly-tuned recurrent architecture. Our Contribution: In this work, we release the first attention-based recurrent neural sense clas- sifier, specifically developed for Chinese implicit discourse relations. Inspired by Zhou et al. (2016), our system is a practical adaptation of the recent advances in relation modeling extended by a novel sampling scheme.Contrary to previous assertions by , our model demonstrates superior performance over traditional bag-of-words ap- proaches with feedfoward networks by treating discourse arguments as a joint sequence. We eval- uate our method within an independent frame- work and show that it performs very well beyond standard class-level predictions, achieving state- of-the-art accuracy on the CDTB test set.We illustrate how our model's attention mech- anism provides means to highlight those parts of an input sequence that are relevant for the classi- fication decision, and thus, it may enable a bet- ter understanding of the implicit discourse pars- ing problem. Our proposed network architecture is flexible and largely language-independent as it operates only on word embeddings. It stands out due to its structural simplicity and builds a solid ground for further development towards other tex- tual domains. We propose the use of an attention-based bidi- rectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997, LSTM) network to pre- dict senses of discourse relations. The model draws upon previous work on LSTM, in particu- lar its bidirectional mode of operation ( Graves and Schmidhuber, 2005), attention mechanisms for re- current models ( Bahdanau et al., 2014;Hermann et al., 2015), and the combined use of these tech- niques for entity relation recognition in annotated sequences ( Zhou et al., 2016). More specifically, our model is a flexible recurrent neural network with capabilities to sequentially inspect tokens and to highlight which parts of the input sequence are most informative for the discourse relation recog- nition task, using the weighting provided by the at- tention mechanism. Furthermore, the model bene- fits from a novel sampling scheme for arguments, as elaborated below. The system is learned in an end-to-end manner and consists of multiple layers, which are illustrated in Figure 1.First, token sequences are taken as input and special markers (&lt;ARG1&gt;, &lt;/ARG1&gt;, etc.) are inserted into the corresponding positions to inform the model on the start and end points of argument spans. This way, we can ensure a general flexi- bility in modeling discourse units and could eas- ily extend them with additional context, for in- stance. In our experiments on implicit arguments, only the tokens in the respective spans are con- sidered. Note that, unlike previous works, our ap- proach models Arg1-Arg2 pairs as a joint sequence and does not first compute intermediate represen- tations of arguments separately.Second, an input layer encodes tokens using one-hot vector representations (t i for tokens at po- sitions i ∈ [1, k]), and a subsequent embedding layer provides a dense representation (e i ) to serve as input for the recurrent layers. The embedding layer is initialized using pre-trained word vectors, in our case 300-dimensional Chinese Gigaword vectors ( Graff and Chen, 2005). 3 These embed- dings are further tuned as the network is trained towards the prediction task. Embeddings for un- known tokens, e.g., markers, are trained by back- propagation only. Note that, tokens, markers and the pre-trained vectors represent the only source of information for the prediction task.For the recurrent setup, we use a layer of LSTM networks in a bidirectional manner, in order to bet- ter capture dependencies between parts of the in- put sequence by inspection of both left and right- hand-side contexts at each time step. The LSTM holds a state representation as a continuous vector passed to the subsequent time step, and it is capa- ble of modeling long-range dependencies due to its gated memory. The forward (A ) and backward (A ) LSTMs traverse the sequence e i , producing sequences of vectors h i and h i respectively, which are then summed together (indicated by ⊕ in Fig- ure 1).The resulting sequence of vectors h i is reduced into a single vector and fed to the final softmax output layer in order to classify the sense label y of the discourse relation. This vector may be ob- tained either as the final vector h produced by an LSTM, or through pooling of all h i , or by using attention, i.e., as a weighted sum over h i . While the model may be somewhat more difficult to opti- mize using attention, it provides the added benefit of interpretability, as the weights highlight to what extent the classifier considers the LSTM state vec- tors at each token during modeling. This is par- ticularly interesting for discourse parsing, as most previous approaches have provided little support for pinpointing the driving features in each argu- ment span.Finally, the attention layer contains the trainable vector w (of the same dimensionality as vectors h i ) which is used to dynamically produce a weight vector α over time steps i by:where H is a matrix consisting of vectors h i . The output layer r is the weighted sum of vectors in H: r = Hα T Partial Argument Sampling: For the purpose of enlarging the instance space of training items in the CDTB, and thus, in order to improve the predictive performance of the model, we propose a novel partial sampling scheme of arguments, whereby the model is trained and validated on se- quences containing both arguments, as well as sin- gle arguments. A data point (a 1 , a 2 , y), with a i be- ing the token sequence of argument i, is expanded into {(a 1 , a 2 , y), (a 1 , a 2 , y), (a 1 , y), (a 2 , y)}. We duplicate bi-argument samples (a 1 , a 2 , y) (in training and development data only) to balance their frequencies against single-argument samples. Two lines of motivation support the inclusion of single argument training examples, grounded in linguistics and machine learning, respectively. First, it has been shown that single arguments in isolation can evoke a strong expectation towards a certain implicit discourse relation, cf. Asr and Demberg (2015) and, in particular, Rohde and Horton (2010) in their psycholinguistic study on implicit causality verbs. Second, the procedure may encourage the model to learn better represen- tations of individual argument spans in support of modeling of arguments in composition, cf. LeCun et al. (2015). Due to these aspects, we believe this data augmentation technique to be effective in re- inforcing the overall robustness of our model. Implementational Details: We train the model using fixed-length sequences of 256 tokens with zero padding at the beginning of shorter sequences and truncate longer ones. Each LSTM has a vector dimensionality of 300, matching the embedding size. The model is regularized by 0.5 dropout rate between the layers and weight decay (2.5e −6 ) on the LSTM inputs. We employ Adam optimization (Kingma and Ba, 2014) using the cross-entropy loss function with mini batch size of 80. 4 % accuracy Rank System % accuracy 1 Wang and Lan (2016) 73. 53 1 Wang and Lan (2016) 72.42 2 Qin et al. (2016) 71.57 2 Schenk et al. (2016) 71.87 3 Schenk et al. (2016) 70. 59 3  70.47 4  68. 30 4 Qin et al. (2016) 67.41 5 Weiss and Bajec (2016) 66.67 5 Weiss and Bajec (2016) 64.07 6 Weiss and Bajec (2016) 61. 44 6 Weiss and Bajec (2016) 63.51 7 Jian et al. (2016) 21.90 7 Jian et al. (2016) 21.73 tions on the test set, outperforming the best feed- forward system of Wang and Lan (2016) and all other word order-agnostic approaches. Develop- ment and test set performances suggest the robust- ness of our approach and its ability to generalize to unseen data. Table 2: Implicit sense labels in the CDTB.We evaluate our recurrent model on the CoNLL 2016 shared task data 5 which include the official training, development and test sets of the CDTB; cf. Table 2 for an overview of the implicit sense distribution. 6 In accordance with previous setups (Rutherford et al., 2016), we treat entity relations (ENTREL) as implicit and exclude ALTLEX relations. In the evaluation, we focus on the sense-only track, the subtask for which gold arguments are provided and a system is supposed to label a given argu- ment pair with the correct sense. The results are shown in Table 1.With our proposed architecture it is possible to correctly label 257/352 (73.01%) of implicit rela- Ablation Study: We perform an ablation study to quantitatively assess the contribution of two of the characteristic aspects of our model. First, we com- pare the use of the attention mechanism against the simpler alternative of feeding the final LSTM hidden vectors (h k and h 1 ) directly to the output layer. When attention is turned off, this yields an absolute decrease in performance of 2.70% on the test set, which is substantial and significant ac- cording to a Welch two-sample t-test (p &lt; .001). Second, we independently compare the use of the partial sampling scheme against training on the standard argument pairs in the CDTB. Here, the absence of the partial sampling scheme yields an absolute decrease in accuracy of 5.74% (p &lt; .001), which demonstrates its importance for achieving competitive performance on the task.Performance on the PDTB: As a side experi- ment, we investigate the model's language inde- pendence by applying it to the implicit argument pairs of the English PDTB. Due to computational time constraints we do not optimize hyperparam- eters, but instead train the model using identical settings as for Chinese, which is expected to lead to suboptimal performance on the evaluation data. Nevertheless, we measure 27.09% accuracy on the PDTB test set (surpassing the majority class base- line of 22.01%), which shows that the model has potential to generalize across implicit discourse relations in a different language. CONJUNCTION:In the talks, they discussed some principles and specific questions in depth, and reached some understandingsBoth sides agree that the talks have positive results ENTREL:He said: We hope that the Macao government will continue to pay attention to these three issues,in order to find a final proper solution Visualizing Attention Weights: Finally, in Fig- ure 2, we illustrate the learned attention weights which pinpoint important subcomponents within a given implicit discourse relation. For the im- plicit CONJUNCTION relation the weights indicate a peak on the transition between the argument boundary, establishing a connection between the semantically related terms understandings-agree. Most ENTRELs show an opposite trend: here sec- ond arguments exhibit larger intensities than Arg1, as most entity relations follow the characteristic writing style of newspapers by adding additional information by reference to the same entity.ily adapted to similar relation recognition tasks. In future work, we intend to extend our approach to different languages and domains, e.g., to the recent data sets on narrative story understanding or ques- tion answering (Mostafazadeh et al., 2016;Feng et al., 2015). We believe that recurrent modeling of implicit discourse information can be a driving force in successfully handling such complex se- mantic processing tasks. 7 In this work, we have presented the first attention- based recurrent neural sense labeler specifically developed for Chinese implicit discourse relations. Its ability to model discourse units sequentially and jointly has been shown to be highly benefi- cial, both in terms of state-of-the-art performance on the CDTB (outperforming word order-agnostic feedforward approaches), and also in terms of insightful observations into the inner workings of the model through its attention mechanism. The architecture is structurally simple, benefits from partial argument sampling, and can be eas-
