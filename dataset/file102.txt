Deep learning [1] has led to many breakthroughs including speech and image recognition [2,3,4,5,6,7]. A subfamily of deep models, the Sequence-to-Sequence (seq2seq) neural net- works have proved to be very successful on complex transduction tasks, such as machine translation [8,9,10], speech recognition [11,12,13], and lip-reading [14]. Seq2seq networks can typi- cally be decomposed into modules that implement stages of a data processing pipeline: an encoding module that transforms its inputs into a hidden representation, a decoding (spelling) module which emits target sequences and an attention module that com- putes a soft alignment between the hidden representation and the targets. Training directly maximizes the probability of observing desired outputs conditioned on the inputs. This discriminative training mode is fundamentally different from the generative "noisy channel" formulation used to build classical state-of-the art speech recognition systems. As such, it has benefits and limitations that are different from classical ASR systems. Understanding and preventing limitations specific to seq2seq models is crucial for their successful development. Discrimina- tive training allows seq2seq models to focus on the most infor- mative features. However, it also increases the risk of overfitting to those few distinguishing characteristics. We have observed that seq2seq models often yield very sharp predictions, and only a few hypotheses need to be considered to find the most likely transcription of a given utterance. However, high confidence reduces the diversity of transcripts obtained using beam search.During typical training the models are conditioned on ground truth transcripts and are scored on one-step ahead predictions. By itself, this training criterion does not ensure that all relevant fragments of the input utterance are transcribed. Subsequently, The listener is a multilayer Bi-LSTM network that transforms a sequence of N frames of acoustic features x1, x2, . . . , xN into a possibly shorter sequence of hidden activations h1, h2, . . . , h N/k , where k is a time reduction constant [12,13].The speller computes the probability of a sequence of characters conditioned on the activations of the listener. The probability is computed one character at a time, using the chain rule:iTo emit a character the speller uses the attention mechanism to find a set of relevant activations of the listener α and summa- rize them into a context c. The history of previously emitted characters is encapsulated in a recurrent state s:We implement the recurrent step using a single LSTM layer.The attention mechanism is sensitive to the location of frames selected during the previous step and employs the convolutional filters over the previous attention weights [11]. The output char- acter distribution is computed using a SoftMax function.Our speech recognizer computes the probability of a character conditioned on the partially emitted transcript and the whole utterance. It can thus be trained to minimize the cross-entropy between the ground-truth characters and model predictions. The training loss over a single utterance isWe have analysed the impact of model confidence by separating its effects on model accuracy and beam search effectiveness. We also propose a practical solution to the partial transcriptions problem, relating to the coverage of the input utterance.i c where T (yi, c) denotes the target label function. In the baseline model T (yi, c) is the indicator [yi = c], i.e. its value is 1 for the correct character, and 0 otherwise. When label smoothing is used, T encodes a distribution over characters.Decoding new utterances amounts to finding the character se- quence y * that is most probable under the distribution computed by the network:y yDue to the recurrent formulation of the speller function, the most probable transcript cannot be found exactly using the Viterbi algorithm. Instead, approximate search methods are used. Typically, best results are obtained using beam search. The search begins with the set (beam) of hypotheses containing only the empty transcript. At every step, candidate transcripts are formed by extending hypothesis in the beam by one character. The candidates are then scored using the model, and a certain number of top-scoring candidates forms the new beam. The model indicates that a transcript is considered to be finished by emitting a special EOS (end-of-sequence) token.Model confidence is promoted by the the cross-entropy training criterion. For the baseline network the training loss (7) is mini- mized when the model concentrates all of its output distribution on the correct ground-truth character. This leads to very peaked probability distributions, effectively preventing the model from indicating sensible alternatives to a given character, such as its homophones. Moreover, overconfidence can harm learning the deeper layers of the network. The derivative of the loss backprop- agated through the SoftMax function to the logit corresponding to character c equals [yi = c]−p(yi|y&lt;i, x), which approaches 0 as the network's output becomes concentrated on the correct character. Therefore whenever the spelling RNN makes a good prediction, very little training signal is propagated through the attention mechanism to the listener.Model overconfidence can have two consequences. First, next-step character predictions may have low accuracy due to overfitting. Second, overconfidence may impact the ability of beam search to find good solutions and to recover from errors.We first investigate the impact of confidence on beam search by varying the temperature of the SoftMax function. Without retraining the model, we change the character probability distri- bution to depend on a temperature hyperparameter T :The simplest solution to include a separate language model is to extend the beam search cost with a language modeling term [12,4,15]:y where coverage refers to a term that promotes longer transcripts described it in detail in Section 3.3.We have identified two challenges in adding the language model. First, due to model overconfidence deviations from the best guess of the network drastically changed the term − log p(y|x), which made balancing the terms in eq. (9) dif- ficult. Second, incomplete transcripts were produced unless a recording coverage term was added.Equation (9) is a heuristic involving the multiplication of a conditional and unconditional probabilities of the transcript y. We have tried to justify it by adding an intrinsic language model suppression term log p(y) that would transform p(y|x) into p(x|y) ∝ p(y|x)/p(y). We have estimated the language modeling capability of the speller p(y) by replacing the encoded speech with a constant, separately trained, biasing vector. The per character perplexity obtained was about 6.5 and we didn't observe consistent gains from this extension of the beam search criterion.At increased temperatures the distribution over characters be- comes more uniform. However, the preferences of the model are retained and the ordering of tokens from the most to least probable is preserved. Tuning the temperature therefore allows to demonstrate the impact of model confidence on beam search, without affecting the accuracy of next step predictions.Decoding results of a baseline model on the WSJ dev93 data set are presented in Figure 1. We haven't used a language model. At high temperatures deletion errors dominated. We didn't want to change the beam search cost and instead constrained the search to emit the EOS token only when its probability was within a narrow range from the most probable token. We compare the default setting (T = 1), with a sharper distribution (T = 0.5) and smoother distributions (T ∈ {1.3, . . . , 4}). All strategies lead to the same greedy decoding accuracy, because temperature changes do not affect the selection of the most probable character. As temperature increases beam search finds better solutions, however care must be taken to prevent truncated transcripts.A elegant solution to model overconfidence was problem pro- posed for the Inception image recognition architecture [16]. For the purpose of computing the training cost the ground-truth label distribution is smoothed, with some fraction of the probability mass assigned to classes other than the correct one. This in turn prevents the model from learning to concentrate all probability mass on a single token. Additionally, the model receives more training signal because the error function cannot easily saturate.Originally uniform label smoothing scheme was proposed in which the model is trained to assign β probability mass to he Figure 1: Influence of beam width and SoftMax temperature on decoding accuracy. In the baseline case (no label smoothing) increasing the temperature reduces the error rate. When label smoothing is used the next-character prediction improves, as witnessed by WER for beam size=1, and tuning the temperature does not bring additional benefits. LM cost Model cost log p(y) log p(y|x) "chase is nigeria's registrar and the society is an independent organi- zation hired to count votes" -108. 5 -34.5 "in the society is an independent organization hired to count votes" -64.6 -19.9"chase is nigeria's registrar" -40.6 -31.2 "chase's nature is register" -37.8 -20.3 "" -3.5 -12.5 correct label, and spread the 1 − β probability mass uniformly over all classes [16]. Better results can be obtained with unigram smoothing which distributes the remaining probability mass proportionally to the marginal probability of classes [17]. In this contribution we propose a neighborhood smoothing scheme that uses the temporal structure of the transcripts: the remaining 1 − β probability mass is assigned to tokens neighboring in the transcript. Intuitively, this smoothing scheme helps the model to recover from beam search errors: the network is more likely to make mistakes that simply skip a character of the transcript.We have repeated the analysis of SoftMax temperature on beam search accuracy on a network trained with neighborhood smoothing in Figure 1. We can observe two effects. First, the model is regularized and greedy decoding leads to nearly 3 percentage smaller error rate. Second, the entropy of network predictions is higher, allowing beam search to discover good solutions without the need for temperature control. Moreover, the since model is trained and evaluated with T = 1 we didn't have to control the emission of EOS token.utterance. The last rows demonstrate severely incomplete tran- scriptions that may be discovered when decoding is performed with even wider beam sizes.We compare three strategies designed to prevent incomplete transcripts. The first strategy doesn't change the beam search criterion, but forbids emitting the EOS token unless its probabil- ity is within a set range of that of the most probable token. This strategy prevents truncations, but is inefficient against omissions in the middle of the transcript, such as the failure shown in Ta- ble 1. Alternatively, beam search criterion can be extended to promote long transcripts. A term depending on the transcript length was proposed for both CTC [4] and seq2seq [12] net- works, but its usage was reported to be difficult because beam search was looping over parts of the recording and additional constraints were needed [12]. To prevent looping we propose to use a coverage term that counts the number of frames that have received a cumulative attention greater than τ :j iWhen a language model is used wide beam searches often yield incomplete transcripts. With narrow beams, the problem is less visible due to implicit hypothesis pruning. We illustrate a failed decoding in Table 1. The ground truth (first row) is the least prob- able transcript according both to the network and the language model. A width 100 beam search with a trigram language model finds the second transcript, which misses the beginning of the The coverage criterion prevents looping over the utterance because once the cumulative attention bypasses the threshold τ a frame is counted as selected and subsequent selections of this frame do not reduce the decoding cost. In our implementation, the coverage is recomputed at each beam search iteration using all attention weights produced up to this step.In Figure 2 we compare the effects of the three methods when decoding a network that uses label smoothing and a tri- gram language model. Unlike [12] we didn't experience looping when beam search promoted transcript length. We hypothe- size that label smoothing increases the cost of correct character emissions which helps balancing all terms used by beam search. We observe that at large beam widths constraining EOS emis- sions is not sufficient. In contrast, both promoting coverage and transcript length yield improvements with increasing beams. However, simply maximizing transcript length yields more word insertion errors and achieves an overall worse WER.We conducted all experiments on the Wall Street Journal dataset, training on si284, validating on dev93 and evaluating on eval92 Parameters dev93 eval92CTC [3] 26.5M - 27.3 seq2seq [12] 5.7M - 18.6 seq2seq [24] 5.9M - 12.9 seq2seq [22] - - 10.  Table 3: Results withextended trigram language model on WSJ.Figure 2: Impact of using techniques that prevent incomplete transcripts when a trigram language models is used on the dev93 WSJ subset. Results are averaged across two networksset. The models were trained on 80-dimensional mel-scale filter- banks extracted every 10ms form 25ms windows, extended with their temporal first and second order differences and per-speaker mean and variance normalization. Our character set consisted of lowercase letters, the space, the apostrophe, a noise marker, and start-and end-of sequence tokens. For comparison with previously published results, experiments involving language models used an extended-vocabulary trigram language model built by the Kaldi WSJ s5 recipe [18]. We have use the FST framework to compose the language model with a "spelling lexicon" [6,12,19]. All models were implemented using the Tensorflow framework [20]. Our base configuration implemented the Listener using 4 bidirectional LSTM layers of 256 units per direction (512 total), interleaved with 3 time-pooling layers which resulted in an 8-fold reduction of the input sequence length, approximately equating the length of hidden activations to the number of characters in the transcript. The Speller was a single LSTM layer with 256 units. Input characters were embedded into 30 dimensions. The attention MLP used 128 hidden units, previous attention weights were accessed using 3 convolutional filters spanning 100 frames. LSTM weights were initialized uniformly over the range ±0.075. Networks were trained using 8 asynchronous replica workers each employing the ADAM algorithm [21] with default parameters and the learning rate set initially to 10 −3 , then reduced to 10 −4 and 10 −5 after 400k and 500k training steps, respectively. Static Gaussian weight noise with standard deviation 0.075 was applied to all weight matrices after 20000 training steps. We have also used a small weight decay of 10 −6 .Baseline + Cov 12.6 8.9 Unigram LS + Cov.9.9 7.0 Temporal LS + Cov. 9.7 6.7mance achieved with very deep and sophisticated encoders [22]. Table 3 gathers results that use the extended trigram lan- guage model. We report averages of two runs. For each run we have tuned beam search parameters on the validation set and applied them on the test set. A typical setup used beam width 200, language model weight λ = 0.5, coverage weight γ = 1.5 and coverage threshold τ = 0.5. Our best result surpasses CTC- based networks [6] and matches the results of a DNN-HMM and CTC ensemble [23].We have compared two label smoothing methods: unigram smoothing [17] with the probability of the correct label set to 0.95 and neighborhood smoothing with the probability of correct token set to 0.9 and the remaining probability mass distributed symmetrically over neighbors at distance ±1 and ±2 with a 5 : 2 ratio. We have tuned the smoothing parameters with a small grid search and have found that good results can be obtained for a broad range of settings.We have gathered results obtained without language models in Table 2. We have used a beam size of 10 and no mechanism to promote longer sequences. We report averages of two runs taken at the epoch with the lowest validation WER. Label smoothing brings a large error rate reduction, nearly matching the perfor- Label smoothing was proposed as an efficient regularizer for the Inception architecture [16]. Several improved smoothing schemes were proposed, including sampling erroneous labels instead of using a fixed distribution [25], using the marginal label probabilities [17], or using early errors of the model [26]. Smoothing techniques increase the entropy of a model's pre- dictions, a technique that was used to promote exploration in reinforcement learning [27,28,29]. Label smoothing prevents saturating the SoftMax nonlinearity and results in better gradient flow to lower layers of the network [16]. A similar concept, in which training targets were set slightly below the range of the output nonlinearity was proposed in [30].Our seq2seq networks are locally normalized, i.e. the speller produces a probability distribution at every step. Alternatively normalization can be performed globally on whole transcripts. In discriminative training of classical ASR systems normaliza- tion is performed over lattices [31]. In the case of recurrent networks lattices are replaced by beam search results. Global normalization has yielded important benefits on many NLP tasks including parsing and translation [32,33]. Global normalization is expensive, because each training step requires running beam search inference. It remains to be established whether globally normalized models can be approximated by cheaper to train lo- cally normalized models with proper regularization such as label smoothing.Using source coverage vectors has been investigated in neu- ral machine translation models. Past attentions vectors were used as auxiliary inputs in the emitting RNN either directly [34], or as cumulative coverage information [35]. Coverage embeddings vectors associated with source words end modified during train- ing were proposed in [36]. Our solution that employs a coverage penalty at decode time only is most similar to the one used by the Google Translation system [10].
