Learning in statistical models via gradient descent is straightforward when the objective function and its gradients are tractable. In the presence of latent variables, however, many objectives become intractable. For neural generative models with latent variables, there are currently a few dominant approaches: optimizing lower bounds on the marginal log-likelihood [1,2], restricting to a class of invertible models [3], or using likelihood-free methods [4,5,6,7]. In this work, we focus on the first approach and introduce filtering variational objectives (FIVOs), a tractable family of objectives for maximum likelihood estimation (MLE) in latent variable models with sequential structure.Specifically, let x denote an observation of an X -valued random variable. We assume that the process generating x involves an unobserved Z-valued random variable z with joint density p(x, z) in some family P. The goal of MLE is to recover p ∈ P that maximizes the marginal log-likelihood, log p(x) = log p(x, z) dz 1 . The difficulty in carrying out this optimization is that the log- likelihood function is defined via a generally intractable integral. To circumvent marginalization, a common approach [1,2] is to optimize a variational lower bound on the marginal log-likelihood [8,9]. The evidence lower bound L(x, p, q) (ELBO) is the most common such bound and is defined by a variational posterior distribution q(z|x) whose support includes p's, L(x, p, q) = E q(z|x) log p(x, z) q(z|x) = log p(x) − KL(q(z|x) p(z|x)) ≤ log p(x) .L(x, p, q) lower-bounds the marginal log-likelihood for any choice of q, and the bound is tight when q is the true posterior p(z|x). Thus, the joint optimum of L(x, p, q) in p and q is the MLE. In practice, it is common to restrict q to a tractable family of distributions (e.g., a factored distribution) and to jointly optimize the ELBO over p and q with stochastic gradient ascent [1,2,10,11]. Because of the KL penalty from q to p, optimizing (1) under these assumptions tends to force p's posterior to satisfy the factorizing assumptions of the variational family which reduces the capacity of the model p. One strategy for addressing this is to decouple the tightness of the bound from the quality of q. For example, [12] observed that Eq. (1) can be interpreted as the log of an unnormalized importance weight with the proposal given by q, and that using N samples from the same proposal produces a tighter bound, known as the importance weighted auto-encoder bound, or IWAE.Indeed, it follows from Jensen's inequality that the log of any unbiased positive Monte Carlo estimator of the marginal likelihood results in a lower bound that can be optimized for MLE. The filtering variational objectives (FIVOs) build on this idea by treating the log of a particle filter's likelihood estimator as an objective function. Following [13], we call objectives defined as log-transformed likelihood estimators Monte Carlo objectives (MCOs). In this work, we show that the tightness of an MCO scales like the relative variance of the estimator from which it is constructed. It is well-known that the variance of a particle filter's likelihood estimator scales more favourably than simple importance sampling for models with sequential structure [14,15]. Thus, FIVO can potentially form a much tighter bound on the marginal log-likelihood than IWAE.The main contributions of this work are introducing filtering variational objectives and a more careful study of Monte Carlo objectives. In Section 2, we review maximum likelihood estimation via maximizing the ELBO. In Section 3, we study Monte Carlo objectives and provide some of their basic properties. We define filtering variational objectives in Section 4, discuss details of their optimization, and present a sharpness result. Finally, we cover related work and present experiments showing that sequential models trained with FIVO outperform models trained with ELBO or IWAE in practice.We briefly review techniques for optimizing the ELBO as a surrogate MLE objective. We restrict our focus to latent variable models in which the model p θ (x, z) factors into tractable conditionals p θ (z) and p θ (x|z) that are parameterized differentiably by parameters θ. MLE in these models is then the problem of optimizing log p θ (x) in θ. The expectation-maximization (EM) algorithm is an approach to this problem which can be seen as coordinate ascent, fully maximizing L(x, p θ , q) alternately in q and θ at each iteration [16,17,18]. Yet, EM rarely applies in general, because maximizing over q for a fixed θ corresponds to a generally intractable inference problem.Instead, an approach with mild assumptions on the model is to perform gradient ascent following a Monte Carlo estimator of the ELBO's gradient [19,10]. We assume that q is taken from a family of distributions parameterized differentiably by parameters φ. We can follow an unbiased estimator of the ELBO's gradient by sampling z ∼ q φ (z|x) and updating the parameters by θ = θ + η∇ θ log p θ (x, z) and φ = φ + η(log p θ (x, z) − log q φ (z|x))∇ φ log q φ (z|x), where the gradients are computed conditional on the sample z and η is a learning rate. Such estimators follow the ELBO's gradient in expectation, but variance reduction techniques are usually necessary [10,20,13].A lower variance gradient estimator can be derived if q φ is a reparameterizable distribution [1,2,21]. Reparameterizable distributions are those that can be simulated by sampling from a distribution ∼ d( which does not depend on φ, and then applying a deterministic transformation z = f φ (x, When p θ , q φ , and f φ are differentiable, an unbiased estimator of the ELBO gradient consists of sampling and updating the parameter by (θ , φ ) = (θ, φ) + η∇ (θ,φ) (log p θ (x, f φ (x, − log q φ (f φ (x, Given the gradients of the sampling process can flow through z = f φ (x, Unfortunately, when the variational family of q φ is restricted, following gradients of −KL(q φ (z|x) p θ (z|x)) tends to reduce the capacity of the model p θ to match the assumptions of the variational family. This KL penalty can be "removed" by considering generalizations of the ELBO whose tightness can be controlled by means other than the closenesss of p and q, e.g., [12]. We consider this in the next section. they are lower bounds on the marginal log-likelihood, and thus can be used for MLE. Motivated by the previous section, we present results on the convergence of generic MCOs to the marginal log-likelihood and show that the tightness of an MCO is closely related to the variance of the estimator that defines it.One can verify that the ELBO is a lower bound by using the concavity of log and Jensen's inequality,This argument only relies only on unbiasedness of p(x, z)/q(z|x) when z ∼ q(z|x). Thus, we can generalize this by considering any unbiased marginal likelihood estimatorˆpestimatorˆ estimatorˆp N (x) and treating E[logˆplogˆ logˆp N (x)] as an objective function over models p. Here N ∈ N indexes the amount of computation needed to simulatê p N (x), e.g., the number of samples or particles.For example, the ELBO is constructed from a single unnormalized importance weightˆpweightˆ weightˆp(x) = p(x, z)/q(z|x). The IWAE bound [12] takesˆptakesˆ takesˆp N (x) to be N averaged i.i.d. importance weights,We consider additional examples in the Appendix. To avoid notational clutter, we omit the arguments to an MCO, e.g., the observations x or model p, when the default arguments are clear from context. Whether we can compute stochastic gradients of L N efficiently depends on the specific form of the estimator and the underlying random variables that define it.Many likelihood estimatorsˆpestimatorsˆ estimatorsˆp N (x) converge to p(x) almost surely as N → ∞ (known as strong consistency). The advantage of a consistent estimator is that its MCO can be driven towards log p(x) by increasing N . We present sufficient conditions for this convergence and a description of the rate:Proposition 1. Properties of Monte Carlo Objectives. Let L N (x, p) be a Monte Carlo objective defined by an unbiased positive estimatorˆpestimatorˆ estimatorˆp N (x) of p(x). Then,Proof. See the Appendix for the proof and a sufficient condition for controlling the first inverse moment whenˆpwhenˆ whenˆp N (x) is the average of i.i.d. random variables.In some cases, convergence of the bound to log p(x) is monotonic, e.g., IWAE [12], but this is not true in general. The relative variance of estimators, var(ˆ p N (x)/p(x)), tends to be well studied, so property (c) gives us a tool for comparing the convergence rate of distinct MCOs. For example, [14,15] study marginal likelihood estimators defined by particle filters and find that the relative variance of these estimators scales favorably in comparison to naive importance sampling. This suggests that a particle filter's MCO, introduced in the next section, will generally be a tighter bound than IWAE.1: FIVO(x 1:T , p, q, N ):if resampling criteria satisfied by {w11:16:4 Filtering Variational Objectives (FIVOs)The filtering variational objectives (FIVOs) are a family of MCOs defined by the marginal likelihood estimator of a particle filter. For models with sequential structure, e.g., latent variable models of audio and text, the relative variance of a naive importance sampling estimator tends to scale exponentially in the number of steps. In contrast, the relative variance of particle filter estimators can scale more favorably with the number of steps-linearly in some cases [14,15]. Thus, the results of Section 3 suggest that FIVOs can serve as tighter objectives than IWAE for MLE in sequential models.Let our observations be sequences of T X -valued random variables denoted x 1:T , where x i:j ≡ (x i , . . . , x j ). We also assume that the data generation process relies on a sequence of T unobserved Z-valued latent variables denoted z 1:T . We focus on sequential latent variable models that factor as a series of tractable conditionals, p(x 1:T , z 1:T ) = p 1 (x 1 , z 1 ) T t=2 p t (x t , z t |x 1:t−1 , z 1:t−1 ). A particle filter is a sequential Monte Carlo algorithm, which propagates a population of N weighted particles for T steps using a combination of importance sampling and resampling steps, see Alg. 1. In detail, the particle filter takes as arguments an observation x 1:T , the number of particles N , the model distribution p, and a variational posterior q(z 1:T |x 1:T ) factored over t, T q(z 1:T |x 1:T ) = q t (z t |x 1:t , z 1:t−1 ) .(6) t=1The particle filter maintains a population {w and renormalized. If the current weights w i t satisfy a resampling criteria, then a resampling step is performed and N particles z i 1:t are sampled in proportion to their weights from the current population with replacement. Common resampling schemes include resampling at every step and resampling if the effective sample size (ESS) of the population ( [22]. After resampling the weights are reset to 1. Otherwise, the particles z i 1:t are copied to the next step along with the accumulated weights. See Fig. 1 for a visualization.Instead of viewing Alg. 1 as an inference algorithm, we treat the quantity E[logˆplogˆ logˆp N (x 1:T )] as an objective function over p. Becausê p N (x 1:T ) is an unbiased estimator of p(x 1:T ), proven in the Appendix and in [23,24,25,26], it defines an MCO, which we call FIVO: Definition 2. Filtering Variational Objectives. Let logˆplogˆ logˆp N (x 1:T ) be the output of Alg. 1 with inputsis a strongly consistent estimator [23,24]Resampling is the distinguishing feature of L N ; if resampling is removed, then FIVO reduces to IWAE. Resampling does add an amount of immediate variance, but it allows the filter to discard low weight particles with high probability. This has the logˆplogˆ logˆp 1 logˆplogˆ logˆp 2 logˆplogˆ logˆp 3 logˆplogˆ logˆp 1 logˆplogˆ logˆp 2 logˆplogˆ logˆp 3 logˆplogˆ logˆp 4 ∇ logˆplogˆ logˆp 4 logˆplogˆ logˆp 4 ∇ log w i 1:3 ) gradients Figure 1: Visualizing FIVO; (Left) Resample from particle trajectories to determine inheritance in next step, (middle) propose with q t and accumulate loss logˆplogˆ logˆp t , (right) gradients (in the reparameterized case) flow through the lattice, objective gradients in solid red and resampling gradients in dotted blue.effect of refocusing the distribution of particles to regions of higher mass under the posterior, and in some sequential models can reduce the variance from exponential to linear in the number of time steps [14,15]. Resampling is a greedy process, and it is possible that a particle discarded at step t, could have attained a high mass at step T . In practice, the best trade-off is to use adaptive resampling schemes [22]. If for a given x 1:T , p, q a particle filter's likelihood estimator improves over simple importance sampling in terms of variance, we expect L The FIVO bound can be optimized with the same stochastic gradient ascent framework used for the ELBO. We found in practice it was effective simply to follow a Monte Carlo estimator of the biased gradient E[∇ (θ,φ) logˆplogˆ logˆp N (x 1:T )] with reparameterized z i t . This gradient estimator is biased, as the full FIVO gradient has three kinds of terms: it has the term E[∇ θ,φ logˆplogˆ logˆp N (x 1:T )], where ∇ θ,φ logˆplogˆ logˆp N (x 1:T ) is defined conditional on the random variables of Alg. 1; it has gradient terms for every distribution of Alg. 1 that depends on the parameters; and, if adaptive resampling is used, then it has additional terms that account for the change in FIVO with respect to the decision to resample. In this section, we derive the FIVO gradient when z i t are reparameterized and a fixed resampling schedule is followed. We derive the full gradient in the Appendix.In more detail, we assume that p and q are parameterized in a differentiable way by θ and φ. Assume that q is from a reparameterizable family and that z i t of Alg. 1 are reparameterized. Assume that we use a fixed resampling schedule, and let I(resampling at step t) be an indicator function indicating whether a resampling occured at step t. Now, L FIVO N depends on the parameters via logˆplogˆ logˆp N (x 1:T ) and the resampling probabilities wGiven a single forward pass of Alg. 1 with reparameterized z i t , the terms inside the expectation form a Monte Carlo estimator of Eq. (8). However, the terms from resampling events contribute to the majority of the variance of the estimator. Thus, the gradient estimator that we found most effective in practice consists only of the gradient ∇ (θ,φ) logˆplogˆ logˆp N (x 1:T ), the solid red arrows of Figure 1. We explore this experimentally in Section 6.3.As with the ELBO, FIVO is a variational objective taking a variational posterior q as an argument. An important question is whether FIVO achieves the marginal log-likelihood at its optimal q. We can only guarantee this for models in which z 1:t−1 and x t are independent given x 1:t−1 . Proposition 2. Sharpness of Filtering Variational Objectives. Let L FIVO N (x 1:T , p, q) be a FIVO, and. If p has independence structure such that p(z 1:t−1 |x 1:t ) = p(z 1:t−1 |x 1:t−1 ) for t ∈ {2, . . . , T }, thenProof. See Appendix.Most models do not satisfy this assumption, and deriving the optimal q in general is complicated by the resampling dynamics. For the restricted the model class in Proposition 2, the optimal q t does not condition on future observations x t+1:T . We explored this experimentally with richer models in Section 6.4, and found that allowing q t to condition on x t+1:T does not reliably improve FIVO. This is consistent with the view of resampling as a greedy process that responds to each intermediate distribution as if it were the final. Still, we found that the impact of this effect was outweighed by the advantage of optimizing a tighter bound.The marginal log-likelihood is a central quantity in statistics and probability, and there has long been an interest in bounding it [27]. The literature relating to the bounds we call Monte Carlo objectives has typically focused on the problem of estimating the marginal likelihood itself. [28,29] use Jensen's inequality in a forward and reverse estimator to detect the failure of inference methods. IWAE [12] is a clear influence on this work, and FIVO can be seen as an extension of this bound. The ELBO enjoys a long history [8] and there have been efforts to improve the ELBO itself. [30] generalize the ELBO by considering arbitrary operators of the model and variational posterior. More closely related to this work is a body of work improving the ELBO by increasing the expressiveness of the variational posterior. For example, [31,32] augment the variational posterior with deterministic transformations with fixed Jacobians, and [33] extend the variational posterior to admit a Markov chain.Other approaches to learning in neural latent variable models include [34], who use importance sampling to approximate gradients under the posterior, and [35], who use sequential Monte Carlo to approximate gradients under the posterior. These are distinct from our contribution in the sense that for them inference for the sake of estimation is the ultimate goal. To our knowledge the idea of treating the output of inference as an objective in and of itself, while not completely novel, has not been fully appreciated in the literature. Although, this idea shares inspiration with methods that optimize the convergence of Markov chains [36].We note that the idea to optimize the log estimator of a particle filter was independently and concurrently considered in [37,38]. In [37] the bound we call FIVO is cast as a tractable lower bound on the ELBO defined by the particle filter's non-parameteric approximation to the posterior. [38] additionally derive an expression for FIVO's bias as the KL between the filter's distribution and a certain target process. Our work is distinguished by our study of the convergence of MCOs in N , which includes FIVO, our investigation of FIVO sharpness, and our experimental results on stochastic RNNs.In our experiments, we sought to: (a) compare models trained with ELBO, IWAE, and FIVO bounds in terms of final test log-likelihoods, (b) explore the effect of the resampling gradient terms on FIVO, (c) investigate how the lack of sharpness affects FIVO, and (d) consider how models trained with FIVO use the stochastic state. To explore these questions, we trained variational recurrent neural networks (VRNN) [39] with the ELBO, IWAE, and FIVO bounds using TensorFlow [40] on two benchmark sequential modeling tasks: natural speech waveforms and polyphonic music. These datasets are known to be difficult to model without stochastic latent states [41].The VRNN is a sequential latent variable model that combines a deterministic recurrent neu- ral network (RNN) with stochastic latent states z t at each step. The observation distri- bution over x t is conditioned directly on z t and indirectly on z 1:t−1 via the RNN's state h t (z t−1 , x t−1 , h t−1 ). For a length T sequence, the model's posterior factors into the condition- als T t=1 p t (z t |h t (z t−1 , x t−1 , h t−1 ))g t (x t |z t , h t (z t−1 , x t−1 , h t−1 )), and the variational posterior factors as T t=1 q t (z t |h t (z t−1 , x t−1 , h t−1 ), x t ). All distributions over latent variables are factorized Gaussians, and the output distributions g t depend on the dataset. The RNN is a single-layer LSTM and the conditionals are parameterized by fully connected neural networks with one hidden layer of the same size as the LSTM hidden layer. We used the residual parameterization [41] for the variational posterior.  For FIVO we resampled when the ESS of the particles dropped below N/2. For FIVO and IWAE we used a batch size of 4, and for the ELBO, we used batch sizes of 4N to match computational budgets (resampling is O(N ) with the alias method). For all models we report bounds using the variational posterior trained jointly with the model. For models trained with FIVO we report L We evaluated VRNNs trained with the ELBO, IWAE, and FIVO bounds on 4 polyphonic music datasets: the Nottingham folk tunes, the JSB chorales, the MuseData library of classical piano and orchestral music, and the Piano-midi.de MIDI archive [42]. Each dataset is split into standard train, valid, and test sets and is represented as a sequence of 88-dimensional binary vectors denoting the notes active at the current timestep. We mean-centered the input data and modeled the output as a set of 88 factorized Bernoulli variables. We used 64 units for the RNN hidden state and latent state size for all polyphonic music models except for JSB chorales models, which used 32 units. We report bounds on average log-likelihood per timestep in Table 1. Models trained with the FIVO bound significantly outperformed models trained with either the ELBO or the IWAE bounds on all four datasets. In some cases, the improvements exceeded 1 nat per timestep, and in all cases optimizing FIVO with N = 4 outperformed optimizing IWAE or ELBO for N = {4, 8, 16}.The TIMIT dataset is a standard benchmark for sequential models that contains 6300 utterances with an average duration of 3.1 seconds spoken by 630 different speakers. The 6300 utterances are divided into a training set of size 4620 and a test set of size 1680. We further divided the training set into a validation set of size 231 and a training set of size 4389, with the splits exactly as in [41]. Each TIMIT utterance is represented as a sequence of real-valued amplitudes which we split into a sequence of 200-dimensional frames, as in [39,41]. Data preprocessing was limited to mean centering and variance normalization as in [41]. For TIMIT, the output distribution was a factorized Gaussian, and we report the average log-likelihood bound per sequence relative to models trained with ELBO. Again, models trained with FIVO significantly outperformed models trained with IWAE or ELBO, see Table 1.All models in this work (except those in this section) were trained with gradients that did not include the term in Eq. (8) that comes from resampling steps. We omitted this term because it has an outsized effect on gradient variance, often increasing it by 6 orders of magnitude. To explore the effects of this term experimentally, we trained VRNNs with and without the resampling gradient term on the TIMIT and polyphonic music datasets. When using the resampling term, we attempted to control its variance   using a moving-average baseline linear in the number of timesteps. For all datasets, models trained without the resampling gradient term outperformed models trained with the term by a large margin on both the training set and held-out data. Many runs with resampling gradients failed to improve beyond random initialization. A representative pair of train log-likelihood curves is shown in Figure  2 -gradients without the resampling term led to earlier convergence and a better solution. We stress that this is an empirical result -in principle biased gradients can lead to divergent behaviour. We leave exploring strategies to reduce the variance of the unbiased estimator to future work.FIVO does not achieve the marginal log-likelihood at its optimal variational posterior q * , because the optimal q * does not condition on future observations (see Section 4.2). In contrast, ELBO and IWAE are sharp, and their q * s depend on future observations. To investigate the effects of this, we defined a smoothing variant of the VRNN in which q takes as additional input the hidden state of a deterministic RNN run backwards over the observations, allowing q to condition on future observations. We trained smoothing VRNNs using ELBO, IWAE, and FIVO, and report evaluation on the training set (to isolate the effect on optimization performance) in Table 2 . Smoothing helped models trained with IWAE, but not enough to outperform models trained with FIVO. As expected, smoothing did not reliably improve models trained with FIVO. Test set performance was similar, see the Appendix for details.A known pathology when training stochastic latent variable models with the ELBO is that stochastic states can go unused. Empirically, this is associated with the collapse of variational posterior q(z|x) network to the model prior p(z) [43]. To investigate this, we plot the KL divergence from q(z 1:T |x 1:T ) to p(z 1:T ) averaged over the dataset (Figure 2). Indeed, the KL of models trained with ELBO collapsed during training, whereas the KL of models trained with FIVO remained high, even while achieving a higher log-likelihood bound.We introduced the family of filtering variational objectives, a class of lower bounds on the log marginal likelihood that extend the evidence lower bound. FIVOs are suited for MLE in neural latent variable models. We trained models with the ELBO, IWAE, and FIVO bounds and found that the models trained with FIVO significantly outperformed other models across four polyphonic music modeling tasks and a speech waveform modeling task. Future work will include exploring control variates for the resampling gradients, FIVOs defined by more sophisticated filtering algorithms, and new MCOs based on differentiable operators like leapfrog operators with deterministically annealed temperatures. In general, we hope that this paper inspires the machine learning community to take a fresh look at the literature of marginal likelihood estimators-seeing them as objectives instead of algorithms for inference.Other Examples of MCOs.There is an extensive literature on marginal likelihood estimators [44,45,46,47,48,49,50]. Each defines an MCO, and we consider two in more detail, annealed importance sampling [48] and multiple importance sampling [44,51]. Let x denote an observation of an X -valued random variable generated in a process with an unobserved Z-valued random variable z. Let p(x, z) be the joint density.Annealed Importance Sampling MCO. Annealed importance sampling (AIS) is a generalization of importance sampling [48]. We present an MCO derived from a special case of the AIS algorithm. Let q(z|x) be a variational posterior distribution and let β i be a sequence of real numbers for i ∈ {1, . . . , N + 1} such that 0 ≤ β i ≤ 1 and β 1 = 0 and β N +1 = 1. Let T i (z |z, x) be a Markov transition distribution whose stationary distribution is proportional to q(z|x)1−βi p(x, z) βi . Then for z 1 ∼ q(z|x) and z i ∼ T i (z |z i−1 , x) for i ∈ {2, . . . , N } we have the following unbiased estimator,Notice two things. First, there is no assumption that the states z i are at equilibrium, and second, we did not require a transition operator keeping p(x, z) as an invariant distribution. All together, we can define the AIS MCO,This is a sharp objective, if we take q as the true posterior, q(z|x) = p(z|x), and T i (z |z, x) = δ(z − z) to be the Dirac delta copy operator. The difficulty in applying this MCO is finding T i , which are scalable and easy to optimize. Generalizations of the AIS procedure have been proposed in [52]. The resulting Sequential Monte Carlo samplers procedures also provide an unbiased estimator of the marginal likelihood and are structurally identical to the particle algorithm presented in this paper.Multiple Importance Sampling MCO. Multiple importance sampling (MIS) [44] is another generalization of importance sampling. Let q i (z|x) be N possibly distinct variational posterior distributions and w i (x) ≥ 0 be such that N i=1 w i (x) = 1. There are a variety of distinct estimators that could be formed from the q i [51]. We present just one. Let z i ∼ q i (z|x), then we have the following unbiased estimatorNotice that the latent sample z i ∼ q i (z|x) is evaluated under all q i 's. One can view this as a Rao-Blackwellized estimator corresponding to the mixture distributionAgain, this objective is sharp, if we take any q i (z|x) = p(z|x) and w i (x) = 1. The difficulty in making this objective more useful is optimizing it in a way that distinguishes the q i and assigns the appropriate w i .Let E[ˆ p N (x)] = p(x) and define L N (x, p) = E[logˆplogˆ logˆp N (x)] as the Monte Carlo objective defined byˆp byˆ byˆp N (x).(a) By the concavity of log and Jensen's inequality,a.s.−→ p(x) as N → ∞.• logˆplogˆ logˆp N (x) is uniformly integrable. That is, let (Ω, F, µ) be the probability space on which logˆplogˆ logˆp N (x) is defined. The random variables {logˆp{logˆ {logˆp N (x)} ∞ N =1 are uniformly integrable if E[| logˆplogˆ logˆp N (x)|] &lt; ∞ and if for any &gt; 0, there exists δ &gt; 0, such that for all N and E ∈ F, µ(E) &lt; δ implies E[| logˆplogˆ logˆp N (x)|I(E)] &lt; where I(E) is an indicator function of the set E. Then by continuity of log, logˆplogˆ logˆp N (x) converges almost surely to log p(x). By Vitali's convergence theorem (using the uniform integrability assumption), we get6 ], and assume lim supThen the bias log p(x) − L N (x, p) = −E[log(1 + ∆)]. Now, Taylor expand log(1 + ∆) about 0,Our aim is to show ∆In particular, by Cauchy-and again by Cauchy-SchwarzThis concludes the proof.Controlling the first inverse moment.We provide a sufficient condition that guarantees that the inverse moment of the average of i.i.d. random variables is bounded, a condition used in Proposition 1 (c). Intuitively, this is a fairly weak condition, because it only requires that the mass in an arbitrarily small neighbourhood of zero is bounded. Proof. Let M, C, &gt; 0 be such that P(w i &lt; w) ≤ Cw 1+ for w ∈ [0, M ). We proceed in two cases., so the same condition is sufficient for any N . The AM-GM inequality tells us thatand by Lyapunov's inequality, we haveThis concludes the proof.Unbiasedness ofˆpofˆ ofˆp N (x 1:T ) from the particle filter.We sketch an argument that the random variablê p N (x 1:T ) defined by Algorithm 1 is an unbiased estimator of the marginal likelihood p(x 1:T ). This is a well-known fact [23,52,25,26], and our sketch is based on [25]. The strategy is to cast the particle filter's estimatorˆpestimatorˆ estimatorˆp N (x 1:T ) as a single importance weight over an extended space. The lack of bias in the particle filter therefore reduces to the unbiasedness of importance sampling. Key to this is identifying the target and proposal distributions in the extended space. The target distribution is called "conditional sequential Monte Carlo", Algorithm 2. The proposal distribution is the particle filter itself, Algorithm 1.We argue that it is enough to consider just an arbitrary fixed (non-adaptive) resampling schedule that always resamples at step T . First, consider adaptive resampling criteria, i.e. criteria that are deterministic functions of the weights w i t . For such criteria the joint density of random variables in Algorithm 1 will be piecewise continuous, composed of 2 T regions corresponding to a sequence of resample/no-resample decisions. This density has a form on each piece that is exactly the same as the density for some fixed resampling schedule. Moreover, it is globally normalized, because of the sequential structure of the filter. Because Algorithm 2 makes the same decisions, it also is partitioned along the same sets and each piece has the same fixed resampling schedule. Thus, it is Algorithm 2 Conditional SMC 1: CSMC(x 1:T , p, q, N ): 2: y 1:T ∼ p(z 1:T |x 1:T ) 3: j = 1 4: {w 10:if resampling criteria satisfied by {w t } i=1 then5: for t ∈ {1, . . . , T } do 13:j ∼ Uniform{1, . . . , N } 1:t = y 1:t j 7:for i = j do enough to consider only a fixed resampling schedule. Second, notice that in the final step, step T , of Algorithms 1 and 2 resampling has no effect onˆponˆ onˆp N (x 1:T ). Thus, we assume that the resampling criteria of Algorithms 1 and 2 at step T is set to always resample. All together it is safe to assume a fixed resampling schedule with R resampling events, 1 ≤ R ≤ T , at steps k r ∈ {1, . . . , T } for r ∈ {0, . . . , R} with k R = T and k 0 = 0.Now we derive the joint density of Algorithm 1 and 2 taken at each iteration after possibly resampling.To avoid notational clutter we let g, f (omitting their arguments) represent the densities of the variables in Algorithms 1 and 2. Technically, we should also be keeping track of the indices that indicate the inheritance of the resampling step. So, let the random variables {{wbe the particles before resampling and s(i) ∈ {1, . . . , N } be the index that is selected for inheritance of the ith particle after resampling. Then the density corresponding to Algorithm 1 is and pushing in the marginal likelihoodone can show that for this sequence of resampling times the weight w j kr telescopes intoand the estimatorˆpestimatorˆ estimatorˆp N (x 1:T ) = T t=1ˆpt=1ˆ t=1ˆp t telescopes intoand thusThe result follows. An intuitive way to understand this result is the following: Algorithm 2 matches the distribution of every random variable in the particle filter except it interleaves a true posterior sample into the set of particles with uniform probability. The only mismatch in the densities are the normalization terms of the resampling probabilities of that privileged posterior sample, with terms N i i=1 kr k=kr−1+1 α k (z 1:k ) coming from the filter's resampling and terms N from conditional SMC's resampling. Of course, we never run Algorithm 2, it just serves to define the target density.We formulate unbiased gradients of L FIVO N (x 1:T , p, q) by considering Algorithm 1 as a method for simulating FIVO. We consider the cases when the sampling of z i t is and is not reparameterized. We also consider the case where we make adaptive resampling decisions.First, we assume that the decision to resample is not adaptive (i.e., depends in some way on the random variables already produced until that point in Algorithm 1), and are fixed ahead of time. When the sampling z i t is not reparameterized there are three terms to the gradient: (1) the gradients of logˆplogˆ logˆp N (x 1:T ) with respect to the parameters conditional on the latent states, (2) gradients of the densities q t with respect to their parameters, and (3) gradients of the resampling probabilities with respect to the parameters. All together, the following is a gradient of FIVO,where I(A) is an indicator function. If z i t is reparameterized, then the first and third terms suffice for an unbiased gradient,In this work we only considered reparameterized q t s, and we dropped the terms of the gradient that arise from resampling.Second, when the decision to resample is adaptive, the domain of the random variables involved in simulating logˆplogˆ logˆp N (x 1:T ) can be partitioned into 2 T regions, over each of which the density is differentiable. Between those regions, the density experiences a jump discontinuity. Thus, there are additional terms to the gradient of L FIVO N (x 1:T , p, q) that correspond to the change in the regions of continuity as the parameters change. These terms can be written as surface integrals over the boundaries of the regions. We drop these terms in practice.Assume p(z 1:t−1 |x 1:t ) = p(z 1:t−1 |x 1:t−1 ) for all t ∈ {2, . . . , T }. We will show L FIVO N (x 1:T , p, q) = log p(x 1:T ) at q(z t |z 1:t−1 , x 1:t ) = p(z t |z 1:t−1 , x 1:t ). We will do this by induction, showing that every particle has a constant weight and thatˆpthatˆ thatˆp N (x 1:T ) = p(x 1:T ) is a constant. For t = 1 we haveThus, all particles have the same weight andˆpandˆ andˆp 1 = p 1 (x 1 ). Now for any t we have that the weights must be 1/N since the particles all have the same weight and α i t (z 1:t ) = p t (x t , z t |z 1:t−1 , x 1:t−1 ) p(z t |z 1:t−1 , x 1:t )= p(z 1:t , x 1:t ) p(z 1:t−1 , x 1:t−1 )p(z t |z 1:t−1 , x 1:t )= p(x 1:t ) p(x 1:t−1 ) p(z 1:t |x 1:t ) p(z 1:t−1 |x 1:t−1 )p(z t |z 1:t−1 , x 1:t )and thus,t=2We initialized weights using the Xavier initialization [53] and used the Adam optimizer [54] with a batch size of 4. During training, we did not truncate sequences and performed full backpropagation through time for all datasets. For the results presented in Sections 6.1 and 6.2 we performed a grid search over learning rates {3 × 10 −4 , 1 × 10 −4 , 3 × 10 −5 , 1 × 10 −5 } and picked the run and early stopping step by the validation performance.Comparing models trained with different log-likelihood lower bounds is challenging because calculat- ing the actual log-likelihood is intractable. Burda et al. [12] showed that the IWAE bound is at least as tight as the ELBO and monotonically increases with N . This suggests comparing models based on the IWAE bound evaluated with a large N . However, we found that IWAE and ELBO bounds tended to diverge for models trained with FIVO.Although FIVO is not provably a tighter bound than the ELBO or IWAE, our experiments suggest that this tends to be the case in practice. In Figure 3, we plotted all three bounds over training for a representative experiment. All plots use the same model architecture, but the training objective changes in each panel. For the model trained with IWAE, the FIVO and IWAE bounds are tighter than their counterparts on the model trained with ELBO, suggesting that the model trained with IWAE is superior. The ELBO bound evaluated on the model trained with IWAE, however, is lower than its counterpart on the model trained with the ELBO. For the model trained with FIVO, both IWAE and ELBO bounds seem to diverge, but the FIVO bound outperforms the FIVO bounds on both of the other models. As in the figure, we generally found that the same model evaluated with FIVO, IWAE, and ELBO produced values descending in that order.We suspect that q distributions trained under the FIVO bound are more entropic than those trained under ELBO or IWAE because of the resampling operation. During training under FIVO, q is able to propose state transitions that could poorly explain the observations because the bad states will be resampled away without harming the final bound value. Then, when a FIVO-trained q is evaluated with ELBO or IWAE it proposes poor states that are not resampled away, leading to a poor final bound value. Conversely, qs trained with ELBO and IWAE are not able to fully leverage the resampling operation when evaluated with the FIVO bound.Because of this behavior, we chose to optimistically evaluate models trained with IWAE and ELBO by reporting the maximum across all the bounds. For models trained with FIVO, we reported only the FIVO bound. We felt this evaluation scheme provided the strongest comparison to existing bounds. We reported log-likelihood scores for TIMIT relative to an ELBO baseline instead of raw log- likelihoods. Previous papers (e.g., [39,41]) report the log-likelihood of data that have been mean centered and variance normalized, but it would be more proper to report the results on the un- standardized data. Specifically, if the training set has mean µ and variance σ 2 and the model outputsˆµoutputsˆ outputsˆµ andˆσandˆ andˆσ 2 , then the un-standardized test data would be evaluated under a N (ˆ µσ + µ, ˆ σ 2 σ 2 ) distribution.Log-likelihoods produced by these approaches differ by a constant offset that depends on σ. Because the offset is a function of only training set statistics, it does not affect relative comparison between methods. Because of this we chose to report log-likelihoods relative to a baseline instead of absolute numbers. Absolute numbers calculated on standardized data are reported in Tables 3, 4    128 for FIVO models. All models were trained with N = 4 and a learning rate of 3 × 10 −5 . The JSB Chorales model used 32 units and the Musedata model used 256 units. All other models used 64 units. TIMIT results were calculated on data that was standardized (mean-centered and scaled to unit variance) using training set statistics.
