The ability to generate sentences is a core com- ponent of many NLP systems, including ma- chine translation (Kalchbrenner and Blunsom, 2013;Koehn et al., 2007), summarization (Hahn and Mani, 2000;Nallapati et al., 2016), speech recognition (Ju- rafsky and Martin, 2000), and dialogue ( Ritter et al., 2011). State-of-the-art models are largely based on recurrent neural language models (NLMs) that gen- erate sentences from scratch, often in a left-to-right manner. It is often observed that such NLMs suf- fer from the problem of favoring generic utterances such as "I don't know" ( . At the same time, naive strategies to increase diversity have been shown to compromise grammaticality ( Shao et al., 2017), suggesting that current NLMs may lack the inductive bias to faithfully represent the full diver- sity of complex utterances.Indeed, it is difficult even for humans to write complex text from scratch in a single pass; we of- ten create an initial draft and incrementally revise it (Hayes and Flower, 1986). Inspired by this process, we propose a new generative model of text which we call the prototype-then-edit model, illustrated in Fig- ure 1. It first samples a random prototype sentence from the training corpus, and then invokes a neural editor, which draws a random "edit vector" and gen- erates a new sentence by attending to the prototype while conditioning on the edit vector. The motiva- tion is that sentences from the corpus provide a high quality starting point: they are grammatical, natu- rally diverse, and exhibit no bias towards shortness or vagueness. The attention mechanism ( Bahdanau et al., 2015) then extracts the rich information from the prototype and generalizes to novel sentences.We train the neural editor by maximizing an approximation to the generative model's log- likelihood. This objective is a sum over lexically- similar sentence pairs in the training set, which we can scalably approximate using locality sensitive hashing. We also show empirically that most lexi- cally similar sentences are also semantically similar, thereby endowing the neural editor with additional semantic structure. For example, we can use the neural editor to perform a random walk from a seed sentence to traverse semantic space. We compare our prototype-then-edit model against approaches which generate from scratch on two fronts: language generation quality and seman- tic properties. For the former, our model generates higher quality generations according to human eval- uations, and improves perplexity by 13 points on the Yelp corpus and 7 points on the One Billion Word Benchmark. For the latter, we show that latent edit vectors outperform standard sentence variational au- toencoders ( Bowman et al., 2016) on semantic sim- ilarity, locally-controlled text generation, and a sen- tence analogy task.where both prototype x and edit vector z are latent variables.Our formulation stems from the observation that many sentences in a large corpus can be represented as minor transformations of other sentences. For ex- ample, in the Yelp restaurant review corpus (Yelp, 2017) we find that 70% of the test set is within word- token Jaccard distance 0.5 of a training set sentence, even though almost no sentences are repeated ver- batim. This implies that a neural editor which mod- els lexically similar sentences should be an effective generative model for large parts of the test set.A secondary goal for the neural editor is to cap- ture certain semantic properties; we focus on the fol- lowing two in particular:1. Semantic smoothness: an edit should be able to alter the semantics of a sentence by a small and well-controlled amount, while multiple edits should make it possible to accumulate a larger change.Our primary goal is to learn a generative model of sentences. In particular, we model sentence genera- tion as a prototype-then-edit process: 1 2. Consistent edit behavior: the edit vector z should model/control the variation in the type of edit that is performed. When we apply the same edit vector on different sentences, the neural editor should perform semantically analogous edits across the sentences.1. Prototype selector: Given a training corpus of sentences X , we randomly sample a prototype sentence, x ∼ p(x ) (in our case, uniform over X ).In Section 4, we show that the neural editor can successfully capture both properties, as reported by human evaluations.2. Neural editor: First we draw an edit vector z, from an edit distribution p(z), which encodes the type of edit. Then, we draw the final sen- tence x from p edit (x | x , z), which takes the prototype x and the edit vector z.Under this model, the likelihood of a sentence is:x ∈XWe would like to train our neural editor by maxi- mizing the marginal likelihood (Equation 1), but the exact likelihood is difficult to maximize because it involves a sum over all latent prototypes x (expen- sive) and integration over the latent edit vector z (in- tractable). We therefore propose two approximations to over- come these challenges:z 1 For many applications such as machine translation or dia- logue generation, there is a context (e.g. foreign sentence, di- alogue history), which can be supplied to both the prototype selector and the neural editor. This paper focuses on the uncon- ditional case.1. We approximate the sum over all prototypes x by only summing over x that are lexically sim- ilar to x.2. We lower bound the integral over latent edit vectors by modeling z with a variational au- toencoder, which admits tractable inference via the evidence lower bound (ELBO), which in- cidentally also provides additional semantic structure.We describe and motivate both of these approxima- tions in the subsections below.Equation 1 defines the probability of generating a sentence x as the total probability of reaching x via edits from every prototype x ∈ X . However, most prototypes are unrelated and should have very small probability of transforming into x. Therefore, we approximate the marginal distribution over pro- totypes by only considering the prototypes x with high lexical overlap with x, as measured by word token Jaccard distance d J . Formally, define a lexi- cal similarity neighborhood as N (x) = {x ∈ X :The neighborhoods N (x) can be constructed efficiently with locality sensitive hash- ing (LSH) and minhashing. The full procedure is described in Appendix 6.1. Then the log-likelihood of the prototype- then-edit process is lower bounded by: sentences from a corpus grounded in real world events, most lexically similar sentences are also se- mantically similar. For example, given "my son en- joyed the delicious pizza", we are far more likely to see "my son enjoyed the delicious macaroni", versus "my son hated the delicious pizza".Human evaluations of 250 edit pairs sampled from lexical similarity neighborhoods on the Yelp corpus support this conclusion. 35.2% of the sen- tence pairs were judged to be exact paraphrases, while 84% of the pairs were judged to be at least roughly equivalent. Sentence pairs were negated or change in topic only 7.2% of the time. Thus, a neu- ral editor trained on this distribution should prefer- entially generate semantically similar edits.Now let us tackle integration over the latent edit vec- tors. To do this, let us introduce the evidence lower bound (ELBO) to the integral over z in Equation 1:The important elements of x ) are the neural editor p edit (x | x , z), the edit prior p(z), and the approximate edit posterior q(z | x, x ) (we describe each of these shortly). Combining the ELBO with Equation 3, our final objective function is:where the inequalities follow from summing over fewer terms, multiplying by a number that is at most 1, and Jensen's inequality. Treating |N (x)| −1 as a constant 2 and summing over the training set, we arrive at the following objective:Interlude: lexical similarity semantics. We have motivated lexical similarity neighborhoods via com- putational considerations, but we find that lexical similarity training also captures semantic similar- ity. One can certainly construct sentences with small lexical distance that differ semantically (e.g., inser- tion of the word "not"). However, since we mine L ELBO is now maximized over the parameters of both p edit and q. Note that q is only used for train- ing, and discarded at test time. With the introduction of q, we are now training p(x | x ) as a conditional variational autoencoder (C-VAE) (it is conditioned on x ). We maximize the objective via SGD, approx- imating the gradient using the usual Monte Carlo es- timate for VAEs ( Kingma and Welling, 2014).on an edit vector z by concatenating z to the input of the decoder at each time step. Further details are given in Appendix 6.2. cated norm. Then,∝ exp(κz Edit prior p(z): We sample z from the prior by drawing a random magnitude z norm ∼ Unif(0, 10) and then drawing a random direction z dir ∼ vMF (0), where vMF(0) is a uniform distribution over the unit sphere (von-Mises Fisher distribution with concen- tration = 0). The resulting z = z norm z dir .Approximate edit posterior q(z | x, x ): q is named the approximate edit posterior because the ELBO is tight when q matches the true posterior: the best possible estimate of the edit z given both the prototype x and the revision x. Our design of q treats the edit vector z as a generalization of word vectors. In the case of a single word insertion, a good edit vector would be the word vector of the in- serted word. Extending this to multi-word edits, we would like multi-word insertions to be represented as the sum of the individual word vectors.Since q is an encoder observing both the protoype x and the revised sentence x, it can directly observe the word differences between x and x . Define I = x\x to be the set of words added to the prototype, and D = x \x to be the words deleted.We would then like q to output a z equal to where the resulting z = z dir z norm . The result- ing distribution q has three parameters: (Φ, κ, where κ, are hyperparameters. This distribution is straightforward to use as part of a variational autoen- coder, as sampling can be easily performed using the reparameterization trick and the rejection sam- pler of Wood (1994). Furthermore, the KL term has a closed form expression independent of z.w∈I w∈D where Φ(w) is the word vector for word w and ⊕ denotes concatenation. The word embeddings Φ are parameters of q. However, q cannot deterministically output f (x, x ) -without any entropy in q, the KL term in equation 5 would be infinity and training would be infeasible. Hence, we design q to output a noisy, perturbed version of f : we perturb the norm of f by adding uniform noise, and we perturb the direction of f by adding von-Mises Fisher noise. The von- Mises Fisher distribution is a distribution over vec- tors with unit norm, with a mean µ and a precision κ such that the log-likelihood of drawing a vector decays linearly with the cosine similarity to µ.Let f norm = and f dir = f /f norm . Further- more, define˜fdefine˜ define˜f norm = min(f norm , 10) to be the trun- where I n (κ) is the modified Bessel function of the first kind, and Γ is the gamma function.Our design of q differs substantially from the stan- dard choice of a standard normal distribution with a given mean ( Bowman et al., 2016;Kingma and Welling, 2014) for two reasons:First, by construction, edit vectors are sums of word vectors and since cosine distances are tradi- tionally used to measure distances between word vectors, it would be natural to encode distances be- tween edit vectors by the cosine distance. The von- Mises Fisher distribution captures this idea, as the log likelihood of transforming f dir into z dir decays linearly with the cosine similarity.Second, our parameterization avoids collapsing the latent code, which is a serious problem with variational autoencoders in practice ( Bowman et al., 2016). With a Gaussian latent noise variable, the KL-divergence term is instance-dependent, and thus the encoder must decide how much information to pass to the decoder. Even with training techniques such as annealing, the model often learns to ignore the encoder entirely. In our case, this does not oc- cur, since the KL divergence between a von-Mises Fisher with parameter κ and the uniform distribu- tion is independent of the mean f dir , allowing us to optimize κ separately using binary search. In prac- tice, we never observe issues with encoder collapse using standard gradient training.We divide our experimental results into two parts. In Section 4.3, we evaluate the merits of the prototype- then-edit model as a generative modeling strategy, measuring its improvements on language modeling (perplexity) and generation quality (human evalua- tions of diversity and plausibility). In Section 4.4, we focus on the semantics learned by the model and its latent edit vector space. We demonstrate that it possesses interpretable semantics, enabling us to smoothly control the magnitude of edits, incremen- tally optimize sentences for target properties, and perform analogy-style sentence transformations.5. SVAE: the sentence variational autoencoder of Bowman et al. (2016), sharing the same de- coder architecture as NEURALEDITOR. We compare the edit vector to differences of sen- tence vectors in the SVAE. 4 We evaluate perplexity on the Yelp review corpus (YELP, Yelp (2017)) and the One Billion Word Lan- guage Model Benchmark (BILLIONWORD, Chelba (2013)). For qualitative evaluations of generation quality and semantics, we focus on YELP as our pri- mary test case, as we found that human judgments of semantic similarity were much better calibrated in this focused setting. For both corpora, we used the named-entity rec- ognizer (NER) in spaCy 3 to replace named entities with their NER categories. We replaced tokens out- side the top 10,000 most frequent tokens with an "out-of-vocabulary" token.Throughout our experiments, we compare the fol- lowing generative models:1. NEURALEDITOR: the proposed approach.2. NLM: a standard left-to-right neural language model generating from scratch. For fair com- parison, we use the exact same architecture as the decoder of NEURALEDITOR.3. KN5: a standard 5-gram Kneser-Ney language model in KenLM (Heafield et al., 2013).Perplexity. We start by evaluating NEURALEDI- TOR's value as a language model, measured in terms of perplexity. For the NEURALEDITOR, we use the likelihood lower bound in Equation 3 where we sum over training set instances within Jaccard distance &lt; 0.5, and for the VAE term in NEURALEDITOR, we use the one-sample approximation to the lower bound used in Kingma (2014) and Bowman (2016). Compared to NLM, our initial result is that NEU- RALEDITOR is able to drastically improve log- likelihood for a significant number of sentences in the test set ( Figure 2) when considering test sen- tences with at least one similar sentence in the train- ing set. However, it places lower log-likelihood and on sentences which are far from any prototypes, as it was not trained to make extremely large edits. Prox- imity to a prototype seems to be the chief deter- miner of NEURALEDITOR's performance. To eval- uate NEURALEDITOR's perplexity, we use smooth- ing with NLM to account for rare sentences not within our Jaccard distance threshold. 5 We find NEURALEDITOR improves perplexity over NLM and KN5. Table 1 shows that this is the case for both YELP and the more general BILLIONWORD, which contains substantially fewer test-set sentences close to the training set. On YELP, we surpass even the best ensemble of NLM and KN5, while on BIL- LIONWORD we nearly match their performance.Since NEURALEDITOR draws its strength from sentences in the training set, we also compared against a simpler alternative, in which we ensem- ble the NLM and MEMORIZATION (retrieval with- out edits). NEURALEDITOR performs dramatically better than this alternative.   Prototype x Revision x i had the fried whitefish taco which was decent, but i've had much better.i had the &lt;unk&gt; and the fried carnitas tacos, it was pretty tasty, but i've had better. "hash browns" are unsea- soned, frozen potato shreds burnt to a crisp on the outside and mushy on the inside. the hash browns were crispy on the outside, but still the taste was missing.i'm currently giving &lt;car- dinal&gt; stars for the service alone. Human evaluation. We now turn to human eval- uation of generation quality, focusing on grammat- icality and plausibility. 6 We evaluate generations from NEURALEDITOR against a NLM with a tem- perature parameter on the per-token softmax 7 , which is a popular technique for suppressing incoherent and ungrammatical sentences. Many NLM systems have noted a undesireable tradeoff between gram- maticality and diversity, where a temperature low enough to enforce grammaticality results in short and generic utterances ( . Figure 3 illustrates that both the grammaticality and plausibility of NEURALEDITOR with a temper- ature of 1.0 is on par with the best tuned temperature for NLM, with a far higher diversity, as measured by unigram entropy. We also find that decreasing the temperature of the NEURALEDITOR can be used to slightly improve the grammaticality, without sub- stantially reducing the diversity of the generations.A key advantage of edit-based models thus emerges: Prototypes sampled from the training set organically inject diversity into the generation pro- cess, even if the temperature of the decoder in the NEURALEDITOR is zero. Hence, we can keep the decoder at a very low temperature to maximize grammaticality and plausibility, without sacrificing sample diversity. In contrast, a zero temperature NLM would collapse to outputting one generic sen- tence. 6 Human raters were asked, "How plausible is it for this sen- tence to appear in the corpus?" on a scale of 1-3. 7 If si is the softmax logit for token wi and τ is a temperature parameter, the temperature-adjusted distribution is p(wi) ∝ exp(si/τ ).This also suggests that the temperature parameter for the NEURALEDITOR captures a more natural no- tion of diversity -higher temperature encourages more aggressive extrapolation from the training set while lower temperatures favor more conservative mimicking. This is likely to be more useful than the tradeoff for generation-from-scratch, where the temperature also affects the quality of generations. In this section, we investigate learned semantics of the NEURALEDITOR, focusing on the two desider- ata discussed in Section 2: semantic smoothness, and consistent edit behavior.In order to establish a baseline for these proper- ties, we consider existing sentence generation tech- niques which can sample semantically similar sen- tences. We are not aware of other approaches which attempt to learn a vector space for edits, but there are many approaches which learn a vector space for sentences. Of particular relevance is the sentence variational autoencoder (SVAE) which also imposes semantic structure onto a latent vector space, but uses the latent vector to represent the entire sen- tence, rather than just an edit. To use the SVAE to "edit" a target sentence into a semantically similar sentence, we perturb its underlying latent sentence vector and then decode the result back into a sen- tence -the same method used in Bowman et al. (2016). The neural editor frequently generates para- phrases and similar sentences while avoiding unrelated and degenerate ones. In contrast, the SVAE frequently generates identical and unrelated sentences and rarely generates paraphrases.Semantic smoothness. A good editing system should have fine-grained control over the semantics of a sentence: i.e., each edit should only alter the se- mantics of a sentence by a small and well-controlled amount. We call this property semantic smoothness.To study smoothness, we first generate an "edit sequence" by randomly selecting a prototype sen- tence, and then repeatedly editing via the neural edi- tor (with edits drawn from the edit prior p(z)) to pro- duce a sequence of revisions. We then ask human annotators to rate the size of the semantic changes between revisions. An example is given in Table 3.For the SVAE baseline, we generate a similar se- quence of sentences by first encoding the prototype sentence, and then decoding after the addition of a random Gaussian with variance 0.4. 8 This process is repeated to produce a sequence of sentences which we can view as the SVAE equivalent of the edit se- quence. Figure 4 shows that the neural editor frequently generates paraphrases despite being trained on lex- ical similarity, and only 1% of edits are unrelated from the prototype. In contrast, the SVAE often re- peats sentences exactly, and when it makes an edit it is equally likely to generate unrelated sentences. This suggests that the neural editor produces sub- stantially smoother sentence sequences with a sur- prisingly high frequency of paraphrases.Qualitatively (Table 3), NEURALEDITOR seems to generate long, diverse sentences which smoothly change over time, while the SVAE biases towards short sentences with several semantic jumps, pre- sumably due to the difficulty of training a suffi- ciently informative SVAE encoder. RALEDITOR have the same average human similarity judge- ment between two successive sentences. This avoids situations where the SVAE produces completely unrelated sentence due to the perturbation size.10 545 similarity assessments pairs were collected through Amazon Mechanical Turk following Agirre (2014), with the same scale and prompt. Similarity judgements were converted to descriptions by defining Paraphrase (5), Roughly Equivalent (4-3), Same Topic (2-1), Unrelated (0). NEURALEDITOR SVAE this food was amazing one of the best i've tried, service was fast and great.this food was amazing one of the best i've tried, service was fast and great. this is the best food and the best service i've tried in &lt;gpe&gt;.this place is a great place to go if you want a quick bite. some of the best &lt;norp&gt; food i've had in &lt;date&gt; i've lived in &lt;gpe&gt;. the food was good, but the service was terrible. i have to say this is the best &lt;norp&gt; food i've had in &lt;gpe&gt;.this is the best &lt;norp&gt; food in &lt;gpe&gt;. best &lt;norp&gt; food i've had since moving to &lt;gpe&gt; &lt;date&gt;.this place is a great place to go if you want to eat. this was some of the best &lt;norp&gt; food i've had in the &lt;gpe&gt;.this is the best &lt;norp&gt; food in &lt;gpe&gt;. i've lived in &lt;gpe&gt; for &lt;date&gt; and every time we come in this is great the food was good, the service was great. i've lived in &lt;gpe&gt; for &lt;date&gt; and have enjoyed my haircut at &lt;gpe&gt; since &lt;date&gt;. the food was good, but the service was terrible. Table 3: Example random walks from NEURALEDITOR, where the top sentence is the prototype.Figure 5: Neural editors can shorten sentences (left), include common words (center, the word 'service') and rarer words (right 'pizza') while maintaining similarity.the coffee ice cream was one of the best i've ever tried. the coffee ice cream was one of the best i've ever tried. some of the best ice cream we've ever had! the &lt;unk&gt; was very good and the food was good. just had the best ice -cream i've ever had! the food was good, but not great. some of the best pizza i've ever tasted! the food was good, but not great. that was some of the best pizza i've had in the area. the food was good, but the service was n't bad. Table 4: Examples of word inclusion trajectories for 'pizza'. The NeuralEditor produces smooth chains that lead to word inclusion, but the SVAE gets stuck on generic sentences.Smoothly controlling sentences. We now show that we can selectively choose edits sampled from the neural editor to incrementally optimize a sen- tence towards desired attributes. This task serves as a useful measure of semantic coverage: if an edit model has high coverage of sentences that are se- mantically similar to a prototype, it should be able to satisfy the target attribute while deviating mini- mally from the prototype's original meaning.get attribute. We repeat this process for a large num- ber of prototypes.We use almost the same procedure for the SVAE, but instead of selecting by highest likelihood, we select the sequence whose endpoint has shortest la- tent vector distance from the prototype (as this is the SVAE's metric of semantic similarity).We focus on controlling two simple attributes: compressing a sentence to below a desired length (e.g. 7 words), and inserting a target keyword into the sentence (e.g. "service" or "pizza").Given a prototype sentence, we try to discover a semantically similar sentence satisfying the target attribute using the following procedure: First, we generate 1000 edit sequences using the procedure described earlier. Then, we select the sequence with highest likelihood whose endpoint possesses the tar- In Figure 5, we then aggregate the sentences from the collected edit sequences, and plot their seman- tic similarity to the prototype against their success in satisfying the target attribute. Not surprisingly, as target attribute satisfaction rises, semantic simi- larity drops. However, we also see that the neural editor sacrifices less semantic similarity to achieve the same level of attribute satisfaction as the SVAE. The SVAE is reasonable on tasks involving common words (such as the word service), but fails when the model is asked to generate rarer words such as pizza. Examples from these word inclusion problems show that the SVAE often becomes stuck generating short, generic sentences (Table 4).Consistent edit behavior: sentence analogies. In the previous results, we showed that edit models learn to generate semantically similar sentences. We now assess whether the edit vector possesses glob- ally consistent semantics. Specifically, applying the same edit vector to different sentences should result in semantically analogous edits.Formally, suppose we have two sentences, x 1 and x 2 , which are related by some underlying semantic relation r. Given a new sentence y 1 , we would like to find a y 2 such that the same relation r holds be- tween y 1 and y 2 .Our approach is to estimate the edit vector be- tween x 1 and x 2 asˆzasˆ asˆz = f (x 1 , x 2 ) -the mode of our edit posterior q. We then apply this edit vector to y 1 using the neural editor to yieldˆyyieldˆ yieldˆy 2 = argmax x p edit (x | y 1 , ˆ z). Since it is difficult to outputˆyoutputˆ outputˆy 2 exactly matching y 2 , we take the top k candidate outputs of p edit (us- ing beam search) and evaluate whether the gold y 2 appears among the top k elements.Evaluation for this task can be difficult, as two ar- bitrary sentences x 1 and x 2 are often not related by any well-defined relationship. However, prior work already provide well-established sets of word analo- gies ( Mikolov et al., 2013a;Mikolov et al., 2013b). We leverage these to generate a new dataset of sen- tence analogies, using a simple strategy: given an analogous word pair (w 1 , w 2 ), we mine the Yelp corpus for sentence pairs (x 1 , x 2 ) such that x 1 is transformed into x 2 by inserting w 1 and removing w 2 (allowing for reordering and inclusion/exclusion of stop words).For example, given the words (good, best), we mine the sentence pair x 1 ="this was a good restau- rant" and x 2 ="this was the best restaurant". Given a new sentence y 1 ="The cake was great", we ex- pect y 2 to be "The cake was the greatest".For this task, we initially compared against the SVAE, but it had a top-k accuracy close to zero. Hence, we instead compare to the baseline of ran- domly sampling an edit vectorˆzvectorˆ vectorˆz ∼ p(z), instead usingˆzusingˆ usingˆz derived from f (x 1 , x 2 ). We can also com- pare to numbers on the original word analogy task, restricted to the relationships we find in the Yelp cor- pus although these numbers are for the easier task of computing word-level (not sentence) analogies.Interestingly, the top-10 performance of our model in Table 5 is nearly as good as the perfor- mance of GloVe vectors on the simpler lexical anal- ogy task, despite the fact that the sentence predic- tion task is harder. In some categories the neural editor at top-10 actually performs better than word vectors, since the neural editor has an understanding of which words are likely to appear in the context of a Yelp review. Examples in Table 6 show the model is accurate and captures lexical analogies requiring word reorderings.Our work connects with a broad literature on neu- ral retrieval and attention-based generation meth- ods, semantically meaningful representations for sentences, and nonparametric statistics.Neural language models ( Bengio et al., 2003) based upon recurrent neural networks and sequence- to-sequence architectures (Sutskever et al., 2014) have been widely used due to their flexibility and performance across a wide-range of NLP tasks (Jurafsky and Martin, 2000;Kalchbrenner and Blunsom, 2013;Hahn and Mani, 2000;Ritter et al., 2011). Our work is motivated by an emerging con- sensus that attention-based mechanisms ( Bahdanau et al., 2015) can substantially improve performance on various sequence to sequence tasks by captur- ing more information from the input sequence ( Wu et al., 2016;Vaswani et al., 2017). Our work ex- tends the applicability of attention mechanisms be- yond sequence to sequence tasks by deriving a train- ing method for models which attend to randomly sampled sentences.There is a growing literature on applying re- trieval mechanisms to augment neural sequence-to- sequence models. For example, Song (2016) en- sembled a retrieval system and an NLM for dia- logue, using the NLM to transform the retrieved ut- terance, and Gu (2017) used an off-the-shelf search engine system to retrieve and condition on train- ing set examples. Both of these approaches rely on a deterministic retrieval mechanism which se- lects a prototype x using some input c. In contrast, our work treats the prototype x as a latent vari-  = my son actually looks forward to going to the dentist! Table 6: Examples of lexical analogies correctly answered by NEURALEDITOR. Sentence pairs generating the analogy relationship are shortened to only their lexical differences.able, and marginalizes over all possible prototypes -a challenge which motivates our new lexical sim- ilarity training method in Section 3.1. Practically, marginalization over x makes our model attend to training examples based on similarity of output se- quences, while retrieval models attend to examples based on similarity of the input sequences.In terms of generation techniques that capture semantics, the sentence variational autoencoder (SVAE) (Bowman et al., 2016) is closest to our work in that it attempts to impose semantic structure on a latent vector space. However, the SVAE's latent vec- tor is meant to represent the entire sentence, whereas the neural editor's latent vector represents an edit. Our results from Section 4.4 suggest that local vari- ation over edits is easier to model than global varia- tion over sentences.Our use of lexical similarity neighborhoods is comparable to context windows used in word vector training ( Mikolov et al., 2013a). Proximity of words within text and lexical similarity both serve as a fil- ter which reveals semantics through distributional statistics of the corpus. More generally, results in manifold learning demonstrate that a weak metric such as lexical similarity can be used to extract semantic similarity through distributional statistics ( Tenenbaum et al., 2000;Hashimoto et al., 2016).Prototype-then-edit is a semi-parametric ap- proach that remembers the entire training set and uses a neural editor to generalize meaningfully be- yond the training set. The training set provides a strong inductive bias -that the corpus can be char- acterized by prototypes surrounded by semantically similar sentences reachable by edits. Beyond im- provements on generation quality as measured by perplexity, the approach also reveals new semantic structures via the edit vector. A natural next step is to apply these ideas in the conditional setting for tasks such as dialogue generation.https://github.com/kelvinguu/neural-editor.https://worksheets.codalab.org/worksheets/ 0xa915ba2f8b664ddf8537c83bde80cc8c.The LSH maps each sentence to other lexically sim- ilar sentences in the corpus, representing a graph over sentences. To speed up corpus construction, we apply breadth-first search (BFS) over the LSH sen- tence graph started at randomly selected seed sen- tences. We store the edges encountered during the BFS, and uniformly sample from this set to form the training set. The BFS ensures that every query to the LSH index returns a valid edit pair, at the cost of adding bias to our training set.Encoder. The prototype encoder is a 3-layer biL- STM. The inputs to each layer are the concatena- tion of the forward and backward hidden states of the previous layer, with the exception of the first layer, which takes word vectors as input. Word vec- tors were initialized with GloVe ( Pennington et al., 2014).Decoder. The decoder is a 3-layer LSTM with at- tention. At each time step, the hidden state of the top layer is used to compute attention over the top- layer hidden states of the prototype encoder. The re- sulting attention context vector is then concatenated with the decoder's top-layer hidden state and used to compute a softmax distribution over output tokens.
