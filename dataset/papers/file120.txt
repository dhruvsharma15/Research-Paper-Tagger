Neural network models can require a lot of space on disk and in memory. They can also need a substantial amount of time for inference. This is especially im- portant for models that we put on devices like mobile phones. There are several approaches to solve these problems. Some of them are based on sparse compu- tations. They also include pruning or more advanced methods. In general, such approaches are able to provide a large reduction in the size of a trained net- work, when the model is stored on a disk. However, there are some problems when we use such models for inference. They are caused by high computation time of sparse computing. Another branch of methods uses different matrix- based approaches in neural networks. Thus, there are methods based on the usage of Toeplitz-like structured matrices in [1] or different matrix decomposi- tion techniques: low-rank decomposition [1], TT-decomposition (Tensor Train decomposition) [2,3]. Also [4] proposes a new type of RNN, called uRNN (Uni- tary Evolution Recurrent Neural Networks).In this paper, we analyze some of the aforementioned approaches. The mate- rial is organized as follows. In Section 2, we give an overview of language mod- eling methods and then focus on respective neural networks approaches. Next we describe different types of compression. In Section 3.1, we consider the sim- plest methods for neural networks compression like pruning or quantization. In Section 3.2, we consider approaches to compression of neural networks based on different matrix factorization methods. Section 3.3 deals with TT-decomposition. Section 4 describes our results and some implementation details. Finally, in Sec- tion 5, we summarize the results of our work.Consider the language modeling problem. We need to compute the probability of a sentence or sequence of words (w 1 , . . . , w T ) in a language L.The use of such a model directly would require calculation P(w t |w 1 , . . . , w t−1 ) and in general it is too difficult due to a lot of computation steps. That is why a common approach features computations with a fixed value of N and approximate (1) with P(w t |w t−N , . . . , w t−1 ). This leads us to the widely known N -gram models [5,6]. It was very popular approach until the middle of the 2000s. A new milestone in language modeling had become the use of recurrent neural networks [7]. A lot of work in this area was done by Thomas Mikolov [8].Consider a recurrent neural network, RNN, where N is the number of timesteps, L is the number of recurrent layers, x t−1 ℓ is the input of the layer ℓ at the moment t. Here t ∈ {1, . . . , N }, ℓ ∈ {1, . . . , L}, and x t 0 is the embedding vector. We can describe each layer as follows:where W ℓ and V ℓ are matrices of weights and σ is an activation function. The output of the network is given byThen, we defineWhile N -gram models even with not very big N require a lot of space due to the combinatorial explosion, neural networks can learn some representations of words and their sequences without memorizing directly all options. Now the mainly used variations of RNN are designed to solve the problem of decaying gradients [9]. The most popular variation is Long Short-Term Memory (LSTM) [7] and Gated Recurrent Unit (GRU) [10]. Let us describe one layer of LSTM:where again t ∈ {1, . . . , N }, ℓ ∈ {1, . . . , L}, c t ℓ is the memory vector at the layer ℓ and time step t. The output of the network is given the same formula 4 as above.Approaches to the language modeling problem based on neural networks are efficient and widely adopted, but still require a lot of space. In each LSTM layer of size k × k we have 8 matrices of size k × k. Moreover, usually the first (or zero) layer of such a network is an embedding layer that maps word's vocabulary number to some vector. And we need to store this embedding matrix too. Its size is n vocab ×k, where n vocab is the vocabulary size. Also we have an output softmax layer with the same number of parameters as in the embedding, i.e. k ×n vocab . In our experiments, we try to reduce the embedding size and to decompose softmax layer as well as hidden layers.We produce our experiments with compression on standard PTB models. There are three main benchmarks: Small, Medium and Large LSTM models [11]. But we mostly work with Small and Medium ones.In this subsection, we consider maybe not very effective but still useful tech- niques. Some of them were described in application to audio processing [12] or image-processing [13,14], but for language modeling this field is not yet well described.Pruning is a method for reducing the number of parameters of NN. In  Fig 1. (left), we can see that usually the majority of weight values are con- centrated near zero. It means that such weights do not provide a valuable con- tribution in the final output. We can set some threshold and then remove all connections with the weights below it from the network. After that we retrain the network to learn the final weights for the remaining sparse connections.Quantization is a method for reducing the size of a compressed neural net- work in memory. We are compressing each float value to an eight-bit integer representing the closest real number in one of 256 equally-sized intervals within the range. Pruning and quantization have common disadvantages since training from scratch is impossible and their usage is quite laborious. In pruning the reason is mostly lies in the inefficiency of sparse computing. When we do quantization, we store our model in an 8-bit representation, but we still need to do 32-bits computations. It means that we have not advantages using RAM. At least until we do not use the tensor processing unit (TPU) that is adopted for effective 8- and 16-bits computations.Low-rank factorization represents more powerful methods. For example, in [1], the authors applied it to a voice recognition task. A simple factorization can be done as follows:After this we can rewrite our equation for RNN:For LSTM it is mostly the same with more complicated formulas. The main advantage we get here from the sizes of matrices W a b a l , U l , U l . They have the sizes r × n and n × r, respectively, where the original W l and V l matrices have size n × n. With small r we have the advantage in size and in multiplication speed. We discuss some implementation details in Section 4.In the light of recent advances of tensor train approach [2,3], we have also decided to apply this technique to LSTM compression in language modeling.The tensor train decomposition was originally proposed as an alternative and more efficient form of tensor's representation [15]. The TT-decomposition (or TT-representation) of a tensor A ∈ R n1×...×n d is the set of matrices G k [j k ] ∈ R r k−1 ×r k , where j k = 1, . . . , n k , k = 1, . . . , d, and r 0 = r d = 1 such that each of the tensor elements can be represented as A(j 1 , j 2 , . . . ,In the same paper, the author proposed to consider the input matrix as a multi- dimensional tensor and apply the same decomposition to it. If we have matrix A of size N × M , we can fix d and such n 1 , . . . , n d , m 1 , . . . , m d that the following conditions are fulfilled:Then we reshape our ma- trix A to the tensor A with d dimensions and size n 1 m 1 × n 2 m 2 × . . . × n d m d . Finally, we can perform tensor train decomposition with this tensor. This ap- proach was successfully applied to compress fully connected neural networks [2] and for developing convolution TT layer [3].In its turn, we have applied this approach to LSTM. Similarly, as we describe it above for usual matrix decomposition, here we also describe only RNN layer. We apply TT-decomposition to each of the matrices W and V in equation 2 and get:Here TT(W ) means that we apply TT-decomposition for matrix W . It is nec- essary to note that even with the fixed number of tensors in TT-decomposition and their sizes we still have plenty of variants because we can choose the rank of each tensor.For testing pruning and quantization we choose Small PTB Benchmark. The results can be found in Table 1. We can see that we have a reduction of the size with a small loss of quality. For matrix decomposition we perform experiments with Medium and Large PTB benchmarks. When we talk about language modeling, we must say that the embedding and the output layer each occupy one third of the total network size. It follows us to the necessity of reducing their sizes too. We reduce the output layer by applying matrix decomposition. We describe sizes of LR LSTM 650- 650 since it is the most useful model for the practical application. We start with basic sizes for W and V , 650 × 650, and 10000 × 650 for embedding. We reduce each W and V down to 650 × 128 and reduce embedding down to 10000 × 128. The value 128 is chosen as the most suitable degree of 2 for efficient device implementation. We have performed several experiments, but this configuration is near the best. Our compressed model, LR LSTM 650-650, is even smaller than LSTM 200-200 with better perplexity. The results of experiments can be found in Table 2.In TT decomposition we have some freedom in way of choosing internal ranks and number of tensors. We fix the basic configuration of an LSTM-network with  [16]. The average training time for each is about 5-6 hours on GeForce GTX TITAN X (Maxwell architecture), but unfortunately none of them has achieved acceptable quality. The best obtained result (TT LSTM 600-600) is even worse than LSTM-200-200 both in terms of size and perplexity. In this article, we have considered several methods of neural networks compres- sion for the language modeling problem. The first part is about pruning and quantization. We have shown that for language modeling there is no difference in applying of these two techniques. The second part is about matrix decompo- sition methods. We have shown some advantages when we implement models on devices since usually in such tasks there are tight restrictions on the model size and its structure. From this point of view, the model LR LSTM 650-650 has nice characteristics. It is even smaller than the smallest benchmark on PTB and demonstrates quality comparable with the medium-sized benchmarks on PTB.
