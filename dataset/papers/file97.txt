Visual question answering (VQA) [1,17] asks an agent to generate an accurate answer to a natural language question that queries an image (see Figure 1). composite task involves a variety of artiicial intelligence such as computer vision, natural lan- guage answering, knowledge representation and reasoning. With the great success of deep learning in these an eeective VQA agent can be built with applications of deep learning models. A typical design is to use an answer generator based on a joint rep- resentation of visual and textual inputs [1]. A considerable body of research has been conducted on appropriately combining visual and textual representations [11,19,26,39,41], while the funda- mental question of learning these representations speciically for visual question answering has not generated a lot of interests. In this work, we perform a detailed analysis on text data in VQA and design textual representation methods that are appropriate for this task.In VQA, the subtask of extracting visual information can be well addressed by models commonly used in computer vision tasks like object detection [12,25] and image classiication [12,22,24,33,36], because they share similar demands for visual representations. Deep convolutional neural networks (CNNs) [24] have achieved sig- niicant breakthroughs in computer vision and can be directly used in VQA. In natural language processing, recurrent neural networks (RNNs) [32] are widely used to learn textual representations in tasks like text sentiment classiication [7,10,18,20,23,34,42], language modeling [4,6,8,28], and machine translation [2,35,38]. Parallel to image representations, most previous deep learning models on VQA [1,11,19,26,39,41] directly rely on RNNs to extract textual information. However, our detailed analysis on text data reveals some special properties of text data in VQA. results indicate that RNNs may not be the best for text representation in VQA.With the above analysis and insights, we propose to apply CNNs for learning textual representations in VQA. CNNs for texts have been explored in simple text classiication task [7,10,18,20,42] and shown comparable results with RNNs. Our experiments show that a very simple CNN-based model outperforms a RNN-based model that has much more parameters, a result that is consistent with our analysis. In further work, we incorporate techniques from CNNs for images and RNNs and make specialized improvements to build wider and deeper networks. Diierent methods for text vectorization are also explored and analyzed. Our best model yields a substantial improvement as compared to VQA models with RNN- based textual representations.Recent study on text classiication also shows that a shallow model like fastText [16] can achieve comparable accuracy with deep learning models in text classiication. result contradicts the common belief that deep learning models have higher repre- sentation power than shallow ones. It is speculated that simple text classiication only needs shallow representation power. We conduct experiments on learning textual representations using fast- Text in VQA and observe a signiicant decrease in accuracy. demonstrates the diierent requirements for textual representations in VQA, which makes the use of deep learning models necessary. We argue that VQA is an appropriate task to evaluate diierent textual representation models.In this section, we describe two basic families of neural networks, namely convolutional neural networks [24] and recurrent neural networks [32], and their use in visual question answering [1,17]. units (GRUs) [5]. In natural language processing, text data is natu- rally a type of sequential data so that RNNs are widely used in tasks like text sentiment classiication [7,10,18,20,23,34,42], language modeling [4,6,8,28], and machine translation [2,35,38]. Similar to CNNs for image data, RNNs work as feature extractors in these tasks by encoding sequential data into vector representations. In addition, as generative models, RNNs are also used as decoders in language modeling and machine translation.As compared to using CNNs on sequential data, applying RNNs on image data is more natural. In terms of data size, CNNs for sequences must solve the problem of variable lengths while RNNs for images do not have this problem. With diierent ways to serialize matrix data, RNNs have been shown to be eeective to some extent. Meanwhile, for video data, combination of RNNs and CNNs is an active research topic [3,9].In the of image data processing, convolutional kernel is one of the most useful tools for feature detection. combination of several kernels, which can detect simple features like corners and edges, will lead to a detector for speciic shapes or even more complex features. Based on this motivation, convolutional neural networks (CNNs), which apply trainable convolutional kernels in neural networks, have outperformed many other methods in computer vision tasks, such as object detection [12,25], image classiication [12,22,24,33,36], and image captioning [40]. CNN module serves as the feature extractor of image inputs in these tasks. Unlike in image data processing, where convolutional kernels are hardwired to be primitive feature detectors, CNNs learn the parameters of kernels through training, deciding what kinds of features are important to each speciic task. By stacking several convolution layers, CNNs compute a hierarchy of increasingly high- level features from images. high-level features, represented by feature maps, are then used as inputs to the next module, such as a classiier, a text generator, or a decoder, depending on the tasks.Because of the success on image tasks, CNNs are considered as a natural choice for matrix data which have sizes. Sequential data like text data usually have variable lengths and recurrent neural networks (Section 2.2) are commonly considered to be a beeer choice. Currently, applications of CNNs on sequential data have not been explored much. However, recent studies have shown that applying CNNs on sequential data with appropriate pooling layer is feasible and eeective. We will discuss these in detail in Section 3.As CNNs are specialized for matrix data, recurrent neural networks (RNNs) are specialized for processing sequential data. Similar to how human beings process a sequence, RNNs process a sequence of values one by one from one end to the other to obtain a compre- hensive representation of the data. requires RNNs to have a memory for processed data until the end of the sequence. In prac- tice, this requirement causes problems called gradient vanishing and explosion, which leads to the development of improved RNNs like long short-term memories (LSTMs) [13] and gated recurrent Visual question answering (VQA) requires an agent to answer a natural language question based on a corresponding image. It has drawn considerable aaention from artiicial intelligence, deep learn- ing, computer vision, and natural language processing research communities. As compared to most traditional computer vision and natural language processing agents, an agent that solves the VQA problems is more likely to pass the Turing Test, since both visual and textual understanding and knowledge reasoning are needed in VQA. Deep learning has shown its power in a variety of AI tasks. However, training deep learning models demands a large amount of data. To this end, various datasets aimed at VQA are collected and made available [17]. In [1] a VQA dataset (COCO- VQA) with a well-deened quantitative evaluation metric was made available. makes it feasible to develop computational methods based on recent achievements in deep learning. also held a public VQA challenge based on this dataset in 2015. Most current deep learning models for COCO-VQA share a similar paaern. is, they usually consist of four basic components: an image fea- ture extractor, a text feature extractor, a feature combiner and a classiier.Image Feature Extractor: EEective CNN-based visual repre- sentation models for computer vision, such as VGG [33], ResNet [12], and GooLeNet [36], have been used as the image feature extrac- tor. performance is consistent among tasks; that is, beeer image classiication models yield beeer results when used in VQA models. implies the image classiication task shares similar requirements with the VQA task for feature extraction of image information. However, this is not the case on text side, as discussed in Section 3.Text Feature Extractor: In VQA, RNNs like LSTMs and GRUs are commonly used as the text feature extractor. To the best of our knowledge, only a very simple CNN model has been used in [41] and achieved similar performance as RNNs. work provides a detailed exploration of CNN-based text feature extractor and obtains considerably beeer results on VQA tasks.Feature Combiner: Since VQA was initially proposed, most eeorts have been devoted to improving the method for combin- ing image and text feature vectors into a joint representation that contains the information needed to answer the question. In [1] element-wise multiplication or concatenation was proposed. In [11] the multimodal compact bilinear (MCB) pooling, which approxi- mately computes the out-product of two vectors, was proposed and won the VQA challenge on COCO-VQA [1]. MCB was improved in [19] by adding more parameters in the layer. In contrast, a RNN- based episodic memory architecture was proposed in [39] and used as the combiner. Moreover, aaention mechanism, especially spa- tial visual aaention, has been shown to be eeective as part of the combiner with its ability to extract highly related information from images [26,41].Classiier: As proposed in [1], during training, the top K fre- quent answers in the training set are chosen to form an answer vocabulary. casts the VQA problem into a K-class classiication problem. classiier used in VQA models is the Soomax function, which is typically used in other classiication problems like image classiication and text sentiment classiication. For testing, the ac- curacy is computed by a speciic evaluation metric as discussed in Section 4.1.In this section, we discuss the special properties of text data in VQA.motivates us to apply CNNs as the text feature extractor. We perform detailed analysis of text data in VQA and develop CNN models that achieve promising performance. their answers' categories. Meanwhile, objects and their relation- ships are usually nouns and prepositional phrases, respectively, in the question sentence. provide guidance on locating answer- related facts in the image, which is the fundamental module of the aaention mechanism in VQA models. Now the task of text feature extraction becomes clear; that is, to obtain a feature vector consisting of information about the question type and objects being queried. To be more speciic, the textual rep- resentation is supposed to extract what the starting words, nouns as well as prepositional phrases represent. Considering words and phrases as features of text, a model specializing on feature detection should be an appropriate choice. While both CNNs and RNNs serve as feature extractors in their general usage, RNNs, including LSTMs and GRUs, do not have explicit feature extraction units. In contrast to convolutional connections in CNNs, the connections within and between units in RNNs are mostly fully-connected. To summarize, CNNs are conceptually more appropriate as the text feature extrac- tor in VQA, which is also validated by our experiments. Additional advantages provided by CNNs are fewer parameters and easily parallelable, which accelerate training and testing while reducing the risk of over---ing.Natural language questions in VQA are diierent from other text data in several aspects. First, people tend to ask short questions, according to diierent VQA datasets [17]. For example, the longest question in the training set of COCO-VQA contains only 22 words, and the average length is 6.2. Most questions have 4 to 10 words. Second, the required level for textual understanding in VQA diiers from that in conventional natural language processing tasks such as text sentiment classiication. In sentiment analysis of movie reviews [18,34], the learned text feature vector is given to a clas- siier to determine whether the author of a post likes or dislikes a movie. So the agent, or the feature extractor, will focus on emo- tional words but pay liile aaention to other contents. In VQA, in order to answer a question, a comprehensive understanding is required since a question can ask anything. As a result, the text feature extractor in VQA should be more powerful and computes comprehensive information from the raw texts. questions are diierent from declarative sentences. Words in a question in VQA are highly related to the contents of its corresponding image.Based on these properties, we argue that, as compared to RNNs, CNNs are the beeer choices for text feature extraction in VQA. By analyzing how human beings process questions, we observe that there are two keys in understanding questions: one is understand- ing the question type, which is usually determined by the a few words, and the other is understanding the objects mentioned in the question and the relationships among them. In many cases, the question type directly describes what the answer looks like [1]. Answers to questions starting with "Is the", "Is there", "Do" are typically "yes" or "no". "What number" and "How many" questions must have numbers as answers. beginning with "What color", "What animal", "What sport" and so on all explicitly indicate A challenge of applying CNNs for text data is how to convert raw texts in a format that CNNs can take, as they are originally designed for matrix data like images. To apply CNNs on texts directly, we need to represent text data in the same way as how image data are represented. Each image is typically a 3-dimensional tensor, where the three dimensions correspond to height, width (in terms of number of pixels), and number of channels, respectively. Intuitively, a text sentence can be considered as an image with height equal to 1. Inspired by the bag-of-words model in natural language processing, a vocabulary is built to transform texts into pseudo images. vocabulary can be either word-based that contains words appearing in the texts, or character-based, which is for a particular language. It is also reasonable for the vo- cabulary to include punctuation as single words or characters. width of text data is deened based on the vocabulary; that is, for word-level representations the width is number of words in a sen- tence; for character-level representation we count the number of characters. To make it concrete, we take the word-based vocabulary as an example, and the character-based case can be easily general- ized in Section 3.3. Similar to pixels in an image, if we can convert each word as a vector, the length of the vector is the number of channels. problem is then reduced to word vectorization, which is usually done by one-hot vectorization.Given a vocabulary V , each word can be represented as a one-hot vector; namely a |V |-component vector with one 1 at the position corresponding to the index of the word in V and 0s for other entries, where |V | is the size of V . With one-hot vectorization, the number of channels becomes |V |. As a result, a sentence with L words is treated as a 1 × L pseudo image with |V | channels, and it can be given into CNNs directly by modifying the height of convolutional kernels into 1 consistently. Although one-hot embedding works well as inputs to CNNs in some cases [14,15], it is sometimes preferable to have a lower dimensional embedding. are two primary reasons. First, if |V | is large, which is usually the case for word-based vocabulary, computation eeciency is low due to the sparsity and high dimensionality of inputs. Second, one-hot embedding is semantically meaningless. an extra embedding layer is usually inserted before CNNs. layer maps the |V |- component vectors into d-component vectors, where d is much smaller than |V | [6,10,18,20]. embedding layer is basically a multiplication of one-hot vectors with a |V | ×d matrix to perform a look-up operation. distributed representations can capture a variety of syntactic and semantic relationships between words. embedding matrix can be trained as part of the networks, which are task-specialized, or can be pre-trained using word embedding like Word2Vec [27,29,30] or GloVe [31]. AAer the embedding layer, the pseudo images will have d channels. Figure 2 provides a complete view of the transformations.are explored in our experiments. Nevertheless, due to the special properties of texts in VQA, character-based vocabulary seems to have liile eeect on the overall performance.Note that once a vocabulary is built, the remaining process to transform text data follows the same path for diierent vocabularies.It is clear that the vocabulary V deenes the pixels in the pseudo image obtained from text data. In the above example, each word becomes a pixel. If the vocabulary is character-based, each character, including space character and single punctuation, will be a pixel. In this case, suppose a sentence has C characters in total, the resulting pseudo image has a size of 1 × C with |V | channels. embedding layer then converts |V | channels into d channels. main advantage of character-based vocabulary is that it pro- duces much longer inputs since C is usually larger than L. makes it possible for using deeper models. For long texts, trans- forming text data using character-based vocabulary and applying very deep CNNs leads to impressive performance [7,42]. Another advantage is that characters may include knowledge about how to form words. However, for short texts, the size of the transformed data is still small even with character-based vocabulary. Our ex- periments show that models that work well for long texts with character-based vocabulary fail to obtain high performance in VQA (Section 3.5). It is believed that the inputs are too short for the models to learn that space is the delimiter for words, which is nat- urally given in word-based vocabulary case. combination of character-based and word-based vocabularies for short tests has been explored in [10] and achieved comparable results. In this method, characters corresponding to each word are grouped to- gether. Instead of computing the sentence representation directly from characters, word representation is generated intermediately. Each group of characters is transformed by character-based vocab- ulary and then fed into a smaller textual representation model to generate a word vector. word vector is then concatenated with the corresponding word embedding from word-based vocabulary to form a larger word representation. More details are given in Fig- ure 2. two methods that involve character-based vocabulary Another problem for text data is that each sentence is composed of diierent numbers of words, and this leads to variable sizes of inputs and outputs of convolution layers. However, the outputs of the whole CNN module are expected to be in order to serve as inputs to next module. Moreover, the sizes of inputs to CNNs should also be consistent in consideration of training.Inspired by the pooling layers applied for down-sampling aaer convolution layers in CNNs for images [24], several pooling layers specialized for text data of variable lengths have been proposed [14,18,20]. One basic idea is to apply pooling for the whole sentence and select the k largest values instead of performing pooling repeatedly for each local area of images. is called k-max pooling. By k for the last pooling layer of the CNN-based module, the requirement for outputs is satissed. If k = 1, it results in max-pooling over the whole length. More details are given in Figure 2.While pooling layers can provide outputs regardless of the size of inputs, the same size for all inputs is also required due to particular optimization techniques such as batch training. solution to this requirement is to perform padding and cropping. Cropping is usually used in the case of long texts, especially with character-based vocabulary, which simply cuts the part longer than a length. For short texts like questions in VQA, zero padding is typically used to pad each input to the same length of the longest sentence. involves a minor problem that we are only aware of the longest length in the training set while there can be longer data during test. in practice, a combination of padding and cropping is used. Note that with inputs, pooling over each local area as in images is also feasible [7]. In this work, we perform zero-padding and max-pooling over the whole sentence for most experiments based on the properties of texts in VQA. Cropping is only used in validation and testing.Note that in the training set of COCO-VQA, the average length of questions is only 6.2 words, while the longest one consists of 22 words. Character-based vocabulary results in longer inputs where the average is 30.9 and the longest training sample in COCO-VQA becomes 100 characters. in batch training of texts in VQA, zero-padding is heavily used in both cases. AAer padding, the lengths of inputs seem to be appropriate for a model with multiple convolution layers. However, our analysis and experiments show that adding layers actually leads to worse results. First, if a local max-pooling is added aaer convolution layers, it usually hurts the performance since a local area may be all zero-padded, which makes the outputs meaningless. Second, when a global max-pooling is applied, more layers also do not work well. In this case, zero- padding does not aaect the outputs, but the module will actually obtain the same results as applied directly on the original short texts. Deeper networks are known to suuer from over---ing when the input size is small. In fact, comparing to long texts, where most  Figure 2: An example of applying CNNs on text sentiment classiication. Given a word-based vocabulary V , whose size is |V | = 1000, we transform the L = 6-word sentence into a 1 × 6 image with 1000 channels by one-hot vectorization. blue units represent 1 and white units represent 0 for this layer. through the embedding layer, the number of channels is reduced to d = 300. Now we are able to apply CNNs on the inputs just like on images. Note that the convolutional kernel is 1-D, since the height of our pseudo image is always 1. AAer convolution, a max-pooling over the whole sentence is performed to provide inputs for the classiier (Section 3.4). Wider and deeper convolution layers can be added to this model easily. part in the dotted box illustrates the case where character-based vocabulary is used to supplement word embedding vectors from word-based vocabulary (Section 3.3). In addition to a word-based vocabulary, a character-based vocabulary V c, whose size is |V c | = 50, is provided. It transforms the C = 3-character word you into a 1 × 3 image with 50 channels through the same one-hot vectorization process. the embedding layer changes the number of channels to d c = 15. A CNN-based module followed by a max-pooling over the whole word generates a word embedding, which is then concatenated to the word embedding obtained from word-based vocabulary, generating the pseudo image with d + d c channels. process is the same for other words. Note that the CNN module is shared among diierent words.samples have more than 1000 characters [7] and multi-layer CNNs work well, the length of texts in VQA is not enough for obtaining promising outcomes from multi-layer CNNs (Section 3.6). observations imply that the CNNs for texts in VQA should not be deep. Our experiments show that one-layer models achieved beeer performance.For long texts and images, deeper networks are important and beneecial. Obstacles on going deeper are that very deep networks become hard to train and suuer from the degradation problem [12]. Residual networks (ResNet) [12] overcame these obstacles by adding skip connections from inputs to outputs of one layer or several layers. skip connections are named residual connections.enable CNNs with hundreds of layers to be trained eeciently and avoid the accuracy saturation problem. Modiied ResNet with 49 layers for long texts has been explored in text classiication with character-based vocabulary [7]. Note that each sample in text data used in [7] has more than 1000 characters.We experiment with a ResNet with 8 layers on texts in VQA with character-based vocabulary. results indicate that the inputs are too short, and deeper networks suuer from over---ing instead of training and degradation problems. Also, residual connections are used when we add one more layer to the one-layer models but it also hurts the performances. It turns out that, unlike mappings learned by intermediate layers in very deep models, the mappings learned by the text feature extractor in VQA is not similar to identity function, making the application of skip connections inappropriate.In our experiments, we explore both methods and combined gates with inception modules, where diierent-sized kernels also generate diierent gates. We achieve our best results with the method in Eq. (3).Inception modules, proposed by [20,36], involve combining convo- lutional kernels of diierent sizes in one convolution layer. tech- nique enables wider convolution layers. motivation for using inception modules for texts is straight-forward; that is, diierent- sized kernels extract features from phrases of diierent lengths. Based on this interpretation, the choice of the number of kernels and their corresponding sizes should be data-dependent, because diierent-sized phrases may have diverse importance in various text data. We explore the seeings and several improvements in our experiments.It is commonly believed that deep models like CNNs and RNNs are more powerful in general. In [16] a shallow model termed fastText was proposed, and it achieved comparable results with deep learning models on several text classiication tasks. In fastText, the embedding vectors of text data are used as inputs, and the CNN module is replaced by a simple average operation. Formally, on a word-based vocabulary, since the 1 × L pseudo image with d channels is actually a concatenation of L d-component word vectors, the average over L word vectors results in a d-component sentence vector. sentence representation is given directly into the classiier. As compared to deep learning models that use CNNs and RNNs, fastText obtains improvements in terms of accuracy while achieving a 15, 000-fold speed-up due to the small number of parameters.performance of fastText casts doubts on using deep models, but it is argued that simple text classiication tasks may not take full advantage of the higher representation power of deep learning [16]. As stated in Section 3.1, the task of text understanding in VQA is much more complicated and comprehensive, which makes it a beeer way to evaluate the capability of diierent models. According to our experiments, deep learning methods are superior to fastText in VQA, a result that is consistent with our analysis.LSTMs and GRUs improve RNNs by adding gates to control infor- mation In particular, the output gate controls information along the sequential dimension. With this functionality, the output gate can be used on any deep learning models. In [37] an output gate is also applied on CNNs. Unlike LSTMs and GRUs that use fully-connected connections, convolutional connections are used when generating output gates in CNNs. Given an input to CNNs, which in our case is the transformed data I ∈ R 1×L×d from text data, two independent 1-D convolutional kernels K and K are used to form the output O of the convolution layer as follows:We report experimental results on COCO-VQA dataset [1] 1 , which consists of 204, 721 MSCOCO real images with 614, 163 questions. data are divided into 3 subsets: training (82, 783 images with 248, 349 questions), validation (40, 504 images with 121, 512 ques- tions) and testing (81, 434 images with 244, 302 questions). In COCO- VQA, answers from ten diierent individuals are collected for each question as ground truths. For training, the top K = 3000 frequent answers among all answers of the training set were chosen to build the answer vocabulary. In each iteration, an in-vocabulary answer is sampled as the label from ten ground truths of each question. If all of the ten answers are out of the answer vocabulary, the ques- tion is skipped. To evaluate the accuracy of a generated answer, following evaluation metric was proposed [1]:where is the output gate, σ is the sigmoid function, * represents convolution, denotes element-wise multiplication, b and b are bias terms. Gated convolutional networks for language modeling was proposed in [8], and the activation function for the original outputs was removed. is, Eq. (2) is replaced with where the generated answer is compared with each of the ten ground truth answers, and the corresponding accuracy is computed. Since evaluation on the testing set can only be processed on remote servers during the VQA challenge [1], and the testing labels are not published, we choose to train and validate our models on the training set only instead of the training+validation set like [1,11], and test on the validation set.  2(512)+3(512)+4(512)+5(512) 61.03 1(512)+3(512)+5(512)+7 (512) 60.96 3(1024)+5(512)+7 (512) 60.97 1(512)+3 (1024) Our baseline model is the challenge winner [11], which uses a 2-layer LSTM as the text feature extractor. model is retrained on the training set only. Meanwhile, unlike in [11], we do not use additional data sources like the pre-trained word embedding (Word2Vec, GloVe) and other dataset (Visual Genome [21]) to aug- ment training. In order to explore the power of models, we argue that additional data will narrow the performance gap of diierent models. For comparison, we only improve the text feature extrac- tors using CNN-based models in all experiments. All the results are reported in Table 1. retrained baseline model is shown as "LSTM (baseline)" in Part 1 of the table. Our code is publicly available 2 .Several CNN-based text feature extractors on word-based vocabu- lary are implemented. word-based vocabulary, which includes all words that appear in the training set, has size |V | = 13321. For word embedding, we the dimension d = 300. Dropout is applied on textual representations before they are given into next module. Part 2 in Table 1 shows the results of these models."CNN Non-Inception" model is a one-layer model with one 1× 3 convolutional kernel. With max-pooling over the whole sentence, it produces a 2048-component textual vector representation. simple CNN-based model already outperforms the baseline model, demonstrating that CNN-based model is beeer than RNN-based one in VQA."CNN Inception (word)" model explores wider CNNs by re- placing the single 1 × 3 kernel in "CNN Non-Inception" model with several diierent-sized kernels in the same layer, as stated in Sec- tion 3.7. Diierent kernel seeings are explored and their results are given in Table 2. Seeings are named in the format "width of kernel (number of feature maps output by this kernel)". Note that the height of kernel is always 1. resulting textual vector rep- resentation has 2048 components. All these models outperform "CNN Non-Inception" model, showing that features extracted from phrases of diierent lengths complement each other. Table 1 in- cludes the best results. For all models using inception modules, diierent kernel seeings are explored. We only report the best result for other models."CNN Inception + Residual" model tries going deeper. It adds an identical layer with a residual connection from inputs to outputs to "CNN Inception (word)" model (Section 3.6). best kernel seeing is 1(512)+3(512)+5 (512)+7(512). extra layer is supposed to further extract text features but hurt performance in experiments. We conjecture that there is no need to go deeper for the short inputs in VQA. Character-based vocabulary will result in longer inputs and deeper models on it are discussed in Section 4.3."CNN Inception + Bottleneck" model is inspired by the bot- tleneck architecture proposed by [12]. We apply booleneck on the convolution layer of "CNN Inception (word)" model with kernel seeing 3(1024) + 5(1024). For deep models on image tasks, this architecture improves the accuracies while reducing the number of parameters. However, it causes a signiicant decrease in accuracy to our one-layer model for VQA, which indicates that the booleneck design is only suitable to very deep models."CNN Inception + Gate (tanh)" model and "Inception + Gate" model are CNN-based models with output gates introduced in Sec- tion 3.8, with Eqs. (2) and (3), respectively. Note that we combine the gate architecture with the inception module: for each kernel K in the same convolution layer, there is a corresponding K . Both methods improve "CNN Inception (word)" model by adding out- put gates. With Eq. (3), we achieve our best text feature extractor with 61.33% accuracy. See Figure 3 for a comparison in accuracy per question type between "Inception + Gate" model and "LSTM (baseline)" model. We can see for most question types, "Inception + Gate" model outperforms "LSTM (baseline)" model.Results for models that involve character-based vocabulary are reported in parts 3 and 4 in Table 1. two models in part 3 use character-based vocabulary only, while the model in part 4 uses a combination of both vocabularies (Section 3.3). character-based vocabulary collects |V c | = 45 characters: all lowercase characters in English, punctuation as well as the space character. kernel seeings for both inception-like models below are 2(512) + 3(512) + 4(512) + 5(512). Dropout is also applied."CNN Inception (char)" model applies the same inception mod- ule as "CNN Inception (word)" model but replaces the word-based inputs with character-based inputs. accuracy drops drastically. As explained in Section 3.3, it is due to the short length of the in- puts, which is not enough for the model to learn how to separate characters into words.  Figure 3: Comparison of accuracy per question type between the "Inception + Gate" model and "LSTM (baseline)" model."CNN Deep Residual" model aaempts to take advantage of the longer inputs provided by character-based vocabulary. We stack 5 convolution layers with residual connections and 3 local pooling layers to build a deep model. Contrast to the results of [7,42], the model fails to work well. Again, comparison indicates the input length as the cause of failure."CNN Inception (char+word)" model makes use of both word- based and character-based vocabularies as shown in Figure 2. In our model, the characters of each word generate a 150-component word embedding vector, which is concatenated with the 150-component word embedding from word-based vocabulary to form a 300-component vector representing the word. As compared to "CNN Inception (word)" model, it leads to a slight accuracy decrease. demon- strates that using character-based vocabulary is not able to provide useful information from constituent characters of the word. Based on these experiments, we conclude that character-based vocabulary is not helpful in short input cases like texts in VQA.We compare the numbers of parameters of CNN-based text fea- ture extractor with LSTM-based ones in Table 3. While CNNs improve the accuracy, much fewer parameters are needed to train them. reduces the risk of over---ing. is averaged to generate part of the word embedding. results are given in Table 4. We can see the performance gap between deep learning models and fastText. Clearly, VQA is an appropriate task to evaluate textual representation methods and demonstrates the power of deep models.As introduced in Section 3.9, fastText is a shallow model that achieves comparable results with deep learning models like CNNs and RNNs in text classiication tasks [16]. result contradicts the common belief that deep models can learn beeer representa- tions. It has been conjectured [16] that the simple text classiication task may not be the right one to evaluate textual representation methods. Given the higher requirement for textual understand- ing in VQA, we compare these models in VQA. In addition to the original fastText model ("fastText (word)"), which averages word embedding to obtain sentence representation, we also explore fast- Text ("fastText (char+word)") with character-based vocabulary. Similar to the idea in Section 3.3, character embedding of each wordWe propose to apply CNNs on textual representations as the text feature extractor in VQA. By incorporating recent research achieve- ments in CNNs for images, our best model improves textual rep- resentations and the overall accuracy of VQA. By comparing deep models with the fastText, we show that while beeer textual repre- sentations lead to beeer results in VQA, VQA is in turn an appro- priate task to evaluate textual representation methods due to its comprehensive requirement for texts. Based on our research, we believe that our CNN-based textual representation methods can be extensively used for learning textual representations in other tasks with texts of similar properties.work was supported in part by National Science Foundation grant IIS-1633359, and by Washington State University. We grate- fully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research. 
