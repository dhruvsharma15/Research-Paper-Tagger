Question answering (QA) is the task of automatically pro- ducing an answer to a question given a corresponding doc- ument. It not only provides humans with efficient ac- cess to vast amounts of information, but also acts as an important proxy task to assess machine literacy via read- ing comprehension. Thanks to the recent release of sev- eral large-scale machine comprehension/QA datasets (Hermann et al., 2015;Rajpurkar et al., 2016;Dunn et al., 2017;Trischler et al., 2016;Nguyen et al., 2016), the field has un- dergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks ( Wang et al., 2017;Shen et al., 2016;Seo et al., 2016). However, previous models do not treat QA as a task of natural language generation (NLG), but of pointing to an answer span within a document.Although the question answering and asking tasks appear symmetric, there are some key differences. First, answer- ing the questions in most existing QA datasets is extractive -it requires selecting some span of text within the doc- ument -while question asking is comparatively abstrac- tive -it requires generation of text that may not appear in the document. Furthermore, a (document, question) pair typically specifies a unique answer. Conversely, a typical (document, answer) pair may be associated with multiple questions, since a valid question can be formed from any information or relations which uniquely specify the given answer.Alongside QA, question generation has also gained in- creased popularity ( Du et al., 2017;Yuan et al., 2017). The task is to generate a natural-language question conditioned on an answer and the corresponding document. Among its many applications, question generation has been used to improve QA systems (Buck et al., 2017;Serban et al., 2016;Yang et al., 2017 To tackle the joint task, we construct an attention- based ( Bahdanau et al., 2014) sequence-to-sequence model (Sutskever et al., 2014) that takes a document as input and generates a question (answer) conditioned on an an- swer (question) as output. To address the mixed extrac- tive/abstractive nature of the generative targets, we use the pointer-softmax mechanism ) that learns to switch between copying words from the docu- ment and generating words from a prescribed vocabulary. Joint training is realized by alternating the input data be- tween question-answering and question-generating exam- ples for the same model. We demonstrate empirically that this model's QA performance on SQuAD, while not state of the art, improves by about 10% with joint training. A key novelty of our joint model is that it can generate (partially) abstractive answers.Joint-learning on multiple related tasks has been explored previously (Collobert et al., 2011;Firat et al., 2016). In machine translation, for instance, Firat et al. (2016) demon- strated that translation quality clearly improves over mod- els trained with a single language pair when the atten- tion mechanism in a neural translation model is shared and jointly trained on multiple language pairs. of asking questions to benefit question answering, without training the model to answer the generated questions.In question answering, Wang &amp; Jiang (2016) proposed one of the first neural models for the SQuAD dataset. SQuAD defines an extractive QA task wherein answers consist of word spans in the corresponding document. Wang &amp; Jiang (2016) demonstrated that learning to point to answer boundaries is more effective than learning to point sequen- tially to the tokens making up an answer span. Many later studies adopted this boundary model and achieved near- human performance on the task ( Wang et al., 2017;Shen et al., 2016;Seo et al., 2016). However, the boundary- pointing mechanism is not suitable for more open-ended tasks, including abstractive QA ( Nguyen et al., 2016) and question generation. While "forcing" the extractive bound- ary model onto abstractive datasets currently yields state- of-the-art results ( Wang et al., 2017), this is mainly because current generative models are poor and NLG evaluation is unsolved.Our proposed model adopts a sequence-to-sequence frame- work ( Sutskever et al., 2014) with an attention mecha- nism ( Bahdanau et al., 2014) and a pointer-softmax de- coder ( Gulcehre et al., 2016). Specifically, the model takes a document (i.e., a word sequence) D = (w , and the answer word sequence in question-generation mode (q-gen). We also at- tach a binary variable to indicate whether a data-point is in- tended for a-gen or q-gen. Intuitively, this should help the model learn the two modalities more easily. Empirically, QA performance improves slightly with this addition.A word w i in an input sequence is first embedded with an embedding layer into vector e w i . Character-level informa- tion is captured with the final states e ch Earlier work on question generation has resorted to either rule-based reordering methods (Heilman &amp; Smith, 2010;Agarwal &amp; Mannem, 2011;Ali et al., 2010) or slot-filling with question templates (Popowich &amp; Winne, 2013;Chali &amp; Golestanirad, 2016;Labutov et al., 2015). These tech- niques often involve pipelines of independent components that are difficult to tune for final performance measures. Partly to address this limitation, end-to-end-trainable neu- ral models have recently been proposed for question gen- eration in both vision ( Mostafazadeh et al., 2016) and lan- guage. For example, Du et al. (2017) used a sequence-to- sequence model with an attention mechanism derived from the encoder states. Yuan et al. (2017) proposed a similar architecture but in addition improved model performance through policy gradient techniques. To better encode the condition, we also extract the encod- ings of the document words that appear in the condition sequence. This procedure is particularly helpful in q-gen mode, where the condition (answer) sequence is typically extractive. These extracted vectors are then fed into a con- dition aggregation BiLSTM to produce the extractive con- dition encoding h  Buck et al. (2017) used policy gradient with a QA reward to train a sequence-to-sequence paraphrase model to reformulate questions in an existing QA dataset ( Dunn et al., 2017). The generated questions were then used to further train an existing QA model ( Seo et al., 2016). A key distinction of our model is that we harness the process J in a-gen mode (for encoding questions) and h e K in q-gen mode (for encoding answers).The RNN-based decoder employs the pointer-softmax mechanism ( Gulcehre et al., 2016). At each generation step, the decoder decides adaptively whether (a) to gen- erate from a decoder vocabulary or (b) to point to a word in the source sequence (and copy over). Recurrence of the pointing decoder is implemented with two LSTM cells c 1 and c 2 :Here, w &lt;t corresponds to the embeddings y (t−1) in Equa- tion (1) and (4). During training, gold targets are used to teacher-force the sequence generation for training, i.e., w &lt;t = w &lt;t , while during inference, generation is condi- tioned on the previously generated words, i.e.,where s1 and s 2 are the recurrent states, y (t−1) is the embedding of decoder output from the previous time step, and v (t) is the context vector (to be defined shortly in Equa- tion (3)).The pointing decoder computes a distribution α (t) over the document word positions (i.e., a document attention, Bahdanau et al. 2014). Each element is defined as:For words with multiple occurrence, since their exact ref- erences in the document cannot be reiabled determined, we aggregate the probability of these words in the encoder and the pointing decoder (similar to Kadlec et al. 2016). At test time, beam search is used to enhance fluency in the question-generation output. 1 The decoder also keeps an explicit history of previously generated words to avoid rep- etition in the output.where f is a two-layer MLP with tanh and softmax activa- tion, respectively. The context vector v (t) used in Equa- tion (2) is the sum of the document encoding weighted by the document attention:5. ExperimentsThe generative decoder, on the other hand, defines a dis- tribution over a prescribed decoder vocabulary with a two- layer MLP g:Finally, the switch scalar s (t) at each time step is computed by a three-layer MLP h:We conduct our experiments on the SQuAD corpus (Rajpurkar et al., 2016), a machine comprehension dataset con- sisting of over 100k crowd-sourced question-answer pairs on 536 Wikipedia articles. Simple preprocessing is per- formed, including lower-casing all texts in the dataset and using NLTK (Bird, 2006) for word tokenization. The test split of SQuAD is hidden from the public. We there- fore take 5,158 question-answer pairs (self-contained in 23 Wikipedia articles) from the training set as validation set, and use the official development data to report test results. Note that answers in this dataset are strictly extractive, and we therefore constrain the pointer-softmax module to point at all decoding steps in answer generation mode.The first two layers of h use tanh activation and the final layer uses sigmoid activation, and highway connections are present between the first and the second layer. We also attach the entropy of the softmax distributions to the in- put of the final layer, postulating that the quantities should help guide the switching mechanism by indicating the con- fidence of pointing vs generating. The addition is empiri- cally observed to improve model performance.We first establish two baselines without multi-task train- ing. Specifically, model A-gen is trained only to gener- ate an answer given a document and a question, i.e., as a conventional QA model. Analogously, model Q-gen is trained only to generate questions from documents and an- swers. Joint-training (in model JointQA) is realized by feeding answer-generation and question-generation data to the model in an alternating fashion between mini-batches.The resulting switch is used to interpolate the pointing and the generative probabilities for predicting the next word:The optimization objective for updating the model param- eters θ is to maximize the negative log likelihood of the generated sequences with respect to the training data D:In addition, we compare answer-generation performance with the sequence model variant of the match-LSTM (mLSTM) model (Wang &amp; Jiang, 2016). As mentioned ear- lier, in contrast to existing neural QA models that point to the start and end boundaries of extractive answers, this model predicts a sequence of document positions as the an- swer. This makes it most comparable to our QA setup. Note, however, that our model has the additional capacity 1 The effectiveness of beam search can be undermined by the generally diminished output length. We therefore do not use beam search in a-gen mode, which also saves training time. to generate abstractively from the decoder vocabulary.We use F1 and Exact Match (EM, Rajpurkar et al. 2016) against the gold answer sequences to evaluate answer gen- eration, and BLEU 2 ( Papineni et al., 2002) against the gold question sequences to evaluate question generation. How- ever, existing studies have shown that the task of question generation often exhibits linguistic variance that is seman- tically admissible; this renders it inappropriate to judge a generated question solely by matching against a gold se- quence ( Yuan et al., 2017). We therefore opt to assess the quality of generated questions Y q with two pretrained neu- ral models as well: we use a language model to compute the perplexity of Y q , and a QA model to answer Y q . We measure the F1 score of the answer produced by this QA model. out by earlier studies, automatic metrics often do not cor- relate well with the generation quality assessed by humans ( Yuan et al., 2017). We thus consider the overall outcome to be positive.We choose mLSTM as the pretrained QA model and train it on SQuAD with the same split as mentioned in Sec- tion 5.1. Performance on the test set (i.e., the official val- idation set of SQuAD) is 73.78 F1 and 62.7 EM. For the pretrained language model, we train a single-layer LSTM language model on the combination of the text8 corpus 3 , the Quora Question Pairs corpus 4 , and the gold questions from SQuAD. The latter two corpora were included to tai- lor to our purpose of assessing question fluency, and for this reason, we ignore the semantic equivalence labels in the Quora dataset. Validation perplexity is 67.2 for the pre- trained language model.Meanwhile, although our model does not perform as well as mLSTM on the QA task, it has the added capability of generating questions. mLSTM uses a more advanced en- coder tailored to QA, while our model uses only a bidi- rectional LSTM for encoding. Our model uses a more ad- vanced decoder based on the pointer-softmax that enables it to generate abstactively and extractively.Evaluation results are provided in Table 1. We see that A-gen performance improves significantly with the joint model: both F1 and EM increase by about 10 percentage points. Performance of q-gen worsens after joint training, but the decrease is relatively small. Furthermore, as pointed For a finer grained analysis, we first categorize test set answers based on their entity types, then stratify the QA performance comparison between A-gen and JointQA. The categorization relies on Stanford CoreNLP ( Manning et al., 2014) to generate constituency parses, POS tags, and NER tags for answer spans (see Rajpurkar et al. 2016 for more details). As seen in Figure 1, the joint model sig- nificantly outperforms the single model in all categories. Interestingly, the moving average of the performance gap (dashed curve above bars) exhibits an upward trend as the A-gen model performance decreases across answer types, suggesting that the joint model helps most where the single model performance is weakest. Qualitatively, we have observed interesting "shifts" in at- tention before and after joint training. For example, in the positive case in Table 2, the gold question asks about the direct object,Nixon, of the verb endorse, but the A-gen model predicts the indirect object, Kennedy, instead. In contrast, the joint model asks about the appositive of vice president during question generation, which presumably Table 2. Examples of QA behaviour changes possibly induced by joint training. Gold answers correspond to text spans in green. In both the positive and the negative cases, the answers produced by the joint model are highly related (and thus presumably influenced) by the generated questions.Positive Document in the 1960 election to choose his successor , eisenhower endorsed his own vice president , republican richard nixon against democrat john f. kennedy . Q gold who did eisenhower endorse for president in 1960 ? Q gen what was the name of eisenhower 's own vice president ? Answer A-gen: john f. kennedy JointQA: richard nixonNegative Document in 1870 , tesla moved to karlovac , to attend school at the higher real gymnasium , where he was profoundly influenced by a math teacher martin sekuli´csekuli´c Q gold why did tesla go to karlovac ? Q gen what did tesla do at the higher real gymnasium ? Answer A-gen: to attend school at the higher real gymnasium JointQA: he was profoundly influenced by a math teacher martin sekuli´csekuli´c "primes" the model attention towards the correct answer Nixon. Analogously in the negative example, QA attention in the joint model appears to be shifted by joint training to- wards an answer that is incorrect but closer to the generated question.Note that the examples from Table 2 come from the vali- dation set, and it is thus not possible for the joint model to memorize the gold answers from question-generation mode -the priming effect must come from some form of knowl- edge transfer between q-gen and a-gen via joint training.Implementation details of the proposed model are as fol- lows. The encoder vocabulary indexes all words in the dataset. The decoder vocabulary uses the top 100 words sorted by their frequency in the gold questions in the train- ing data. This encourages the model to generate frequent words (e.g. wh-words and function words) from the de- coder vocabulary and copy less frequent ones (e.g., topical words and entities) from the document.We proposed a neural machine comprehension model that can jointly ask and answer questions given a document. We hypothesized that question answering can benefit from syn- ergistic interaction between the two tasks through parame- ter sharing and joint training under this multitask setting. Our proposed model adopts an attention-based sequence- to-sequence architecture that learns to dynamically switch between copying words from the document and generating words from a vocabulary. Experiments with the model con- firm our hypothesis: the joint model outperforms its QA- only counterpart by a significant margin on the SQuAD dataset.Although evaluation scores are still lower than the state- of-the-art results achieved by dedicated QA models, the proposed model nonetheless demonstrates the effectiveness of joint training between QA and question generation, and thus offers a novel perspective and a promising direction for advancing the study of QA.The word embedding matrix is initialized with the 300- dimensional GloVe vectors ( Pennington et al., 2014). The dimensionality of the character representations is 32. The number of hidden units is 384 for both of the en- coder/decoder RNN cells. Dropout is applied at a rate of 0.3 to all embedding layers as well as between the hidden states in the encoder/decoder RNNs across time steps.
