In the past decade, deep neural network models based on RNNs 1 and/or CNNs 2 have been shown to be very powerful sequence modelers. Their ability to model joint distribu- tions of real-world data has been demonstrated through remarkable achievements in a broad spectrum of genera- tive tasks such as; image synthesis (van den Oord et al., 2016b;c;Salimans et al., 2017;Theis &amp; Bethge, 2015), image description (Karpathy &amp; fei Li, 2015;Xu et al., 2015;Johnson et al., 2016;Pedersoli et al., 2016;Vinyals et al., 2015), video description ( Donahue et al., 2015), speech and audio synthesis (van den Oord et al., 2016a), handwriting recognition (Graves &amp; Schmidhuber, 2008;Bluche, 2016), handwriting synthesis (Graves, 2013), ma- chine translation ( Cho et al., 2014;Bahdanau et al., 2014;Kalchbrenner et al., 2016;Sutskever et al., 2014), speechOne class of sequence models employ the so-called encoder- decoder ( Cho et al., 2014) or sequence-to-sequence ( Sutskever et al., 2014) architecture, wherein an encoder encodes a source sequence into feature vectors, which a de- coder employs to produce the target sequence. The source and target sequences may either belong to the same modality (e.g. in machine translation use-cases) or different modal- ities (e.g. in image-to-text, text-to-image, speech-to-text); the encoder / decoder sub-models being constructed ac- cordingly. The entire model is trained end-to-end using supervised-learning techniques 3 . In recent years, this archi- tecture has been augmented with an attention and align- ment model which selects a subset of the feature vectors for decoding. It has been shown to help with longer se- quences ( Bahdanau et al., 2014;Luong et al., 2015). Among other things, this architecture has been used for image- captioning.We employ a variant of this architecture to map images of math formulas into L A T E X markup code. The contributions of this paper are: 1) Solves the Im2Latex problem 100 and achieves the best reported BLEU score. 2) Pushes the limits of the neural encoder-decoder archi- tecture with visual attention. 3) Analyses variations of the model and cost function. Specifically we note the changes to the base model ( Xu et al., 2015) and what impact those had on performance.1 Saratoga, California, USA. Correspondence to: Sumeet Singh &lt;sumeet@singhonline.info&gt;. 100 https://openai.com/requests-for-research/#im2latex 1 Recurrent Neural Network 2 Convolutional Neural Networks and variants such as dilated CNNs (Yu &amp; Koltun, 2015) The IM2LATEX Problem is a request for research proposed by OpenAI. The challenge is to build a Neural Markup Generation model that can be trained end-to-end to generate the L A T E X markup of a math formula given its image. Data for this problem (see section 2.5) was produced by rendering single-line real-world L A T E X 2 ε formulas obtained from the KDD Cup 2003 dataset. The resulting grayscale images were used as the input samples while the original markup was used as the label/target sequence. Each training/test sample s, is comprised of an image x (s) -the input -and a corresponding target markup-sequence y (s) -the output 3 generally backprop or reinforcement learning t , belongs to the vocabulary of the dataset plus two special tokens, begin-of-sequence 'bos' and end-of-sequence 'eos' ( Figure  1). Denoting image dimensions as H I , W I and C I and the vocabulary as a set V of K words and dropping the superscript (s) for brevity, we represent: ´ H and´Wand´ and´W are the visual feature map's dimensions along the height and width axes of the image respectively. Table 1 shows the configuration of the Encoder CNN. All convolution kernels have shape (3,3), stride (1,1) and tanh non-linearity, whereas all maxpooling windows have shape (2,2) and stride (2,2). See section A.1 for further comments around this architecture.The task is to generate markup that a L A T E X 2 ε compiler will render back to the original image. Therefore, our model needs to generate syntactically and semantically correct markup, by simply 'looking' at the image: i.e. it should jointly model vision and language.Our model (Figure 2) has the same basic architecture as Xu et al. (2015) in the way the encoder, decoder and a visual attention interact. 4 Output IMAGE PREPROCESSING: All images are standardized to the same size by centering the text and padding with white pixels. Then they are linearly transformed (whitened) to lie in the range [-0.5,0.5].  Probability of the output sequence for image a follows:Pooling allows varying the receptive fields of the feature vectors by changing the stride. We share results of two models, one with stride [1,1] (i.e. no pooling) and the other [4,1]  (Table 2). Each pooled feature vector is a window into a distinct spatial region of the image; i.e. the rectangle projected by its receptive field. 5 The idea behind this is to partition the image into spatially localized regional en- codings and setup a decoder architecture (Section 2.2) that selects/emphasizes only relevant regions at a given time-step t, while ignoring/de-emphasizing the rest. Bahdanau et al. 2014 showed that such piecewise encoding enables mod- eling longer sequences as opposed to models that encode the entire input into a single feature vector (Sutskever et al., 2014;Cho et al., 2014). 6 We represent A as a flattened sequence a of pooled feature vectors a l (Equation 2):The decoder (Figure 3) is an LSTM (Hochreiter &amp; Schmidhuber, 1997) based model with an internal state C t that holds relevant information (features) regarding the output sequence unfolded thus far, the image regions attended to and an initial state generated at t = 0. In addition, it re- ceives the previous word y t−1 and encoded image a as inputs. This architecture enables it to condition its output on the entire previous sequence and the image. Representing it as a function f D , we have:It is comprised of the following sub-models ( Figure 3):  Graves, 2008) encoder whose receptive field does encompass the entire input anyway! (Although that does not necessarily mean that the bi-LSTM will encode the entire image). Likewise Deng et al. 2017 who also solve the IM2LATEX problem also employ a bi-directional LSTM stacked on top of a CNN-encoder in order to get full view of the image. In contrast, our visual feature vectors hold only spatially local information which we found are sufficient to achieve good accuracy. This is probably owing to the nature of the problem; i.e. transcribing a one-line math formula into L A T E Xsequence requires only local information at each step.7 This is now a very standard way to model sequence probabili- ties in neural sequence-generators. See ( Sutskever et al., 2014) for example. 1) A LSTM-Stack responsible for memorizing C t and producing a recurrent activation H t .2) A Visual Attention and Alignment model responsible for selecting relevant regions of the encoded image for input to the LSTM-Stack.NOTE: The LSTM-Stack and Visual Attention and Align- ment model jointly form a Conditioned Attentive LSTM (CALSTM); H t and C t being its activa- tion and internal state respectively. 8 3) A Deep Output Layer ( Pascanu et al., 2013) that pro- duces the output probabilities p t .4) Init Model: A model that generates the initial state C 0 .5) An embedding matrix E (learnt through training) that transforms y t into a dense representation ∈ R m .After the model is trained, the output sequence is generated by starting with the word 'bos' and then repeatedly sampling from p t until 'eos' is produced. The sequence of words thus sampled is the predicted sequencê y (Figure 2). Figure 4. Focal-regions learnt by the attention model: at the top by I2L-STRIPS and at the bottom by I2L-NOPOOL. Image darkness is proportional to αt. The focal-regions are 3-4 blocks big and surround the symbol (∆) whose markup is being produced at that step. Also, the model is able to utilize the finer granularity of I2L- NOPOOL and produces a tighter focal region than I2L-STRIPS. More samples at our website.For this procedure (also called 'decoding' 9 ) we use beam search decoding with a beam width of 10.As previously alluded, the decoder selects/emphasizes rel- evant (encoded) image regions at each step. This is im- plemented via. a 'soft attention' mechanism 10 which com- putes a weighted mean z t (equation 9) of the pooled feature vectors a l . The visual attention model f att , computes the weight distribution α t .L While there is a potential for α t to end up in a flat distri- bution over {a 1 . . . a L }, in practice we see that most of its mass gets concentrated on a 2-4 region neighborhood (see Figure 4). That is, the model learns to focus its attention on small focal-regions of the image, tending towards the 'hard attention' formulation described by ( Xu et al., 2015); except in this case the focal-regions are learnt and there- fore adaptive to the dataset and problem. Also, notice that for a given sample (Figure 4), the attention model is able to utilize the extra granularity of feature-map available in the I2L-NOPOOL case and consequently generates a much smaller focal-region than I2L-STRIPS. It would be interest- ing to experiment with an even more granular image-map to see if the attention model refines its focal-region even further.Furthermore, the model aligns its attention with the word it is producing and scans the image left-to-right (I2L-STRIPS) or left-right/up-down (I2L-NOPOOL) just like a human would read it ( Figure 5).f att is modeled by an MLP (Table 3):The core sequence-generator of the decoder is a multilayer LSTM (Hochreiter &amp; Schmidhuber, 1997; Graves, 2013) (  8 Our source-code implements the CALSTM as a RNN cell which may be used as a drop-in replacement for a RNN cell. 9 The term 'decoding' is borrowed from Hidden Markov Mod- els. For further details see for e.g. (Graves, 2008) 10 'soft' attention as defined by Xu et al. (2015) and originally proposed by ( Bahdanau et al., 2014)  LST M q is the LSTM cell at position q with x q q t , h t and c q t being its input, hidden activation and cell state respectively. LST M 1 receives the stack's input: soft attention context z t and previous output word Ey t−1 . LST M Q produces the stack's output H t = h Q t , which is sent up to the Deep Output Layer. Accordingly, the stack's activation (H t ) and state (C t ) are defined as:We do not use skip or residual connections between the cells. Both of our models have two LSTM layers with n = 1500. More discussion on this model can be found in section A.3.We use a a Deep Output Layer ( Pascanu et al., 2013) to produce the final output probabilities p t : Figure 5. Attention scan. The movement of focal-region is aligned with the output word (above the image). Continued in Figure 11.f out is modeled by an MLP ( Table 4). Note that the output layer receives skip connections from the LSTM-Stack input (Equation 13). More discussion on this model can be found in section A.4.  The Init Model f init , produces the initial state C 0 of the LSTM-Stack, based on the pooled feature vectors:f init is modeled as an MLP with common hidden layers and 2Q distinct output layers, one for each element of C 0 , connected as in Figure 7 and specified in Table 5. That said, since it only provides a very small improvement in performance in exchange for over 7 million parameters, its presence could be questioned (see Section A.5). The The first term in equation 17 is the average (per-word) log perplexity of the predicted sequence 11 and is the main ob- jective. R is the L2-regularization term, equal to L2-norm of the model's parameters θ (weights and biases). A is an optional penalty term intended to bias the distribution of α l (Equation 17e) -the amount of attention placed on image-location l. Observe that while L l α t,l = 1, there is no constraint on how the attention is distributed across the L locations of the image. The term λ A A serves to steer the variance of α l by penalizing any deviation from a desired value. ASE (Alpha Squared Error) is the sum of squared-difference between α l and its mean τ /L; and ASE N is its normalized value 12 ∈ [0,100] 13 . Therefore ASE N ∝ ASE ∝ σ . ASE T which is the desired value of ASE N , is a hyperparameter that needs to be discovered through experimentation 14 . λ R and λ A are also hyperpa- rameters that need to be tuned. See section A.2 for more discussion around this topic.Function Output 2Q n tanh Hidden 1 100 tanh For experimentation we split the dataset into two fixed parts: 1) training dataset = 90-95% of the data and 2) test dataset 5-10%. At the beginning of each run, 5% of the training dataset was randomly held out as the validation-set and the remainder was used for training. Therefore, each such run had a different training/validation data-split, thus naturally cross-validating our learnings through the duration of the project. We trained the model in minibatches of 56 using the ADAM optimizer (Kingma &amp; Ba, 2014) for updating param- idea behind the init model is to setup the decoder to extract relevant features from the image (if any) before it starts generating the sequence.11 Also variously known as cross-entropy, negative log- likelihood or negative log-probability 12 It can be shown that τ 2 L−1 LThe entire model was trained end-to-end by minimizing the objective function J (Equation 17) using back propagation is the maximum theoretical value of ASE 13 We normalize ASE so that it may be compared across batches, runs and models 14 Start with ASET = 0, observe where ASEN settles after training, then set ASET to that value and repeat until approximate convergence.eters; periodically evaluating it over the validation set 15 . For efficiency, we batched the data such that each minibatch had similar length samples. For the final evaluation however, we fixed the training and validation dataset split and retrained our models for about 100 epochs (2 1 2 days on I2L-140K dataset). We then picked the model-snapshots with the best validation BLEU score and evaluated the model over the test-dataset for publication. We executed on two Nvidia GeForce 1080Ti graphics cards in a parallel towers configu- ration. Our implementation 102 uses the Tensorflow toolkit and is distributed under AGPL license.Given that there are multiple possible L A T E Xsequences that will render the same math image, ideally we should perform a visual evaluation. However, since there is no widely ac- cepted visual evaluation metric, we report corpus BLEU (1,2,3 &amp; 4 grams) and per-word Levenstein Edit Distance bigger than 1086 × 126 and formulas longer than 150. Pro- cessing the Im2latex-100k dataset 104 (103559 samples) as above resulted in the Im2latex-90k dataset which has 93741 samples. Of these, 4648 were set aside as the test dataset and the remaining 89093 were split into training (95%) and validation (5%) sets before each run (section 2.3). We found the Im2latex-90k dataset too small for good generalization and therefore augmented it with additional samples from KDD Cup 2003. This resulted in the I2L-140K dataset with 114406 (training), 14280 (validation) and 14280 (test) samples. Since the normalized formulas are already space separated token sequences, no additional tokenization step was necessary. The vocabulary was therefore produced by simply identifying the set of unique space-separated words in the dataset. Both datasets are available at our website 105 and the data processing code is available in our source-code repository 102 .scores (see table 7). We also report a (non-standard) exact visual match score 103 which reports the percentage of ex- act visual matches, discarding all partial matches. While the predicted and targeted images match in at least 70%Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua.Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.of the cases, the model generates different but correct se- quences (i.e. y = ˆ y) in about 40% of the cases (Figure 8). For the cases where the images do not exactly match, the differences in most cases are minor (Figure 9). Overall, our models produce syntactically correct sequences 17 for at least 99.85% of the test samples (Table 7 and Figure 10). Bluche, Théodore. Joint line segmentation and transcription for end-to-end handwritten paragraph recognition. In NIPS, 2016.Bluche, Théodore, Ney, Hermann, and Kermorvant, Christo- pher. A comparison of sequence-trained deep neural networks and recurrent neural networks optical modeling for handwriting recognition. In SLSP, 2014.Datasets were created from single-line L A T E X math formulas extracted from scientific papers and subsequently processed as follows: 1) Normalize the formulas to minimize spuri- ous ambiguity. 18 2) Render the normalized formulas us- ing pdflatex and discard ones that didn't compile or render successfully. 3) Remove duplicates. 4) Remove formu- las with low-frequencey words (frequency-threshold = 24 for Im2latex-90k and 50 for I2l-140K  103 We use the 'match without whitespace' algorithm provided by Deng et al. (2017) wherein two images count as matched if they match pixel-wise discarding white columns and allowing for upto 5 pixel image translation (a pdflatex quirk). It outputs a binary match/no-match verdict for each sample -i.e. partial matches however close, are considered a non-match. 18 Normalization was performed using the method and software used by (Deng et al., 2017) which parses the formulas into an AST and then converts them back to normalized sequences.    2123 Dsî R3132 38Figure 8. A sample of correct predictions by I2L-STRIPS. We've shown the long predictions hence lengths are touching 150. Note that at times the target length is greater than the predicted length and at times the reverse is true (though the original and predicted images were identical). All such cases would evaluate to a less than perfect BLEU score or edit-distance. This happens in about 40% of the cases. For more examples visit our website.3.4β [2|2] (y)=−9y       . Figure 9. A random sample of mistakes made by I2L-STRIPS. Observe that usually the model gets most of the formula right and the mistake is only in a small portion of the overall formula (e.g. sample # 1; generating one subscript t instead of an l ). In some cases the mistake is in the font and in some cases the images are identical but were incorrectly flagged by the image-match evaluation software (e.g. sample # 0 &amp; #17). In some cases the predicted formula appears more correct than the original! (sample # 10 where position of the subscript β has been 'corrected' by I2L-STRIPS). Sample # 20 is clearly bad probably owing to very few training samples with underbraces. For more examples visit our website.Model  Table 6. Training metrics. λR = 0.00005 and β2 = 0.9 for all runs. Training BLEU scores were calculated over 100 mini-batches after decoding the sequences using CTC-decoding ( Graves et al., 2006). * denotes that the row corresponds to  got us good results. Further reducing number of layers to 5 yielded the same performance, therefore we stuck with that configuration (Table 1). In additon, we experimented with I2L-STRIPS because it reduces the rectangular image- map to a linear map, thereby presumably making the align- ment model's task easier because now it would only need to scan in one-dimension. However, it performed around the same as I2L-NOPOOL and therefore that hypothesis was debunked.A In this section we present further analyses and discussion of our experiments and comparison with related work.We initially experimented with the output of the VGG16 model (Simonyan &amp; Zisserman, 2014) -per Xu et al. (2015). However (presumably since VGG16 was trained on a differ- ent dataset and a different problem) the BLEU score didn't improve beyond 40%. Then we started training VGG16 along with our model but the end-to-end model didn't even start learning (the log-loss curve was flat) -possibly due to the large overall depth of the end-to-end model. Reduc- ing the number of convolution layers to 6 and changing the non-linearity to tanh (to keep the activations in check) tic optimization'. Our formulation uses the true mean of α l , τ /L instead of 1, normalizes it to a fixed range so that it can be compared across models and more importantly, includes a target-ASE term ASE T . Without this term, i.e. with ASE T = 0, A would bias the attention model towards uniformly scanning all the L image locations. This is unde- sirable since there are many empty regions of the images where it makes no sense for the attention model to spend much time. Conversely, there are some densely populated regions (e.g. a symbol with complex superscript and sub- scripts) where the model would reasonably spend more time because it would have to produce a longer output sequence. In other words, the optimal scanning pattern would have to be non-uniform -ASE T = 0. Also, the scanning pattern would vary from sample to sample, but ASE T is set to a single value (even if zero) for all samples. Therefore we pre- ferred to remove the attention-model bias altogether from the objective function by setting λ A = 0 in all situations except when the attention model needed a 'nudge' in order to 'get off the ground'. In such cases we set ASE T based on observed values of ASE N ( Table 6).¯ hv Γ hv=Tµν =T37 Figure 10. A random sample of predictions of I2L-STRIPS containing both good and bad predictions. Note that though this is a random sample, prediction mistakes are not obvious and it takes some effort to point them out! For more examples visit our website.During experimentation our penultimate LSTM-stack which had 3 LSTM layers with 1000 units each, gave us a vali- dation score of 87.45%. At that point experimental obser- vations suggested that the LSTM stack was the accuracy 'bottleneck' because other sub-models were performing very well. Increasing the number of LSTM units to 1500 got us better validation score -but a worse overfit. Reducing the number of layers down to 2 got us the best overall validation score. In comparison, Xu et al. (2015) have used a single LSTM layer with 1000 cells.Note that the output layer receives skip connections from the LSTM-Stack input (Equation 13). We observed a 2% impact on the BLEU score with the addition of input-to- output skip-connections. This leads us to believe that adding skip-connections within the LSTM-stack may help further improve model accuracy. Overall accuracy also improved by increasing the number of layers from 2 to 3. Lastly, observe that this sub-model is different from Xu et al. (2015) wherein the three inputs are affine-transformed into D dimensions, summed and then passed through one fully-connected layer.After experimenting with their model we ultimately chose to instead feed the inputs (concatenated) to a fully-connected layer thereby allowing the MLP to naturally learn the input- to-output function. We also increased the number of layers to 3, changed activation function of hidden units from relu to tanh 101 and ensured that each layer had at least as many units as the softmax layer (K).We questioned the need for the Init Model and experimented just using zero values for the initial state. That caused a slight but consistent decline (&lt; 1%) in the validation score, indicating that the initial state learnt by our Initial State Model did contribute in some way towards learning and generalization. Note however that our Init Model is different than ( Xu et al., 2015), in that our version uses all L feature vectors of a while theirs takes the average. We also added a hidden layer and used tanh activation function instead of relu. We did start off with their version but that did not provide an appreciable impact to the bottom line (validation). This made us hypothesize that perhaps taking an average of the feature vectors was causing a loss of information; and we mitigated that by taking in all the L feature vectors without summing them. After making all these changes, the Init Model yields a consistent albiet small performance improvement (Table. 8). But given that it consumes ∼7.5 Figure 11. Attention scan continuing from Figure 5. The move- ment of focal-region is aligned with the output word (above the image). million parameters, its usefulness remains in question.Init  Table 8. Impact of the Init Model on overall performance. Since it comprises 10-12% of the total params, it may as well be omitted in exchange for a small performance hit.Default values of β 1 andβ 2 of the ADAM optimizer -0.9 and 0.99 -yielded very choppy validation score curves with frequent down-spikes where the validation score would fall to very low levels, ultimately resulting in lower peak scores.Reducing the first and second moments (i.e. β 1 andβ 2 ) fixed the problem suggesting that the default momentum was too high for our 'terrain'. We did not use dropout for regular- ization. However increasing the data-set size and raising the minimum-word-frequency threshold from 24 to 50 did yield better generalization and overall test scores (Table 7). Finally, normalizing the data (Section 2.5) yielded about 25% more accuracy than without.
