In spite of their impressive performance on challeng- ing tasks in computer vision such as image classifica- tion [17,33,34,35,11,12] and semantic segmentation [22,1,4,38,39], deep neural networks are shown to be highly vulnerable to adversarial examples, i.e. carefully crafted samples looking similar to natural images but de- signed to mislead a pre-trained model. This phenomenon was first studied in [36], and may hinder the applications of deep networks on visual tasks, or pose security concerns.Consider a classification network K trained on natural images from C different classes. The classifier assigns a label K(x) ∈ {1, . . . , C} to each input image x . We consider two types of adversarial perturbations: Uni- versal and Image-dependent. Image-dependent perturba- tions can vary for different images in the dataset. To gen- erate image-dependent perturbations, we require a function f : N → A K which takes a natural image, and outputs an adversarial image. We approximate this function with a deep neural network. Universal perturbations are fixed perturbations which when added to natural images can sig- nificantly degrade the accuracy of the pre-trianed network. In this case, we seek a perturbation U ∈ [0,1] n 1 . We assume that images are normalized to [0,1] range. Let N ⊂ [0,1] n represent the space of natural images 2 . We assume that K achieves a high accuracy on natural images. Therefore, if we denote the correct class for image x by c x , we have K(x) = c x for most x ∈ N. Let A K stand for the space of adversarial examples for the network K. Images in with small magnitude such that for most x ∈ N, x + U ∈ A K . Un- like the iterative approaches proposed in the literature, we consider trainable networks for learning the universal per- turbation.From another viewpoint, adversarial attacks can be cat- egorized as targeted and non-targeted. In targeted adver- sarial attacks, we seek adversarial images that can change the prediction of a model to a specific target label. In non- targeted attacks we want to generate adversarial examples for which the model's prediction is any label other than the ground-truth label. Considering all the possible com- binations, we can have four types of adversarial examples: targeted universal, non-targeted universal, targeted image- dependent and non-targeted image-dependent. We elabo- rate on each of them in the following sections.First introduced in [24], universal perturbations are fixed perturbations which after being added to natural images can mislead a pre-trained model for most of the images. The algorithm in [24] iterates over samples in a target set, and gradually builds the universal perturbation by aggre- gating image-dependent perturbations and normalizing the result. [27] presents a data independent approach for gener- ating image-agnostic perturbations. Its objective is to max- imize the product of mean activations at multiple layers of the network when the input is the universal perturbation. They constrain the perturbation to have a norm smaller than a fixed threshold. While this method obviates the need for training data, the results are not as strong as [24]. A method for generating targeted universal adversarial pertur- bations for semantic segmentation models is presented in [23]. Their approach is similar to [24] in that they also cre- ate the universal perturbation by adding image-dependent perturbations and clipping the result to limit the norm. [25] proposes a quantitative analysis of the robustness of classi- fiers to universal perturbations based on the geometric prop- erties of decision boundaries. when they are combinatorial and non-decomposable. [28] generates images unrecognizable to humans but classified with high confidence as members of a recognizable class. It uses evolutionary algorithms and gradient ascent to fool deep neural networks. Our work bears a resemblance to [2] in that it also considers training a network for generating ad- versarial examples. However, [2] does not provide a fixed bound on the perturbation magnitude, which might make perturbations detectable at inference time. It is also limited to targeted image-dependent perturbations. [37] extends ad- versarial examples from the task of image classification to semantic segmentation and object detection. For each im- age, it applies gradient ascent in an iterative procedure until the number of correctly predicted targets becomes zero or a maximum iteration is reached. Similar to [36] and [3], this method suffers from being slow at inference time.In this section we discuss our approach for generating adversarial examples.Various approaches have been proposed for creating image-dependent adversarial examples.Optimization- based approaches such as [36] and [3] define a cost function based on the magnitude of the perturbation and a measure of difference between predictions of the model and the tar- get label. Then they use gradient ascent in pixel space with optimizers such as L-BFGS or Adam [16] to create the per- turbation. While these approaches yield better results than other methods, they are slow at inference time as they need to forward the input to the model several times, and update it until it can fool the classifier.[10] proposes a Fast Gradient Sign Method (FGSM) to generate adversarial examples. It computes the gradient of the loss function with respect to pixels, and moves a single step based on the sign of the gradient. While this method is fast, using only a single direction based on the linear ap- proximation of the loss function often leads to sub-optimal results. Based on this work, [26] presents an iterative al- gorithm to compute the adversarial perturbation by assum- ing that the loss function can be linearized around the cur- rent data point at each iteration. [18] introduces the Itera- tive Least-Likely Class method, an iterative gradient-based method choosing the least-likely prediction as the desired class. This method is applied to ImageNet in [19]. It also discusses how to effectively include adversarial examples in training to increase model's robustness. [5] proposes a method for directly optimizing performance measures, even Universal Perturbations were first proposed in the sem- inal work of Dezfooli et al. [24]. The paper proposes an iterative algorithm to generate the universal perturba- tion. It constructs the universal perturbation by adding image-dependent perturbations obtained from [26] and scal- ing the result. Unlike the iterative approach of [24], we seek an end-to-end trainable model for generating a quasi- imperceptible universal perturbation. Let us denote the set of universal perturbations for the network K by U K = {U ∈ [0,1] n | for most x ∈ N : x + U ∈ A K }. In this case, we do not want the perturbation to directly depend on any input image from the dataset. We seek a function f : [0,1] n → U K which can transform any random pattern to a universal perturbation. By changing the input pattern, we can obtain a diverse set of universal perturbations. In practice, we approximate f (·) with a deep neural network f Θ (·) with weights Θ. This setting resembles Generative Adversarial Networks (GANs) [9,31,8,20,13] in which a random vector is sampled from a latent space, and is trans- formed to a natural-looking image by a generator. In our case the range of the mapping is U K instead of N, and the generator is trained with a fooling loss instead of the dis- criminative loss used in GANs. We also tried using a com- bination of fooling and discriminative losses; however, it led to sub-optimal results.There are several options for the architecture of the im- age transformation network f Θ (·). We consider two ar- chitectures used in recent image-to-image translation net- works such as [14] and [40]. The U-Net architecture [32] is an encoder-decoder network with skip connections be- tween the encoder and the decoder. The other architecture is Figure 1: Training architecture for generating universal adversarial perturbations. A fixed pattern, sampled from a uniform distribution, is passed through the generator. The scaled result is the universal perturbation which, when added to natural images, can mislead the pre-trained model. We consider both U-Net (illustrated here) and ResNet Generator architectures.ResNet Generator which was introduced in [15], and is also used in [40] for transforming images from one domain to another. It consists of several downsampling layers, residual blocks and upsampling layers. In most of our experiments, the ResNet Generator outperforms U-Net. Figure 1 illustrates the architecture for generating uni- versal perturbations. A fixed pattern Z ∈ [0,1] In practice, we found that losses defined in equations 1 and 2 lead to competitive results.For targeted perturbations we consider the cross-entropy with the one-hot encoding of the target:, is fed to a generator f Θ to create the perturbation. The output of the generator f Θ (Z) is then scaled to have a fixed norm. More specifi- cally, we multiply it by min 1, where t represents the target. Note that for the classification task, t ∈ {1, . . . , C} is the target class while in semantic segmentation, t ∈ {1, . . . , C} n is the target label map. in which is the maximum permissible L p norm. Similar to related works in the literature, we consider p = 2 and p = ∞ in exper- iments. The resulting universal perturbation U is added to natural images to create the perturbed ones. Before feeding the perturbed image to the generator, we clip it to keep it in the valid range of images on which the network is trained. Since we are dealing with perturbations with small norm, this has a minor effect on the perturbed image. We feed the clipped imagê x to the network K to obtain the output probabilities k(ˆ x). Let 1 cx denote the one-hot encoding of the ground-truth for image x. In the case of semantic seg- mentation c x ∈ {1, . . . , C} n is the ground-truth label map, and k(ˆ x) contains the class probabilities for each pixel inˆx inˆ inˆx. For non-targeted attacks we want the prediction k(ˆ x) to be different from 1 cx . Hence, we define the loss for train- ing the generator to be a decreasing function of the cross- entropy H(k(ˆ x), 1 cx ). We found that the following fooling loss gives good results in experiments:Alternatively, as proposed by [18] and [19], we can consider the least likely class k ll (x) = arg min k(x), and set it as the target for training the model:We consider the task of perturbing images as a transfor- mation from the domain of natural images to the domain of adversarial images. In other words, we require a map- ping f : N → A K which generates a perturbed image f (x) ∈ A K for each natural image x ∈ N. A desirable function f (·) must result in a low accuracy and a high fool- ing ratio. Accuracy denotes the proportion of samples x for which K(f (x)) = c x , while fooling ratio represents the ra- tio of images x for which K(f (x)) = K(x), i.e. predictions of the pre-trained network K are different for the original image x and the perturbed image f (x). Since we assume that the model achieves a high accuracy on natural images, these two metrics are highly correlated.We consider two slightly different approaches for ap- proximating f (·). The first approach is to parametrize it directly using a neural network f Θ (·). Hence, we seek Θ such that for most x ∈ N: K(f Θ (x)) = K(x). We also require that the perturbed image f Θ (x) look similar to the original image x. Hence, d(x, f Θ (x)) needs to be small for most x ∈ N, where d(·, ·) is a proper distance function.The second approach is to approximate the difference of natural and adversarial images with a neural network f Θ (·). We require that for most x ∈ N : K(x + f Θ (x)) = K(x) ≈ c x , and the L p norm of the additive perturbation Θ (x) p needs to be small in order for it to be quasi-imperceptible. We empirically found that the second approach leads to bet- ter results. Hence, we will focus on this approach hereafter.   Input image x is passed through the generator to create the perturbation f Θ (x). The pertur- bation is then scaled to constrain its norm. The result is the image-dependent perturbation which is added to the input image. We feed the clipped imagê x to the network to ob- tain the output probabilities k(ˆ x). For non-targeted attacks we use a loss function similar to the universal case:For targeted perturbations we consider a loss similar to equation 3:is a distance function, is a pre-specified threshold and c x is the ground-truth for x. We can con- sider both universal and image-dependent perturbations. In the case of universal perturbations, we seek a mapping F : [0, 1] n → A K generating adversarial examples from in- put patterns. In practice, the function is approximated with a deep neural network F Θ . Figure 3 depicts the correspond- ing architecture. It is similar to figure 1 other than that the resulting perturbed imagê x is fed to each of the pre-trained models. The loss function for training the generator is a lin- ear combination of fooling losses of pre-trained models as defined in equations 1-5. Hence, we have: in which t represents the target.At inference time, we can discard the pre-trained model, and use only the generator to produce adversarial examples. This obviates the need for iterative gradient computations, and allows us to generate perturbations fast.in which {λ 1 , . . . , λ m } ⊂ IR is a set of weights cho- sen based on the difficulty of deceiving each target model. The architecture for image-dependent perturbations is sim- ilar except that inputs to the generator are natural images.Using generative models for creating adversarial pertur- bations enables us to train sophisticated models. For in- stance, we can consider training a single model for mislead- ing multiple networks simultaneously. Suppose we have models K 1 , K 2 , . . . , K m trained on natural images. Let A K denote the space of adversarial examples for these tar- get models, i.e.We generate adversarial examples for fooling classifiers pre-trained on the ImageNet dataset [7]. For the Euclidean distance as the metric, we scale the output of the generator to have a fixed L 2 norm. We can also scale the generator's output to constrain its maximum value when dealing with the L ∞ norm. All results are reported on the 50,000 images   Non-targeted Universal Perturbations. This setting cor- responds to the architecture in figure 1 with the loss func- tions defined in equations 1 and 2. Results are given in Ta- bles 1 and 2 for L 2 and L ∞ norms respectively. For most cases our approach outperforms that of [24]. Similar to [24], a value of 2000 is set as the L 2 -norm threshold of the uni- versal perturbation, and a value of 10 is set for the L ∞ -norm . We use U- Net and ResNet Generator for L 2 and L ∞ norms respec- tively. We visualize the results in figure 4. Notice that the L 2 perturbation consists of a bird-like pattern in the top left. Intuitively, the network has learned that in this constrained problem it can successfully fool the classifier for the largest number of images by converging to a bird perturbation. On the other hand, when we optimize the model based on L ∞ norm, it distributes the perturbation to make use of the max- imum permissible magnitude at each pixel. More examples are shown in figure 10.Targeted Universal Perturbations. In this case we seek a single pattern which can be added to any image in the dataset to mislead the model into predicting a specified tar- get label. We perform experiments with fixed L ∞ norm of 10, and use the ResNet generator for fooling the Inception- v3 model. We use the loss function defined in equation 3  to train the generator. Figure 5 depicts the perturbations for various targets. It also shows the top-1 target accuracy on the validation set, i.e. the ratio of perturbed samples clas- sified as the desired target. We observe the the universal perturbation contains patterns resembling the target class. While this task is more difficult than the non-targeted one, our model achieves high target accuracies. To the best of our knowledge, we are the first to present effective targeted universal perturbations on the ImageNet dataset. Figure 11 illustrates more examples. To make sure that the model performs well for any target, we train it on 10 randomly sampled classes. The resulting average target accuracy for L ∞ = 10 is 52.0%, demonstrating generalizability of the model across different targets. 4.2. Image-dependent Perturbations [3] proposes a strong method for creating targeted image-dependent perturbations. However, its iterative al- gorithm is very slow at inference time. It reports attacks that take several minutes to run for each image, making it infeasible in real-time scenarios in which the input image changes constantly. FGSM [10] is a fast attack method but is not very accurate. In this work, we present adversarial attacks that are both fast and accurate. Non-targeted Image-dependent Perturbations. The cor- responding architecture is given in figure 2 with the loss function defined in equation 4. We use ResNet genera- tor with 6 blocks for generating the perturbations. Similar to most of the related works on image-dependent pertur- bations, we focus on L ∞ norm as the metric. Results are shown for various perturbation norms and pre-trained clas- sifiers in Table 3. Figure 6 illustrates the perturbed images. In this case the model converges to simple patterns which can change the prediction for most images. As we observe, the perturbations contain features from the corresponding input images. More examples are shown in the figure 12.turbed images for fooling the Inception-v3 model. The per- turbations are barely perceptible, yet they can obtain high target accuracies. Moreover, the perturbation itself has fea- tures resembling the target class and the input image. See figure 13 for more examples. We also evaluate performance of the model on 10 randomly sampled classes. The average target accuracy for L ∞ = 10 is 89.1%, indicating generaliz- ability of the proposed model across different target classes. The average inference time for generating a perturbation to fool the Inception-v3 model is 0.28 ms per image, showing that our method is considerably faster than [3] 7 . Refer to the appendix for a detailed runtime analysis.Targeted Image-dependent Perturbations. For this task we use the training scheme shown in figure 2 with the loss function in equation 5. Figure 13 shows samples of per- Several works have demonstrated that adversarial exam- ples generated for one model may also be misclassified by other models. This property is referred to as transferability, and can be leveraged to perform black-box attacks against target models [36,10,29,30,21]. We show that our gener- ated perturbations can be transferred across different mod-VGG16 VGG19 ResNet152 VGG1693.9% 89.6% 52.2% VGG1988.0% 94.9% 49.0% ResNet15231.9% 30.6% 79.5% VGG16 + VGG19 90.5% 90.1% 54.1% Table 4: Transferability of non-targeted universal perturba- tions. The network is trained to fool the pre-trained model shown in each row, and is tested on the model shown in each column. Perturbation magnitude is set to L 2 = 2000. The last row indicates joint training on VGG-16 and VGG-19.(a) Target: Soccer Ball, Top-1 target accuracy: 91.3%are hand-engineered for the specific task, and are slow at in- ference. We demonstrate that our proposed architectures are generalizable across different tasks. More specifically, we show that architectures similar to those used in the classi- fication task yield strong results on fooling segmentation models. We leave extension to tasks other than classifi- cation and segmentation as future work. Experiments are performed on the Cityscapes dataset [6]. It contains 2975 training and 500 validation images with a resolution of 2048 × 1024 pixels. Similar to [23], we downsample im- ages and label maps to 1024 × 512 pixels using bilinear and nearest-neighbor interpolation respectively. els. Table 4 shows the fooling ratio of a non-targeted uni- versal attack trained on one network and evaluated on other networks. Each row corresponds to the pre-trained model based on which the attack model is learned. The last row of the table corresponds to a model trained to jointly mislead VGG-16 and VGG-19 models based on the architecture de- picted in figure 3. We see that joint optimization results in better transferability than training on a single target net- work. This is expected as the network has seen more models during training, so it can generalize better to unseen models.We first consider the more challenging case of targeted attacks in which a desired target label map is given. We use the same setting as in the classification task, i.e. the train- ing architecture in figure 1 with the fooling loss defined in equation 3. In order for our results to be comparable with [23], we consider FCN-8s [22] as our segmentation model, and use L ∞ norm as the metric. Our setting corresponds to the static target segmentation in [23]. We use the same target as the paper, and consider our performance metric to be success rate, i.e. the categorical accuracy between the prediction k(ˆ x) and the target t. Table 5 demonstrates our results. Our method outperforms the algorithm proposed in [23] for most of the perturbation norms. We also visualize the results in figure 8. We observe that the generator fools the segmentation model by creating a universal perturbation which resembles the target label map. Additional examples are shown in figures 14-16. We also demonstrates the re- sulting mean IoU for non-targeted attacks in Table 6, and figures 20-22 depict samples of non-targeted universal per- turbations with L ∞ = 5, 10 and 20 respectively.Current methods for fooling semantic segmentation models such as [37] and [23] use iterative algorithms, whichThe targeted image-dependent task corresponds to the architecture in figure 2 with the loss function in equation 5. We use the same target as the universal case. Results 79.5% 92.1% 97.2% UAP-Seg [23] 80.3% 91.0% 96.3%98.2% Table 5: Success rate of targeted universal perturbations for fooling the FCN-8s segmentation model. Results are ob- tained on the validation set of the Cityscapes dataset.  for various norms are given in Table 7. As we expect, re- laxing the constraint of universality leads to higher success rates. Figure 9 illustrates the perturbations for L ∞ = 10. By closely inspecting the perturbations, we can observe pat- terns from both the target and the input image. More ex- amples are given in figures 17-19. Finally, non-targeted image-dependent perturbations are shown in figures 23-25. In this case, the predictions for perturbed images are usu- ally either road or background, and the perturbations have a repetitive structure. As shown in In this paper, we demonstrate the efficacy of genera- tive models for creating adversarial examples. Four types of adversarial attacks are considered: targeted universal, non-targeted universal, targeted image-dependent and non- targeted image-dependent. We achieve high fooling rates on all tasks in the small perturbation norm regime. The perturbations can successfully transfer across different tar- get models. Moreover, we demonstrate that similar archi- tectures can be effectively used for fooling both classifica- tion and semantic segmentation models. This eliminates the need for designing task-specific attack methods, and paves the way for extending adversarial examples to other tasks. Future avenues of research include incorporating various properties such as transformation-invariance into the per- turbations and extending the proposed framework to tasks other than classification and semantic segmentation. Note that inference time is not an issue for universal per- turbations as we just need to add the perturbation to the in- put image during inference. Therefore, we provide running time only for image-dependent perturbations. In this case, we need to forward the input image to the generator and get the resulting perturbation. Table 8 demonstrates the infer- ence time for image-dependent perturbations. It also shows the generator's architecture for each task including the num- ber of filters in the first layer. We perform model-level par- allelization across two GPUs, and batch size is set to be one. Notice that inference time is in the order of milliseconds, allowing us to generate perturbations in real-time. Table 9 shows inference time for the segmentation task. Two archi- tectures with similar performance are given. Here we deal with 1024 × 512 images in the Cityscapes dataset, and we need models with more capacity; hence, the inference time is larger compared with the classification task.Architecture   
