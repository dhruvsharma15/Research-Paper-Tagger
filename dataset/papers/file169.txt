Many cognitive tasks require learning useful representations on multiple abstraction levels. Hier- archical latent variable models are an appealing approach for learning a hierarchy of abstractions. The classical way of learning such models is by postulating an explicit parametric model for the distributions of random variables. The inference procedure, which evaluates the posterior distribution of the unknown variables, is then derived from the model -an approach adopted in probabilistic graphical models (see, e.g., [5]).The success of deep learning can, however, be explained by the fact that popular deep models focus on learning the inference procedure directly. For example, a deep classifier like AlexNet [19] is trained to produce the posterior probability of the label for a given data sample. The representations that the network computes at different layers are related to the inference in an implicit latent variable model but the designer of the model does not need to know about them.However, it is actually tremendously valuable to understand what kind of inference is required by different types of probabilistic models in order to design an efficient network architecture. Ladder networks [22,28] are motivated by the inference required in a hierarchical latent variable model. By design, the Ladder networks aim to emulate a message passing algorithm, which includes a bottom-up pass (from input to label in classification tasks) and a top-down pass of information (from label to input). The results of the bottom-up and top-down computations are combined in a carefully selected manner.The original Ladder network implements only one iteration of the inference algorithm but complex models are likely to require iterative inference. In this paper, we propose a recurrent extension of the Ladder network for iterative inference and show that the same architecture can be used for temporal modeling. We also show how to use the proposed architecture as an inference engine in more complex models which can handle multiple independent objects in the sensory input. Thus, the proposed architecture is suitable for the type of inference required by rich models: those that can learn a hierarchy of abstractions, can handle temporal information and can model multiple objects in the input. In this paper, we present a recurrent extension of the Ladder networks which is conducive to iterative inference and temporal modeling. Recurrent Ladder (RLadder) is a recurrent neural network whose units resemble the structure of the original Ladder networks [22,28] (see Fig. 1a). At every iteration t, the information first flows from the bottom (the input level) to the top through a stack of encoder cells. Then, the information flows back from the top to the bottom through a stack of decoder cells. Both the encoder and decoder cells also use the information that is propagated horizontally. Thus, at every iteration t, an encoder cell in the l-th layer receives three inputs: 1) the output of the encoder cell from the level below e l−1 (t), 2) the output d l (t − 1) of the decoder cell from the same level from the previous iteration, 3) the encoder state s l (t − 1) from the same level from the previous iteration. It updates its state value s l (t) and passes the same output e l (t) both vertically and horizontally:The encoder cell in the bottom layer typically sends observed data (possibly corrupted by noise) as its output e 1 (t). Each decoder cell is stateless, it receives two inputs (the output of the decoder cell from one level above and the output of the encoder cell from the same level) and produces one outputwhich is passed both vertically and horizontally. The exact computations performed in the cells can be tuned depending on the task at hand. In practice, we have used LSTM [15] or GRU [8] cells in the encoder and cells inspired by the original Ladder networks in the decoder (see Appendix A).Similarly to Ladder networks, the RLadder is usually trained with multiple tasks at different abstrac- tion levels. Tasks at the highest abstraction level (like classification) are typically formulated at the highest layer. Conversely, the output of the decoder cell in the bottom level is used to formulate a low-level task which corresponds to abstractions close to the input. The low-level task can be denoising (reconstruction of a clean input from the corrupted one), other possibilities include object detection [21], segmentation [3,23], or in a temporal setting, prediction. A weighted sum of the costs at different levels is optimized during training.The RLadder architecture is designed to mimic the computational structure of an inference procedure in probabilistic hierarchical latent variable models. In an explicit probabilistic graphical model, inference can be done by an algorithm which propagates information (messages) between the nodes of a graphical model so as to compute the posterior distribution of the latent variables (see, e.g., [5]).For static graphical models implicitly assumed by the RLadder (see Fig. 1b), messages need to be propagated from the input level up the hierarchy to the highest level and from the top to the bottom, as shown in Fig. 1a. In Appendix B, we present a derived iterative inference procedure for a simple static hierarchical model to give an example of a message-passing algorithm. We also show how that inference procedure can be implemented in the RLadder computational graph.In the case of temporal modeling, the type of a graphical model assumed by the RLadder is shown in Fig. 1c. If the task is to do next step prediction of observations x, an online inference procedure should update the knowledge about the latent variables y t , z t using observed data x t and compute the predictive distributions for the input x t+1 . Assuming that the distributions of the latent variables at previous time instances (τ &lt; t) are kept fixed, the inference can be done by propagating messages from the observed variables x t and the latent variables y, z bottom-up, top-down and from the past to the future, as shown in Fig. 1c. The architecture of the RLadder (Fig. 1a) is designed so as to emulate such a message-passing procedure, that is the information can propagate in all the required directions: bottom-up, top-down and from the past to the future. In Appendix C, we present an example of the message-passing algorithm derived for a temporal hierarchical model to show how it is related to the RLadders's computation graph.Even though the motivation of the RLadder architecture is to emulate a message-passing procedure, the nodes of the RLadder do not directly correspond to nodes of any specific graphical model. 1 The RLadder directly learns an inference procedure and the corresponding model is never formulated explicitly. Note also that using stateful encoder cells is not strictly motivated by the message-passing argument but in practice these skip connections facilitate training of a deep network.As we mentioned previously, the RLadder is usually trained with multiple tasks formulated at different representation levels. The purpose of tasks is to encourage the RLadder to learn the right inference procedure, and hence formulating the right kind of tasks is crucial for the success of training. For example, the task of denoising encourages the network to learn important aspects of the data distribution [1,2]. For temporal modeling, the task of next step prediction plays a similar role. The RLadder is most useful in problems that require accurate inference on multiple abstraction levels, which is supported by the experiments presented in this paper.The RLadder architecture is similar to that of other recently proposed models for temporal modeling [10,11,9,27,20]. In [9], the recurrent connections (from time t − 1 to time t) are placed in the lateral links between the encoder and the decoder. This can make it easier to extend an existing feed-forward network architecture to the case of temporal data as the recurrent units do not participate in the bottom-up computations. On the other hand, the recurrent units do not receive information from the top, which makes it impossible for higher layers to influence the dynamics of lower layers. The architectures in [10,11,27] are quite similar to ours but they could potentially derive further benefit from the decoder-to-encoder connections between successive time instances (green links in Fig. 1b). The aforementioned connections are well justified from the message-passing point of view: When updating the posterior distribution of a latent variable, one should combine the latest information from the top and from the bottom, and it is the decoder that contains the latest information from the top. We show empirical evidence to the importance of those connections in Section 3.1.In this section, we demonstrate that the RLadder can learn an accurate inference algorithm in tasks that require temporal modeling. We consider datasets in which passing information both in time and in abstraction hierarchy is important for achieving good performance.We use a dataset where we know how to do optimal inference in order to be able to compare the results of the RLadder to the optimal ones. To this end we designed the Occluded Moving MNIST dataset. It consists of MNIST digits downscaled to 14 × 14 pixels flying on a 32 × 32 white background with white vertical and horizontal occlusion bars (4 pixels in width, and spaced by 8 visible pixels apart) which, when the digit flies behind them, occludes the pixels of the digit (see Fig. 2). We also restrict the velocities to be randomly chosen in the set of eight discrete velocities {(1, ±2), (−1, ±2), (2, ±1), (−2, ±1)} pixels/frame, so that apart from the bouncing, the movement is deterministic. The digits are split into training, validation, and test sets according to the original MNIST split. The primary task is then to classify the digit which is only partially observable at any given moment, at the end of five time steps.In order to do optimal classification, one would need to assimilate information about the digit identity (which is only partially visible at any given time instance) by keeping track of the observed pixels (see the bottom row of Fig. 2) and then feeding the resultant reconstruction to a classifier.In order to encourage optimal inference, we add a next step prediction task to the RLadder at the bottom of the decoder: The RLadder is trained to predict the next occluded frame, that is the network never sees the un-occluded digit. This thus mimics a realistic scenario where the ground truth is not known. To assess the importance of the features of the RLadder, we also do an ablation study. In addition, we compare it to three other networks. In the first comparison network, the optimal reconstruction of the digit from the five frames (as shown in Fig. 2) is fed to a static feed-forward network from which the encoder of the RLadder was derived. This is our gold standard, and obtaining similar results to it implies doing close to optimal temporal inference. The second, a temporal baseline, is a deep feed-forward network (the one on which the encoder is based) with a recurrent neural network (RNN) at the top only so that, by design the network can propagate temporal information only at a high level, and not at a low level. The third, a hierarchical RNN, is a stack of convolutional LSTM units with a few convolutional layers in between, which is the RLadder amputated of its decoder. See Fig. 3 and Appendix D.1 for schematics and details of the architectures.Fully supervised learning results. The results are presented in Table 1. The first thing to notice is that the RLadder reaches (up to uncertainty levels) the classification accuracy obtained by the network which was given the optimal reconstruction of the digit. Furthermore, if the RLadder does not have a decoder or the decoder-to-encoder connections, or if it is trained without the auxiliary prediction task, we see the classification error rise almost to the level of the temporal baseline. This means that even if a network has RNNs at the lowest levels (like with only the feed-forward encoder), or if it does not have a task which encourages it to develop a good world model (like the RLadder without the next-frame prediction task), or if the information cannot travel from the decoder to the encoder, the high level task cannot truly benefit from lower level temporal modeling.Next, one notices from Table 1 that the top-level classification cost helps the low-level prediction cost in the RLadder (which in turn helps the top-level cost in a mutually beneficial cycle). This mutually supportive relationship between high-level and low-level inferences is nicely illustrated by the example in Fig. 4. Up until time step t = 3 inclusively, the network believes the digit to be a fiveTemporal baseline network  Optimal reconstruction and static classifier 0.71 ± 0.03Temporal baseline 2.02 ± 0.16 Hierarchical RNN (encoder only)1.60 ± 0.05 RLadder w/o prediction task1.51 ± 0.21 RLadder w/o decoder-to-encoder conn.1.24 ± 0.05As such, at t = 3, the network predicts that the top right part of the five which has been occluded so far will stick out from behind the occlusions as the digit moves up and right at the next time step (Fig. 4b). Using the decoder-to-encoder connections, the decoder can relay this expectation to the encoder at t = 4. At t = 4 the encoder can compare this expectation with the actual input where the top right part of the five is absent (Fig. 4c). Without the decoder-to-encoder connections this comparison would have been impossible. Using the upward path of the encoder, the network can relay this discrepancy to the higher classification layers. These higher layers with a large receptive field can then conclude that since it is not a five, then it must be a three (Fig. 4d). Now thanks to the decoder, the higher classification layers can relay this information to the lower prediction layers so that they can change their prediction of what will be seen at t = 5 appropriately ( Fig. 4e). Without a decoder which would bring this high level information back down to the low level, this drastic update of the prediction would be impossible. With this information the lower prediction layer can now predict that the top-left part of the three (which it has never seen before) will appear at the next time step from behind the occlusion, which is indeed what happens at t = 5 (Fig. 4f).Semi-supervised learning results. In the following experiment, we test the RLadder in the semi- supervised scenario when the training data set contains 1.000 labeled sequences and 59.000 unlabeled ones. To make use of the unlabeled data, we added an extra auxiliary task at the top level which was the consistency cost with the targets provided by the Mean Teacher (MT) model [26]. Thus, the RLadder was trained with three tasks: 1) next step prediction at the bottom, 2) classification at the top, 3) consistency with the MT outputs at the top. As shown in Table 2, the RLadder improves dramatically by learning a better model with the help of unlabeled data independently and in addition to other semi-supervised learning methods. The temporal baseline model also improves the classification accuracy by using the consistency cost but it is clearly outperformed by the RLadder.In this section, we evaluate the RLadder on the midi dataset converted to piano rolls [6]. The dataset consists of piano rolls (the notes played at every time step, where a time step is, in this case, an eighth note) of various piano pieces. We train an 18-layer RLadder containing five convolutional LSTMs and one fully-connected LSTM. More details can be found in Appendix D.2. Table 3  Notice how at t = 1, the network does not yet know in which direction the digit will move and so it predicts a superposition of possible movements. Notice further (red annotations a-f), that until t = 3, the network thought the digit was a five, but when the top bar of the supposed five did not materialize on the other side of the occlusion as expected at t = 4, the network immediately concluded correctly that it was actually a three. negative log-likelhoods of the next-step prediction obtained on the music dataset, where our results are reported as mean plus or minus standard deviation over 10 seeds. We see that the RLadder is competitive with the best results, and gives the best results amongst models outputting the marginal distribution of notes at each time step.The fact that the RLadder did not beat [16] on the midi datasets shows one of the limitations of RLadder. Most of the models in Table 3 output a joint probability distribution of notes, unlike RLadder which outputs the marginal probability for each note. That is to say, those models, to output the probability of a note, take as input the notes at previous time instances, but also the ground truth of the notes to the left at the same time instance. RLadder does not do that, it only takes as input the past notes played. Even though, as the example in 3.1 of the the digit five turning into a three after seeing only one absent dot, shows that internally the RLadder models the joint distribution.In this section, we show that the RLadder can be used as an inference engine in a complex model which benefits from iterative inference and temporal modeling. We consider the task of perceptual grouping, that is identifying which parts of the sensory input belong to the same higher-level perceptual  [14] which also extends Tagger to a recurrent neural network, but does so from an expectation maximization point of view.Tagger is a model designed for perceptual grouping. When applied to images, the modeling assump- tion is that each pixel˜xpixel˜ pixel˜x i belongs to one of the K objects, which is described by binary variables z i,k : z i,k = 1 if pixel i belongs to object k and z i,k = 0 otherwise. The reconstruction of the whole image using object k only is µ µ µ k which is a vector with as many elements µ i,k as there are pixels. Thus, the assumed probabilistic model can be written as follows:where z k is a vector of elements z i,k and h k is (a hierarchy of) latent variables which define the shape and the texture of the objects. See Fig. 5a for a graphical representation of the model and Fig. 5b for possible values of model variables for the textured MNIST dataset used in the experiments of Section 4.2. The model in (4) is defined for noisy image˜ximage˜ image˜x because Tagger is trained with an auxiliary low-level task of denoising. The inference procedure in model (4) should evaluate the posterior distributions of the latent variables z k , µ µ µ k , h k for each of the K groups given corrupted datã x. Making the approximation that the variables of each of the K groups are independent a posteriorik the inference procedure could be implemented by iteratively updating each of the K approximate distributions q(z k , µ µ µ k , h k ), if the model (4) and the approximation (5) were defined explicitly.Tagger does not explicitly define a probabilistic model (4) but learns the inference procedure directly. The iterative inference procedure is implemented by a computational graph with K copies of the same Ladder network doing inference for one of the groups (see Fig. 5c). At the end of every iteration, the inference procedure produces the posterior probabilities π i,k that pixel i belongs to object k and the point estimates of the reconstructions µ µ µ k (see Fig. 5c). Those outputs, are used to form the low-level cost and the inputs for the next iteration (see more details in [13]). In this paper, we replace the original Ladder engine of Tagger with the RLadder. We refer to the new model as RTagger.The goal of the following experiment is to test the efficiency of RTagger in grouping objects using the texture information. To this end, we created a dataset that contains thickened MNIST digits with 20 textures from the Brodatz dataset [7]. An example of a generated image is shown in Fig. 6a. To create a greater diversity of textures (to avoid over-fitting), we randomly rotated and scaled the 20 Brodatz textures when producing the training data.The network trained on the textured MNIST dataset has the architecture presented in Fig. 5c with three iterations. The number of groups was set to K = 3. The details of the RLadder architecture are presented in Appendix D.3. The network was trained on two tasks: The low-level segmentation task was formulated around denoising, the same way as in the Tagger model [13]. The top-level cost was the log-likelihood of the digit class at the last iteration. Table 4 presents the obtained performance on the textured MNIST dataset in both fully supervised and semi-supervised settings. All experiments were run over 5 seeds. We report our results as mean plus or minus standard deviation. In some runs, Tagger experiments did not converge to a reasonable solution (because of unstable or too slow convergence), so we did not include those runs in our evaluations. Following [13], the segmentation accuracy was computed using the adjusted mutual information (AMI) score [29] which is the mutual information between the ground truth segmentation and the estimated segmentation π π π k scaled to give one when the segmentations are identical and zero when the output segmentation is random.For comparison, we trained the Tagger model [13] on the same dataset. The other comparison method was a feed-forward convolutional network which had an architecture resembling the bottom-up pass (encoder) of the RLadder and which was trained on the classification task only. One thing to notice is that the results obtained with the RTagger clearly improve over iterations, which supports the idea that iterative inference is useful in complex cognitive tasks. We also observe that RTagger outperforms Tagger and both approaches significantly outperform the convolutional network baseline in which the classification task is not supported by the input-level task. We have also observed that the top-level classification tasks makes the RTagger faster to train in terms of the number of updates, which also supports that the high-level and low-level tasks mutually benefit from each other: Detecting object  The same RTagger model can perform perceptual grouping in video sequences using motion cues. To demonstrate this, we applied the RTagger to the moving MNIST [25] 2 sequences of length 20 and the low-level task was prediction of the next frame. When applied to temporal data, the RTagger assumes the existence of K objects whose dynamics are independent of each other. Using this assumption, the RTagger can separate the two moving digits into different groups. We assessed the segmentation quality by the AMI score which was computed similarly to [13,12] ignoring the background in the case of a uniform zero-valued background and overlap regions where different objects have the same color. The achieved averageAMI score was 0.75. An example of segmentation is shown in Fig. 7. When we tried to use Tagger on the same dataset, we were only able to train successfully in a single seed out of three. This is possibly because speed is intermediate level of abstraction not represented at the pixel level. Due to its reccurent connections, RTagger can keep those representations from one time step to the next and segment accordingly, something more difficult for Tagger to do, which might explain the training instability.In the paper, we presented recurrent Ladder networks. The proposed architecture is motivated by the computations required in a hierarchical latent variable model. We empirically validated that the recurrent Ladder is able to learn accurate inference in challenging tasks which require modeling dependencies on multiple abstraction levels, iterative inference and temporal modeling. The proposed model outperformed strong baseline methods on two challenging classification tasks. It also produced competitive results on a temporal music dataset. We envision that the purposed Recurrent Ladder will be a powerful building block for solving difficult cognitive tasks.Decoder cells receive two inputs: vector v top from the top and vector h from the encoder and produce output y.where BN() is batch normalization and f (u, w) = w 0 sigmoid(w 1 u + w 2 ) + w 3 u + w 4 (6) with w i being the elements of vector w and the element-wise product. In the convolutional version ConvG1 of that cell, u in the first formula is computed with the convolution operation * :where f is defined in (6) and LN() is layer normalization.where LN() is layer normalization.Encoder: For all the experiments all non-recurrent layers in the encoder, whether convolutional or not, are normalized using batch normalization. In the convolution LSTM, there is a layer normalization after the all the first convolutions of the inputs in the RLadder experiments and batch normalization in the RTagger experiments, these normalizations were important to get things working.Decoder: In the decoder gating functions, there is always a layer normalization after the first convolution over the concatenation of the inputs in the RLadder experiments, and this normalization was replaced by batch normalization in the RTagger experiments. A normalization at this point was critical for optimal performance.The structure of the RLadder is designed to emulate a message-passing algorithm in a hierarchical latent variable model. To illustrate this, let us consider a simple hierarchical latent variable model with three one-dimensional variables whose joint probability distribution is given by p(x, y, z) = p(z)p(y|z)p(x|y)  where N (·|m, v) denotes the Gaussian probability density function with mean m and variance v. We want to derive a denoising algorithm that recovers clean observation x from its corrupted versioñ x, where the corruption is also modeled to be Gaussian:The reason we look at denoising is because denoising is a task which can be used for unsupervised learning of the data distribution [1,2]. The graphical representation of the model shown in Fig. 8a.In order to do optimal denoising, one needs to evaluate the expectation of p(x|˜xx|˜x), which can be done by learning the joint posterior distribution of the unknown variables x, y and z. For this linear Gaussian model, it is possible to derive an inference algorithm that is guaranteed to produce the exact posterior distribution in a finite number of steps (see, e.g., [5]). However, in more complex models, the posterior distribution of the latent variables is impossible to represent exactly and therefore a neural network that learns the inference procedure needs to represent an approximate distribution. To this end, we derive an approximate inference procedure for this simple probabilistic model. Using the variational Bayesian approach, we can approximate the joint posterior distribution by a distribution of a simpler form. For example, all the latent variables can be modeled to be independent Gaussian variables a posteriori:p(x, y, z|˜xz|˜x) ≈ q(x, y, z) = q(x)q(y)q(z),and the goal of the inference procedure is to estimate parameters m x , v x , m y , v y , m z , v z of the approximate posterior (12)-(15) for the model in (7)-(11) with fixed parametersThe optimal posterior approximation q(x, y, z) can be found by minimizing the lower bound of the Kullback-Leibler divergence between the true distribution and the approximation:where denotes the expectation over q(x, y, z). This can be done with the following iterative procedure:mz,t−1 y t−1 y t y t+1 y t−1 y t y t+1˜m t+1˜ t+1˜m y,t−1 m y,t my,t−1 Thus, in order to update the posterior distribution of a latent variable, one needs to combine the information coming from one level above and from one level below. For example, in order to update q(y), one needs information from below (m x /w x and w 2 x /σ 2 x ) and from above (w yz m z and 1/σ 2 y ). We can think of the information needed for updating the parameters describing the posterior distributions of the latent variables as 'messages' propagating between the nodes of a probabilistic graphical model (see Fig. 8b). Since there are mutual dependencies between m x , m y and m z in (17)-(19), the update rules need to be iterated multiple times until convergence. This procedure can be implemented using the RLadder computational graph with the messages shown in Fig. 8c. Note that in practice, the computations used in the cells of the RLadder are not dictated by any particular explicit probabilistic model. The original Ladder networks [22] contain only one iteration of the bottom-up and top-down passes. The RLadder extends the model to multiple passes. Note also that the computations used in the decoder (top-down pass) of the Ladder networks are inspired by the gating structure of the update rules (17)-(19) in simple Gaussian models.In this section we consider a simple hierarchical and temporal model and look at the relationship between temporal inferance in that model and the computational structure of RLadder. Consider a simple probabilistic model with three levels of hierarchy x t , y t , z t in which variables vary in time (see Fig. 9a). The conditional distributions given the latent variables in the past y t−1 , z t−1 are defined as follows: The cost function minimized in the variational Bayesian approach is:where the expectation is taken over q(y t , z t , y t−1 , z t−1 , . . . , y 1 , z 1 ) = t s=1 q(y s )q(z s ). Let us elaborate the terms of C VB . The first term isThe second term is (mThe last term is a function of variational parameters m y,t−1 , m z,t−1 , v y,t−1 , v z,t−1 which we do not update at time instance t. Taking the derivative of C VB wrt the variational parameters yields:Equating the derivatives to zero yields:These computations can be done by first estimating m y,t , m z,t before observing x t and then correcting using the observation. Let us show how it is done for y. We define m y,t to be the posterior mean before observing x t (next step decoder prediction), ˜ m y,t (encoder posterior) to be the posterior means after observing x t but before updating the approximate posterior of higher latent variables (z in this case), and finally m y,t to be the posterior mean after observing x t and updating all posteriors (decoder, same step prediction). Thus, we can write m y,t = w y m y,t−1 + w yz m z,ty which if we eliminate the tilded and primed m's, give exactly the same equations as previously.Generalizing the results for an arbitrary number of variables in the chain we have that for a generic level l in a hierarchical chain, we have:These equations suggest a computational graph as shown in Fig. 9c. This is, if we literally interpret˜m interpret˜ interpret˜m to be the activations of the encoder and m and m to be the activations of the decoder, then we would conclude that we need upward-diagonal (the grey dotted arrow in Fig. 9c) decoder to encoder connections, and a recurrent decoder (curved dotted arrow in Fig. 9c). But we impose no such constraint, and the dotted lines are in fact only skip connections. So as long an information can be copied through accross neural network layers (which it can), and as long as each layer has sufficient capacity, we can remove those skip connections and retain the same computational capabilities. This is what is done in RLadder. However, it would be interesting to see if adding those skip connections help with training on temporal tasks.Using gatings in the bottom-up computations are justified by (28). Even though the top-down updates (27) and (31) are simple summations in the case of this linear Gaussian model, the more general gatings in the top-down pass provides more flexibility for nonlinear models. The activations in the level above can select one of a few possible evolution scenarios on the level below.As we discussed previously, RLadder networks are usually trained by introducing tasks that encourage the network to learn the optimal inference of the latent variables. For temporal models, the natural low-level task is predicting the input at the next time instance. Thus the cost is − log(p(x t+1 |x 1..t )):if we assume a Gaussian distribution forˆxforˆ forˆx.The RLadder architecture trained for the occluded moving MNIST dataset is presented in Table 5. The architecture of the baseline temporal model and the static feed-forward network trained on optimal digit reconstructions are shown in Tables 6-7. Initially, we trained the network only on the training set using the Adam optimizer [18] with an initial learning rate of 0.001. Every time the classification error on the validation set rose, we divided the learning rate by 2 until a minimum value of 0.0001. We then trained the network using both the training and validation sets following the learning rate schedule learned previously.   The RLadder architecture trained for the music dataset is presented in Table 8. We trained using the Adam optimizer with a learning rate of 0.01. Because the network overfited very rapidly, we determined the optimal stopping time using the validation set and then trained on the training and validation set for the same number of epochs (between 3 and 10 depending on the hyperparameters). The RTagger architecture used to train on Brodatz-textured MNIST data is in Table 9. The baseline convolutional network is model C from [24]. The networks were trained using the Adam optimizer with learning rate 0.0004. All training results were evaluated at update 160000. The size of the mini-batch was 16 for all the models. Batch normalization was applied to encoder and decoder layers. 
