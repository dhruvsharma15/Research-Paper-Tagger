The recurrent neural network (RNN) has proved to be an effective solution for natural language processing (NLP) through the advancement in the last three decades [1,2]. At the cell level, the long short-term memory (LSTM) [3] and the gated recurrent unit (GRU) [4] are often adopted by an RNN as its low- level building element. Built upon these cells, various RNN models have been proposed to solve the SISO problem. To name a few, there are the bidirectional RNN (BRNN) [5], the encoder-decoder model [4,6,7,8] and the deep RNN [9].LSTM and GRU cells were designed to enhance the memory length of RNNs and address the gradient vanishing/exploding issue [3,10,11], yet thorough analysis on their memory decay property is lacking. The first objective of this research is to analyze the memory length of three RNN cells -simple RNN (SRN) [1,2], LSTM and GRU. It will be conducted in Sec. 2. Our analysis is different from the investigation of gradient vanishing/exploding problem in the following sense. The gradient vanishing/exploding problem occurs in the training process while memory analysis is conducted on a trained RNN model.Based on the analysis, we further propose a new design in Sec. 3 to extend the memory length of a cell, and call it the extended long short-term memory (ELSTM).As to the macro RNN model, one popular choice is the BRNN [5]. Since elements in BRNN output sequences should be independent of each other, the BRNN cannot be used to solve dependent output sequence problem alone. Nev- ertheless, most language tasks do involve dependent output sequences. Another choice is the encoder-decoder system, where the attention mechanism was intro- duced to improve its performance in [7,8]. We show that the encoder-decoder system is not an efficient learner by itself. A better solution is to exploit the encoder-decoder and the BRNN jointly so as to overcome their individual lim- itations. Following this line of thought, we propose a new multi-task model, called the dependent bidirectional recurrent neural network (DBRNN), in Sec.To demonstrate the performance of the DBRNN model with the ELSTM cell, we conduct a series of experiments on the part of speech (POS) tagging and the dependency parsing (DP) problems in Sec. 5. Finally, concluding remarks are given and future research direction is pointed out in Sec. 6.For a large number of NLP tasks, we are concerned with finding seman- tic patterns from input sequences. It was shown by Elman [1] that an RNN builds an internal representation of semantic patterns. The memory of a cell characterizes its ability to map input sequences of certain length into such a representation. Here, we define the memory as a function that maps elements of the input sequence to the current output. Thus, the memory of an RNN is not only about whether an element can be mapped into the current output but also how this mapping takes place. It was reported by Gers et al. [12] that an SRN only memorizes sequences of length between 3-5 units while an LSTM could memorize sequences of length longer than 1000 units. In this section, we conduct memory analysis on SRN, LSTM and GRU cells.For ease of analysis, we begin with Elman's SRN model [1] with a linear hidden-state activation function and a non-linear output activation function since such a cell model is mathematically tractable while its performance is equivalent to Jordan's model [2] and Tensorflow variations.The SRN model can be described by the following two equations:where subscript t is the time unit index, W c ∈ R N ×N is the weight matrix for hidden-state vector c t−1 ∈ R N , W in ∈ R N ×M is the weight matrix of input vector X t ∈ R M , h t ∈ R N in the output vector, and f (·) is an element-wise non-linear activation function. Usually, f (·) is a hyperbolic-tangent or a sigmoid function. Throughout this paper, we omit the bias terms by including them in the corresponding weight matrices. The multiplication between two equal-sized vectors in this paper is element-wise multiplication.By induction, c t can be written ask=1 where c 0 is the initial internal state of the SRN. Typically, we set c 0 = 0. Then,Let λ max be the largest singular value of W c . Then, we haveHere, we are only interested in the case of memory decay when λ max &lt; 1. Since the contribution of X k , k &lt; t, to output h t decays at least in form of λ |t−k| max , we conclude that SRN's memory decays exponentially with its memory length |t − k|. By following the work of Hochreiter et al. [3], we plot the diagram of the LSTM cell in Fig. 1. In this figure, φ, σ and ⊗ denote the hyperbolic tangent function, the sigmoid function and the multiplication operation, respectively.All of them operate in an element-wise fashion. The LSTM cell has an input gate, an output gate, a forget gate and a constant error carousal (CEC) module.Mathematically, the LSTM cell can be written aswhere c t ∈ R N , column vector I t ∈ R (M +N ) is a concatenation of the current input, X t ∈ R M , and the previous output,and W in are weight matrices for the forget gate, the input gate, the output gate and the input, respectively.Under the assumption c 0 = 0, the hidden-state vector of the LSTM can be derived by induction as We see from the above that W t−k c and t j=k+1 σ(W f I j ) play the same memory role for the SRN and the LSTM, respectively.As given in Eqs. (5) and (11), the impact of input I k on the output of the LSTM lasts longer than that of the SRN. This is the case if an appropriate weight matrix, W f , of the forget gate is selected.The GRU was originally proposed for neural machine translation [4]. It provides an effective alternative for the LSTM. Its operations can be expressed by the following four equations:where X t , h t , z t and r t denote the input, the hidden-state, the update gate and the reset gate vectors, respectively, and W z , W r , W , are trainable weight matrices. Its hidden-state is also its output, which is given in Eq. (15). By setting U z , U r and U to zero matrices, we can obtain the following simplified GRU system:For the simplified GRU with the initial rest condition, we can derive the follow- ing by induction: As discussed above, the LSTM and the GRU have longer memory lengths by introducing the forget and the update gates, respectively. However, from Eqs.(10) and (11), we see that the impact of the proceeding element to the current output at time step t still decays quickly. However, this does not have to be the case. To demonstrate this point, we will present a new model and call it the extended long short-term memory (ELSTM) in this section. In particular, we propose the following two ELSTM cells:• ELSTM-I: the ELSTM with trainable input scaling vectors s i ∈ R N , i = 1, · · · , t − 1, where s i and s j (with i = j) are independent.• ELSTM-II: the ELSTM-I with no forget gate.They are depicted in Figs. 2 (a) and (b), respectively. The ELSTM-I cell can be described bywhere b ∈ R N is a trainable bias vector. The ELSTM-II cell can be written asAs shown above, we introduce scaling factor, s i , i = 1, · · · , t−1, to the ELSTM-I and the ELSTM-II to increase or decrease the impact of input I i in the sequence.To prove that the ELSTM-I has longer memory than the LSTM, we first derive a closed form expression of h t asThen, we select s k such thatBy comparing Eq. (25) with Eq. (11), we conclude that the ELSTM-I has longer memory than the LSTM. It is important to emphasize that we only need to show the sufficient but not necessary conditions such as those in Eq. (25) to explain the memory capability of an RNN system. In other words, there is no need to argue whether such a system is always capable in retaining longer memory as compared to some other systems. As a matter of fact, retaining longer memory is not always desired, which is the reason why a forgetting gate [12] is introduced for LSTM. On the other hand, if the memory of a RNN model is limited by design, it would have negative impact on its performance.To examine the memory capability problem from this viewpoint, we find that scaling factors s k play a role similar to the attention score in various attention models such as Vinyals et al. [7]. The impact of proceeding elements to the current output can be adjusted (either increased or decreased) by s k . The memory capability of the ELSTM-II can be proven in a similar fashion. We should point out that, even the ELSTM-II does not have a forget gate, it can attend or forget a particular position of a sequence as the ELSTM-I through the scaling factor. On the other hand, fewer parameters are used in the ELSTM-II than the ELSTM-I. The numbers of parameters used by various RNN cells are compared in Table 1, where As shown in Table 1, the number of parameters of the ELSTM cell depends on the maximum length, T , of the input sequences, which makes the model size uncontrollable. To address this problem, we choose a fixed T s (with T s &lt; T ) as the upper bound on the number of scaling factors, and set s k = s (k−1) mod Ts+1 , if k &gt; T s and k starts from 1, where mod denotes the modulo operator. In other words, the sequence of scaling factors is a periodic one with period T s , so the elements in a sequence that are distanced by the length of T s will share the same scaling factor.The ELSTM-I cell with periodic scaling factors can be described bywhere t s = (t − 1) mod T s + 1. Similarly, the ELSTM-II cell with periodic scaling factors can be written asWe observe that the choice of T s affects the network performance. Generally speaking, a small T s value is suitable for simple language tasks that demand shorter memory while a larger T s value is desired for complex ones that demand longer memory. For the particular sequence-to-sequence (seq2seq [6,7]) RNN models, a larger T s value is always preferred. We will elaborate the parameter settings in Sec. 5.We are interested in the problem of predicting an output sequence,with The BRNN is used to model the conditional probability density function in form ofIts output is a combination of the output of a forward RNN and the output of a backward RNN. Due to the bidirectional design, the BRNN can utilize the information of the entire input sequence to predict each individual output ele- ment. However, the BRNN does not exploit the predicted output in predicting Y t , and elements in the predicted sequenceare generated from the input sequences only. Since the prediction of each in- dividual output is conducted independently, the inherent correlation among elements in the output sequence is not utilized during the prediction stage. The correlation can be maximally captured by explicitly feeding the previous pre- dicted outputs back to the system to avoid the mis-alignment [8] problem. This idea is exploited by the encoder-decoder system in form ofOn the other hand, the encoder-decoder system is vulnerable to previous er- roneous predictions in the forward path. Recently, the BRNN was introduced to the encoder by Bahdanau et al. [8], yet their design does not address the erroneous prediction problem.Being motivated by observations in Sec. 4.1, we propose a multi-task BRNN model, called the dependent BRNN (DBRNN), to achieve the following objec- tives:and W f and W b are trainable weights. As shown in Eqs. (31), (32)    where u denotes the upper BRNN. To generate forward predictionˆY predictionˆ predictionˆY To show that DBRNN is more robust to previous erroneous predictions than one-directional models, we compare their cross entropy defined aswhere K is the total number of classes (e.g. the size of vocabulary for the language task), ˆ p t is the predicted distribution, and p t is the ground truth distribution with k as the ground truth label. It is in form of one-hot vector.That is,where δ k,k is the Kronecker delta function. Based on Eq. (30), l can be further expressed asWeThe above two equations indicate that the DBRNN can have a cross entropy lower than those of one-directional predictions by pooling opinions from both the forward and the backward predictions. The encoder-decoder design is adopted by [13] but not by the DBRNN.In the experiments, we compare the performance of five RNN macro-models:2. bidirectional RNN (BRNN);3. sequence-to-sequence (seq2seq) RNN [6] (a variant of the encoder-decoder);4. seq2seq with attention [7];5. dependent bidirectional RNN (DBRNN), which is proposed in this work.For each RNN model, we compare four cell designs: LSTM, GRU, ELSTM-I and ELSTM-II.We conduct experiments on two problems: part of speech (POS) tagging and dependency parsing (DP). We report the testing accuracy for the POS tagging problem and the unlabeled attachment score (UAS) and the labeled attachment score (LAS) for the DP problem. The POS tagging task is an easy one which requires shorter memory while the DP task demands much longer memory. For the latter, there exist more complex relations between the input and the output.  The input is first fed into a trainable embedding layer [17] before it is sent to the actual network. Table 2 shows the detailed network and training specifica- tions. We do not finetune network hyper-parameters or apply any engineering trick (e.g. feeding additional inputs other than the raw embedded input se- quences) for the best possible performance since our main goal is to compare the performance of the LSTM, GRU, ELSTM-I and ELSTM-II cells under var-ious macro-models.   The ELSTM-I cell even outperforms the bi-Att model, which was designed specifically for the DP task. For the POS tagging problem, the advantage of the ELSTM cells is not as obvious. This is probably due to the shorter memory requirement in this simple task. In this context, ELSTM cells are over-parameterized, and they converge slower and tend to overfit the training data.The ELSTM-I and ELSTM-II cells with large T s value perform particularly well for the seq2seq (with and without attention) model. The hidden state, used to predict the transition from a tail word to its headword. The convseq2seqis an end-to-end convolutional neural network (CNN) with an attention mecha- nism. We used the default settings for the TDP and the convseq2seq as reported in [14] and [15], respectively. For the TDP, we do not use the ground truth POS tags but the predicted dependency relation labels as the input to the parsing tree for the next prediction.We see from Table 5 that the ELSTM-based models learn much faster than the CNN-based convseq2seq model with fewer parameters. The convseq2seq uses dropout while the ELSTM-based models do not. It is also observed that convseq2seq does not converge if Adagrad is used as its optimizer. The ELSTM- I-based seq2seq with attention even outperforms the TDP, which was specifi- cally designed for the DP task. Without a good pretrained word embedding scheme, the UAS and LAS of TDP drop drastically to merely 8.93% and 0.30% respecively.Although the memory of the LSTM and GRU celles fades slower than that of the SRN, it is still not long enough for complicated language tasks such as dependency parsing. To address this issue, we proposed the ELSTM-I and the ELSTM-II to enhance the memory capability of an RNN cell. Besides, we For example, is the ELSTM cell also helpful in more sophisticated RNN models such as the deep RNN? Is it possible to make the DBRNN deeper and better?They are left for future study.Declarations of interest: none     Training Steps 
