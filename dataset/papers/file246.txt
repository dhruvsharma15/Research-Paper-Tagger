Recently proposed residual networks (ResNets) get state-of-the-art performance on the ILSVRC 2015 classification task (Deng et al., 2009) and allow training of extremely deep networks up to more than 1000 layers ( He et al., 2015b). Similar to highway networks, residual networks make use of identity shortcut connections that enable flow of information across layers without attenuation that would be caused by multiple stacked non-linear transformations, resulting in improved optimization ( Srivastava et al., 2015). In residual networks, shortcut connections are not gated and untransformed input is always transmitted. While the empirical performance of ResNets in He et al. (2015b) is very impressive, current residual network architectures have several potential limitations: identity connections as implemented in the current ResNet leads to accumulation of a mix of levels of feature representations at each layer, even though in a deep network some features learned by earlier layers may no longer provide useful information in later layers ( Gers et al., 2000).A hypothesis of the ResNet architecture is that learning identity weights is difficult, but by the same argument, it is difficult to learn the additive inverse of identity weights needed to remove informa- tion from the representation at any given layer. The fixed size layer structure of the residual block modules also enforces that residuals must be learned by shallow subnetworks, despite evidence that deeper networks are more expressive (Bianchini &amp; Scarselli, 2014). We introduce a generalized residual architecture that combines residual networks and standard convolutional networks in paral- lel residual and non-residual streams. We show architectures using generalized residual blocks retain the optimization benefits of identity shortcut connections while improving expressivity and the ease of removing unneeded information. We then present a novel architecture, ResNet in ResNet (RiR), which incorporates these generalized residual blocks within the framework of a ResNet and demon- strate state-of-the-art performance of the RiR architecture on CIFAR-100.The modular unit of the generalized residual network architecture is a generalized residual block consisting of parallel states for a residual stream, r, which contains identity shortcut connections and is similar to the structure of a residual block from the original ResNet with a single convolu- tional layer (parameters W l,r→r ), and a transient stream, t, which is a standard convolutional layer (W l,t→t ). Two additional sets of convolutional filters in each block (W l,r→t , W l,t→r ) also transfer information across streams.Same-stream and cross-stream activations are summed (along with the shortcut connection for the residual stream) before applying batch normalization and ReLU nonlinearities (together σ) to get the output states of the block (Equation 1) (Ioffe &amp; Szegedy, 2015). The function of the residual stream r resembles that of the original structure of the ResNet ( He et al., 2015b) with shortcut connections between each unit of processing, while the transient stream t adds the ability to process information from either stream in a nonlinear manner without shortcut connections, allowing information from earlier states to be discarded. The form of the shortcut connection can be an identity function with the appropriate padding or a projection as in He et al. (2015b). We implement the generalized residual block as a single convolutional layer with a modified initialization, which we call ResNet Init (see Appendix 6.1 for detail). . By repeating the generalized residual block several times, the generalized residual architecture has the expres- sivity to learn anything in between, including the standard 2-layer ResNet block ( Figure 1c). This architecture allows the network to learn residuals with a variable effective number of processing steps before addition back into the residual stream, which we investigate by visualizations. The generalized residual block is not specific to CNNs, and can be applied to standard fully connected layers and other feedforward layers. Replacing each of the convolutional layers within a residual block from the original ResNet ( Figure 1a) with a generalized residual block (Figure 1b) leads us to a new architecture we call ResNet in ResNet (RiR) (Figure 1d). In Figure 2, we summarize the relationship between standard CNN, ResNet Init, ResNet, and RiR architectures.We evaluate our architectures on CIFAR-10 and CIFAR-100 datasets (Krizhevsky &amp; Hinton, 2009) and report the best results found after a grid search on hyperparameters including learning rate, L2 penalty, initialization among Xavier (Glorot &amp; Bengio, 2010)  Tieleman &amp; Hinton, 2012), and the type of shortcut connections in the residual blocks. We optimize the hyperparameters for the original ResNet architecture and use SGD with momentum of 0.9, a minibatch size of 500, L2 penalty of 0.0001, and train for 82 epochs. The learning rate was scaled by 0.1 after epochs 42 and 62 and MSR initialization was used for all weight tensors. Test time batch normalization statistics were approximated using an exponential moving average of training batch normalization statistics. A projection with a 3x3 convolution was used for residual blocks that increase dimensionality, and all other shortcut connections were identity. We use equal numbers of filters for the residual and transient streams of the generalized residual network, but optimizing this hyperparameter could lead to further potential improvements.  In our experiments, the ResNet Init architecture shows consistent improvement over standard CNN architectures, and the RiR architecture outperforms the original ResNet (Table 3). We find the RiR architecture performs well across a range of numbers of blocks and layers in each block and that ResNet Init applied to existing architectures, such as ALL-CNN-C ( Springenberg et al., 2014), yields improvement over standard initialization (Tables 4, 5). Because each stream uses only half the total filters, we investigate the effect of our architectures on a wider 18-layer network (Tables 1,  2). We find this RiR architecture is remarkably effective, obtaining competitive results on CIFAR-10 with only standard augmentation by random crops and horizontal flips, and state-of-the-art results on CIFAR-100. We visualize the effect of zeroing learned connections of each stream in a trained ResNet Init model a single layer at a time, which shows both streams contribute to accuracy and relative use of residual and transient streams changes at different stages of processing ( Figure 3). In Figure 4, we show performance of the RiR architecture is robust to increasing depth of residual blocks and the RiR architecture allows training of deeper residuals compared to the original ResNet.Interacting transformation streams in which only one stream includes shortcut connections are also used in blocks of LSTM and Grid-LSTM networks (Hochreiter &amp; Schmidhuber, 1997;Kalchbrenner et al., 2015). However, in contrast to highway networks which control flow through shortcut con- nections via input-dependent carry and transform gates, and to memory and hidden states of LSTM and Grid-LSTM blocks, flow of information between the residual and transient states of the gener- alized residual block does not use gates and can thus be implemented with no additional parameters over a standard feedforward network ( Srivastava et al., 2015). Another difference between previous architectures and the generalized residual block we present is that while memory (m) and hidden (h) states in an LSTM or Grid-LSTM block are calculated sequentially (with h l = o l tanh(m l )), transformation of residual and memory streams of the generalized residual block occurs in parallel and depends only on the learned convolutional filters at each layer without further constraints on their relation. The SCRN architecture of Mikolov et al. (2014) also uses hidden and context units to- gether within a single layer to learn longer term information, which behave similarly to the transient and residual streams, but SCRN only allows unidirectional flow from context to hidden units and connections between context units are fixed, in contrast to bidirectional flow between streams and learned connections for both transient and residual streams in our generalized residual architecture.We present a generalized residual architecture which be simply implemented by a modified initial- ization scheme, ResNet Init, and apply it to the original ResNet to create a novel RiR architecture which achieves state-of-the-art results. Future work includes additional study of RiR and related residual models to further determine the cause of their beneficial effects.    We implement the generalized residual block with a modified initialization of a standard convo- lutional or fully connected (FC) layer that combines the identity shortcut with the desired linear transformation (convolution or matrix multiplication), which we call ResNet Init, and concatenat- ing tensors for the residual r and transient t streams to form a single tensor x. Because the identity shortcut and the results of the same-stream and cross-stream transformations are summed to give the output for each stream, linear operations on r and t can be composed into a single linear operation on x (see Equation 2 for an example of ResNet Init applied to an FC layer).To implement ResNet Init in a single FC layer, we concatenate weight matrices initialized by any existing scheme and then add a partial identity matrix (with 1s only on the first half of the diagonal) to the concatenated weight matrix. To implement ResNet Init in a single convolutional layer, we first concatenate convolutional kernels along the input dimension (W l,x→r = W l,r→r + W l,t→r and W l,x→t = W l,r→t + W l,t→t ) and the filter dimension (W l,x→x = W l,x→r + W l,x→t ), and then similarly add half of an identity kernel. The output of the generalized residual block implemented  as a standard layer with modified initialization (W l ) is exactly equivalent to the output if it were implemented as separate linear operations (W l,r→r , W l,t→r , W l,r→t , W l,t→t , and I).The generalized residual block implemented using modified initialization will perform differently from one implemented as separate linear operations when weight regularization that pulls all weights towards zero is present, such as L2 regularization. To maintain equivalence of implementation, we subtract the partial identity from the weights prior to application of weight decay.
