Present day Knowledge Bases (KBs) such as YAGO ( Suchanek et al., 2007), Freebase (Bollacker et al., 2008) or the Google Knowledge Vault ( Dong et al., 2014) provide immense collections of struc- tured knowledge. Relationships in these KBs of- ten exhibit regularities and models that capture these can be used to predict missing KB entries. A com- mon approach to KB completion is via tensor fac- torization, where a collection of fact triplets is rep- resented as a sparse mode-3 tensor which is decom- posed into several low-rank sub-components. Tex- tual relations, i.e. relations between entity pairs ex- tracted from text, can aid the imputation of missing KB facts by modelling them together with the KB relations ( Riedel et al., 2013).The general merit of factorization methods for KB completion has been demonstrated by a variety For further notation let E and R be sets of enti- ties and relations, respectively. We denote a fact f stating a relation r ∈ R between subject s ∈ E and object o ∈ E as f = (s, r, o). Our goal is to learn embeddings for larger sub-constituents of f than just s, r, and o: we want to learn em- beddings also for the entity pair bigram (s, o) as well as the relation-entity bigrams (s, r) and (r, o). As an example, consider Freebase facts with rela- tion eating/practicer of diet/diet and object Veganism. Overall only two objects are ob- served for this relation and it thus makes sense to learn a joint embedding for bigrams (r, o) together, instead of distinct embeddings for each atom alone and then having to learn their compatibility.While Riedel et al. (2013) have trained embed- dings only for entity pairs, we will in this pa- per explore the role of general bigram embeddings for KB completion, i.e. also the embeddings for other possible pairs of entities and relations. This is achieved using a Factorization Machine (FM) framework (Rendle, 2010) that is modular in its feature components, allowing us to selectively add or discard certain bigram embeddings and compare their relative importance. All models are empirically compared and evaluated on the fb15k237 dataset from Toutanova et al. (2015).In summary, our main contributions are: i) Adressing the question of generic bigram embed- dings in a KB completion model for the first time;ii) The adaption of Factorization Machines for this matter; iii) Experimental findings for comparing different bigram embedding models on fb15k237.train. Factorization Machines have already been ap- plied in a similar setting to ours by Petroni et al. (2015) who use them with contextual features for an Open Relation Extraction task, but without bigrams.A Factorization Machine (FM) is a quadratic regres- sion model with low-rank constraint on the quadratic interaction terms. 1 Given a sparse input feature vec-In the Universal Schema model (model F), Riedel et al. (2013) factorize KB entries together with rela- tions of entity pairs extracted from text, embedding textual relations in the same vector space as KB re- lations. Singh et al. (2015) extend this model to include a variety of other interactions between en- tities and relations, using different relation vectors to interact with subject, object or both. Jenatton et al. (2012) also recognize the need to integrate rich higher-order interaction information into the score. Like Nickel et al. (2011) however, their model spec- ifies relationships as relation-specific bilinear forms of entity embeddings. Other embedding methods for KB completion include DistMult ( Yang et al., 2014) with a trilinear score, and TransE (Bordes et al., 2013) which offers an intriguing geometrical in- tuition. Among the aforementioned methods, em- beddings are mostly learned for individual subjects, relations or objects; merely model F ( Riedel et al., 2013) constitutes the exception. Some methods rely on more expressive compo- sition functions to deal with non-compositionality or interaction effects, such as the Neural Tensor Networks ( Socher et al., 2013) or the recently in- troduced Holographic Embeddings ( Nickel et al., 2015). In comparison to the otherwise used (gen- eralized) dot products, the composition functions of these models enable richer interactions between unit constituent embeddings. However, this comes with the potential disadvantage of presenting less well- behaved optimisation problems and being slower to where v ∈ R n , and ∀i, j = 1, . . . n : w i , w j ∈ R k are model parameters with k n and ·· denotes the dot product. Instead of allowing for an individ- ual quadratic interaction coefficient per pair (i, j), the FM assumes that the matrix of quadratic interac- tion coefficients has low rank k; thus the interaction coefficient for feature pair (i, j) is represented by an inner product of k-dimensional vectors w i and w j . The low rank constraint (i) provides a strong form of regularisation to this otherwise over-parameterized model, (ii) pools statistical strength for estimating similarly profiled interaction coefficients and (iii) re- tains a total number of parameters linear in n. In summary, with a FM one can efficiently harness a large set of sparse features and interactions between them while retaining linear memory complexity.For the KB completion task we will use a FM with unit and bigram indicator features to learn low-rank embeddings for both. To formalize this, we will refer to the elements of the set U f = {s, r, o} as units of fact f , and to the elements of B f = {(s, r), (r, o), (o, s)} as bigrams of fact f . Let ι u ∈ R |E|+|R| be the one-hot indicator vector that encodes a particular unit 2 u ∈ (E ∪ R). Furthermore we de- fine ι (s,r) ∈ R |E||R| , ι (r,o) ∈ R |R||E| and ι (o,s) ∈ R |E| 2 to be the one-hot indicator vectors encoding par- ticular bigrams. Our feature vector φ(f ) for fact f = (s, r, o) then consists of simply the concatena- tion of indicator vectors for all its units and bigrams:Given sets of true training facts Ω + and sampled negative facts Ω − , we minimize the following loss:This sparse set of features provides a rich represen- tation of a fact with indicators for subject, relation and object, as well as any pair thereof.Harnessing the expressive benefits of a sigmoid link function for relation modelling (Bouchard et al., 2015), we define the truth score of a fact as g(f ) = σ(X f ) where σ is the sigmoid function and X f is given as output of the FM model (1) with unit and bigram features φ(f ) as defined in (2):Since our feature vector φ(f ) is sparse with only six active entries, we can re-express (3) in terms of the activated embeddings which we directly index by their respective units and bigrams:where the parametrization of X f is learned. We use the hyperparameter η ∈ R + for denoting the ratio of negative facts that are sampled per positive fact so that the contributions of true and false facts are balanced even if there are more negative facts than positives. The loss differs from a standard negative log-likelihood objective with logistic link, but we found that it performs better in practice. The intu- ition comes from the fact that instead of penalizing badly classified positive facts, we put more emphasis (i.e. negative loss) on positive facts that are correctly classified. Since we used an L 2 regularization and the loss is asymptotically linear, the resulting objec- tive is continuous and bounded from below, guaran- teeing a well defined local minimum.This score comprises all possible interactions be- tween any of the units and bigrams of f .The bigram embedding models are tested on fb15k237 (Toutanova et al., 2015), a dataset com- prising both Freebase facts and lexicalized depen- dency path relationships between entities.The score (4) can easily be modified and individual summands removed from it. In particular, when dis- carding all but one summand, model F is recovered, i.e. with c 1 = (s, o); c 2 = r. On the other hand, al- ternatives to model F with other bigrams than entity pairs can be tested by removing all summands but the one of a single bigram b ∈ B f vs. the remaining complementary unit u ∈ U f :This general formulation offers us a method for in- vestigating the relative impact of all combinations of bigram vs. unit embeddings besides model F , namely the models with u = s; b = (r, o) and with u = o; b = (s, r).Training Details and Evaluation We optimized the loss using AdaM ( Kingma and Ba, 2015) with minibatches of size 1024, using initial learn- ing rate 1.0 and initialize model parameters from N (0, 1). Furthermore, a hyperparameter τ &lt; 1 like in ( Toutanova et al., 2015 in % and best result in bold. The optimal value for τ is indicated as well.DistMult as baseline and employ the same ranking evaluation scheme as in ( Toutanova et al., 2015) and (Toutanova and Chen, 2015), computing filtered MRR and HITS scores whilst ranking true test facts among candidate facts with altered object. Particular bigrams that have not been observed during training have no learned embedding; a 0-embedding is used for these. This nullifies their impact on the score and models the back-off to using nonzero embeddings.Results Table 1 gives an overview of the general results for the different models. Clearly, some of the bigram models can obtain an improvement over the unit DistMult model. In a more fine-grained analy- sis of model performances, characterized by whether entity pairs of test facts had textual mentions avail- able in training (with TM) or not (without TM), the results exhibit a similar pattern like in ( Toutanova et al., 2015): most models perform worse on test facts with TM, only model F , which can learn very lit- tle without relations has a reversed behavior. A side observation is that several models achieved highest overall MRR with τ = 0, i.e. when not using TM. The sum of the three more light-weight bigram models performs better than the full FM, even though the same types of embeddings are used. A possible explanation is that applying the same em- bedding in several interactions with other embed- dings (as in the full FM) instead of only one interac- tion (like in (*)+(**)+(***)) makes it harder to learn since its multiple functionalities are competing.Another interesting finding is that some bigram types achieve much better results than others, in par- ticular model (**). A possible explanation becomes apparent with closer inspection of the test set: a given test fact f usually contains at least one bigram b ∈ B f which has never been observed yet. In these cases the bigram embedding is 0 by design and only the offset values are used. The proportions of test facts for which this happens are 73%, 10% and 24% respectively for the bigrams (s, o), (r, o), and (s, r). Thus models (**) and (***) already have a definite advantage over model (*) that originates purely from the nature of the data. A trivial but somehow im- portant lesson we can learn from this is that if we know about the relative prevalence of different bi- grams (or more generally: sub-tuples) in our dataset, we can incorporate and exploit this in the sub-tuples we choose.Finally, for the initial example with relation eating/practicer of diet/diet and ob- ject Veganism, we indeed find that in all instances model (**) with its (r, o) embedding gives the cor- rect fact in the top 2 predictions, while the purely compositional DistMult model ranks it far outside the top 10. More generally, cases in which only a single object co-appeared with a test fact relation during training had 95, 3% HITS@1 with model (**) while only 52, 6% for DistMult. This supports the intuition that bigram embeddings of (r, o) are in fact better suited for cases in which very few objects are possible for a relation.We have demonstrated that FM provide an approach to KB completion that can incorporate embeddings for bigrams naturally. The FM offers a compact uni- fied framework in which various tensor factorization models can be expressed, including model F.Extensive experiments have demonstrated that bi- gram models can improve prediction performances substantially over more straightforward unigram models. A surprising but important result is that bi- grams other than entity pairs are particularly appeal- ing.The bigger question behind our work is about compositionality vs. non-compositionality in a broader class of knowledge bases involving higher order information such as time, origin or context in the tuples. Deciding which modes should be merged into a high order embedding without having to rely on heavy cross-validation is an open question.
