Deep neural networks have shown great success in various applications such as objection recognition (see, e.g., ( Krizhevsky et al., 2012)) and speech recognition (see, e.g., (Dahl et al., 2012)). Fur- thermore, many recent works showed that neu- ral networks can be successfully used in a num- ber of tasks in natural language processing (NLP). These include, but are not limited to, language modeling ( Bengio et al., 2003), paraphrase detec- tion (Socher et al., 2011) and word embedding ex- traction ( Mikolov et al., 2013). In the field of sta- tistical machine translation (SMT), deep neural networks have begun to show promising results. (Schwenk, 2012) summarizes a successful usage of feedforward neural networks in the framework of phrase-based SMT system.The proposed RNN Encoder-Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French. We train the model to learn the translation probabil- ity of an English phrase to a corresponding French phrase. The model is then used as a part of a stan- dard phrase-based SMT system by scoring each phrase pair in the phrase table. The empirical eval- uation reveals that this approach of scoring phrase pairs with an RNN Encoder-Decoder improves the translation performance.We qualitatively analyze the trained RNN Encoder-Decoder by comparing its phrase scores with those given by the existing translation model. The qualitative analysis shows that the RNN Encoder-Decoder is better at capturing the lin- guistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance. The further anal- ysis of the model reveals that the RNN Encoder- Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase. A recurrent neural network (RNN) is a neural net- work that consists of a hidden state h and an optional output y which operates on a variable- length sequence x = (x 1 , . . . , x T ). At each time step t, the hidden state h of the RNN is updated bywhere f is a non-linear activation func- tion.f may be as simple as an element- wise logistic sigmoid function and as com- plex as a long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997).An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence. In that case, the output at each timestep t is the conditional distribution p(x t | x t−1 , . . . , x 1 ). For example, a multinomial distribution (1-of-K coding) can be output using a softmax activation function for all possible symbols j = 1, . . . , K, where w j are the rows of a weight matrix W. By combining these probabilities, we can compute the probabil- ity of the sequence x using T should note that the input and output sequence lengths T and T may differ. The encoder is an RNN that reads each symbol of an input sequence x sequentially. As it reads each symbol, the hidden state of the RNN changes according to Eq. (1). After reading the end of the sequence (marked by an end-of-sequence sym- bol), the hidden state of the RNN is a summary c of the whole input sequence.The decoder of the proposed model is another RNN which is trained to generate the output se- quence by predicting the next symbol y t given the hidden state h . However, unlike the RNN de- scribed in Sec. 2.1, both y t and h are also con- ditioned on y t−1 and on the summary c of the input sequence. Hence, the hidden state of the decoder at time t is computed by,From this learned distribution, it is straightfor- ward to sample a new sequence by iteratively sam- pling a symbol at each time step. and similarly, the conditional distribution of the next symbol is P (y t |y t−1 , y t−2 , . . . , y 1 , c) = g h , y t−1 , c .In this paper, we propose a novel neural network architecture that learns to encode a variable-length sequence into a fixed-length vector representation and to decode a given fixed-length vector rep- resentation back into a variable-length sequence. From a probabilistic perspective, this new model is a general method to learn the conditional dis- tribution over a variable-length sequence condi- tioned on yet another variable-length sequence, e.g. p(y 1 , . . . , y T | x 1 , . . . , x T ), where one for given activation functions f and g (the latter must produce valid probabilities, e.g. with a soft- max). See Fig. 1 for a graphical depiction of the pro- posed model architecture.The two components of the proposed RNN Encoder-Decoder are jointly trained to maximize the conditional log-likelihoodn=1 where θ is the set of the model parameters and each (x n , y n ) is an (input sequence, output se- quence) pair from the training set. In our case, as the output of the decoder, starting from the in- put, is differentiable, we can use a gradient-based algorithm to estimate the model parameters.Once the RNN Encoder-Decoder is trained, the model can be used in two ways. One way is to use the model to generate a target sequence given an input sequence. On the other hand, the model can be used to score a given pair of input and output sequences, where the score is simply a probability p θ (y | x) from Eqs. (3) and (4).  (8) for the detailed equations of r, z, h and˜hand˜ and˜h.In addition to a novel model architecture, we also propose a new type of hidden unit (f in Eq. (1)) that has been motivated by the LSTM unit but is much simpler to compute and implement. 1 Fig. 2 shows the graphical depiction of the proposed hid- den unit. Let us describe how the activation of the j-th hidden unit is computed. First, the reset gate r j is computed bywhere σ is the logistic sigmoid function, and [.] j denotes the j-th element of a vector. x and h t−1 are the input and the previous hidden state, respec- tively. W r and U r are weight matrices which are learned. Similarly, the update gate z j is computed byThe actual activation of the proposed unit h j is then computed by only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation.On the other hand, the update gate controls how much information from the previous hidden state will carry over to the current hidden state. This acts similarly to the memory cell in the LSTM network and helps the RNN to remember long- term information. Furthermore, this may be con- sidered an adaptive variant of a leaky-integration unit ( .As each hidden unit has separate reset and up- date gates, each hidden unit will learn to capture dependencies over different time scales. Those units that learn to capture short-term dependencies will tend to have reset gates that are frequently ac- tive, but those that capture longer-term dependen- cies will have update gates that are mostly active.In our preliminary experiments, we found that it is crucial to use this new unit with gating units. We were not able to get meaningful result with an oft-used tanh unit without any gating.where˜h where˜ where˜hIn a commonly used statistical machine translation system (SMT), the goal of the system (decoder, specifically) is to find a translation f given a source sentence e, which maximizesIn this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the pre- vious hidden state and reset with the current inputThe LSTM unit, which has shown impressive results in several applications such as speech recognition, has a mem- ory cell and four gating units that adaptively control the in- formation flow inside the unit, compared to only two gating units in the proposed hidden unit. For details on LSTM net- works, see, e.g., (Graves, 2012). where the first term at the right hand side is called translation model and the latter language model (see, e.g., (Koehn, 2005)). In practice, however, most SMT systems model log p(f | e) as a log- linear model with additional features and corre- sponding weights:where f n and w n are the n-th feature and weight, respectively. Z(e) is a normalization constant that does not depend on the weights. The weights are often optimized to maximize the BLEU score on a development set.In the phrase-based SMT framework introduced in ( Koehn et al., 2003) and ( Marcu and Wong, 2002), the translation model log p(e | f ) is factorized into the translation probabilities of matching phrases in the source and target sentences. 2 These probabilities are once again considered additional features in the log-linear model (see Eq. (9)) and are weighted accordingly to maximize the BLEU score.Since the neural net language model was pro- posed in ( Bengio et al., 2003), neural networks have been used widely in SMT systems. In many cases, neural networks have been used to rescore translation hypotheses (n-best lists) (see, e.g., (Schwenk et al., 2006)). Recently, however, there has been interest in training neural networks to score the translated sentence (or phrase pairs) using a representation of the source sentence as an additional input. See, e.g., (Schwenk, 2012), (Son et al., 2012) and ( Zou et al., 2013). pairs in the original corpus. With a fixed capacity of the RNN Encoder-Decoder, we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities, i.e., distin- guishing between plausible and implausible trans- lations, or learning the "manifold" (region of prob- ability concentration) of plausible translations.Once the RNN Encoder-Decoder is trained, we add a new score for each phrase pair to the exist- ing phrase table. This allows the new scores to en- ter into the existing tuning algorithm with minimal additional overhead in computation.As Schwenk pointed out in (Schwenk, 2012), it is possible to completely replace the existing phrase table with the proposed RNN Encoder- Decoder. In that case, for a given source phrase, the RNN Encoder-Decoder will need to generate a list of (good) target phrases. This requires, how- ever, an expensive sampling procedure to be per- formed repeatedly. In this paper, thus, we only consider rescoring the phrase pairs in the phrase table.Machine TranslationHere we propose to train the RNN Encoder- Decoder (see Sec. 2.2) on a table of phrase pairs and use its scores as additional features in the log- linear model in Eq. (9) when tuning the SMT de- coder.When we train the RNN Encoder-Decoder, we ignore the (normalized) frequencies of each phrase pair in the original corpora. This measure was taken in order (1) to reduce the computational ex- pense of randomly selecting phrase pairs from a large phrase table according to the normalized fre- quencies and (2) to ensure that the RNN Encoder- Decoder does not simply learn to rank the phrase pairs according to their numbers of occurrences. One underlying reason for this choice was that the existing translation probability in the phrase ta- ble already reflects the frequencies of the phrase Before presenting the empirical results, we discuss a number of recent works that have proposed to use neural networks in the context of SMT.Schwenk in (Schwenk, 2012) proposed a simi- lar approach of scoring phrase pairs. Instead of the RNN-based neural network, he used a feedforward neural network that has fixed-size inputs (7 words in his case, with zero-padding for shorter phrases) and fixed-size outputs (7 words in the target lan- guage). When it is used specifically for scoring phrases for the SMT system, the maximum phrase length is often chosen to be small. However, as the length of phrases increases or as we apply neural networks to other variable-length sequence data, it is important that the neural network can han- dle variable-length input and output. The pro- posed RNN Encoder-Decoder is well-suited for these applications.Similar to (Schwenk, 2012), Devlin et al. ( Devlin et al., 2014) proposed to use a feedfor- ward neural network to model a translation model, however, by predicting one word in a target phrase at a time. They reported an impressive improve- ment, but their approach still requires the maxi- mum length of the input phrase (or context words) to be fixed a priori.Although it is not exactly a neural network they train, the authors of ( Zou et al., 2013) proposed to learn a bilingual embedding of words/phrases. They use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an SMT system.In (Chandar et al., 2014), a feedforward neural network was trained to learn a mapping from a bag-of-words representation of an input phrase to an output phrase. This is closely related to both the proposed RNN Encoder-Decoder and the model proposed in (Schwenk, 2012), except that their in- put representation of a phrase is a bag-of-words. A similar approach of using bag-of-words repre- sentations was proposed in ( Gao et al., 2013) as well. Earlier, a similar encoder-decoder model us- ing two recursive neural networks was proposed in (Socher et al., 2011), but their model was re- stricted to a monolingual setting, i.e. the model reconstructs an input sentence. More recently, an- other encoder-decoder model using an RNN was proposed in ( Auli et al., 2013), where the de- coder is conditioned on a representation of either a source sentence or a source context.One important difference between the pro- posed RNN Encoder-Decoder and the approaches in ( Zou et al., 2013) and (Chandar et al., 2014) is that the order of the words in source and tar- get phrases is taken into account. The RNN Encoder-Decoder naturally distinguishes between sequences that have the same words but in a differ- ent order, whereas the aforementioned approaches effectively ignore order information.The closest approach related to the proposed RNN Encoder-Decoder is the Recurrent Contin- uous Translation Model (Model 2) proposed in (Kalchbrenner and Blunsom, 2013). In their pa- per, they proposed a similar model that consists of an encoder and decoder. The difference with our model is that they used a convolutional n-gram model (CGM) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder. They, however, evaluated their model on rescoring the n-best list proposed by the conventional SMT system and computing the per- plexity of the gold standard translations.Large amounts of resources are available to build an English/French SMT system in the framework of the WMT'14 translation task. The bilingual corpora include Europarl (61M words), news com- mentary (5.5M), UN (421M), and two crawled corpora of 90M and 780M words respectively. The last two corpora are quite noisy. To train the French language model, about 712M words of crawled newspaper material is available in addi- tion to the target side of the bitexts. All the word counts refer to French words after tokenization.It is commonly acknowledged that training sta- tistical models on the concatenation of all this data does not necessarily lead to optimal per- formance, and results in extremely large mod- els which are difficult to handle. Instead, one should focus on the most relevant subset of the data for a given task. We have done so by applying the data selection method proposed in (Moore and Lewis, 2010), and its extension to bi- texts (Axelrod et al., 2011). By these means we selected a subset of 418M words out of more than 2G words for language modeling and a subset of 348M out of 850M words for train- ing the RNN Encoder-Decoder. We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT, and newstest2014 as our test set. Each set has more than 70 thousand words and a single refer- ence translation.For training the neural networks, including the proposed RNN Encoder-Decoder, we limited the source and target vocabulary to the most frequent 15,000 words for both English and French. This covers approximately 93% of the dataset. All the out-of-vocabulary words were mapped to a special token ( [UNK]).The baseline phrase-based SMT system was built using Moses with default settings. This sys- tem achieves a BLEU score of 30.64 and 33.3 on the development and test sets, respectively (see Ta- ble 1).We evaluate our approach on the English/French translation task of the WMT'14 workshop.The RNN Encoder-Decoder used in the experi- ment had 1000 hidden units with the proposed gates at the encoder and at the decoder. The in- put matrix between each input symbol x and the hidden unit is approximated with two lower-rank matrices, and the output matrix is approximated similarly. We used rank-100 matrices, equivalent to learning an embedding of dimension 100 for each word. The activation function used for˜hfor˜ for˜h in Eq. (8) is a hyperbolic tangent function. The com- putation from the hidden state in the decoder to the output is implemented as a deep neural net- work ( Pascanu et al., 2014) with a single interme- diate layer having 500 maxout units each pooling 2 inputs ( Goodfellow et al., 2013).All the weight parameters in the RNN Encoder- Decoder were initialized by sampling from an isotropic zero-mean (white) Gaussian distribution with its standard deviation fixed to 0.01, except for the recurrent weight parameters. For the re- current weight matrices, we first sampled from a white Gaussian distribution and used its left singu- lar vectors matrix, following (Saxe et al., 2014).We used Adadelta and stochastic gradient descent to train the RNN Encoder-Decoder with hyperparameters = 10 −6 and ρ = 0.95 (Zeiler, 2012). At each update, we used 64 randomly selected phrase pairs from a phrase ta- ble (which was created from 348M words). The model was trained for approximately three days.Details of the architecture used in the experi- ments are explained in more depth in the supple- mentary material. tem add up or are redundant.We trained the CSLM model on 7-grams from the target corpus.Each input word was projected into the embedding space R 512 , and they were concatenated to form a 3072- dimensional vector. The concatenated vector was fed through two rectified layers (of size 1536 and 1024) (Glorot et al., 2011). The output layer was a simple softmax layer (see Eq. (2)). All the weight parameters were initialized uniformly be- tween −0.01 and 0.01, and the model was trained until the validation perplexity did not improve for 10 epochs. After training, the language model achieved a perplexity of 45.80. The validation set was a random selection of 0.1% of the corpus. The model was used to score partial translations dur- ing the decoding process, which generally leads to higher gains in BLEU score than n-best list rescor- ing ( Vaswani et al., 2013).To address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stack- search performed by the decoder. Only when the buffer is full, or a stack is about to be pruned, the n-grams are scored by the CSLM. This allows us to perform fast matrix- matrix multiplication on GPU using Theano ( Bergstra et al., 2010;Bastien et al., 2012). In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder- Decoder, we also tried a more traditional approach of using a neural network for learning a target language model (CSLM) (Schwenk, 2007). Espe- cially, the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder-Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT sys- We tried the following combinations:  Table 2: The top scoring target phrases for a small set of source phrases according to the translation model (direct translation probability) and by the RNN Encoder-Decoder. Source phrases were randomly selected from phrases with 4 or more words. ? denotes an incomplete (partial) character. r is a Cyrillic letter ghe.The results are presented in Table 1. As ex- pected, adding features computed by neural net- works consistently improves the performance over the baseline performance.The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder-Decoder. This suggests that the contributions of the CSLM and the RNN Encoder- Decoder are not too correlated and that one can expect better results by improving each method in- dependently. Furthermore, we tried penalizing the number of words that are unknown to the neural networks (i.e. words which are not in the short- list). We do so by simply adding the number of unknown words as an additional feature the log- linear model in Eq. (9). 3 However, in this case we were not able to achieve better performance on the test set, but only on the development set. 3 To understand the effect of the penalty, consider the set of all words in the 15,000 large shortlist, SL. All words x i / ∈ SL are replaced by a special token [UNK] before being scored by the neural networks. Hence, the conditional probability of any x i t / ∈ SL is actually given by the model as In order to understand where the performance im- provement comes from, we analyze the phrase pair scores computed by the RNN Encoder-Decoder against the corresponding p(f | e) from the trans- lation model. Since the existing translation model relies solely on the statistics of the phrase pairs in the corpus, we expect its scores to be better esti- mated for the frequent phrases but badly estimated for rare phrases. Also, as we mentioned earlier in Sec. 3.1, we further expect the RNN Encoder- Decoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus.We focus on those pairs whose source phrase is long (more than 3 words per source phrase) andAs a result, the probability of words not in the shortlist is always overestimated. It is possible to address this issue by backing off to an existing model that contain non-shortlisted words (see (Schwenk, 2007)) In this paper, however, we opt for introducing a word penalty instead, which counteracts the word probability overestimation.where x&lt;t is a shorthand notation for xt−1, . . . , x1.   Table 2. We show the top-5 target phrases out of 50 samples. They are sorted by the RNN Encoder-Decoder scores. frequent. For each such source phrase, we look at the target phrases that have been scored high either by the translation probability p(f | e) or by the RNN Encoder-Decoder. Similarly, we per- form the same procedure with those pairs whose source phrase is long but rare in the corpus. Table 2 lists the top-3 target phrases per source phrase favored either by the translation model or by the RNN Encoder-Decoder. The source phrases were randomly chosen among long ones having more than 4 or 5 words.other phrase pairs that were scored radically dif- ferent (see Fig. 3). This could arise from the proposed approach of training the RNN Encoder- Decoder on a set of unique phrase pairs, discour- aging the RNN Encoder-Decoder from learning simply the frequencies of the phrase pairs from the corpus, as explained earlier.In most cases, the choices of the target phrases by the RNN Encoder-Decoder are closer to ac- tual or literal translations. We can observe that the RNN Encoder-Decoder prefers shorter phrases in general.Interestingly, many phrase pairs were scored similarly by both the translation model and the RNN Encoder-Decoder, but there were as many Furthermore, in Table 3, we show for each of the source phrases in Table 2, the generated sam- ples from the RNN Encoder-Decoder. For each source phrase, we generated 50 samples and show the top-five phrases accordingly to their scores. We can see that the RNN Encoder-Decoder is able to propose well-formed target phrases with- out looking at the actual phrase table. Importantly, the generated phrases do not overlap completely with the target phrases from the phrase table. This encourages us to further investigate the possibility of replacing the whole or a part of the phrase table with the proposed RNN Encoder-Decoder in the future.Since the proposed RNN Encoder-Decoder is not specifically designed only for the task of machine translation, here we briefly look at the properties of the trained model. It has been known for some time that continuous space language models using neural networks are able to learn seman- tically meaningful embeddings (See, e.g., (Bengio et al., 2003;Mikolov et al., 2013)). Since the proposed RNN Encoder-Decoder also projects to and maps back from a sequence of words into a continuous space vector, we expect to see a similar property with the proposed model as well.The left plot in Fig. 4 shows the 2-D embedding of the words using the word embedding matrix learned by the RNN Encoder-Decoder. The pro- jection was done by the recently proposed Barnes- Hut-SNE (van der Maaten, 2013). We can clearly see that semantically similar words are clustered with each other (see the zoomed-in plots in Fig. 4).The proposed RNN Encoder-Decoder naturally generates a continuous-space representation of a phrase. The representation (c in Fig. 1) in this case is a 1000-dimensional vector. Similarly to the word representations, we visualize the representa- tions of the phrases that consists of four or more words using the Barnes-Hut-SNE in Fig. 5.From the visualization, it is clear that the RNN Encoder-Decoder captures both semantic and syn- tactic structures of the phrases. For instance, in the bottom-left plot, most of the phrases are about the duration of time, while those phrases that are syntactically similar are clustered together. The bottom-right plot shows the cluster of phrases that are semantically similar (countries or regions). On the other hand, the top-right plot shows the phrases that are syntactically similar.In this paper, we proposed a new neural network architecture, called an RNN Encoder-Decoder that is able to learn the mapping from a sequence of an arbitrary length to another sequence, possi- bly from a different set, of an arbitrary length. The proposed RNN Encoder-Decoder is able to either score a pair of sequences (in terms of a conditional probability) or generate a target sequence given a source sequence. Along with the new architecture, we proposed a novel hidden unit that includes a re- set gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequence.We evaluated the proposed model with the task of statistical machine translation, where we used the RNN Encoder-Decoder to score each phrase pair in the phrase table. Qualitatively, we were able to show that the new model is able to cap- ture linguistic regularities in the phrase pairs well and also that the RNN Encoder-Decoder is able to propose well-formed target phrases.The scores by the RNN Encoder-Decoder were found to improve the overall translation perfor- mance in terms of BLEU scores. Also, we found that the contribution by the RNN Encoder- Decoder is rather orthogonal to the existing ap- proach of using neural networks in the SMT sys- tem, so that we can improve further the perfor- mance by using, for instance, the RNN Encoder- Decoder and the neural net language model to- gether.Our qualitative analysis of the trained model shows that it indeed captures the linguistic regu- larities in multiple levels i.e. at the word level as well as phrase level. This suggests that there may be more natural language related applications that may benefit from the proposed RNN Encoder- Decoder.The proposed architecture has large potential for further improvement and analysis. One ap- proach that was not investigated here is to re- place the whole, or a part of the phrase table by letting the RNN Encoder-Decoder propose target phrases. Also, noting that the proposed model is not limited to being used with written language, it will be an important future research to apply the proposed architecture to other applications such as speech transcription. sion under the project MateCat, and by DARPA under the BOLT project.
