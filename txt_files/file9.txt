In [1]: ! head -15 data/daquar/qa.894.raw.train.format_triple what is on the right side of the black telephone and on the left side of the red chair ? desk image3 what is in front of the white door on the left side of the desk ? telephone image3 what is on the desk ? book, scissor, papers, tape dispenser image3 what is the largest brown objects ? carton image3 what color is the chair in front of the white wall ? red image3Note that the format is: question, answer (could be many answer words), and the image. Let us have a look at Figure 1. The figure lists images with associated question-answer pairs. It also comments on challenges associated with question-answer- image triplets. We see that to answer properly on the wide range of questions, an answerer not only needs to understand the scene visually or to just understand the question, but also, arguably, has to resort to the common sense knowledge, or even know the preferences of the person asking the question, e.g. what 'behind' exactly means in 'What is behind the table?'. Hence, architectures that answer questions about images have to face many challenges. Ambiguities make it also difficult to judge the provided answers. We revisit this issue in a later section. Meantime, a curious reader may try to answer the following question.Can you spot ambiguities that are present in the first column of the figure? Think of a spatial relationship between an observer, object of interest, and the world.The following code returns a dictionary of three views on the DAQUAR dataset. For now, we look only into the 'text' view. dp ['text'] returns a function from a dataset split into the dataset's textual view. Executing the following code makes it more clear.In [ ]: #TODO: Execute the following procedure (Shift+Enter in the Notebook) from kraino.utils import data_provider dp = data_provider.select['daquar-triples'] train_text_representation = dp ['text'](train_or_test='train')This view specifies how questions are ended ('?'), answers are ended ('.'), answer words are delimited (DAQUAR sometimes has a set of answer words as an answer, for instance 'knife, fork' may be an answer answer), but most important, it has questions (key 'x'), answers (key 'y'), and names of the corresponding images (key 'img name'). Summary DAQUAR consists of question-answer-image triplets. Question-answer pairs for different folds are accessible from executing the following code.data_provider.select ['text'] Finally, as we see in Figure 1, DAQUAR poses many challenges in designing good architectures, or evaluation metrics.We have an access to a textual representation of questions. This is however not very helpful since neural networks expect a numerical input, and hence we cannot really work with the raw text. We need to transform the textual input into some numerical value or a vector of values. One particularly successful representation is called one-hot vector and it is a binary vector with exactly one non-zero entry. This entry points to the corresponding word in the vocabulary. See the illustration shown in Figure 2.The reader can pause here a bit to answer the following questions.Can you sum up the one-hot vectors for the 'What table is behind the table?'. How would you interpret the resulting vector? Why is it a good idea to work with one-hot vector represetantions of the text?As we see from the illustrative example above, we first need to build a suitable vocabulary from our raw textual training data, and next transform them into one-hot representations. The following code can do this.  In many parts of this tutorial, we use Kraino, which was developed for the purpose of this tutorial to simplify the development of various 'question answering' models through prototyping.from kraino.utils.input_output_space import build_vocabulary # This function takes wordcounts, # and returns word2index -mapping from words into indices, # and index2word -mapping from indices to words.In addition, we use a few special, extra symbols that do not occur in the training dataset. Most important are &lt; pad &gt; and &lt; unk &gt;. We use the former to pad sequences in order to have the same number of temporal elements; we use the latter for words (at test time) that do not exist in the training set. Armed with the vocabulary, we can build one-hot representations of the training data. However, this is not necessary and may even be wasteful. Our one-hot representation of the input text does not explicitly build long and sparse vectors, but instead it operates on indices. The example from Figure 2 would be encoded as [0,1,4,2,7,3].Due to the sparsity existing in the one-hot representation, we can more efficiently operate on indices instead of performing full linear transformations by matrix-vector multiplications. This is reflected in the following claim.Claim: Let x be a binary vector with exactly one value 1 at the position index, that iswhere W [:, b] denotes a vector built from a column b of W . This shows that matrix-vector multiplication can be replaced by retrieving a right vector of parameters according to the index.Can you show that the claim is valid?We can encode textual questions into one-hot vector representations by executing the following code. At the last step, we encode test questions. We need them later to see how well our models generalize to new question-answer- image triplets. Remember, however, that we should use the vocabulary we generated from the training samples.Why should we use the training vocabulary to encode test questions?In [ ]: test_text_representation = dp ['text'] With the encoded question-answer pairs we finish this section. However, before delving into details of building and training new models, let us have a look at the summary to see bigger picture.Summary We started from raw questions from the training set. We use them to build a vocabulary. Next, we encode questions into sequences of one-hot vectors based on the vocabulary. Finally, we use the same vocabulary to encode questions from the test set. If a word is absent, we use an extra token &lt; unk &gt; to denote this fact, so that we encode the &lt; unk &gt; token, not the word.As you may already know, we train models by weights updates. Let x and y be training samples (an input, and an output), and y) be an objective function. The formula for weights updates is:with α that we call the learning rate, and ∇ that is a gradient wrt. the weights w. The learning rate is a hyper-parameter that must be set in advance. The rule shown above is called the SGD update, but other variants are also possible. In fact, we use its variant called ADAM [Kingma and Ba, 2014].We cast the question answering problem into a classification framework, so that we classify an input x into some class that represents an answer word. Therefore, we use, commonly used in the classification, logistic regression as the objective:where C is a set of all classes, and p(y | x, w) is the softmax: e x) . Here φ(x) denotes an output of a model (more precisely, it is often a response of a neural network to the input, just before softmax of the neural network is applied). Note, however, that another variant of providing answers, called the answer generation, is also possible . For training, we need to execute the following code.training(gradient_of_the_model, optimizer='Adam') Summary Given a model, and an optimization procedure (SGD, Adam, etc.) all we need is to compute gradient of the model ∇ y; w) wrt. to its parameters w, and next plug it to the optimization procedure.Since computing gradients ∇ y; w) may quickly become tedious, especially for more complex models, we search for tools that could automatize this process. Imagine that you build a model M and you get its gradient ∇M by just executing the tool, something like the following piece of code.This would definitely speed up prototyping. Theano [Bastien et al., 2012] is such a tool that is specifically tailored to work with deep learning models. For a broader understanding of Theano, you can check a suitable tutorial 6 .The following coding example defines ReLU, a popular activation function defined as ReLU (x) = max(x, 0), as well as derive its derivative using Theano. Note however that, with this example, we obviously only scratch the surface. Can you derive a derivative of ReLU on your own? Consider two cases.It should also be mentioned that ReLU is a non-differentiable function at the point 0, and therefore, technically, we compute its sub-gradient -this is however still fine for Theano.Summary To compute gradient symbolically, we can use Theano. This speeds up prototyping, and hence developing new question answering models.Keras [Chollet, 2015] builds upon Theano, and significantly simplifies creating new deep learning models as well as training such models, effectively speeding up the prototyping even further. Keras also abstracts away from some technical burden such as a symbolic variable creation. Many examples of using Keras can be found by following the links: https://keras.io/getting-started/sequential-model-guide/, and https://keras.io/ getting-started/functional-api-guide/. Note that, in the tutorial we use an older sequential model. Please also pay attention to the version of the Keras, since not all versions are compatible with this tutorial.For the purpose of the Visual Turing Test, and this tutorial, we have compiled a light framework that builds on top of Keras, and simplify building and training 'question answering' machines. With the tradition of using Greek names, we call it Kraino. Note that some parts of the Kraino, such as a data provider, were already covered in this tutorial.In the following, we will go through BOW and LSTM approaches to answer questions about images, but, surprisingly, without the images. It turns out that a substantial fraction of questions can be answered without an access to an image, but rather by resorting to a common sense (or statistics of the dataset). For instance, 'what can be placed at the table?', or 'How many eyes this human have?'. Answers like 'chair' and '2' are quite likely to be good answers. Figure 3 illustrates the BOW (Bag Of Words) method. As we have already seen before, we first encode the input sentence into one-hot vector representations. Such a (very) sparse representation is next embedded into a denser space by a matrix W e . Next, the denser representations are summed up and classified via 'Softmax'. Notice that, if W e were an identity matrix, we would obtain a histogram of the word's occurrences.What is your biggest complain about such a BOW representation? What happens if instead of 'What is behind the table' we would have 'is What the behind table'? How does the BOW representation change?Let us now define a BOW model using our tools.In [ ]: #== Model definition # First we define a model using keras/kraino from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers.core import Dropout from keras.layers.core import TimeDistributedMerge from keras.layers.embeddings import Embedding from kraino.core.model_zoo import AbstractSequentialModel from kraino.core.model_zoo import AbstractSingleAnswer from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer from kraino.core.model_zoo import Config from kraino.core.keras_extensions import DropMask from kraino.core.keras_extensions import LambdaWithMask from kraino.core.keras_extensions import time_distributed_masked_ave # This model inherits from AbstractSingleAnswer, # and so it produces single answer words # To use multiple answer words, # you need to inherit from AbstractSequentialMultiplewordAnswer class BlindBOW(AbstractSequentialModel, AbstractSingleAnswer):""" BOW Language only model that produces single word answers.   Figure 4 illustrates, the (temporarily) first word embedding is given to an RNN unit. The RNN unit next processes such an embedding and outputs to the second RNN unit. This unit takes both the output of the first RNN unit and the 2nd word embedding as inputs, and outputs some algebraic combination of both inputs. And so on. The last recurrent unit builds the representation of the whole sequence. Its output is next given to Softmax for the classification. One among the challenges that such approaches have to deal with is maintaining long-term dependencies. Roughly speaking, as new inputs are coming in the following steps it is getting easier to 'forget' information from the beginning (the first temporal step). LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Cho et al., 2014] are two particularly popular Recurrent Neural Networks that can preserve such longer dependencies to some extent 7 .Let us create a Recurrent Neural Network in the following.  The curious reader is encouraged to experiment with the language-only models. For instance, to see the influence of particular modules to the overall performance, the reader can do the following exercise.Change the number of hidden states. Change the number of epochs used to train a model. Modify models by using more RNN layers, or deeper classifiers.Summary RNN models, as opposite to BOW, consider order of the words in the question. Moreover, a substantial number of questions can be answered without any access to images. This can be explained as models learn some specific dataset statistics, some of them can be interpreted as common sense knowledge.To be able to monitor a progress on a task, we need to find ways to evaluate architectures on the task. Otherwise, we would not know how to judge architectures, or even worse, we would not even know what the goal is. Moreover, we should also aim at automatic evaluation measures, otherwise reproducibility is questionable, and the evaluation costs are high.Although an early work on the Visual Turing Test argues for keeping the answer words from a fixed vocabulary in order to keep an evaluation simpler Fritz, 2014a,b, 2015], it is still difficult to automatically evaluate architectures due to ambiguities that occur in the answers. We have ambiguities in naming objects, sometimes due to synonyms, but sometimes due to fuzziness. For instance, is 'chair' == 'armchair' or 'chair' != 'armchair' or something in between? Such semantic boundaries become even more fuzzy when we increase the number of categories. We could easily find a mutually exclusive set of 10 different categories, but what if there are 1000 categories, or 10000 categories? Arguably, we cannot think in terms of an equivalence class anymore, but rather in terms of similarities. That is 'chair' is semantically more similar to 'armchair', than to 'horse'. This simple example shows the main drawback of a traditional, binary evaluation measure which is Accuracy. This metric scores 1 if the names are the same and 0 otherwise. So that Acc('chair', 'armchair') == Acc('chair', 'horse'). We call these ambiguities, word-level ambiguities, but there are other ambiguities that are arguably more difficult to handle. For instance, the same question can be phrased in multiple other ways.  Figure 1 shows a few ambiguities that exists in DAQUAR.Given an ontology a Wu-Palmer Similarity between two words (or broader concepts) is a soft measure defined aswhere lca(a, b) is the least common ancestor of a and b, and depth(a) is depth of a in the ontology. Figure 5 shows a toy-sized ontology. The curious reader can, based on Figure 5, address the following questions.What is WuP(Dog, Horse) and WuP(Dog, Dalmatian) according to the toy-sized ontology? Can you calculate Acc(Dog, Horse) and Acc(Dog, Dalmatian)? Wu-Palmer Similarity depends on the choice of ontology. One popular, large ontology is WordNet [Miller, 1995, Fellbaum, 1999. Although Wu-Palmer Similarity may work on shallow ontologies, we are rather interested in ontologies with hundreds or even thousands of categories. In indoor scenarios, it turns out that many indoor 'things' share similar levels in the ontology, and hence Wu-Palmer Similarities are very small between two entities. The following code exemplifies the issue.In [ ]: from nltk.corpus import wordnet as wn armchair_synset = wn.synset('armchair.n.01') chair_synset = wn.synset('chair.n.01') wardrobe_synset = wn.synset('wardrobe.n.01') print(armchair_synset.wup_similarity(armchair_synset)) print(armchair_synset.wup_similarity(chair_synset)) print(armchair_synset.wup_similarity(wardrobe_synset)) wn.synset('chair.n.01').wup_similarity(wn.synset('person.n.01'))As we can see that 'armchair' and 'wardrobe' are surprisingly close to each other. It is because, for large ontologies, all the indoor 'things' are semantically 'indoor things'. This issue has motivated us to define thresholded Wu-Palmer Similarity Score, defined as followswhere τ is a hand-chosen threshold. Empirically, we found that τ = 0.9 works fine on DAQUAR [Malinowski and Fritz, 2014a]. Moreover, since DAQUAR has answers as sets of answer words, so that 'knife,fork' == 'fork,knife', we have extended the above measure to work with the sets. We call it Wu-Palmer Set score, or shortly WUPS.A detailed exposition of WUPS is beyond this tutorial, but a curious reader is encouraged to read the 'Performance Measure' paragraph in Malinowski and Fritz [2014a]. Note that the measure in Malinowski and Fritz [2014a] is defined broader, and essentially it abstracts away from any particular similarities such as Wu-Palmer Similarity, or an ontology. WUPS at 0.9 is WUPS with threshold τ = 0.9. It is worth noting, that a practical implementation of WUPS needs to deal with synsets. Thus it is recommended to download the script from http://datasets.d2.mpi-inf.mpg.de/ mateusz14visual-turing/calculate_wups.py or re-implement it with caution.The consensus measure handles ambiguities that are caused by various interpretations of a question or an image. In this tutorial, we do not cover this measure. A curious reader is encouraged to read the 'Human Consensus' in Malinowski et al. [2015].We present a few caveats when using WUPS. These can be especially useful if one wants to adapt WUPS to other datasets.Lack of coverage Since WUPS is based on an ontology, not always it recognizes words. For instance 'garbage bin' is missing, but 'garbage can' is perfectly fine. You can check it by yourself, either with the source code provided above, or by using an online script 8 .Synsets The execution of the following code wn.synsets('chair') produces a list with many elements. These elements are semantically equivalent 9 .For instance the following definition of 'chair'wn.synset('chair.n.03').definition() indicates a person (e.g. a chairman). Indeed, the following gives quite high value wn.synset('chair.n.03').wup_similarity(wn.synset('person.n.01'))however the following one has a more preferred, much lower value wn.synset('chair.n.01').wup_similarity(wn.synset('person.n.01'))How to deal with such a problem? In DAQUAR we take an optimistic perspective and always consider the highest similarity score. This works with WUPS 0.9 and a restricted indoor domain with a vocabulary based only on the training set. To sum up, this issue should be taken with a caution whenever WUPS is adapted to other domains.Ontology Since WUPS is based on an ontology, specifically on WordNet, it may give different scores on different ontolo- gies, or even on different versions of the same ontology.Threshold A good threshold τ is dataset dependent. In our case τ = 0.9 seems to work well, while τ = 0.0 is too forgivable and is rather reported due to the 'historical' reasons. However, following our papers, you should still consider to report plain set-based accuracy scores (so that Acc('knife,'fork','fork,knife')==1; it can be computed by our script 10 using the argument -1 to WUPS.WUPS is an evaluation measure that works with sets and word-level ambiguities. Arguably, WUPS at 0.9 is the most practical measure.Once the training of our models is over, we can evaluate their performance on a previously unknown test set. In the following, we show how to make predictions using the already discussed blind models.We start from encoding textual input into one-hot vector representations.In [ ]: test_text_representation = dp ['text'] Let us see the predictions.In [ ]: from numpy import random test_image_name_list = test_text_representation['img_name'] indices_to_see = random.randint(low=0, high=len(test_image_name_list), size=5) for index_now in indices_to_see:Without looking at images, a curious reader may attempt to answer the following questions.Do you agree with the answers given above? What are your guesses? Of course, neither you nor the model have seen any images so far.However, what happens if the reader can actually see the images?Execute the code below. Do your answers change after seeing the images?In [1]: from matplotlib.pyplot import axis from matplotlib.pyplot import figure from matplotlib.pyplot import imshow import numpy as np from PIL import Image Finally, let us also see the ground truth answers by executing the following code.In [ ]: print('question, prediction, ground truth answer') for index_now in indices_to_see:In the code above, we have randomly taken questions, and hence different executions of the code may lead to different answers.Let us do similar predictions with a Recurrent Neural Network. This time, we use Kraino, to make the code shorter.In A curious reader is encouraged to try the following exercise.Visualise question, predicted answers, ground truth answers as before.Check also images.All the considered so far architectures predict answers based only on questions, even though the questions concern images. Therefore, in this section, we also build visual features. As shown in Figure 6, a quite common practice is to:1. Take an already pre-trained CNN; often pre-training is done in some large-scale classification task such as ImageNet [Russakovsky et al., 2014]. 2. 'Chop off' a CNN representation after some layer. We use responses of that layer as visual features.In this tutorial, we use features extracted from the second last 4096 dimensional layer of the VGG Net [Simonyan and Zisserman, 2014]. We have already extracted features in advance using Caffe  -another excellent framework for deep learning, particularly good for CNNs. Given visual features, we can now build a full model that answer questions about images. As we can see in Figure 1, it is hard to answer correctly on questions without seeing images.Let us create an input as a pair of textual and visual features using the following code.In [ ]: train_input = [train_x, train_visual_features] In the following, we investigate two approaches to question answering: an orderless BOW, and an RNN.Similarly to our blind model, we start with a BOW encoding of a question. Here, we explore two ways of combining both modalities (circle with 'C' in Figure 7): concatenation, and piece-wise multiplication. For the sake of simplicity, we do not fine-tune the visual representation (dotted line symbolizes the barrier that blocks back-propagation in Figure 7). In [ ]: #== Model definition # First we define a model using keras/kraino output_dim=len(word2index_y.keys()), visual_dim=train_visual_features.shape [1] At the end of this section, a curious reader can try to answer the following questions.If we merge language and visual features with 'mul', do we need to set both embeddings to have the same number of dimensions? That is, do we require to have textual_embedding_dim == visual_embedding_dim?Now, we repeat the BOW experiments but with RNN. Figure 8 depicts the architecture.In [ ]: #== Model definition # First we define a model using keras/kraino from keras.models import Sequential from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers.core import Dropout from keras.layers.core import Layer  We can do training with batch size set to 512. If an error occurs due to a memory consumption, lowering the batch size should help.In [ ]: #== Model training text_image_rnn_model.fit( train_input, train_y, batch_size=512, nb_epoch=40, validation_split=0.1, show_accuracy=True)A curious reader may experiment with a few different batch sizes, and answer the following questions.Can you experiment with batch-size=1, and next with batch-size=5000? Can you explain both issues regarding the batch size? When do you get the best performance, with multiplication, concatenation, or summation?Summary As previously, using RNN makes the sequence processing order-aware. This time, however, we combine two modalities so that the whole model 'sees' images. Finally, it is also important how both modalities are combined. The models that we have built so far can be transferred to other dataset. Let us consider a recently introduced large-scale dataset, which is named VQA [Antol et al., 2015]. In this section, we train and evaluate VQA models. Since the reader should already be familiar with all the pieces, we just quickly jump into coding. For the sake of simplicity, we use only BOW architectures. Since VQA hides the test data for the purpose of challenge, we use the publicly available validation set to evaluate the architectures.In  The task that tests machines via questions about the content of images is a quite new research direction that recently has gained popularity. Therefore, many opportunities are available. We end the tutorial by enlisting a few possible directions.• Global Representation In this tutorial, we use a global, full-frame representation of the images. Such a represen- tation may destroy too much information. Therefore, it seems a fine-grained alternatives should be valid options. Maybe we should use detections, or object proposals (e.g. Ilievski et al. [2016] use a question dependent detections, and Mokarian Forooshani et al. [2016] use object proposals to enrich a visual representation). We could also use attention models, which become quite successful in answering questions about images [Lu et al., 2016]. However, there is still a hope for global representations if they are trained end-to-end for the task, and question dependent. In the end, our global representation is extracted from CNNs trained on a different dataset (ImageNet), and for different task (object classification).• 3D Scene Representation Most of current approaches, and all neural-based approaches, are trained on 2D images.However, some spatial relations such as 'behind' may need a 3d representation of the scene (in fact Malinowski and Fritz [2014a] design spatial rules using a 3d coordinate system). DAQUAR is built on top of Silberman et al. [2012] that provides both modes (2D images, and 3D depth), however, such a richer visual information is currently not fully exploited.• Recurrent Neural Networks There is disturbingly small gap between BOW and RNN models. As we have seen in the tutorial, some questions clearly require an order, but such questions at the same time become longer, semantically more difficult, and require better a visual understanding of the world. To handle them we may need other RNNs architectures, or better ways of fusing two modalities, or better Global Representation.• Logical Reasoning There are few questions that require a bit more sophisticated logical reasoning such as negation.Can Recurrent Neural Networks learn such logical operators? What about compositionality of the language? Perhaps, we should aim at mixed approaches, similar to the work of Andreas et al. [2016].• Language + Vision There is still a small gap between Language Only and Vision + Language models. But clearly, we need pictures to answer questions about images. So what is missing here? Is it due to Global Representation, 3D Scene Representation or there is something missing in fusing two modalities? The latter is studied, with encouraging results, in Fukui et al. [2016].• Learning from Few Examples In the Visual Turing Test, many questions are quite unique. But then how the models can generalize to new questions? What if a question is completely new, but its parts have been already observed (compositionality)? Can models guess the meaning of a new word from its context? • Ambiguities How to deal with ambiguities? They are all inherent in the task, so cannot be just ignored, and should be incorporated into question answering methods as well as evaluation metrics.• Evaluation Measures Although we have WUPS and Consensus, both are far from being perfect. Consensus has higher annotation cost for ambiguous tasks, and is unclear how to formally define good consensus measure. WUPS is an ontology dependent, which may be quite costly to build for all interesting domains? Finally, the current evaluation metrics ignore the tail of the answer distribution encouraging models to focus only on a few most frequent answers.
