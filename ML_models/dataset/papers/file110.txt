Statistical language models are a crucial component of speech recognition, machine translation and information retrieval systems. In order to handle the data sparsity problem associated with tradi- tional n-gram language models (LMs), neural network language models (NNLMs) ( Bengio et al., 2001) represent the history context in a continuous vector space that can be learned towards error rate reduction by sharing data among similar contexts. Instead of using fixed number of words to represent context, recurrent neural network language models (RNNLMs) (Mikolov et al., 2010) use a recurrent hidden layer to represent longer and variable length histories. RNNLMs significantly outperform traditional n-gram LMs, and are therefore becoming an increasingly popular choice for practitioners ( Mikolov et al., 2010;Sundermeyer et al., 2013;Devlin et al., 2014).Consider a standard RNNLM, depicted in Figure 1. The network has an input layer x, a hidden layer s (also called context layer or state) with a recurrent connection to itself, and an output layer y. Typically, at time step t the network is fed as input x t ∈ R V , where V denotes the vocabulary size, and s t−1 ∈ R h , the previous state. It produces a hidden state s t ∈ R h , where h is the size of the hidden layer, which in turn is transformed to the output y t ∈ R V . Different layers are fully connected, with the weight matrices denoted by Ω = {W For language modeling applications, the input x t is a sparse vector of a 1-of-V (or one-hot) encoding with the element corresponding to the input word w t−1 being 1 and the rest of components of x t set to 0; the state of the network s t is a dense vector, summarizing the history context {w t−1 , · · · , w 0 }  Figure 1: The network architecture of a standard RNNLM and its unrolled version for an example input sentence: &lt;s&gt; A cat is sitting on a sofa &lt;/s&gt;.preceding the output word w t ; and the output y t is a dense vector, with the i-th element denoting the probability of the next word being w i , that is, p(w i |w t−1 , · · · , w 0 ), or more concisely, p(w i |s t ).The input to output transformation occurs via:where σ(v) = 1/(1 + exp(−v)) is the sigmoid activation function, and f (·) is the softmax function f (u i ) := exp(u i )/ V j=1 exp(u j ). One can immediately see that if x t uses a 1-of-V encoding, then the computations in equation (1) are relatively inexpensive (typically h is of the order of a few thousand, and the computations are O(h 2 )), while the computations in equation (2) are expensive (typically V is of the order of a million, and the computations are O(V h)). Similarly, back propagating the gradients from the output layer to the hidden layer is expensive. Consequently, the training times for some of the largest models reported in literature are of the order of weeks (Mikolov et al., 2011;Williams et al., 2015).In this paper, we ask the following question: Can we design an approximate training scheme for RNNLM which will improve on the state of the art models, while using significantly less compu- tational resources? Towards this end, we propose BlackOut an approximation algorithm to effi- ciently train massive RNNLMs with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a weighted sampling strategy which significantly reduces com- putation while improving stability, sample efficiency, and rate of convergence. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE) (Gutmann &amp; Hyvärinen, 2012;Mnih &amp; Teh, 2012), and demonstrate that BlackOut mitigates some of the limitations of both previous methods. Our experiments, on the recently released one billion word language modeling benchmark ( Chelba et al., 2014), demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, achieving the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single CPU machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words.One way to understand BlackOut is to view it as an extension of the DropOut strategy ( Srivastava et al., 2014) to the output layer, wherein we use a discriminative training loss and a weighted sam- pling scheme. The connection to DropOut is mainly from the way they operate in model training and model evaluation. Similar to DropOut, in BlackOut training a subset of output layer is sampled and trained at each training batch and when evaluating, the full network participates. Also, like DropOut, a regularization technique, our experiments show that the models trained by BlackOut are less prone to overfitting. A primary difference between them is that DropOut is routinely used at input and/or hidden layers of deep neural networks, while BlackOut only operates at output layer. We chose the name BlackOut in light of the similarities between our method and DropOut, and the complementary they offer to train deep neural networks.We will primarily focus on estimation of the matrix W out . To simplify notation, in the sequel we will use θ to denote W out and θ j to denote the j-th row of W out . Moreover, let ·· denote the dot product between two vectors. Given these notations, one can rewrite equation (2) asRNNLMs with a softmax output layer are typically trained using cross-entropy as the loss function, which is equivalent to maximum likelihood (ML) estimation, that is, to find the model parameter θ which maximizes the log-likelihood of target word w i , given a history context s:whose gradient is given byThe gradient of log-likelihood is expensive to evaluate because (1) the cost of computing p θ (w j |s) is O(V h) and (2) the summation above takes time linear in the vocabulary size O(V ).To alleviate the computational bottleneck of computing the gradient (5), we propose to use the following discriminative objective function for training RNNLM:where S K is a set of indices of K words drawn from the vocabulary, and i / ∈ S K . Typically, K is a tiny fraction of V , and in our experiments we use K ≈ V /200. To generate S K we will sample K words from the vocabulary using an easy to sample distribution Q(w), and set q j := 1 Q(wj ) in order to compute˜pj∈S K Equation 6 is the cost function of a standard logistic regression classifier that discriminates one positive sample w i from K negative samples w j , ∀j ∈ S K . The first term in (6) corresponds to the traditional maximum likelihood training, and the second term explicitly pushes down the probability of negative samples in addition to the implicit shrinkage enforced by the denominator of (7). In our experiments, we found the discriminative training (6) outperforms the maximum likelihood training (the first term of Eq. 6) in all the cases, with varying degree of accuracy improvement depending on K.The weighted softmax function (7) can be considered as a stochastic version of the standard soft- max (3) on a different base measure. While the standard softmax (3) uses a base measure which gives equal weights to all words, and has support over the entire vocabulary, the base measure used in (7) has support only on K + 1 words: the target word w i and K samples from Q(w). The noise portion of (7) has the motivation from the sampling scheme, and the q i term for target word w i is introduced mainly to balance the contributions from target word and noisy sample words. 1 Other justifications are discussed in Sec. 2.1 and Sec. 2.2, where we establish close connections between BlackOut, importance sampling, and noise contrastive estimation.Due to the weighted sampling property of BlackOut, some words might be sampled multiple times according to the proposal distribution Q(w), and thus their indices may appear multiple times in S K . As w i is the target word, which is assumed to be included in computing (7), we therefore set i / ∈ S K explicitly. Substituting (7) into (6) and letting u j = j , s and˜pand˜ and˜p j = ˜ p θ (w j |s), we haveThen taking derivatives with respect to u j , ∀j ∈ {i} ∪ S K , yieldsBy the chain rule of derivatives, we can propagate the errors backward to previous layers and com- pute the gradients with respect to the full model parameters Ω. In contrast to Eq. 5, Eqs. 9 and 10 are much cheaper to evaluate as (1) the cost of computing˜pcomputing˜ computing˜p j is O(Kh) and (2) the summation takes O(K), hence roughly a V /K times of speed-up.Next we turn our attention to the proposal distribution Q(w). In the past, a uniform distribution or the unigram distribution have been advocated as promising candidates for sampling distributions (Bengio &amp; Senécal, 2003;Jean et al., 2015;Bengio &amp; Senécal, 2008;Mnih &amp; Teh, 2012). As we will see in the experiments, neither one is suitable for a wide range of datasets, and we find that the power-raised unigram distribution of Mikolov et al. (2013) is very important in this context:Note that Q α (w) is a generalization of uniform distribution (when α = 0) and unigram distribution (when α = 1). The rationale behind our choice is that by tuning α, one can interpolate smoothly between sampling popular words, as advocated by the unigram distribution, and sampling all words equally. The best α is typically dataset and/or problem dependent; in our experiments, we use a holdout set to find the best value of α. It's worth noting that this sampling strategy has been used by Mikolov et al. (2013) in a similar context of word embedding, while here we explore its effect in the language modeling applications.After BlackOut training, we evaluate the predictive performance of RNNLM by perplexity. To cal- culate perplexity, we explicitly normalize the output distribution by using the exact softmax func- tion (3). This is similar to DropOut ( Srivastava et al., 2014), wherein a subset of network is sampled and trained at each training batch and when evaluating, the full network participates.BlackOut has a close connection to importance sampling (IS). To see this, differentiating the loga- rithm of Eq. 7 with respect to model parameter θ, we haveIn contrast with Eq. 5, it shows that the weighted softmax function (7) corresponds to an IS-based estimator of the standard softmax (3) with a proposal distribution Q(w).Importance sampling has been applied to NNLMs with large output layers in previous works (Bengio &amp; Senécal, 2003;Jean et al., 2015). However, either uniform distribution or unigram distri- bution is used for sampling and all aforementioned works exploit the maximum likelihood learning of model parameter θ. By contrast, BlackOut uses a discriminative training (6) and a power-raised unigram distribution Q α (w) for sampling; these two changes are important to mitigate some of lim- itations of IS-based approaches. While an IS-based approach with a uniform proposal distribution is very stable for training, it suffers from large bias due to the apparent divergence of the uniform distribution from the true data distribution p θ (w|s). On the other hand, a unigram-based IS esti- mate can make learning unstable due to the high variance (Bengio &amp; Senécal, 2003;. Using a power-raised unigram distribution Q α (w) entails a better trade-off between bias and variance, and thus strikes a better balance between these two extremes. In addition, as we will see from the experi- ments, the discriminative training of BlackOut speeds up the rate of convergence over the traditional maximum likelihood learning.The basic idea of NCE is to transform the density estimation problem to the problem of learning by comparison, e.g., estimating the parameters of a binary classifier that distinguishes samples from the data distribution p d from samples generated by a known noise distribution p n ( Gutmann &amp; Hyvärinen, 2012). In the language modeling setting, the data distribution p d will be the distribution p θ (w|s) of interest, and the noise distribution p n is often chosen from the ones that are easy to sample from and possibly close to the true data distribution (so that the classification problem isn't trivial). While Mnih &amp; Teh (2012) uses a context-independent (unigram) noise distribution p n (w), BlackOut can be formulated into the NCE framework by considering a context-dependent noise distribution p n (w|s), estimated from K samples drawn from Q(w), byj∈S K which is a probability distribution function under the expectation that K samples are drawn fromSimilar to Gutmann &amp; Hyvärinen (2012), noise samples are assumed K times more frequent than data samples so that data points are generated from a mixture of two distributions: 1 K+1 p θ (w|s) and K K+1 p n (w|s). Then the conditional probability of sample w i being generated from the data distribution isInserting Eq. 13 into Eq. 14, we havej∈S K which is exactly the weighted softmax function defined in (7). Note that due to the noise distribution proposed in Eq. 13, the expensive denominator (or the partition function Z) of p θ (w j |s) is canceled out, while in Mnih &amp; Teh (2012) the partition function Z is either treated as a free parameter to be learned or approximated by a constant. Mnih &amp; Teh (2012) recommended to set Z = 1.0 in the NCE training. However, from our experiments, setting Z = 1.0 often leads to sub-optimal solutions 2 and different settings of Z sometimes incur numerical instability since the log-sum-exp trick 3 can not be used there to shift the scores of the output layer to a range that is amenable to the exponential function. BlackOut does not have this hyper-parameter to tune and the log-sum-exp trick still works for the weighted softmax function (7). Due to the discriminative training of NCE and BlackOut, they share the same objective function (6).We shall emphasize that according to the theory of NCE, the K samples should be sampled from the noise distribution p n (w|s). But in order to calculate p n (w|s), we need the K samples drawn from Q(w) beforehand. As an approximation, we use the same K samples drawn from Q(w) as the K samples from p n (w|s), and only use the expression of p n (w|s) in (13) to evaluate the noise density value required by Eq. 14. This approximation is accurate since E S K ∼Q(w) (p n (w i |s)) = Q(w i ) as proved in Appendix A, and we find empirically that it performs much better (with improved stability) than using a unigram noise distribution as in Mnih &amp; Teh (2012).Many approaches have been proposed to address the difficulty of training deep neural networks with large output spaces. In general, they can be categorized into four categories:• Hierarchical softmax (Morin &amp; Bengio, 2005;Mnih &amp; Hinton, 2008) uses a hierarchical binary tree representation of the output layer with the V words as its leaves. It allows ex- ponentially faster computation of word probabilities and their gradients, but the predictive performance of the resulting model is heavily dependent on the tree used, which is of- ten constructed heuristically. Moreover, by relaxing the constraint of a binary structure, Le et al. (2011) introduces a structured output layer with an arbitrary tree structure constructed from word clustering. All these methods speed up both the model training and evaluation considerably.• Sampling-based approximations select at random or heuristically a small subset of the out- put layer and estimate gradient only from those samples. The use of importance sampling in Bengio &amp; Senécal (2003; Unfortunately, it only applies to a lim- ited family of loss functions that includes squared error and spherical softmax, while the standard softmax isn't included.As discussed in the introduction, BlackOut also shares some similarity to DropOut ( Srivastava et al., 2014). While DropOut is often applied to input and/or hidden layers of deep neural networks to avoid feature co-adaptation and overfitting by uniform sampling, BlackOut applies to a softmax output layer, uses a weighted sampling, and employs a discriminative training loss. We chose the name BlackOut in light of the similarities between our method and DropOut, and the complementary they offer to train deep neural networks.We implemented BlackOut on a standard machine with a dual-socket 28-core Intel R Xeon R Haswell CPU. To achieve high throughput, we train RNNLM with Back-Propagation Through Time (BPTT) (Rumelhart et al., 1988) with mini-batches ( Chen et al., 2014). We use RMSProp (Hinton, 2012) for learning rate scheduling and gradient clipping ( Bengio et al., 2013) to avoid the gradi- ent explosion issue of recurrent networks. We use the latest Intel MKL library (version 11.3.0) for SGEMM calls, which has improved support for tall-skinny matrix-matrix multiplications, which consume about 80% of the run-time of RNNLMs.It is expensive to access and update large models with billions of parameters. Fortunately, due to the 1-of-V encoding at input layer and the BlackOut sampling at output layer, the model update on W in and W out is sparse, i.e., only the model parameters corresponding to input/output words and the samples in S K are updated at each training batch. However, subnet updates have to be done carefully due to the dependency within RMSProp updating procedure. We therefore propose an approximated RMSProp that enables an efficient subnet update and thus speeds up the algorithm even further. Details can be found in Appendix C.In our experiments, we first compare BlackOut, NCE and exact softmax (without any approxima- tion) using a small dataset. We then evaluate the performance of BlackOut on the recently released one billion word language modeling benchmark ( Chelba et al., 2014) with a vocabulary size of up to one million. We compare the performance of BlackOut on a standard CPU machine versus the state- of-the-arts reported in the literature that are achieved on GPUs or on clusters of CPU nodes. Our implementation and scripts are open sourced at https://github.com/IntelLabs/rnnlm.Corpus Models are trained and evaluated on two different corpora: a small dataset provided by the RNNLM Toolkit 5 , and the recently released one billion word language modeling benchmark 6 , which is perhaps the largest public dataset in language modeling. The small dataset has 10,000 training sentences, with 71,350 words in total and 3,720 unique words; and the test perplexity is evaluated on 1,000 test sentences. The one billion word benchmark was constructed from a mono- lingual/English corpora; after all necessary preprocessing including de-duplication, normalization and tokenization, 30,301,028 sentences (about 0.8 billion words) are randomly selected for training, 6,075 sentences are randomly selected for test and the remaining 300,613 sentences are reserved for future development and can be used as holdout set.We evaluate BlackOut, NCE and exact softmax (without any approximation) on the small dataset described above. This small dataset is used so that we can train the standard RNNLM algorithm with exact softmax within a reasonable time frame and hence to provide a baseline of expected perplexity. There are many other techniques involved in the training, such as RMSProp for learning rate scheduling (Hinton, 2012), subnet update (Appendix C), and mini-batch splicing (Chen et al., 2014), etc., which can affect the perplexity significantly. For a fair comparison, we use the same tricks and settings for all the algorithms, and only evaluate the impact of the different approximations (or no approximation) on the softmax output layer. Moreover, there are a few hyper-parameters that have strong impact on the predictive performance, including α of the proposal distribution Q α (w) for BlackOut and NCE, and additionally for NCE, the partition function Z. We pay an equal amount of effort to tune these hyper-parameters for BlackOut and NCE on the validation set as number of samples increases. Figure 2 shows the perplexity reduction as a function of number of samples K under two different vocabulary settings: (a) a full vocabulary of 3,720 words, and (b) using the most frequent 2,065 words as vocabulary. The latter is a common approach used in practice to accelerate RNNLM computation by using RNNLM to predict only the most frequent words and handling the rest using an n-gram model (Schwenk &amp; Gauvain, 2005). We will see similar vocabulary settings when we evaluate BlackOut on the large scale one billion word benchmark.As can be seen, when the size of the samples increases, in general both BlackOut and NCE improve their prediction accuracy under the two vocabulary settings, and even with only 2 samples both al- gorithms still converge to reasonable solutions. BlackOut can utilize samples much more effectively than NCE as manifested by the significantly lower perplexities achieved by BlackOut, especially when number of samples is small; Given about 20-50 samples, BlackOut and NCE reach similar perplexities as the exact softmax, which is expensive to train as it requires to evaluate all the words in the vocabularies. When the vocabulary size is 2,065, BlackOut achieves even better perplexity than that of the exact softmax. This is possible since BlackOut does stochastic sampling at each training example and uses the full softmax output layer in prediction; this is similar to DropOut that is routinely used in input layer and/or hidden layers of deep neural networks ( Srivastava et al., 2014). As in DropOut, BlackOut has the benefit of regularization and avoids feature co-adaption and is pos- sibly less prone to overfitting. To verify this hypothesis, we evaluate the perplexities achieved on the training set for different algorithms and provide the results in Figure 5 at Appendix B. As can been seen, the exact softmax indeed overfits to the training set and reaches lower training perplexities than NCE and BlackOut. shows that BlackOut enjoys a much faster convergence rate than NCE, especially when number of samples is small (Figure 3(a)); but this advantage gets smaller when number of samples increases (Figure 3(b)). We also observed similar behavior when we evaluated BlackOut and NCE on the large scale one billion word benchmark. Following the experiments in Williams et al. (2015), we evaluate the performance of BlackOut on a vocabulary of 64K most frequent words. This is similar to the scenario in Figure 2(b) where the most frequent words are kept in vocabulary and the rest rare words are mapped to a special &lt;unk&gt; token. We first study the importance of α of the proposal distribution Q α (w) and the discrimina- tive training (6) as proposed in BlackOut. As we discussed in Sec. 2, when α = 0, the proposal distribution Q α (w) degenerates to a uniform distribution over all the words in the vocabulary, and when α = 1, we recover the unigram distribution. Thus, we evaluate the impact of α in the range of [0,1]. Figure 4(a) shows the evolution of test perplexity as a function of α for the RNNLMs with 256 hidden units. As can be seen, α has a significant impact on the prediction accuracy. The com- monly used uniform distribution (when α = 0) and unigram distribution (when α = 1) often yield sub-optimal solutions. For the dataset and experiment considered, α = 0.4 gives the best perplexity (consistent on holdout set and test set). We therefore use α = 0.4 in the experiments that follow. The number of samples used is 500, which is about 0.8% of the vocabulary size.   (6) over the maximum likelihood training (the first term of Eq. 6) on the RNNLMs with 512 hidden units using two different α's. In general, we observe 1-3 points of perplexity reduction due to discriminative training over traditional maximum likelihood training.Finally, we evaluate the scalability of BlackOut when number of hidden units increases. As the dataset is large, we observed that the performance of RNNLM depends on the size of the hidden layer: they perform better as the size of the hidden layer gets larger. As a truncated 64K word vocabulary is used, we interpolate the RNNLM scores with a full size 5-gram to fill in rare word probabilities (Schwenk &amp; Gauvain, 2005;Park et al., 2010). We report the interpolated perplexities BlackOut achieved and compare them with the results from Williams et al. (2015) in Table 1. As can be seen, BlackOut reaches lower perplexities than those reported in Williams et al. (2015) within comparable time frames (often 10%-40% faster). We achieved a perplexity of 42.0 when the hidden layer size is 4096. To the best of our knowledge, this is the lowest perplexity reported on this benchmark. In the final set of experiments, we evaluate the performance of BlackOut with a very large vocabulary of 1,000,000 words, and the results are provided in Table 2. This is the largest vocabulary used on this benchmark that we could find in existing literature. We consider the RNNLM with 1,024 hidden units (about 2 billion parameters) and 2,048 hidden units (about 4.1 billion parameters) and compare their test perplexities with the results from Le et al. (2015). We use 2,000 samples, 0.2% of the vocabulary size, for BlackOut training with α = 0.1. Comparing to the experiments with the 64K word vocabulary, a much smaller α is used here since the sampling rate (0.2%) is much lower than that is used (0.8%) when the vocabulary size is 64K, and a smaller α strikes a better balance between sample coverage per training example and convergence rate. In contrast, NCE with the same setting converges very slowly (similar to Figure 3(a)) and couldn't reach a competitive perplexity within the time frame considered, and its results are not reported here.As the standard RNN/LSTM algorithms (without approximation) are used in Le et al. (2015), a cluster of 32 CPU machines (at least 20 cores each) are used to train the models for about 60 hours. BlackOut enables us to train this large model using a single CPU machine for 175 hours.Since different model architectures are used in the experiments (deep RNN/LSTM vs. standard RNNLM), the direct comparison of test perplexity isn't very meaningful. However, this experiment demonstrates that even though our largest model is about 2-3 times larger than the models evaluated in Le et al. (2015), BlackOut, along with a few other optimization techniques, make this large scale learning problem still feasible on a single box machine without using GPUs or CPU clusters. 68.3Last, we collect all the state of the art results we are aware of on this benchmark and summarize them in Table 3. Since all the models are the interpolated ones, we interpolate our best RNN model 7 from Table 2 with the KN 5-gram model and achieve a perplexity score of 47.3. Again, different papers provide their best models trained with different architectures and vocabulary settings. Hence, an absolutely fair comparison isn't possible. Regardless of these discrepancies, our models, within different groups of vocabulary settings, are very competitive in terms of prediction accuracy and model size. We proposed BlackOut, a sampling-based approximation, to train RNNLMs with very large vocabu- laries (e.g., 1 million). We established its connections to importance sampling and noise contrastive estimation (NCE), and demonstrated its stability, sample efficiency and rate of convergence on the recently released one billion word language modeling benchmark. We achieved the lowest reported perplexity on this benchmark without using GPUs or CPU clusters.As for future extensions, our plans include exploring other proposal distributions Q(w), and theo- retical properties of the generalization property and sample complexity bounds for BlackOut. We will also investigate a multi-machine distributed implementation.Models with Very Large Vocabularies (Supplementary Material) A NOISE DISTRIBUTION p n (w i |s)Theorem 1 The noise distribution function p n (w i |s) defined in Eq. 13 is a probability distribution function under the expectation that K samples in S K are drawn from Q(w) randomly, C SUBNET UPDATE WITH APPROXIMATED RMSPROP RMSProp (Hinton, 2012) is an adaptive learning rate method that has found much success in prac- tice. Instead of using a single learning rate to all the model parameters in Ω, RMSProp dedicates a learning rate for each model parameter and normalizes the gradient by an exponential moving average of the magnitude of the gradient:where β ∈ (0, 1) denotes the decay rate. The model update at time step t is then given bywhere is the learning rate and λ is a damping factor, e.g., λ = 10 −6 . While RMSProp is one of the most effective learning rate scheduling techniques, it requires a large amount of memory to store per-parameter v t in addition to model parameter Ω and their gradients.It is expensive to access and update large models with billions of parameters. Fortunately, due to the 1-of-V encoding at input layer and the BlackOut sampling at output layer, the model update on W in and W out is sparse, e.g., only the model parameters corresponding to input/output words and the samples in S K are to be updated.8 For Eq. 16, however, even a model parameter is not involved in the current training, its v t value still needs to be updated by v t = βv t−1 since its (∇J) 2 = 0. Ignoring this update has detrimental effect on the predictive performance; in our experiments, we observed 5 − 10 point perplexity loss if we ignore this update completely.We resort to an approximation to v t = βv t−1 . Given p u (w) is the probability of a word w being selected for update, the number of time steps elapsed when it is successfully selected follows a geometric distribution with a success rate p u (w), whose mean value is 1/p u (w). Assume that an input/output word is selected according to the unigram distribution p uni (w) and the samples in S K are drawn from Q α (w), Eq. 16 can be approximated bywith p u (w) = p uni (w) × B × T for word w at input layer p uni (w) × B × T + Q α (w) × K × T for word w at output layer,where B is the mini-batch size and T is the BPTT block size. Now we can only update the model parameters, typically a tiny fraction of Ω, that are really involved in the current training, and thus speed up the RNNLM training further.
