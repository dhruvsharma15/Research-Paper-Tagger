Let us consider the set B d of random d−dimensional unit vectors:with the components corresponding to independent Bernoulli variables, γ i ∈ {−1, 1}, i = 1, ..., d, with the probability p = 1/2.Let us assume that ξ, ζ ∈ B d are two random vectors from B d . These two vectors are orthogonal ξ ⊥ γ if their dot product:is equal to zero. For high-dimensional vectors extracted from B d , the expectation value of the dot product is obviously:and the variance is:Also, using the Chernoff bound [8] we obtain:Thus, the probability that the two random vectors ζ, ξ ∈ B d are "almost orthogonal" is given by:This means that for a relatively large dimensionality d, the probability that two random vectors from B d are "almost orthogonal" is quite high. For example, if δ = 0.05 and d = 1, 200 we have: Pr 0.05 (ζ ⊥ ξ) &gt; 0.95. In general, one can show that in a high dimensional space there is an exponentially large number of "almost orthogonal" randomly chosen vectors [7,9]. Following the random indexing approach, in the next section we show that this "intriguing" property can be used to "memorize" random vectors from B d by simply adding them.Let us now consider the following set membership problem: given a set of k random vectors,This is a typical binary decision problem, with the answer TRUE or FALSE. Normally, the solution requires the calculation of the dot product of γ with each vector ξ i ∈ Ξ, i = 1, 2, ..., K. If ∃ξ i ∈ Ξ such that γ T ξ i = 1 then the answer is TRUE, otherwise the answer is FALSE. Thus, the solution to the set membership problem is practically a binary classifier, which also acts as a "set filter" in B d .The above set membership problem can be also reformulated as a "query" problem, by asking to return the vector ξ i * ∈ Ξ, which is most similar to the "query" vector γ. In this case the answer is obtained by taking the k dot products, and searching for the index i * of the product with the highest value:We can see that statistically the set membership problem requires an average of k/2 operations (dot products) in order to provide a correct answer, while the "query" problem requires k operations (dot products) and a sorting procedure. However, here we will show that probabilistically the set membership problem can be solved using only one operation (dot product).Let us consider the sum of all random vectors from the given set Ξ:and the dot product of ζ with the "query" vector γ:Let us first assume that ∃i such that ξ i = γ. Then the expectation of the dot product is: Obviously, if such that ξ i = γ, then the expectation of the dot product is:Also, one can easily see that the variance of the dot product is:j=1In order to illustrate numerically this result we consider d = 10 4 and k = 10 3 , and we plot the value of the dot product η in 10 3 cases where the "query" vector γ is a member, and respectively a non- member, of the set Ξ. The results are shown in Figure 1. Here we have also included the distributions of η for the distinct member and non-member situations.  Figure 2). Since the intersection point of these two distributions is at η * = 1/2, the overlap will be:1.41.2non-members members where Φ(x) is the cumulative distribution function:−∞In fact, the overlap s(σ) is an estimation of the sum of false positive (F P ) and false negative (F N ) classification cases:also because of the perfect symmetry of the intersecting distributions we have:and respectively:where T P and T N are the true positive, and respectively true negative classification cases. Therefore, the classification precision and recall are equal to the following quantity:In order to illustrate numerically this result we consider d = 10 3 and we let k = 2, ..., d. For each k we compute the distributions from T = 10 3 samples with 10 3 cases where the "query" vector γ is a member, and respectively a non-member, of the set Ξ. The obtained results for ρ(σ) are shown in We consider a vocabulary V of n unique words. With each word w ∈ V we associate a randomly drawn vector ξ w from B d . Thus, we associate the vocabulary V = {w i | i = 1, ..., n} with a set.., n} of "almost orthogonal" random vectors.Let us now consider a document D, containing |D| words from the vocabulary V . For each word w( in D with the index i in the vocabulary V , w( ≡ w i , we also consider the context window ofWe define the context γ i of the word w i ∈ V as the sum of the word vectors from all the corresponding context windows extracted from D:where χ(w) is the function that returns the index of the word w in the vocabulary V , and δ is Kronecker delta function:Thus, given a document D, for each word w i we calculate its context as a sum γ i over all windows at the positions where w i appears in the document D. Since, the vectors ξ i ∈ Ξ are "almost orthogonal", and according to the previously obtained result for the "set membership problem", the context sum can accommodate quite a large number of vectors until its "set filtering" properties will significantly deteriorate. For example, let's assume that the context of the word w i is the set Γ i = {w i1 , ..., w im } where each word w ij appears a number of θ ij times. Then, the context vector of the word w i is:j=1One can easily check if a word w i k (associated with the vector ξ i k ) is a "member" of the context γ i by simply taking the dot product:which has the expectation:j j =kConsequently, the variance is:j Therefore, if σ ∈ (0, 0.375] we have a good precision and recall. Now let's assume that we have two words w i and w j with the context vectors γ i and respectively γ j :k=1One can check their similarity by taking their dot product:The expectation of η ij is: whereThus, the similarity between w i and w j is determined by the number of identical words present in their context vectors γ i and γ j .Let us consider an example by using the book The Adventures of Sherlock Holmes by Sir ArthurConan Doyle, which can be downloaded from the Gutenberg Project [10]. The document is processed by removing all the "stop words", which do not bring meaningful information to contexts (words with very high frequency like: "the"), we also eliminate all the non-alphanumeric words, and the remaining words are lemmatized. The resulted corpus has 38,812 words, with a vocabulary of 5,829 unique words.In Figure 4  Another possible application is to search a document in order to find similar sentences to a given "query"sentence. Again, we associate the vocabulary V = {w i | i = 1, ..., n} with a set Ξ = {ξ i | ξ i ∈ B d , i = 1, ..., n} of "almost orthogonal" random vectors, and we assume that each sentence from a document D is represented by the sum of its words. Thus, the ith sentence s i = {w i1 , ..., w imi } of a document D, will be represented by the vector:j=1 where i j ≡ χ(w ij ).Let us also assume that the vector corresponding to the sentence query q = {w 1 , ..., w k } is:where ≡ χ(w ). Finding the most similar sentence to q is equivalent to solving:The expectation of the dot product η i = νζ T γ i is E(η i ) = νn i , where n i is the number of words the sentences q and s i have in common, and ν = ( i −1 . Also, the variance of the dot product is:Thus, with a reasonable high dimensionality d we can obtain a very good precision (recall). For example, let's consider again the book The Adventures of Sherlock Holmes, and we ask the following naive question: "Who is the woman Irene in the photograph, and what is her special connection to Sherlock?" The question is naive because after processing the "query" sentence, only the following words actually are used in the search process: woman, irene, photograph, special, connection, sherlock, and they are also independent of eachother, since only their "sum" is used.The first three sentences returned using the method described above are: (1) "And when he speaks of Irene Adler, or when he refers to her photograph, it is always under the honourable title of the woman."; (2) "And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory."; (3) "The photograph was of Irene Adler herself in evening dress, the letter was superscribed to Sherlock Holmes, Esq."We also used the dot product without normalization η i = ζ T γ i , and the results for the first three returned sentences are also interesting: (1) "To Sherlock Holmes she is always THE woman."; (2) "And when he speaks of Irene Adler, or when he refers to her photograph, it is always under the honourable title of the woman."; (3) "And yet there was but one woman to him, and that woman was the late IreneAdler, of dubious and questionable memory."Therefore, the normalization of the sentence vectors before taking the dot product may or may not be necessary, and both cases may return relevant results.Spam filters are built in order to protect email users from spam and phishing messages. Most spam filters are word-based filters, which simply block any email that contains certain words or phrases.Another approach is based on machine learning techniques such as Bayesian classifiers, which must be trained on large sets of already classified spam and non-spam messages. Here we discuss a different approach, based on the "almost orthogonal" property of random vectors.As in the previously described applications, we associate the vocabulary V = {w i | i = 1, ..., n} with a set Ξ = {ξ i | ξ i ∈ B d , i = 1, ..., n} of "almost orthogonal" random vectors, and we assume that each message is represented by the sum of its words, and equivalently the sum of "almost orthogonal"vectors representing the words. Therefore, we assume that we have m messages g j , j = 1, ..., m, already classified, such that the associated vectors are:where m j is the number of words in the message g j . Also, the class of each message g j , j = 1, ..., m, is known:Now, let us assume that h is a new message, with the associated vector:where k is the number of words in h.In order to classify h as spam or non-spam we simply compute:and we assign to h the class of g j * :Thus, the class attributed to h is the class of the most similar, and already classified message g j * .In order to evaluate this very simple method we use the Ling-Spam corpus [11], as described in the paper Ref. [12]. The data set contains four subdirectories, corresponding to four versions of the corpus:(1) bare: lemmatiser disabled, stop-list disabled; (2) lemm: lemmatiser enabled, stop-list disabled; (3) lemm-stop: lemmatiser enabled, stop-list enabled; (4) stop: lemmatiser disabled, stop-list enabled.In our experiment we used the files from the first subdirectory: "bare: lemmatiser disabled, stop- We preprocessed the messages using the spaCy Python library [13]. The messages were processed by removing the "stop words" and the remaining words were lemmatized, resulting in a vocabulary of 54,442 unique words. The results for 10-fold cross validation are shown in Figure  on the classification precision and recall. Also, one can see that the described method based on "almost orthogonal" random vectors gives better results (recall ≈ 0.967, precision ≈ 0.946 for d = 3000) than the Bayesian approach described in Ref. [12], even though in this case there is no learning involved.In this paper we have explored a different approach to the "vector semantics" problem, which is based on the "almost orthogonal" property of high-dimensional random vectors. We have shown that the "almost orthogonal" property can be used to "memorize" random vectors by simply adding them, and we have provided an efficient probabilistic solution to the set membership problem. Also, we have discussed several applications to word and context vector embeddings, document sentences similarity, and spam filtering. One can easily extend this approach to other problems, like for example sentiment analysis. Contrary to the "expensive" machine learning methods, this method is very simple and it does not even require a "learning" process, however it exhibits similar properties.
