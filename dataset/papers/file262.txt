The class of Recurrent Neural Network models (RNNs) is particularly well suited to dealing with sequential data, and has been successfully applied to a diverse array of tasks, such as language modeling and speech recognition (Mikolov, 2012), machine translation (Mikolov, 2012;Cho et al., 2014a), or acoustic modeling ( Robinson et al., 1993;Graves &amp; Jaitly, 2014) among others. Two factors have been instrumental in allowing this paradigm to be so widely adopted and give rise to the aforementioned successes. On the one hand, recent advances in both hardware and software have had a significant role in bringing the training of recurrent models to tractable time periods. On the other hand, novel units and architectures have allowed recurrent networks to model certain features of sequential data better than Elman's simple RNN architecture (Elman, 1990). These include such developments as the LSTM (Hochreiter &amp; Schmidhuber, 1997) and GRU ( Cho et al., 2014a) units, which can more easily learn to model long range interactions ( Chung et al., 2014), or attention mechanisms that allow the model to focus on a specific part of its history when making a prediction ( . In this work, we focus on another feature of recurrent networks: the ability to efficiently model processes happening at different and possibly varying time scales.Most existing recurrent models take one of two approaches regarding the amount of computation they require. Either the computational load is constant over time, or it follows a fixed (or deter- ministic) schedule (Koutník et al., 2014), (Mikolov et al., 2014). The latter approach has proven especially useful when dealing with sequences which reflect processes taking place at different lev-els (and time scales) ( Bojanowski et al., 2015). However, we believe that taking a more flexible approach could prove useful.Consider sequential data such as video feeds, audio signal, or language. In video data, there are time periods where the frames differ very slightly, and where the underlying model should probably do much less computation than when the scene completely changes. When modeling speech from an audio signal, it is also reasonable to expect that the model should be able do little to no computation during silences. Finally, in the case of character level language modeling, having more computa- tional power at word boundaries can certainly help: after reading the left context The prime. . . , the model should be able to put a higher likelihood on the sequence of characters that make up the word minister. However, we can take this idea one step further: after reading The prime min. . . , the next few characters are almost deterministic, and the model should require little computation to predict the sequence i-s-t-e-r.In this work, we show how to modify two commonly used recurrent unit architectures, namely the Elman and Gated Recurrent Unit, to obtain their variable computation counterparts. This gives rise to two new architecture, the Variable Computation RNN and Variable Computation GRU (VCRNN and VCGRU), which take advantage of these phenomena by deciding at each time step how much computation is required based on the current hidden state and input. We show that the models learn time patterns of interest, can perform fewer operations, and may even take advantage of these time structures to produce better predictions than the constant computation versions.We start by giving an overview of related work in Section 2, provide background on the class of Recurrent Neural Networks in Section 3, describe our model and learning procedure in Section 4, and present experimental results on music as well as bit and character level language modeling in section 5. Finally, Section 6 concludes and lays out possible directions for future work.How to properly handle sequences which reflect processes happening at different time scales has been a widely explored question. Among the proposed approaches, a variety of notable systems based on Hidden Markov Models (HMMs) have been put forward in the last two decades. The Fac- torial HMM model of (Ghahramani &amp; Jordan, 1997) (and its infinite extension in ( Gael et al., 2008)) use parallel interacting hidden states to model concurrent processes. While there is no explicit han- dling of different time scales, the model achieves good held-out likelihood on Bach chorales, which exhibit multi-scale behaviors. The hierarchical HMM model of ( Fine et al., 1998) and (Murphy &amp; Paskin, 2001) takes a more direct approach to representing multiple scales of processes. In these works, the higher level HMM can recursively call sub-HMMs to generate short sequences with- out changing its state, and the authors show a successful application to modeling cursive writing. Finally, the Switching State-Space Model of (Ghahramani &amp; Hinton, 2000) combines HMMs and Linear Dynamical Systems: in this model, the HMM is used to switch between LDS parameters, and the experiments show that the HMM learns higher-level, slower dynamics than the LDS.On the side of Recurrent Neural Networks, the idea that the models should have mechanisms that allow them to handle processes happening at different time scales is not a new one either. On the one hand, such early works as (Schmidhuber, 1991) and (Schmidhuber, 1992) already presented a two level architecture, with an "automatizer" acting on every time step and a "chunker" which should only be called when the automatizer fails to predict the next item, and which the author hypothesizes learns to model slower scale processes. On the other hand, the model proposed in (Mozer, 1993) has slow-moving units as well as regular ones, where the slowness is defined by a parameter τ ∈ [0, 1] deciding how fast the representation changes by taking a convex combination of the previous and predicted hidden state.Both these notions, along with different approaches to multi-scale sequence modeling, have been developed in more recent work. ( Mikolov et al., 2014) expand upon the idea of having slow moving units in an RNN by proposing an extension of the Elman unit which forces parts of the transition matrix to be close to the identity. The idea of having recurrent layers called at different time steps has also recently regained popularity. The Clockwork RNN of (Koutník et al., 2014), for example, has RNN layers called every 1, 2, 4, 8, etc. . . time steps. The conditional RNN of ( Bojanowski et al., 2015) takes another approach by using known temporal structure in the data: in the character level level language modeling application, the first layer is called for every character, while the second is only called once per word. It should also be noted that state-of-the art results for language models have been obtained using multi-layer RNNs (Józefowicz et al., 2016), where the higher layers can in theory model slower processes. However, introspection in these models is more challenging, and it is difficult to determine whether they are actually exhibiting significant temporal behaviors.Finally, even more recent efforts have considered using dynamic time schedules. ( Chung et al., 2016) presents a multi-layer LSTM, where each layer decides whether or not to activate the next one at every time step. They show that the model is able to learn sensible time behaviors and achieve good perplexity on their chosen tasks. Another implementation of the general concept of adaptive time- dependent computation is presented in (Graves, 2016). In that work, the amount of computation performed at each time step is varied not by calling units in several layers, but rather by having a unique RNN perform more than one update of the hidden state on a single time step. There too, the model can be shown to learn an intuitive time schedule.In this paper, we present an alternative view of adaptive computation, where a single Variable Com- putation Unit (VCU) decides dynamically how much of its hidden state needs to change, leading to both savings in the number of operations per time step and the possibility for the higher dimensions of the hidden state to keep longer term memory.Let us start by formally defining the class of Recurrent Neural Networks (RNNs). For tasks such as language modeling, we are interested in defining a probability distribution over sequences w = (w 1 , . . . , w T ). Using the chain rule, the negative log likelihood of a sequence can be written:where F is a filtration, a function which summarizes all the relevant information from the past. RNNs are a class of models that can read sequences of arbitrary length to provide such a summary in the form of a hidden state h t ≈ F(w 1 , . . . , w t−1 ), by applying the same operation (recurrent unit) at each time step. More specifically, the recurrent unit is defined by a recurrence function g which takes as input the previous hidden state h t−1 at each time step t, as well as a representation of the input x t (where h t−1 and x t are D-dimensional vectors), and (with the convention h 0 = 0,) outputs the new hidden state:Elman Unit. The unit described in (Elman, 1990) is often considered to be the standard unit. It is parametrized by U and V , which are square, D-dimensional transition matrices, and uses a sigmoid non-linearity to obtain the new hidden state:In the Elman unit, the bulk of the computation comes from the matrix multiplications, and the cost per time step is O(D 2 ). In the following section, we show a simple modification of the unit which allows it to reduce this cost significantly.Gated Recurrent Unit. The Gated Recurrent Unit (GRU) was introduced in ( Cho et al., 2014b). The main difference between the GRU and Elman unit consists in the model's ability to interpolate between a proposed new hidden state and the current one, which makes it easier to model longer range dependencies. More specifically, at each time step t, the model computes a reset gate r t , an update gate z t , a proposed new hidden state˜hstate˜ state˜h t and a final new hidden state h t as follows:And:Where denotes the element-wise product. As noted in the previous section, the bulk of the computation in the aforementioned settings comes from the linear layers; a natural option to reduce the number of operations would then be to only apply the linear transformations to a sub-set of the hidden dimensions. These could in theory cor- respond to any sub-set indices in {1, . . . , D}; however, we want a setting where the computational cost of the choice is much less than the cost of computing the new hidden state. Thus, we only consider the sets of first d dimensions of R D , so that there is a single parameter d to compute.Our Variable Computation Units (VCUs) implement this idea using two modules: a scheduler de- cides how many dimensions need to be updated at the current time step, and the VCU performs a partial update of its hidden state accordingly, as illustrated in Figure 1. Section 4.1 formally de- scribes the scheduler and partial update operations, and Section 4.2 outlines the procedure to jointly learn both modules.Scheduler. The model first needs to decide how much computation is required at the current time step. To make that decision, the recurrent unit has access to the current hidden state and input; this way, the model can learn to ignore an uninformative input, or to decide on more computation when an it is unexpected given the current hidden state. The scheduler is then defined as a functionwhich decides what portion of the hidden state to change based on the current hidden and input vectors. In this work, we decide to implement it as a simple log-linear function with parameter vectors u and v, and bias b, and at each time step t, we have:Partial update. Once the scheduler has decided on a computation budget m t , the VCU needs to perform a partial update of the first t D dimensions of its hidden state. Soft mask. In practice, the transition function we just defined would require making a hard choice at each time step of the number of dimensions to be updated, which makes the model non-differentiable and can significantly complicate optimization. Instead, we approximate the hard choice by using a gate function to apply a soft mask. Given m t ∈ [0, 1] and a sharpness parameter λ, we use the gating vector e t ∈ R D defined by:where Thres maps all values greater than 1 − and smaller than to 1 and 0 respectively. That way, the model performs an update using the first (m t × D + η) dimensions of the hidden state, where η goes to 0 as λ increases, and leaves its last ((1 − m t ) × D − η) dimensions unchanged. Thus, if g is the recurrence function defined in Equation 2, we have:The computational cost of this model at each step t, defined as the number of multiplications involv- ing possibly non-zero elements is then O(m 2 t D 2 ). The construction of the VCRNN and VCGRU architectures using this method is described in the appendix.Since the soft mask e t is a continuous function of the model parameters, the scheduler can be learned through back-propagation. However, we have found that the naive approach of using a fixed sharp- ness parameter and simply minimizing the negative log-likelihood defined in Equation 1 led to the model being stuck in a local optimum which updates all dimensions at every step. We found that the following two modifications allowed the model to learn better parametrizations.First, we can encourage m t to be either close or no greater than a target ¯ m at all time by adding a penalty term Ω to the objective. For example, we can apply a 1 or 2 penalty to values of m t that are greater than the target, or that simply diverge from it (in which case we also discourage the model from using too few dimensions). The cost function defined in Equation 1 then becomes:Secondly, for the model to be able to explore the effect of using fewer or more dimensions, we need to start training with a smooth mask (small λ parameter), since for small values of λ, the model actually uses the whole hidden state. We can then gradually increase the sharpness parameter until the model truly does a partial update.We ran experiments with the Variable Computation variants of the Elman and Gated Recurrent Units (VCRNN and VCGRU respectively) on several sequence modeling tasks. All experiments were run using a symmetrical 1 penalty on the scheduler m, that is, penalizing m t when it is greater or smaller than target ¯ m, with ¯ m taking various values in the range [0.2, 0.5]. In all experiments, we start with a sharpness parameter λ = 0.1, and increase it by 0.1 per epoch to a maximum value of 1.In each of our experiments, we are interested in investigating two specific aspects of our model. On the one hand, do the time patterns that emerge agree with our intuition of the time dynamics expressed in the data? On the other hand, does the Variable Computation Unit (VCU) yield a good predictive model? More specifically, does it lead to lower perplexity than a constant computation counterpart which performs as many or more operations? In order to be able to properly assess the efficiency of the model, and since we do not know a priori how much computation the VCU uses, we always report the "equivalent RNN" dimension (noted as RNN-d in Table 3) along with the performance on test data, i.e. the dimension of an Elman RNN that would have performed the same amount of computation. Note that the computational complexity gains we refer to are exclusively in terms of lowering the number of operations, which does not necessarily correlate with a speed up of training when using general purpose GPU kernels; it is however a prerequisite to achieving such a speed up with the proper implementation, motivating our effort.We answer both of these questions on the tasks of music modeling, bit and character level language modeling on the Penn Treebank text, and character level language modeling on the Text8 data set as well as two languages from the Europarl corpus.We downloaded a corpus of Irish traditional tunes from https://thesession.org and split them into a training validation and test of 16,000 (2.4M tokens), 1,511 (227,000 tokens) and 2,000 (288,000 tokens) melodies respectively. Each sub-set includes variations of melodies, but no melody has variations across subsets. We consider each (pitch, length) pair to be a different symbol; with rests and bar symbols, this comes to a total vocabulary of 730 symbols. Table 1 compares the perplexity on the test set to Elman RNNs with equivalent computational costs: an VCRNN with hidden dimension 500 achieves better perplexity with fewer operations than an RNN with dimension 250.Looking at the output of the scheduler on the validation set also reveals some interesting patterns. First, bar symbols are mostly ignored: the average value of m t on bar symbols is 0.14, as opposed to 0.46 on all others. This is not surprising: our pre-processing does not handle polyphony or time signatures, so bars en up having different lengths. The best thing for the model to do is then just to ignore them and focus on the melody. Similarly, the model spends lest computation on rests (0.34 average m t ), and pays less attention to repeated notes (0.51 average for m t on the first note of a repetition, 0.45 on the second).  Table 1: Music modeling, test set perplexity on a corpus of traditional Irish tunes. Our model manages to achieve better perplexity with less computation than the Elman RNN.We also notice that the model needs to do more computation on fast passages, which often have richer ornamentation, as illustrated in Table 2. While it is difficult to think a priori of all the sorts of behaviors that could be of interest, these initial results certainly show a sensible behavior of the scheduler on the music modeling task.  Table 2: Average amount of computation (m t ) for various note lengths. More effort is required for the faster passages with 16th notes and triplets.We also chose to apply our model to the tasks of bit level and character level language modeling. Those appeared as good applications since we know a priori what kind of temporal structure to look for: ASCII encoding means that we expect a significant change (change of character) every 8 bits in bit level modeling, and we believe the structure of word units to be useful when modeling text at the character level.We first ran experiments on two English language modeling tasks, using the Penn TreeBank and Text8 data sets. We chose the former as it is a well studied corpus, and one of the few corpora for which people have reported bit-level language modeling results. It is however quite small for our purposes, with under 6M characters, which motivated us to apply our models to the larger Text8 data set (100M characters).  Quantitative Results. We first compare the VCRNN to the regular Elman RNN, as well as to the Conditional RNN of ( Bojanowski et al., 2015), which combines two layers running at bit and character level for bit level modeling, or character and word level for character level modeling. For bit level language modeling, the VCRNN not only performs fewer operations than the standard unit, it also achieves better performance. For character level modeling, the Elman model using a hidden dimension of 1024 achieved 1.47 bits per character, while our best performing VCRNN does slightly better while only requiring as much computation as a dimension 760 Elman unit. While we do slightly more computation than the Conditional RNN, it should be noted that our model is not explicitly given word-level information: it learns how to summarize it from character-level input.The comparison between the constant computation and Variable Computation GRU (VCGRU) fol- lows the same pattern, both on the PTB and Text8 corpora. On PTB, the VCGRU with the best validation perplexity performs as well as a GRU (and LSTM) of the same dimension with less than half the number of operations. On Text8, the VCGRU models with various values of the target ¯ m always achieve better perplexity than other models performing similar or greater numbers of opera- tions. It should be noted that none of the models we ran on Text8 overfits significantly (the training and validation perplexities are the same), which would indicate that the gain is not solely a matter of regularization. Bit Level Scheduler. The scheduler in the bit level language model manages to learn the structure of ASCII encoding: Figure 2 shows that the higher dimensions are modified roughly every 8 bits. We also created some artificial data by taking the PTB text and adding 8 or 24 0 bits between each character. Figure 2, shows that the model learns to mostly ignore these "buffers", doing most of its computation on actual characters.Character Level Scheduler. On character level language modeling, the scheduler learns to make use of word boundaries and some language structures. Figure 3 shows that the higher dimensions are used about once per words, and in some cases, we even observe a spike at the end of each morpheme (long-stand-ing, as shown in Figure 5). While we provide results for the VCRNN specifically in this Section, the VCGRU scheduler follows the same patterns.We also ran our model on two languages form the Europarl corpus. We chose Czech, which has a larger alphabet than other languages in the corpus, and German , which is a language that features long composite words without white spaces to indicate a new unit. Both are made up of about 20M characters. We tried two settings. In the "guide" setting, we use the penalty on m t to encourage the model to use more dimensions on white spaces. The "learn" setting is fully unsupervised, and encourages lower values of m t across the board.Europarl-de    Figure 4 shows that both perform similarly on the Czech dataset, achieving better performance more efficiently than the standard RNN. On German, the guided settings remains slightly more efficient than the fully learned one, but both are more efficient than the RNN and achieve the same performance when using more dimensions. Both learn to use more dimensions at word boundaries as shown in Figure 3. The German model also appears to be learning interesting morphology (Luft- ver-kehrs, eben-falls in Figure 3, An-satz, Um-welt-freund-lich in Figure 5), and grammar (focusing on case markers at the end of articles, Figure 5).In this work, we have presented two kinds of Variable Computation recurrent units: the VCRNN and VCGRU, which modify the Elman and Gated Recurrent Unit respectively to allow the models to achieve better performance with fewer operations, and can be shown to find time patterns of interest in sequential data. We hope that these encouraging results will open up paths for further exploration of adaptive computation paradigms in neural networks in general, which could lead to more computation-efficient models, better able to deal with varying information flow or multi-scale processes. We also see a few immediate possibilities for extensions of this specific model. For example, the same idea of adaptive computation can similarly be applied to yet other commonly used recurrent units, such as LSTMs, or to work within the different layers of a stacked architecture, and we are working on adapting our implementation to those settings. We also hope to investigate the benefits of using stronger supervision signals to train the scheduler, such as the entropy of the prediction, to hopefully push our current results even further.
