uses, e.g., Elman architecture [16]. Specifically, the long short- term memory (LSTM) [17,18] neural networks have three gates that control flows of error signals. The recently proposed gated recurrent neural networks (GRNN) [6] may be considered as a simplified LSTM with fewer gates.Along this line of research on developing more advanced architectures, this paper focuses on a novel neural network ar- chitecture. Inspired by the recent work in [19], we extend the simple RNN with Elman architecture to using an external mem- ory. The external memory stores the past hidden layer activities, not only from the current sentence but also from past sentences. To predict outputs, the model uses input observation together with a content retrieved from the external memory. The pro- posed model performs strongly on a common language under- standing dataset and achieves new state-of-the-art results.This paper is organized as follows. We briefly describe background of this research in Sec. 2. Section 3 presents de- tails of the proposed model. Experiments are in section 4. We relate our research with other works in Sec. 5. Finally, we have conclusions and discussions in Sec. 6.Neural network based methods have recently demonstrated promising results on many natural language processing tasks [1,2]. Specifically, recurrent neural networks (RNNs) based meth- ods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine transla- tion [5,6] tasks.The main task of a language understanding (LU) system is to associate words with semantic meanings [7][8][9]. For example, in the sentence "Please book me a ticket from Hong Kong to Seattle", a LU system should tag "Hong Kong" as the departure- city of a trip and "Seattle" as its arrival city. The widely used approaches include conditional random fields (CRFs) [8,10], support vector machine [11], and, more recently, RNNs [4,12].A RNN consists of an input, a recurrent hidden layer, and an output layer. The input layer reads each word and the output layer produces probabilities of semantic labels. The success of RNNs can be attributed to the fact that RNNs, if successfully trained, can relate the current prediction with input words that are several time steps away. However, RNNs are difficult to train, because of the gradient vanishing and exploding prob- lem [13]. The problem also limits RNNs' memory capacity because error signals may not be able to back-propagated far enough.There have been two lines of researches to address this problem. One is to design learning algorithms that can avoid gradient exploding, e.g., using gradient clipping [14], and/or gradient vanishing, e.g., using second-order optimization meth- ods [15]  t=1 where k is the size of a context window and t indexes the posi- tions in the alignment.The above posterior probability can be computed using a RNN. A RNN consists of an input layer xt, a hidden layer ht, and an output layer yt. In Elman architecture [16], hidden layer activity ht is dependent on both the input xt and also recurrently on the past hidden layer activity ht−1. Because of the recurrence, the hidden layer activity ht is dependent on the observation sequence from its beginning. The posterior probability is therefore computed as followswhere the output yt and hidden layer activity ht are computed asIn the above equation, g(·) is softmax function and σ(·) is sig- moid or tanh function. The above model is denoted as simple RNN, to contrast it with more advanced recurrent neural net- works described below. The current hidden layer activity ht of a simple RNN is related to its past hidden layer activity ht−1 via the nonlinear function in Eq. (4). The non-linearity can cause errors back-propagated from ht to explode or to vanish. This phenomenon prevents simple RNN from learning patterns that are spanned with long time dependence [14].To tackle this problem, long short-term memory (LSTM) neural network was proposed in [17] with an introduction of memory cells, linearly dependent on their past values. LSTM also introduces three gating functions, namely input gate, forget gate and output gate. We follow a variant of LSTM in [18].More recently, a gated recurrent neural network (GRNN) [6] was proposed. Instead of the three gating functions in LSTM, it uses two gates.One is a reset gate rt that relates a candidate activation with the past hidden layer activity ht−1; i.e.,We extend simple RNN in this section to using external mem- ory. Figure 1 illustrates the proposed model, which we denote it as RNN-EM. Same as with the simple RNN, it consists of an in- put layer, a hidden layer and an output layer. However, instead of feeding the past hidden layer activity directly to the hidden layer as with the simple RNN, one input to the hidden layer is from a content of an external memory. RNN-EM uses a weight vector to retrieve the content from the external memory to use in the next time instance. The element in the weight vector is proportional to the similarity of the current hidden layer activ- ity with the content in the external memory. Therefore, content that is irrelevant to the current hidden layer activity has small weights. We describe RNN-EM in details in the following sec- tions. All of the equations to be described are with their bias terms, which we omit for simplicity of descriptions. We imple- mented RNN-EM using Theano [20,21].3.1. Model input and output wherê ht is the candidate activation. W xh and W hh are the matrices relate the current observation xt and the past hidden layer activity. is element-wise product.The second gate is an update gate zt that interpolates the candidate activation and the past hidden layer activity to update the current hidden layer activity; i.e.,The input to the model is a dense vector xt ∈ R d×1 . In the context of language understanding, xt is a projection of input words, also known as word embedding.The hidden layer reads both the input xt and a content ct vector from the memory. The hidden layer activity is computed as followsThese gates are usually computed as functions of the cur- rent observation xt and the past hidden layer activity; i.e.,where Wxr and W hr are the weights to observation and to the past hidden layer activity for the reset gate. Wxz and W hz are similarly defined for the update gate.where σ(·) is tanh function. W ih ∈ R p×d is the weight to the input vector. ct ∈ R m×1 is the content from a read operation to be described in Eq. (15). Wc ∈ R p×m is the weight to the content vector.The output from this model is fed into the output layer as followswhere W ho is the weight to the hidden layer activity and g(·) is softmax function.Notice that in case of ct = ht−1, the above model is simple RNN.RNN-EM has an external memory Mt ∈ R m×n . It can be considered as a memory with n slots and each slot is a vector with m elements. Similar to the external memory in computers, the memory capacity of RNN-EM may be increased if using a large n.The model generates a key vector kt to search for content in the external memory. Though there are many possible ways to generate the key vector, we choose a simple linear function that relates hidden layer activity ht as followsF1 score CRF [26] 92.94 simple RNN [4] 94.11 CNN [27] 94.35 LSTM [28] 94  where W k ∈ R m×p is a linear transformation matrix. Our intu- ition is that the memory should be in the same space of or affine to the hidden layer activity.We use cosine distance K(u, v) = Therefore, memory is only updated if it is to be read. With the above described two gates, the memory is updated as follows where diag(·) transforms a vector to a diagonal matrix with diagonal elements from the vector.Notice that when the number of memory slots is small, it may have similar performances as a gated RNN. Specifically, when n = 1, Eqs. (19) and (6) are qualitatively similar.where the above weight is normalized and sums to 1.0. βt is a scalar larger than 0.0. It sharpens the weight vector when βt is larger than 1.0. Conversely, it smooths or dampens the weight vector when βt is between 0.0 and 1.0. We use the following function to obtain βt; i.e.,where W β ∈ R 1×p maps the hidden layer activity ht to a scalar. Importantly, we also use a scalar coefficient gt to interpo- late the above weight estimate with the past weight as follows:In order to compare the proposed model with alternative mod- eling techniques, we conducted experiments on a well studied language understanding dataset, Air Travel Information System (ATIS) [22][23][24]. The training part of this dataset consists of 4978 sentences and 56590 words. There are 893 sentences and 9198 words for test. The number of semantic label is 127, in- cluding the common null label. We use lexicon-only features in experiments.This function is similar to Eq. (6) in the gated RNN, except that we use a scalar gt to interpolate the weight updates and the gated RNN uses a vector to update its hidden layer activity. The memory content is retrieved from the external memory at time t − 1 usingRNN-EM generates a new content vector vt to be added to its memory; i.e,where Wv ∈ R m×p . We use the above linear function based on the same intuition in Sec. 3.2 that the new content and the hidden layer activity are in the same space of or affine to each other.RNN-EM has a forget gate as followsThe input xt in RNN-EM has a window size of 3, consisting of the current input word and its neighboring two words. We use the AdaDelta method to update gradients [25]. The maximum number of training iterations was 50. Hyper parameters for tun- ing included the hidden layer size p, the number of memory slots n, and the dimension for each memory slot m. The best performing RNN-EM had 100 dimensional hidden layer and 8 memory slots with 40 dimensional memory slot. Table 2 lists performance in F1 score of RNN-EM, together with the previous best results of alternative models in the lit- erature. Since there are no previous results from GRNN, we use our own implementation of it for this study. These results are optimal in their respective systems. The previous best result was achieved using LSTM. A change of 0.38% of F1 score from LSTM result is significant at the 90% confidence level. Results in Table 2 show that RNN-EM is significantly better than the previous best result using LSTM.4.3. Analysis on convergence and averaged performances where et ∈ R n×1 is an erase vector, generated as et = σ(W he ht). Notice that the c-th element in the forget gate is zero only if both read weight wt and erase vector et have their c-th element set to one. Therefore, memory cannot be forgotten if it is not to be read.Results in the previous sections were obtained with models us- ing different sizes. This section further compares neural net- work models given that they have approximately the same num- ber of parameters, listed in Table 3. We use AdaDelta [25] gra- dient update method for all these models. Figure 2      training set entropy with respect to iteration numbers. To better illustrate their convergences, we have converted entropy values to their logarithms. The results show that RNN-EM converges to lower training entropy than other models. RNN-EM also con- verges faster than the simple RNN and LSTM. We further repeated ATIS experiments for 10 times with different random seeds for these neural network models. We evaluated their performances after their convergences. Table 4 lists their averaged F1 scores, together with their maximum and minimum F1 scores. A change of 0.12% is significant at the 90% confidence level, when comparing against LSTM result. Results in Table 4 show that RNN-EM, on average, significantly outperforms LSTM. The best performance by RNN-EM is also significantly better than the best performing LSTM.The RNN-EM is along the same line of research in [19,29] that uses external memory to improve memory capacity of neu- ral networks. Perhaps the closest work is the Neural Turing Machine (NTM) work in [19], which focuses on those tasks that require simple inference and has proved its effectiveness in copy, repeat and sorting tasks. NTM requires complex mod- els because of these tasks. The proposed model is considerably simpler than NTM and can be considered as an extension of simple RNN. Importantly, we have shown through experiments on a common language understanding dataset the promising re- sults from using the external memory architecture.The size of the external memory Mt is proportional to the num- ber of memory slots n. We fixed the dimension of memory slots to 40 and varied the number of slots. Table 5 lists their test set F1 scores. The best performing RNN-EM was with n = 8. No- tice that RNN-EM with n = 1 performed better than the simple RNN with 94.09% F1 score in Table 4. This can be explained as using gate functions in Eqs. (17) and (18) in RNN-EM, which are absent in simple RNNs. RNN-EM with n = 1 also per- formed similarly as the gated RNN with 94.70% F1 score in Table 4, partly because of these gate functions.Memory capacity may be measured using training set en- tropy. Table 5 shows that training set entropy is decreased ini- tially with n increased from 1 to 8, showing that the memory capacity of the RNN-EM is improved. However, the entropy is increased with ns further increased. This suggests that memory In this paper, we have proposed a novel neural network architec- ture, RNN-EM, that uses external memory to improve memory capacity of simple recurrent neural networks. On a common language understanding task, RNN-EM achieves new state-of- the-art results and performs significantly better than the previ- ous best result using long short-term memory neural networks.We have conducted experiments to analyze its convergence and memory capacity. These experiments provide insights for future research directions such as mechanisms of accessing memory contents and methods to increase memory capacity.
