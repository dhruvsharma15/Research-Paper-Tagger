Automatic image description -the task of generating natural language sentences for an image - has thus far been exclusively performed in English, due to the availability of English datasets. How- ever, the applications of automatic image description, such as text-based image search or providing image alt-texts on the Web for the visually impaired, are also relevant for other languages. Cur- rent image description models are not inherently English-language specific, so a simple approach to generating descriptions in another language would be to collect new annotations and then train a model for that language. Nonetheless, the wealth of image description resources for English suggest a cross-language resource transfer approach, which is what we explore here. In other words: How can we best use resources for Language A when generating descriptions for Language B?We introduce multilingual image description and present a multilingual multimodal image descrip- tion model for this task. Multilingual image description is a form of visually-grounded machine translation, in which parallel sentences are grounded against features from an image. This ground- ing can be particularly useful when the source sentence contains ambiguities that need to be resolved in the target sentence. For example, in the German sentence "Ein Rad steht neben dem Haus", "Rad" could refer to either "bicycle" or "wheel", but with visual context the intended meaning can be more easily translated into English. In other cases, source language features can be more precise than noisy image features, e.g. in identifying the difference between a river and a harbour.Our multilingual image description model adds source language features to a monolingual neural image description model (Karpathy &amp; Fei-Fei, 2015;Vinyals et al., 2015, inter-alia). picts the overall approach, illustrating the way we transfer feature representations between models. Image description models generally use a fixed representation of the visual input taken from a object detection model (e.g., a CNN). In this work we add fixed features extracted from a source language model (which may itself be a multimodal image description model) to our image description model. This is distinct from neural machine translation models which train source language feature repre- sentations specifically for target decoding in a joint model (Cho et al., 2014;Sutskever et al., 2014). Our composite model pipeline is more flexible than a joint model, allowing the reuse of models for other tasks (e.g., monolingual image description, object recognition) and not requiring retraining for each different language pair. We show that the representations extracted from source language models, despite not being trained to translate between languages, are nevertheless highly successful in transferring additional informative features to the target language image description model.In a series of experiments on the IAPR-TC12 dataset of images described in English and German, we find that models that incorporate source language features substantially outperform target mono- lingual image description models. The best English-language model improves upon the state-of-the- art by 2.3 BLEU4 points for this dataset. In the first results reported on German image description, our model achieves a 8.8 Meteor point improvement compared to a monolingual image description baseline. The implication is that linguistic and visual features offer orthogonal improvements in multimodal modelling (a point also made by Silberer &amp; Lapata (2014) and Kiela &amp; Bottou (2014)). The models that include visual features also improve over our translation baselines, although to a lesser extent; we attribute this to the dataset being exact translations rather than independently elicited descriptions, leading to high performance for the translation baseline. Our analyses show that the additional features improve mainly lower-quality sentences, indicating that our best models successfully combine multiple noisy input modalities.Our multilingual image description models are neural sequence generation models, with additional inputs from either visual or linguistic modalities, or both. We present a family of models in sequence of increasing complexity to make their compositional character clear, beginning with a neural se- quence model over words and concluding with the full model using both image and source features. See Figure 2 for a depiction of the model architecture.The core of our model is a Recurrent Neural Network model over word sequences, i.e., a neural language model (LM) ( Mikolov et al., 2010). The model is trained to predict the next word in the sequence, given the current sequence seen so far. At each timestep i for input sequence w 0...n , the input word w i , represented as a one-hot vector over the vocabulary, is embedded into a high- dimensional continuous vector using the learned embedding matrix W eh (Eqn 1). A nonlinear func- tion f is applied to the embedding combined with the previous hidden state to generate the hidden state h i (Eqn 2). At the output layer, the next word o i is predicted via the softmax function over the In simple RNNs, f in Eqn 2 can be the tanh or sigmoid function. Here, we use an LSTM 1 to avoid problems with longer sequences (Hochreiter &amp; Schmidhuber, 1997). Sentences are buffered at timestep 0 with a special beginning-of-sentence marker and with an end-of-sequence marker at timestep n. The initial hidden state values h −1 are learned, together with the weight matrices W .The Recurrent Language Model (LM) generates sequences of words conditioned only on the previ- ously seen words (and the hidden layer), and thus cannot use visual input for image description. In the multimodal language model (MLM), however, sequence generation is additionally conditioned on image features, resulting in a model that generates word sequences corresponding to the image. The image features v (for visual) are input to the model at h 0 at the first timestep 2 :Our translation model is analogous to the multimodal language model above: instead of adding image features to our target language model, we add features from a source language model. This feature vector s is the final hidden state extracted from a sequence model over the source language, the SOURCE-LM. The initial state for the TARGET-LM is thus defined as:We follow recent work on sequence-to-sequence architectures for neural machine translation ( Cho et al., 2014;Sutskever et al., 2014) in calling the source language model the 'encoder' and the target language model the 'decoder'. However, it is important to note that the source encoder is Under review as a conference paper at ICLR 2016 a yellow building with white columns in the background ein gelbes Gebäude mit weißen Säulen im Hintergrund Figure 3: Image 00/25 from the IAPR-TC12 dataset with its English and German description.a viable model in its own right, rather than only learning features for the target decoder. We suspect this is what allows our translation model to learn on a very small dataset: instead of learning based on long distance gradients pushed from target to source (as in the sequence-to-sequence architec- ture), the source model weights are updated based on very local LM gradients. Despite not being optimised for translation, the source features turn out to be very effective for initialising the target language model, indicating that useful semantic information is captured in the final hidden state. We use the same description generation process for each model. First, a model is initialised with the special beginning-of-sentence token and any image or source features. At each timestep, the generated output is the maximum probability word at the softmax layer, o i , which is subsequently used as the input token at timestep i+1. This process continues until the model generates the end- of-sentence token, or a pre-defined number of timesteps (30, in our experiments, which is slightly more than the average sentence length in the training data).We use the IAPR-TC12 dataset, originally introduced in the ImageCLEF shared task for object segmentation and later expanded with complete image descriptions ( Grubinger et al., 2006). This dataset contains 20,000 images with multiple descriptions in both English and German. Each sen- tence corresponds to a different aspect of the image, with the most salient objects likely being de- scribed in the first description (annotators were asked to describe parts of the image that hadn't been covered in previous descriptions). We use only the first description of each image. Note that the English descriptions are the originals; the German data was professionally translated from English. Figure 3 shows an example image-bitext tuple from the dataset. We perform experiments using the standard splits of 17,665 images for training, from which we reserve 10% for hyperparameter estimation, and 1,962 for evaluation. We extract the image features from the pre-trained VGG-16 CNN object recognition model (Simonyan &amp; Zisserman, 2015). Specifically, our image features are extracted as fixed represen- tations from the penultimate layer of the CNN, in line with recent work in this area. We use an LSTM (Hochreiter &amp; Schmidhuber, 1997) as f in the recurrent language model. The hidden layer size |h| is set to 256 dimensions. The word embeddings are 256-dimensional and learned along with other model parameters. We also experimented with larger hidden layers (as well as with deeper architectures), and while that did result in improvements, they also took longer to train. The image features v are the 4096-dimension penultimate layer of the VGG-16 object recognition network (Simonyan &amp; Zisserman, 2015) applied to the image.The models are trained with mini-batches of 100 examples towards the objective function (cross- entropy of the predicted words) using the ADAM optimiser (Kingma &amp; Ba, 2014). We do early stopping for model selection based on BLEU4: if validation BLEU4 has not increased for 10 epochs, and validation language model perplexity has stopped decreasing, training is halted.We apply dropout over the image features, source features, and word representations with p = 0.5 to discourage overfitting ( Srivastava et al., 2014). The objective function includes an L2 regularisation term with λ=1e −8 .All results reported are averages over three runs with different Glorot-style uniform weight initiali- sations (Glorot &amp; Bengio, 2010). We report image description quality using BLEU4 ( Papineni et al., 2002), Meteor (Denkowski &amp; Lavie, 2014), and language-model perplexity. Meteor has been shown to correlate better with human judgements than BLEU4 for image description (Elliott &amp; Keller, 2014). The BLEU4 and Meteor scores are calculated using MultEval (Clark et al., 2011).Meteor PPLX En MLM 14.2 ± 0.3 15.4 ± 0.2 6.7 ± 0.0 De LM → En LM 21.3 ± 0.5 19.6 ± 0.2 6.0 ± 0.1 Mao et al. (2015) 20.8 - 6.92De MLM → En MLM 18.0 ± 0.3 18.0 ± 0.2 6.3 ± 0.1 De LM → En MLM 17.3 ± 0.5 17.6 ± 0.5 6.3 ± 0.0 De MLM → En LM 23.1 ± 0.1 20.9 ± 0.0 5.7 ± 0.1  The results for image description in both German and English are presented in Tables 1 and 2; generation examples can be seen in Figures 6, 7, 8 in Appendix B 5 . To our knowledge, these are the first published results for German image description. Overall, we found that English image description is easier than German description, as measured by BLEU4 and Meteor scores. This may be caused by the more complex German morphology, which results in a larger vocabulary and hence more model parameters.The English monolingual image description model (En-MLM) is comparable with state-of-the-art models, which typically report results on the Flickr8K / Flickr30K dataset. En-MLM achieves a BLEU4 score of 15.8 on the Flickr8K dataset, nearly matching the score from Karpathy &amp; Fei-Fei (2015)  (16.0), which uses an ensemble of models and beam search decoding. On the IAPR-TC12 dataset, the En-MLM baseline outperforms Kiros et al. (2014) 6 . Mao et al. (2015) report higher per- formance, but evaluate on all reference descriptions, making the figures incomparable.All multilingual models beat the monolingual image description baseline, by up to 8.9 BLEU4 and 8.8 Meteor points for the best models. Clearly the features transferred from the source models are useful for the TARGET-LM or TARGET-MLM description generator, despite the switch in languages.The translation baseline without visual features performs very well 7 . This indicates the effectiveness of our translation model, even without joint training, but is also an artifact of the dataset. A different dataset with independently elicited descriptions (rather than translations of English descriptions) may result in worse performance for a translation system that is not visually grounded, because the target descriptions would only be comparable to the source descriptions.Overall, the multilingual models that encode the source using an MLM outperform the SOURCE-LM models. On the target side, simple LM decoders perform better than MLM decoders. This can be explained to some extent by the smaller number of parameters in models that do not input the visual features twice. Incorporating the image features on the source side seems to be more effective, possibly because the source is constrained to the gold description at test time, leading to a more coherent match between visual and linguistic features. Conversely, the TARGET-MLM variants tend to be worse sentence generators than the LM models, indicating that while visual features lead to useful hidden state values, there is room for improving their role during generation.What do source features add beyond image features? Source features are most useful when the baseline MLM does not successfully separate related images. The image description models have to compress the image feature vector into the same number of dimensions as the hidden layer in the recurrent network, effectively distilling the image down to the features that correspond to the words in the description. If this step of the model is prone to mistakes, the resulting descriptions will be of poor quality. However, our best multilingual models are initialised with features transferred from image description models in a different language. In these cases, the source language features have already compressed the image features for the source language image description task.Qualitatively, we can illustrate this effect using Barnes-Hut t-SNE projections of the initial hidden representations of our models (van der Maaten, 2014). Figure 4 shows the t-SNE projection of the example from Figure 7 using the initial hidden state of an En MLM (left) and the target side of the De MLM → En MLM (right). In the monolingual example, the nearest neighbours of the target image are desert scenes with groups of people. Adding the transferred source features results in a representation that places importance on the background, due to the fact that it is consistently mentioned in the descriptions. Now the nearest neighbours are images of mountainous snow regions with groups of people.Which descriptions are improved by source or image features? Figure 5 shows the distribution of sentence-level Meteor scores of the baseline models (monolingual MLM and monomodal LM → LM) and the average per-sentence change when moving to our best performing multilingual multimodal model (SOURCE-MLM→ TARGET-LM). The additional source language features (compared to MLM) or additional modality (compared to LM→ LM) result in similar patterns: low quality descriptions are improved, while the (far less common) high quality descriptions deteriorate.Adding image features seems to be riskier than adding source language features, which is unsurpris- ing given the larger distance between visual and linguistic space, versus moving from one language to another. This is also consistent with the lower performance of MLM baseline models compared to LM→LM models. An analysis of the LM→MLM model (not shown here) shows similar behaviour to the MLM→LM model above. However, for this model the decreasing performance starts earlier: the LM→MLM model improves over the LM→LM baseline only in the lowest score bin. Adding the image features at the source side, rather than the target side, seems to filter out some of the noise and complexity of the image features, while the essential source language features are retained. Conversely, merging the source language features with image features on the target side, in the TARGET-MLM models, leads to a less helpful entangling of linguistic and noisier image input, maybe because too many sources of information are combined at the same time (see Eqn 6).The past few years have seen numerous results showing how relatively standard neural network model architectures can be applied to a variety of tasks. The flexibility of the application of these ar- chitectures can be seen as a strong point, indicating that the representations learned in these general models are sufficiently powerful to lead to good performance. Another advantage, which we have exploited in the work presented here, is that it becomes relatively straightforward to make connec- tions between models for different tasks, in this case image description and machine translation.Automatic image description has received a great deal of attention in recent years (see Bernardi et al. (2016) for a more detailed overview of the task, datasets, models, and evaluation issues). Deep neural networks for image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) ( Kiros et al., 2014;Donahue et al., 2014;Vinyals et al., 2015;Karpathy &amp; Fei-Fei, 2015;Mao et al., 2015). The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010;Yang et al., 2011;Li et al., 2011;Mitchell et al., 2012;Elliott &amp; Keller, 2013;Yatskar et al., 2014;Elliott &amp; de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task. As in our MLM, the image-sentence representation in the multimodal RNN is initialised with image features from the final fully-connected layer of a convolutional neural network trained for multi-class object recognition ( Krizhevsky et al., 2012). Alternative formulations input the image features into the model at each timestep ( Mao et al., 2015), or first detect words in an image and generate sentences using a maximum-entropy language model .In the domain of machine translation, a greater variety of neural models have been used for sub- tasks within the MT pipeline, such as neural network language models (Schwenk, 2012) and joint translation and language models for re-ranking in phrase-based translation models ( Le et al., 2012;Auli et al., 2013) or directly during decoding (Devlin et al., 2014). More recently, end-to-end neu- ral MT systems using Long Short-Term Memory Networks and Gated Recurrent Units have been proposed as Encoder-Decoder models for translation Bahdanau et al., 2015), and have proven to be highly effective ( Bojar et al., 2015;Jean et al., 2015).In the multimodal modelling literature, there are related approaches using visual and textual informa- tion to build representations for word similarity and categorization tasks (Silberer &amp; Lapata, 2014;Kiela &amp; Bottou, 2014;Kiela et al., 2015 We introduced multilingual image description, the task of generating descriptions of an image given a corpus of descriptions in multiple languages. This new task not only expands the range of output languages for image description, but also raises new questions about how to integrate features from multiple languages, as well as multiple modalities, into an effective generation model. Our multilingual multimodal model is loosely inspired by the encoder-decoder approach to neural machine translation. Our encoder captures a multimodal representation of the image and the source- language words, which is used as an additional conditioning vector for the decoder, which produces descriptions in the target language. Each conditioning vector is originally trained towards its own objective: the CNN image features are transferred from an object recognition model, and the source features are transferred from a source-language image description model. Our model substantially improves the quality of the descriptions in both directions compared to monolingual baselines.The dataset used in this paper consists of translated descriptions, leading to high performance for the translation baseline. However, we believe that multilingual image description should be based on independently elicited descriptions in multiple languages, rather than literal translations. Linguistic and cultural differences may lead to very different descriptions being appropriate for different lan- guages (For example, a polder is highly salient to a Dutch speaker, but not to an English speaker; an image of a polder would likely lead to different descriptions, beyond simply lexical choice.) In such cases image features will be essential.A further open question is whether the benefits of multiple monolingual references extend to multiple multilingual references. Image description datasets typically include multiple refer- ence sentences, which are essential for capturing linguistic diversity within a single language ( Rashtchian et al., 2010;Elliott &amp; Keller, 2013;Hodosh et al., 2013;Chen et al., 2015). In our ex- periments, we found that useful image description diversity can also be found in other languages instead of in multiple monolingual references.In the future, we would like to explore attention-based recurrent neural networks, which have been used for machine translation ( Bahdanau et al., 2015;Jean et al., 2015) and image description ( Xu et al., 2015). We also plan to apply these models to other language pairs, such as the recently released PASCAL 1K Japanese Translations dataset (Funaki &amp; Nakayama, 2015). Lastly, we aim to apply these types of models to a multilingual video description dataset (Chen &amp; Dolan, 2011).D. Elliott was supported by an Alain Bensoussain Career Development Fellowship. S. Frank is supported by funding from the European Union's Horizon 2020 research and innovation programme under grant agreement Nr. 645452.We thank Philip Schulz, Khalil Sima'an, Arjen P. de Vries, Lynda Hardman, Richard Glassey, Wilker Aziz, Joost Bastings, and´Akosand´ and´Akos Kádár for discussions and feedback on the work. We built the models using the Keras library, which is built on-top of Theano. We are grateful to the Database Architectures Group at CWI for access to their K20x GPUs.
