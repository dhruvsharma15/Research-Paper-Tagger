Highly expressive parametric models have enjoyed great success in supervised learning, where learning objectives and evaluation metrics are typically well-specified and easy to compute. On the other hand, the learning objective for un- supervised settings is less clear. At a fundamental level, the idea is to learn a generative model that minimizes some no- tion of divergence with respect to the data distribution. Min- imizing the Kullback-Liebler divergence between the data distribution and the model, for instance, is equivalent to per- forming maximum likelihood estimation (MLE) on the ob- served data. Maximum likelihood estimators are asymptoti- cally statistically efficient, and serve as natural objectives for learning prescribed generative models (Mohamed and Lakshminarayanan 2016).In contrast, an alternate principle that has recently at- tracted much attention is based on adversarial learning, where the objective is to generate data indistinguishable from the training data. Adversarially learned models such as generative adversarial networks (GAN; ( Goodfellow et al. 2014)) can sidestep specifying an explicit density for any data point and belong to the class of implicit generative mod- els ( Diggle and Gratton 1984). The lack of characterization of an explicit density in GANs is however problematic for two reasons. Several ap- plication areas of deep generative models rely on density estimates; for instance, count based exploration strategies based on density estimation using generative models have recently achieved state-of-the-art performance on challeng- ing reinforcement learning environments ( Ostrovski et al. 2017). Secondly, it makes the quantitative evaluation of the generalization performance of such models challenging. The typical evaluation criteria based on ad-hoc sample quality metrics ( Salimans et al. 2016;Che et al. 2017) do not address this issue since it is possible to generate good samples by memorizing the training data, or missing important modes of the distribution, or both (Theis, Oord, and Bethge 2016). Alternatively, density estimates based on approximate in- ference techniques such as annealed importance sampling (AIS; (Neal 2001;Wu et al. 2017)) and non-parameteric methods such as kernel density estimation (KDE;(Parzen 1962;Goodfellow et al. 2014)) are computationally slow and crucially rely on assumptions of a Gaussian observa- tion model for the likelihood that could lead to misleading estimates as we shall demonstrate in this paper.To sidestep the above issues, we propose Flow-GANs, a generative adversarial network with a normalizing flow generator. A Flow-GAN generator transforms a prior noise density into a model density through a sequence of invert- ible transformations. By using an invertible generator, Flow- GANs allow us to tractably evaluate exact likelihoods using the change-of-variables formula and perform exact posterior inference over the latent variables while still permitting effi- cient ancestral sampling, desirable properties of any proba- bilistic model that a typical GAN would not provide.Using a Flow-GAN, we perform a principled quantitative comparison of maximum likelihood and adversarial learning on benchmark datasets viz. MNIST and CIFAR-10. While adversarial learning outperforms MLE on sample quality metrics as expected based on strong evidence in prior work, the log-likelihood estimates of adversarial learning are or- ders of magnitude worse than those of MLE. The difference is so stark that a simple Gaussian mixture model baseline outperforms adversarially learned models on both sample quality and held-out likelihoods. Our quantitative analysis reveals that the poor likelihoods of adversarial learning can be explained as a result of an ill-conditioned Jacobian ma- trix for the generator function suggesting a mode collapse, rather than overfitting to the training dataset.To resolve the dichotomy of perceptually good-looking samples at the expense of held-out likelihoods in the case of adversarial learning (and vice versa in the case of MLE), we propose a hybrid objective that bridges implicit and pre- scribed learning by augmenting the adversarial training ob- jective with an additional term corresponding to the log- likelihood of the observed data. While the hybrid objective achieves the intended effect of smoothly trading-off the two goals in the case of CIFAR-10, it has a regularizing effect on MNIST where it outperforms MLE and adversarial learning on both held-out likelihoods and sample quality metrics.Overall, this paper makes the following contributions:Hence, evaluating the learning objective for MLE in Eq. (1) requires the ability to evaluate the model density p θ (x). Models that provide an explicit characterization of the like- lihood function are referred to as prescribed generative mod- els (Mohamed and Lakshminarayanan 2016).A generative model can be learned to optimize divergence notions beyond the KL divergence. A large family of diver- gences can be conveniently expressed as:where F denotes a set of parameters, h φ and h1. We propose Flow-GANs, a generative adversarial net- work with an invertible generator that can perform effi- cient ancestral sampling and exact likelihood evaluation.φ are ap- propriate real-valued functions parameterized by φ. Differ- ent choices of F, h φ and h 2. We propose a hybrid learning objective for Flow-GANs that attains good log-likelihoods and generates high- quality samples on MNIST and CIFAR-10 datasets.φ can lead to a variety of f - divergences such as Jenson-Shannon divergence and inte- gral probability metrics such as the Wasserstein distance. For instance, the GAN objective proposed by Goodfellow et al. (2014) can also be cast in the form of Eq. (2) below: (3) 3. We demonstrate the limitations of AIS and KDE for log- likelihood evaluation and ranking of implicit models.We analyze the singular value distribution for the Jaco- bian of the generator function to explain the low log- likelihoods observed due to adversarial learning.We begin with a review of maximum likelihood estimation and adversarial learning in the context of generative models. For ease of presentation, all distributions are w.r.t. any arbi- trary x ∈ R d , unless otherwise specified. We use upper-case to denote probability distributions and assume they all admit absolutely continuous densities (denoted by the correspond- ing lower-case notation) on a reference measure dx.Consider the following setting for learning generative models. Given some datawhere φ denotes the parameters of a neural network function D φ . We refer the reader to (Nowozin, Cseke, and Tomioka 2016;Mescheder, Nowozin, and Geiger 2017b) for further details on other possible choices of divergences. Impor- tantly, a Monte Carlo estimate of the objective in Eq. (2) requires only samples from the model. Hence, any model that allows tractable sampling can be used to evaluate the following minimax objective:As a result, even differentiable implicit models which do not provide a characterization of the model likelihood 1 but allow tractable sampling can be learned adversarially by op- timizing minimax objectives of the form given in Eq. (4). m i=1 sampled i.i.d. from an unknown probability density p data , we are inter- ested in learning a probability density p θ where θ denotes the parameters of a model. Given a parameteric family of mod- els M, the typical approach to learn θ ∈ M is to minimize a notion of divergence between P data and P θ . The choice of divergence and the optimization procedure dictate learning, leading to the following two objectives.In maximum likelihood estimation (MLE), we minimize the Kullback-Liebler (KL) divergence between the data distri- bution and the model distribution. Formally, the learning ob- jective can be expressed as:Since p data is independent of θ, the above optimization problem can be equivalently expressed as:From a statistical perspective, maximum likelihood estima- tors are statistically efficient asymptotically (under some conditions) and hence minimizing the KL divergence is a natural objective for many prescribed models (Huber 1967). However, not all models allow for a well-defined, tractable, and easy-to-optimize likelihood. For example, exact likelihood evaluation and sampling are tractable in directed, fully observed models such as Bayesian networks and autoregressive models (Larochelle and Murray 2011;Oord, Kalchbrenner, and Kavukcuoglu 2016). Hence, they are usually trained by maximum likelihood. Undirected models, on the other hand, provide only unnor- malized likelihoods and are sampled from using expensive Markov chains. Hence, they are usually learned by approx- imating the likelihood using methods such as contrastive divergence (Carreira-Perpinan and Hinton 2005) and pseu- dolikelihood (Besag 1977). The likelihood is generally in- tractable to compute in latent variable models (even directed max 1 This could be either due to computational intractability in eval- uating likelihoods or because the likelihood is ill-defined.ones) as it requires marginalization. These models are typi- cally learned by optimizing a stochastic lower bound to the log-likelihood using variational Bayes approaches (Kingma and Welling 2014). Directed latent variable models allow for efficient ances- tral sampling and hence these models can also be trained using other divergences, e.g., adversarially (Mescheder, Nowozin, and Geiger 2017a;Mao et al. 2017;Song, Zhao, and Ermon 2017). A popular class of latent variable models learned adversarially consist of generative adversarial net- works (GAN; ( Goodfellow et al. 2014)). GANs comprise of a pair of generator and discriminator networks. The gener- atoris a deterministic function differen- tiable with respect to the parameters θ. The function takes as input a source of randomness z ∈ R k sampled from a tractable prior density p(z) and transforms it to a sample G θ (z) through a forward pass. Evaluating likelihoods as- signed by a GAN is challenging because the model density p θ is specified only implicitly using the prior density p(z) and the generator function G θ . In fact, the likelihood for any data point is ill-defined (with respect to the Lesbegue mea- sure over R n ) if the prior distribution over z is defined over a support smaller than the support of the data distribution.GANs are typically learned adversarially with the help of a discriminator network. The discriminator D φ : R d → R is another real-valued function that is differentiable with re- spect to a set of parameters φ. Given the discriminator func- tion, we can express the functions h and h in Eq. (4) as compositions of D φ with divergence-specific functions. For instance, the Wasserstein GAN (WGAN; (Arjovsky, Chin- tala, and Bottou 2017)) optimizes the following objective:A Flow-GAN consists of a pair of generator-discriminator networks with the generator specified as a normalizing flow model . A normalizing flow model specifies a parametric transformation from a prior density p(z) :to another density over the same space, p θ (x) :where R 0 is the set of non- negative reals. The generator transformation G θ :Using the change-of-variables formula and let- ting z = f θ (x), we have:wherewhere F is defined such that D φ is 1-Lipschitz. Empirically, GANs generate excellent samples of natural images (Rad- ford, Metz, and Chintala 2015), audio signals (Pascual, Bonafonte, and Serrà 2017), and of behaviors in imitation learning (Ho and Ermon 2016; Li, Song, and Ermon 2017).As discussed above, generative adversarial networks can tractably generate high-quality samples but have intractable or ill-defined likelihoods. Monte Carlo techniques such as AIS and non-parameteric density estimation methods such as KDE get around this by assuming a Gaussian observation model p θ (x|z) for the generator. 2 This assumption alone is not sufficient for quantitative evaluation since the marginal likelihood of the observed data, p θ (x) = p θ (x, z)dz in this case would be intractable as it requires integrating over all the latent factors of variation. This would then require ap- proximate inference (e.g., Monte Carlo or variational meth- ods) which in itself is a computational challenge for high- dimensional distributions. To circumvent these issues, we propose flow generative adversarial networks (Flow-GAN). denotes the Jacobian of f θ at x. The above formula can be applied recursively over compositions of many invertible transformations to produce a complex final density. Hence, we can evaluate and optimize for the log- likelihood assigned by the model to a data point as long as the prior density is tractable and the determinant of the Ja- cobian of f θ evaluated at x can be efficiently computed.Evaluating the likelihood assigned by a Flow-GAN model in Eq. (6) requires overcoming two major challenges. First, requiring the generator function G θ to be reversible imposes a constraint on the dimensionality of the latent variable z to match that of the data x. Thereafter, we require the trans- formations between the various layers of the generator to be invertible such that their overall composition results in an invertible G θ . Secondly, the Jacobian of high-dimensional distributions can however be computationally expensive to compute. If the transformations are designed such that the Jacobian is an upper or lower triangular matrix, then the de- terminant can be easily evaluated as the product of its diago- nal entries. We consider two such family of transformations. 1. Volume preserving transformations. Here, the Jacobian of the transformations have a unit determinant. For exam- ple, the NICE model consists of several layers perform- ing a location transformation . The top layer is a diagonal scaling matrix with non- zero log determinant. 2. Non-volume preserving transformations. The determinant of the Jacobian of the transformations is not necessarily unity. For example, in Real-NVP, layers performs both location and scale transformations (Dinh, Sohl-Dickstein, and Bengio 2017). For brevity, we direct the reader to Dinh, Krueger, and Bengio (2014) and Dinh, Sohl-Dickstein, and Bengio (2017) for the specifications of NICE and Real-NVP respectively. Crucially, both volume preserving and non-volume preserv- ing transformations are invertible such that the determinant of the Jacobian can be computed tractably.2 The true observation model for a GAN is a Dirac delta distribu- tion, i.e., p θ (x|z) is infinite when x = G θ (z) and zero otherwise.In a Flow-GAN, the likelihood is well-defined and compu- tationally tractable for exact evaluation of even expressive volume preserving and non-volume preserving transforma- tions. Hence, a Flow-GAN can be trained via maximum like- lihood estimation using Eq. (1) in which case the discrimi- nator is redundant. Additionally, we can perform ancestral sampling just like a regular GAN whereby we sample a ran- dom vector z ∼ P z and transform it to a model generated sample viaθ . This makes it possible to learn a Flow-GAN using an adversarial learning objective (for ex- ample, the WGAN objective in Eq. (5)).A natural question to ask is why should one use adversar- ial learning given that MLE is statistically efficient asymp- totically (under some conditions). Besides difficulties that could arise due to optimization (in both MLE and adversar- ial learning), the optimality of MLE holds only when there is no model misspecification for the generator i.e., the true data distribution P data is a member of the parametric family of distributions under consideration (White 1982). This is gen- erally not the case for high-dimensional distributions, and hence the choice of the learning objective becomes largely an empirical question. Unlike other models, a Flow-GAN allows both maximum likelihood and adversarial learning, and hence we can investigate this question experimentally. Che et al. 2017). The Inception scores are computed as:where x is a sample generated by the model, p(y|x) is the softmax probability for the labels y assigned by a pretrained classifier for x, and p(y) is the overall distribution of la- bels in the generated samples (as predicted by the pretrained classifier). The intuition is that the conditional distribution p(y|x) should have low entropy for good looking images while the marginal distribution p(y) has high entropy to en- sure sample diversity. Hence, a generative model can per- form well on this metric if the KL divergence between the two distributions (and consequently, the Inception score for the generated samples) is large. The MODE score given be- low modifies the Inception score to take into account the distribution of labels in the training data, p * (y):Our criteria for evaluation is based on held-out log- likelihoods and sample quality metrics. We focus on nat- ural images since they allow visual inspection as well as quantification using recently proposed metrics. A "good" generative model should generalize to images outside the training data and assign high log-likelihoods to held-out data. Comparing the left vs. right panels in Figure 2, we see that the log-likelihoods attained by ADV are orders of magnitude worse than those attained by MLE after sufficient training. Finally, we note that the WGAN loss (green curves) does not correlate well with NLL estimates. While the WGAN loss stabilizes after few iterations of training, the NLLs con- tinue to increase. This observation is in contrast to prior work showing the loss to be strongly correlated with sam- ple quality metrics (Arjovsky, Chintala, and Bottou 2017).   Log-likelihood. The log-likelihood learning curves for Flow-GAN models learned using MLE and ADV are shown in Figure 2a and Figure 2b respectively. Following conven- tion, we report the negative log-likelihoods (NLL) in nats for MNIST and bits/dimension for CIFAR-10.MLE. In Figure 2a, we see that normalizing flow models attain low validation NLLs (blue curves) after few gradient updates as expected because it is explicitly optimizing for the MLE objective in Eq. (1). Continued training however could lead to overfitting as the train NLLs (red curves) begin to diverge from the validation NLLs.ADV. Surprisingly, ADV models show a consistent in- crease in validation NLLs as training progresses as shown in Figure 2b (for CIFAR-10, the estimates are reported on a log scale!). Based on the learning curves, we can disregard overfitting as an explanation since the increase in NLLs is observed even on the training data. The training and vali- dation NLLs closely track each other suggesting that ADV models are not simply memorizing the training data.The above experiments suggest that ADV can produce ex- cellent samples but assigns low likelihoods to the observed data. However, a direct comparison of ADV with the log- likelihoods of MLE is unfair since the latter is explicitly optimizing for the desired objective. To highlight that gen- erating good samples at the expense of low likelihoods is not a challenging goal, we propose a simple baseline. We compare the adversarially learned Flow-GAN models that achieves the highest MODE/Inception score with a Gaussian Mixture Model consisting of m isotropic Gaussians with equal weights centered at each of the m training points as the baseline Gaussian Mixture Model (GMM). The band- width hyperparameter, σ, is the same for each of the mixture components and optimized for the lowest validation NLL by doing a line search in (0,1]. We show results for CIFAR- 10 in Figure 3. Our observations below hold for MNIST as well; results deferred to Appendix C.We overload the y-axis in Figure 3 to report both NLLs and sample quality metrics. The horizontal maroon and cyan dashed lines denote the best attainable MODE/Inception scores and corresponding validation NLLs respectively at- tained by the adversarially learned Flow-GAN model. The GMM can clearly attain better sample quality metrics since it is explicitly overfitting to the training data for low values of the bandwidth parameter (any σ for which the red curve is above the maroon dashed line). Surprisingly, the simple GMM also outperforms the adversarially learned model with respect to NLLs attained for several values of the bandwidth parameter (any σ for which the blue curve is below the cyan dashed line). Bandwidth parameters for which GMM mod- els outperform the adversarially learned model on both log- likelihoods and sample quality metrics are highlighted using the green shaded area. We show samples from the GMM in the appendix. Hence, a trivial baseline that is memo- rizing the training data can generate high quality samples and better held-out log-likelihoods, suggesting that the log- likelihoods attained by adversarial training are very poor.NLL of ADV model with best inception score   Figure 3: Gaussian Mixture Models outperform adversar- ially learned models on both held-out log-likelihoods and sampling metrics on CIFAR-10 (green shaded region).In the previous section, we observed that adversarially learn- ing Flow-GANs models attain poor held-out log-likelihoods. This makes it challenging to use such models for applica- tions requiring density estimation. On the other hand, Flow- GANs learned using MLE are "mode covering" but do not generate high quality samples. With a Flow-GAN, it is pos- sible to trade-off the two goals by combining the learning objectives corresponding to both these inductive principles. Without loss of generality, let V (G θ , D φ ) denote the min- imax objective of any GAN model (such as WGAN). The hybrid objective of a Flow-GAN can be expressed as:where λ ≥ 0 is a hyperparameter for the algorithm. By vary- ing λ, we can interpolate between plain adversarial training (λ = 0) and MLE (very high λ).We summarize the results from MLE, ADV, and Hy- brid for log-likelihood and sample quality evaluation in Ta- ble 1 and Table 2 for MNIST and CIFAR-10 respectively. The tables report the test log-likelihoods corresponding to the best validated MLE and ADV models and the highest MODE/Inception scores observed during training. The sam- ples generated by models with the best MODE/Inception scores for each objective are shown in Figure 1c.While the results on CIFAR-10 are along expected lines, the hybrid objective interestingly outperforms MLE and ADV on both test log-likelihoods and sample quality met- rics in the case of MNIST. One potential explanation for this is that the ADV objective can regularize MLE to generalize to the test set and in turn, the MLE objective can stabilize the optimization of the ADV objective. Hence, the hybrid objective in Eq. (7) can smoothly balance the two objectives using the tunable hyperparameter λ, and in some cases such as MNIST, the performance on both tasks could improve as a result of the hybrid objective.Our findings are in contrast with prior work which report much better log-likelihoods for adversarially learned mod- els with a standard generator architecture based on an- nealed importance sampling (AIS; ( Wu et al. 2017)) and kernel density estimation (KDE; ( Goodfellow et al. 2014)). These methods rely on approximate inference techniques for log-likelihood evaluation and make assumptions about a Gaussian observation model which does not hold for GANs. Since Flow-GANs allow us to compute exact log- likelihoods, we can evaluate the quality of approximation made by AIS and KDE for density estimation of invertible generators. For a detailed description of the methods, we re- fer the reader to prior work (Neal 2001;Parzen 1962).We consider the MNIST dataset where these meth- ods have been previously applied to by Wu et al. (2017) and Goodfellow et al. (2014) respectively. Since both AIS and KDE inherently rely on the samples generated, we eval- uate these methods for the MLE, ADV, and Hybrid Flow- GAN model checkpoints corresponding to the best MODE scores observed during training. In Table 3, we observe that both AIS and KDE produce estimates of log-likelihood that are far from the ground truth, accessible through the ex- act Flow-GAN log-likelihoods. Even worse, the ranking of log-likelihood estimates for AIS (ADV&gt;Hybrid&gt;MLE) and KDE (Hybrid&gt;MLE&gt;ADV) do not obey the relative rank- ings of the Flow-GAN estimates (MLE&gt;Hybrid&gt;ADV).In order to explain the variation in log-likelihoods attained by various Flow-GAN learning objectives, we investigate the distribution of the magnitudes of singular values for the Jacobian matrix of several generator functions, G θ for MNIST in Figure 4 evaluated at 64 noise vectors z randomly sampled from the prior density p(z). The x-axis of the figure shows the singular value magnitudes on a log scale and for each singular value s, we show the corresponding cumula- tive distribution function value on the y-axis which signifies the fraction of singular values less than s. The results on CIFAR-10 in Appendix D show a similar trend.The Jacobian is a good first-order approximation of the generator function locally. In Figure 4, we observe that the singular value distribution for the Jacobian of an invertible generator learned using MLE (orange curves) is concen- trated in a narrow range, and hence the Jacobian matrix is well-conditioned and easy to invert. In the case of invertible generators learned using ADV with Wasserstein distance (green curves) however, the spread of singular values is very wide, and hence the Jacobian matrix is ill-conditioned.The average log determinant of the Jacobian matrices for MLE, ADV, and Hybrid models are −4170.34, −15588.34, and −5184.40 respectively which translates to the trend ADV&lt;Hybrid&lt;MLE. This indicates that the ADV models are trying to squish a sphere of unit volume centered at a la- tent vector z to a very small volume in the observed space x. Tiny perturbations of training as well as held-out data- points can hence manifest as poor log-likelihoods. In spite of not being limited in the representational capacity to cover the entire space of the data distribution (the dimensions of z (i.e., k) and x (i.e., d) match for invertible generators), ADV prefers to learn a distribution over a smaller support.The Hybrid learning objective (blue curves), however, is able to correct for this behavior, and the distribution of sin- gular value magnitudes matches closely to that of MLE. We also considered variations involving the standard DCGAN architectures with k = d minimizing the Wasserstein dis- tance (red curves) and Jenson-Shannon divergence (purple curves). The relative shift in distribution of singular value magnitudes to lower values is apparent even in these cases. in the context of maximum likelihood estimation of fully ob- served and latent variable models Rezende and Mohamed 2015;Kingma, Salimans, and Welling 2016;Dinh, Sohl-Dickstein, and Bengio 2017).The low dimensional support of the distributions learned by adversarial learning often manifests as lack of sample di- versity and is referred to as mode collapse. In prior work, mode collapse is detected based on visual inspection or heuristic techniques (Goodfellow 2016;Arora and Zhang 2017). Techniques for avoiding mode collapse explicitly fo- cus on stabilizing GAN training such as ( Metz et al. 2016;Che et al. 2017;Mescheder, Nowozin, and Geiger 2017b) rather than quantitative methods based on likelihoods.Any model which allows for efficient likelihood evaluation and sampling can be trained using maximum likelihood and adversarial learning. This line of reasoning has been ex- plored to some extent in prior work that combine the ob- jectives of prescribed latent variable models such as VAEs (maximizing an evidence lower bound on the data) with ad- versarial learning ( Larsen et al. 2015;Mescheder, Nowozin, and Geiger 2017a;Srivastava et al. 2017). However, the ben- efits of such procedures do not come for "free" since we still need some form of approximate inference to get a handle on the log-likelihoods. This could be expensive, for instance combining a VAE with a GAN introduces an additional in- ference network that increases the overall model complexity.Our approach sidesteps the additional complexity due to approximate inference by considering a normalizing flow model. The trade-off made by a normalizing flow model is that the generator function needs to be invertible while other generative models such as VAEs have no such re- quirement. On the positive side, we can tractably evaluate exact log-likelihoods assigned by the model for any data point. Normalizing flow models have been previously used As an attempt to more quantitatively evaluate generative models, we introduced Flow-GAN. It is a generative adver- sarial network which allows for tractable likelihood evalu- ation, exactly like in a flow model. Since it can be trained both adversarially (like a GAN) and in terms of MLE (like a flow model), we can quantitatively evaluate the trade-offs involved. We observe that adversarial learning assigns very low-likelihoods to both training and validation data while generating superior quality samples. To put this observa- tion in perspective, we demonstrate how a naive Gaussian mixture model can outperform adversarially learned models on both log-likelihood estimates and sample quality metrics. Quantitative evaluation methods based on AIS and KDE fail to detect this behavior and can be poor approximations of the true log-likelihood (at least for the models we considered).Analyzing the Jacobian of the generator provides insights into the contrast between maximum likelihood estimation and adversarial learning. The latter have a tendency to learn distributions of low support, which can lead to low likeli- hoods. To correct for this behavior, we proposed a hybrid ob- jective function which involves loss terms corresponding to both MLE and adversarial learning. The use of such models in applications requiring both density estimation and sample generation is an exciting direction for future work. The progression of sample quality metrics for MLE and ADV objectives during training is shown in Figures 5 (a) and (b) for MNIST and CIFAR-10 respectively. Higher scores are reflective of better sample quality. ADV (maroon curves) significantly outperform MLE (cyan curves) with respect to the final MODE/Inception scores achieved. The comparison of GMMs with Flow-GANs trained using adversarial learning is shown in Figure 6. Similar to the ob- servations made for CIFAR-10, the simple GMM outper- forms the adversarially learned model with respect to NLLs and sample quality metrics for any bandwidth parameter within the green shaded area. The samples obtained from the GMM are shown in Fig- ure 7. Since the baseline is fitting Gaussian densities around every training point, the samples obtained for relatively small bandwidths are of high quality. Yet, even the held- out likelihoods for these bandwidths are better than those of ADV models with the best MODE/Inception scores.  The CDF of singular value magnitudes for the CIFAR-10 dataset in Figure 8 again suggests that the Jacobian matrix for the generator function is ill-conditioned for the ADV models (green, red, purple curves) since the distributions have a large spread. Using a hybrid objective (blue curves) can correct for this behavior with the distribution of singu- lar values much more concentrated similar to MLE (orange curves).The log determinant of the Jacobian for the MLE, ADV, and Hybrid models are −12818.84, −21848.09, −14729.51 respectively reflecting the trend ADV&lt;Hybrid&lt;MLE, pro- viding further empirical evidence to suggest that adversarial training shows a strong preference for learning distributions with smaller support. 
