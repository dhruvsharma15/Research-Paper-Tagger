Jigsaw puzzles are a popular form of entertainment, available in different variation of difficulty to challenge children, adults and even professional players. Given n × mJigsaw puzzles were first introduced around 1760 by John Spilsbury, a Londonian engraver and mapmaker. Nevertheless, the first attempt by the scientific commu- nity to computationally solve the problem is attributed to Freeman and Garder [9] who in 1964 presented a solver which could handle up to nine-piece problems. Ever since then, the research focus regarding the problem has shifted from shape-based to merely color-based solvers of square-tile puzzles. In 2010 Cho et al. [4] presented a probabilistic puzzle solver that could handle up to 432 pieces, given some a priori knowledge of the puzzle. Their results were improved a year later by Yang et al. [26] who presented a particle filter-based solver. Furthermore, Pomeranz et al. [20] in- troduced that year, for the first time, a fully automated square jigsaw puzzle solver that could handle puzzles of up to 3,000 pieces. Gallagher [10] has further advanced this by considering a more general variant of the problem, where neither piece ori- entation nor puzzle dimensions are known. Son et al. [24] improved the accuracy of the latter variant using so-called "loop-constraints". Palkin and Tal [19] further improved the accuracy and handled puzzles with missing pieces. Sholomon et al. [21] presented a genetic algorithm (GA)-based solver for puzzles of known orientation which was later generalized to other variants [22,23].As stated earlier, most works focus on the compatibility measure and an estimation metric. A compatibility measure is a function that given two puzzle piece edges (e.g. the right edge of piece 7 versus the upper edge of piece 12) predicts the likelihood that these two edges are indeed placed as neighbors in the correct solution. This measure applies to each possible pair of piece edges. The estimation metric, on the other hand, predict whether two piece edges are adjacent but may not apply to many possible pairs. Following is a more detailed review of the efforts made so far in the field. Cho et al.[4] surveyed four compatibility measures among which they found dis- similarity the most accurate. Dissimilarity is the sum (over all neighboring pixels) of squared color differences (over all color bands). Assuming pieces x i , x j are repre- sented in some three-dimensional color space (like RGB or YUV) by a K × K × 3 matrix, where K is the height/width of a piece (in pixels), their dissimilarity, where x j is to the right of x i , for example, iswhere cb denotes the color band. Pomeranz et al. [20] also used the dissimilarity measure but found empirically that using the (L p ) q norm works better than the usual L 2 norm. Moreover, they presented the high-precision best-buddy metric. Pieces x i and x j are said to best- buddies ifwhere P ieces is the set of all given image pieces and R 1 and R 2 are "complementary" spatial relations (e.g. if R 1 = right, then R 2 = left and vice versa).Gallagher [10] proposed yet another compatibility measure, called the Maha- lanobis gradient compatibility (MGC) as a preferable compatibility measure to those used by Pomeranz et al. [20]. The MGC penalizes changes in intensity gradients, rather than changes in intensity, and learns the covariance of the color channels, us- ing the Mahalanobis distance. Also, Gallagher suggested using dissimilarity ratios. Absolute distances between potential piece edge matches are sometimes not indica- tive (for example in smooth surfaces like sea and sky), so considering the absolute score, divided by the second-best score available seems more indicative.Son et al. [24] suggested "loop-constraints", four or more puzzle piece edges where the compatibility ratio between each pair is in the top ten among all possible pairs of piece edges in the given puzzle. Palkin and Tal [19] proposed a greedy solver based on an L 1 -norm asymmetric dissimilarity and the best-buddies estimation metric.We propose a novel estimation metric called "DNN-Buddies". Our goal is to obtain a classifier which predicts the adjacency likelihood of two puzzle piece edges in the correct puzzle configuration.Note that despite the exponential nature of the problem (as there are O((nm)!) possible arrangements of the pieces, taking into account rotations), the problem can be solved theoretically by assigning correctly, in a consecutive manner, n × m − 1 piece-edge pairs. (This is reminiscent of finding a minimal spanning tree, as noted by [10].) Hence, the classifier's precision is of far greater importance than its recall. A classifier with perfect precision and a recall ofmight achieve a perfect solution by itself.A straight-forward solution might have been to train a neural network against matching-pairs vs. non-matching ones. However, the issue of a jigsaw puzzle piece matching is of an imbalanced nature. In each n × m puzzle, there are O(n × m) matching pairs of piece edges and O(n 2 × m 2 ) possible nonmatching ones. A thor- ough review on the challenges and tactics to avoid them can be found in [13].The trivial approach of random or uninformed undersampling, i.e. randomly choosing the required number of nonmatching pairs leads to a low-precision and high- recall metric, the very opposite of the goal set beforehand. We believe that the reason for this shortcoming is that there exist many "easy-to-spot" mismatches but only a handful of "hard-to-spot" ones. Thus, we resort to informed undersampling, choosing a subset of "good" mismatching pairs according to some criterion. Nevertheless, we avoid using any manual feature selection or other sophisticated image-related means.In the jigsaw puzzle domain, similarly to many other problem domains, the solver does not actually try to reassemble the original image (as this problem is Deep Neural Network Based Estimation Metric for the Jigsaw Puzzle Problem 5 not mathematically defined), but rather tries solving a "proxy problem" which is to achieve an image whose global overall score between abutting-edges is minimal. Thus, we choose using the compatibility measure as the undersampling criterion.For training and cross-validation, we use the 2,755 images of size 360 × 480 pixels from the IAPR TC-12 Benchmark [12]. Each image is first converted to YUV space followed by the normalization of each channel separately (via z-score normalization). Next, each (puzzle) image is divided to 12 × 17 tiles, where each tile is of size 28 × 28 pixels (as in all previous works); finally, we create a balanced set of positive and negative samples of puzzle-piece pairs, using informed undersampling as will be described below. In the end, we obtain a balanced set of 970,224 pairs overall.To balance our dataset, we use the most basic compatibility score which is the dissimilarity between two piece-edges in the YUV color-space, as described in Eq. 1, as an undersampling criterion. For each puzzle piece edge x i,j (i = 1..n×m, j = 1..4), we find its most compatible piece edge x k1,l1 and its second most compatible piece edge x k2,l2 . If the pair of edges x i,j −x k1,l1 is indeed adjacent in the original image, we add this pair to the pool of positively-labeled samples and toss the pair x i,j − x k2,l2 to the pool of negatively-labeled samples. Otherwise, x i,j − x k1,l1 is added to the negatively-labeled samples and the other pair is discarded. The latter is done to avoid training the network on adjacent pieces which happen to be vastly different due to a significant change of the image scenery in the corresponding region. In other words, we restrict our interest to highly compatible piece edges that are indeed adjacent. Since this method leads to more negative samples than positive ones, we eventually randomly throw some negative samples to balance out the set.From each image pair we extract the two columns near the edge, i.e. the column of abutting pixels in each edge and the one next to it. This results is an input of size (28 × 4 × 3 =) 336 pixels. We use a feed-forward neural network (FFNN) of five fully connected layers of size 336, 100, 100, 100, and 2. The output is a softmax layer containing two neurons. We expect (0, 1) for matching pairs and (1, 0) otherwise. The activation used in all layers is the rectified linear unit (ReLU) function, i.e. f (x) = max(0, x). Figure 2 depicts the network's structure.We trained the network in a supervised manner using Stochastic Gradient De- scent that minimizes the negative log likelihood of the error for 100 iterations. The resulting network reaches 95.04% accuracy on the training set and 94.62% on a held-out test set.All dataset preparation and network training was performed using Torch7 [6].For each piece edge x i,j (i = 1..n × m, j = 1..4), if its most compatible piece edge x k,l is classified positively using the DNN-Buddies network, we define x k,l to be x i,j 's DNN-buddy piece edge. Note that each piece edge can have only a single DNN-buddy; Next, we incorporated the estimation metric (due to the proposed DNN-Buddies scheme) into the GA-based solver proposed by us previously [23]. Unfortunately, due to lack of space, no self-contained review of genetic algorithms and the proposed method can be included in this paper. Nevertheless, the modification required with respect to the existing GA framework is rather simple; if a DNN-buddy pair appears in one of the parents, assign this pair in the child. Figure 3 describes the modified crossover operator in the GA framework according to the above (see Step 2, which includes the new DNN-buddy phase).Until (n − 1) relative relations are assigned do 1. Try assigning all common relative relations in the parents.2. Try assigning all DNN-buddy relative relations in the parents. 3. Try assigning all best-buddy relative relations in the parents. 4. Try assigning all existing most-compatible relative relations. 5. Try assigning random relative relations.We ran the augmented solver on the 432-piece puzzle set and on the two ad- ditional datasets proposed by Pomeranz et al. [20] of 540-and 805-piece puzzles. We evaluated our results according to the neighbor comparison which measures the fraction of correct neighbors and the number of puzzles perfectly reconstructed for each set. Table 1 presents the accuracy results of the same solver with and without the DNN-Buddies metric. For each dataset we achieve a considerable improvement in the overall accuracy of the solution, as well as the number of perfectly reconstructed puzzles. Moreover, our enhanced deep neural network-based scheme appears to out- perform the current state-of-the-art results, as it yields accuracy levels of 95.65%, 96.37% and 95.86%, which surpass, respectively, the best results known of 95.4% [19], 94.08% and 94.12% [23].  In this paper we presented the first neural network-based estimation metric for the jigsaw puzzle problem. Unlike previous methods, no manual feature crafting was employed. The novel method exhibits high precision and when combined with a real-world puzzle solver, it significantly improves the solution's accuracy to set a new state-of-the art standard.
