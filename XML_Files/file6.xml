<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\grobid-0.5.1\grobid-0.5.1\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-11-08T08:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Sentences by Editing Prototypes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-09-26">26 Sep 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Oren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Sentences by Editing Prototypes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-09-26">26 Sep 2017</date>
						</imprint>
					</monogr>
					<note>(* equal contribution)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Overpriced , overrated , and tasteless food. The food here is ok but not worth the price. I definitely recommend this restaurante. We propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional models that generate from scratch either left-to-right or by first sampling a latent sentence vector , our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures inter-pretable semantics such as sentence similarity and sentence-level analogies. Sample from the training set Edit Vector Prototype The food here is ok but not worth the price. Edit using a�en�on Generation Generation The food is mediocre and not worth the ridiculous price. The food is good but not worth the horrible customer service. The food here is not worth the drama. The food is not worth the price. Figure 1: The prototype-then-edit model generates a sentence by sampling a random example from the training set and then editing it using a randomly sampled edit vector.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to generate sentences is a core com- ponent of many NLP systems, including ma- chine translation <ref type="bibr" target="#b11">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b13">Koehn et al., 2007)</ref>, summarization <ref type="bibr" target="#b6">(Hahn and Mani, 2000;</ref><ref type="bibr" target="#b17">Nallapati et al., 2016)</ref>, speech recognition (Ju- rafsky and <ref type="bibr" target="#b10">Martin, 2000</ref>), and dialogue ( <ref type="bibr" target="#b19">Ritter et al., 2011</ref>). State-of-the-art models are largely based on recurrent neural language models (NLMs) that gen- erate sentences from scratch, often in a left-to-right manner. It is often observed that such NLMs suf- fer from the problem of favoring generic utterances such as "I don't know" ( . At the same time, naive strategies to increase diversity have been shown to compromise grammaticality ( <ref type="bibr" target="#b20">Shao et al., 2017)</ref>, suggesting that current NLMs may lack the inductive bias to faithfully represent the full diver- sity of complex utterances.</p><p>Indeed, it is difficult even for humans to write complex text from scratch in a single pass; we of- ten create an initial draft and incrementally revise it <ref type="bibr" target="#b8">(Hayes and Flower, 1986)</ref>. Inspired by this process, we propose a new generative model of text which we call the prototype-then-edit model, illustrated in Fig- ure 1. It first samples a random prototype sentence from the training corpus, and then invokes a neural editor, which draws a random "edit vector" and gen- erates a new sentence by attending to the prototype while conditioning on the edit vector. The motiva- tion is that sentences from the corpus provide a high quality starting point: they are grammatical, natu- rally diverse, and exhibit no bias towards shortness or vagueness. The attention mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) then extracts the rich information from the prototype and generalizes to novel sentences.</p><p>We train the neural editor by maximizing an approximation to the generative model's log- likelihood. This objective is a sum over lexically- similar sentence pairs in the training set, which we can scalably approximate using locality sensitive hashing. We also show empirically that most lexi- cally similar sentences are also semantically similar, thereby endowing the neural editor with additional semantic structure. For example, we can use the neural editor to perform a random walk from a seed sentence to traverse semantic space. We compare our prototype-then-edit model against approaches which generate from scratch on two fronts: language generation quality and seman- tic properties. For the former, our model generates higher quality generations according to human eval- uations, and improves perplexity by 13 points on the Yelp corpus and 7 points on the One Billion Word Benchmark. For the latter, we show that latent edit vectors outperform standard sentence variational au- toencoders ( <ref type="bibr" target="#b3">Bowman et al., 2016</ref>) on semantic sim- ilarity, locally-controlled text generation, and a sen- tence analogy task.</p><p>where both prototype x and edit vector z are latent variables.</p><p>Our formulation stems from the observation that many sentences in a large corpus can be represented as minor transformations of other sentences. For ex- ample, in the Yelp restaurant review corpus <ref type="bibr">(Yelp, 2017)</ref> we find that 70% of the test set is within word- token Jaccard distance 0.5 of a training set sentence, even though almost no sentences are repeated ver- batim. This implies that a neural editor which mod- els lexically similar sentences should be an effective generative model for large parts of the test set.</p><p>A secondary goal for the neural editor is to cap- ture certain semantic properties; we focus on the fol- lowing two in particular:</p><p>1. Semantic smoothness: an edit should be able to alter the semantics of a sentence by a small and well-controlled amount, while multiple edits should make it possible to accumulate a larger change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem statement</head><p>Our primary goal is to learn a generative model of sentences. In particular, we model sentence genera- tion as a prototype-then-edit process: 1 2. Consistent edit behavior: the edit vector z should model/control the variation in the type of edit that is performed. When we apply the same edit vector on different sentences, the neural editor should perform semantically analogous edits across the sentences.</p><p>1. Prototype selector: Given a training corpus of sentences X , we randomly sample a prototype sentence, x ∼ p(x ) (in our case, uniform over X ).</p><p>In Section 4, we show that the neural editor can successfully capture both properties, as reported by human evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>2. Neural editor: First we draw an edit vector z, from an edit distribution p(z), which encodes the type of edit. Then, we draw the final sen- tence x from p edit (x | x , z), which takes the prototype x and the edit vector z.</p><p>Under this model, the likelihood of a sentence is:</p><formula xml:id="formula_0">p(x) = p(x | x )p(x )<label>(1)</label></formula><p>x ∈X</p><p>We would like to train our neural editor by maxi- mizing the marginal likelihood (Equation 1), but the exact likelihood is difficult to maximize because it involves a sum over all latent prototypes x (expen- sive) and integration over the latent edit vector z (in- tractable). We therefore propose two approximations to over- come these challenges:</p><formula xml:id="formula_1">p(x | x ) = p edit (x | x , z)p(z)dz,<label>(2)</label></formula><p>z 1 For many applications such as machine translation or dia- logue generation, there is a context (e.g. foreign sentence, di- alogue history), which can be supplied to both the prototype selector and the neural editor. This paper focuses on the uncon- ditional case.</p><p>1. We approximate the sum over all prototypes x by only summing over x that are lexically sim- ilar to x.</p><p>2. We lower bound the integral over latent edit vectors by modeling z with a variational au- toencoder, which admits tractable inference via the evidence lower bound (ELBO), which in- cidentally also provides additional semantic structure.</p><p>We describe and motivate both of these approxima- tions in the subsections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approximate sum on prototypes, x</head><p>Equation 1 defines the probability of generating a sentence x as the total probability of reaching x via edits from every prototype x ∈ X . However, most prototypes are unrelated and should have very small probability of transforming into x. Therefore, we approximate the marginal distribution over pro- totypes by only considering the prototypes x with high lexical overlap with x, as measured by word token Jaccard distance d J . Formally, define a lexi- cal similarity neighborhood as N (x) = {x ∈ X :</p><formula xml:id="formula_2">d J (x, x ) &lt; 0.5}.</formula><p>The neighborhoods N (x) can be constructed efficiently with locality sensitive hash- ing (LSH) and minhashing. The full procedure is described in Appendix 6.1. Then the log-likelihood of the prototype- then-edit process is lower bounded by: sentences from a corpus grounded in real world events, most lexically similar sentences are also se- mantically similar. For example, given "my son en- joyed the delicious pizza", we are far more likely to see "my son enjoyed the delicious macaroni", versus "my son hated the delicious pizza".</p><p>Human evaluations of 250 edit pairs sampled from lexical similarity neighborhoods on the Yelp corpus support this conclusion. 35.2% of the sen- tence pairs were judged to be exact paraphrases, while 84% of the pairs were judged to be at least roughly equivalent. Sentence pairs were negated or change in topic only 7.2% of the time. Thus, a neu- ral editor trained on this distribution should prefer- entially generate semantically similar edits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approximate integration on edit vectors, z</head><p>Now let us tackle integration over the latent edit vec- tors. To do this, let us introduce the evidence lower bound (ELBO) to the integral over z in Equation 1:</p><formula xml:id="formula_3">log p(x | x ) = log p edit (x | x , z)p(z)dz z ≥ x ) = E z∼q(z|x,x ) [log p edit (x | x , z)]   − D KL (q(z | x, x ) (5) log p(x) ≥ log  p(x )p(x | x )  x ∈N (x)   ≥ log  |N (x)| −1 p(x|x )  − log |X | x ∈N (x) ≥ |N (x)| −1 log p(x|x ) − log |X |<label>(3)</label></formula><p>The important elements of x ) are the neural editor p edit (x | x , z), the edit prior p(z), and the approximate edit posterior q(z | x, x ) (we describe each of these shortly). Combining the ELBO with Equation 3, our final objective function is:</p><formula xml:id="formula_4">x (x)</formula><p>where the inequalities follow from summing over fewer terms, multiplying by a number that is at most 1, and Jensen's inequality. Treating |N (x)| −1 as a constant 2 and summing over the training set, we arrive at the following objective:</p><formula xml:id="formula_5">L Lex ≥ L ELBO = x, x (6) x∈X x ∈N (x) L Lex = log p(x | x ). (4) x∈X x ∈N (x)</formula><p>Interlude: lexical similarity semantics. We have motivated lexical similarity neighborhoods via com- putational considerations, but we find that lexical similarity training also captures semantic similar- ity. One can certainly construct sentences with small lexical distance that differ semantically (e.g., inser- tion of the word "not"). However, since we mine L ELBO is now maximized over the parameters of both p edit and q. Note that q is only used for train- ing, and discarded at test time. With the introduction of q, we are now training p(x | x ) as a conditional variational autoencoder (C-VAE) (it is conditioned on x ). We maximize the objective via SGD, approx- imating the gradient using the usual Monte Carlo es- timate for VAEs ( <ref type="bibr" target="#b12">Kingma and Welling, 2014</ref>).</p><p>on an edit vector z by concatenating z to the input of the decoder at each time step. Further details are given in Appendix 6.2. cated norm. Then,</p><formula xml:id="formula_6">q(z dir | x, x ) = vMF (z dir ; f dir , κ)</formula><p>∝ exp(κz Edit prior p(z): We sample z from the prior by drawing a random magnitude z norm ∼ Unif(0, 10) and then drawing a random direction z dir ∼ vMF <ref type="formula">(0)</ref>, where vMF(0) is a uniform distribution over the unit sphere (von-Mises Fisher distribution with concen- tration = 0). The resulting z = z norm z dir .</p><formula xml:id="formula_7">dir f dir ) q z norm | x, x = Unif(z norm ; [ ˜ f norm , ˜ f norm +</formula><p>Approximate edit posterior q(z | x, x ): q is named the approximate edit posterior because the ELBO is tight when q matches the true posterior: the best possible estimate of the edit z given both the prototype x and the revision x. Our design of q treats the edit vector z as a generalization of word vectors. In the case of a single word insertion, a good edit vector would be the word vector of the in- serted word. Extending this to multi-word edits, we would like multi-word insertions to be represented as the sum of the individual word vectors.</p><p>Since q is an encoder observing both the protoype x and the revised sentence x, it can directly observe the word differences between x and x . Define I = x\x to be the set of words added to the prototype, and D = x \x to be the words deleted.</p><p>We would then like q to output a z equal to where the resulting z = z dir z norm . The result- ing distribution q has three parameters: (Φ, κ, where κ, are hyperparameters. This distribution is straightforward to use as part of a variational autoen- coder, as sampling can be easily performed using the reparameterization trick and the rejection sam- pler of <ref type="bibr" target="#b25">Wood (1994)</ref>. Furthermore, the KL term has a closed form expression independent of z.</p><formula xml:id="formula_8">D KL (vMF(κ) = κ I d/2+1 (κ) + I d/2 (κ) d 2κ I d/2 (κ) − d 2κ + d 2 log(κ/2) − log(I d/2 (κ)Γ(d/2 + 1)), f x, x = Φ (w) ⊕ Φ (w) .</formula><p>w∈I w∈D where Φ(w) is the word vector for word w and ⊕ denotes concatenation. The word embeddings Φ are parameters of q. However, q cannot deterministically output f (x, x ) -without any entropy in q, the KL term in equation 5 would be infinity and training would be infeasible. Hence, we design q to output a noisy, perturbed version of f : we perturb the norm of f by adding uniform noise, and we perturb the direction of f by adding von-Mises Fisher noise. The von- Mises Fisher distribution is a distribution over vec- tors with unit norm, with a mean µ and a precision κ such that the log-likelihood of drawing a vector decays linearly with the cosine similarity to µ.</p><p>Let f norm = and f dir = f /f norm . Further- more, define˜fdefine˜ define˜f norm = min(f norm , 10) to be the trun- where I n (κ) is the modified Bessel function of the first kind, and Γ is the gamma function.</p><p>Our design of q differs substantially from the stan- dard choice of a standard normal distribution with a given mean ( <ref type="bibr" target="#b3">Bowman et al., 2016;</ref><ref type="bibr" target="#b12">Kingma and Welling, 2014</ref>) for two reasons:</p><p>First, by construction, edit vectors are sums of word vectors and since cosine distances are tradi- tionally used to measure distances between word vectors, it would be natural to encode distances be- tween edit vectors by the cosine distance. The von- Mises Fisher distribution captures this idea, as the log likelihood of transforming f dir into z dir decays linearly with the cosine similarity.</p><p>Second, our parameterization avoids collapsing the latent code, which is a serious problem with variational autoencoders in practice ( <ref type="bibr" target="#b3">Bowman et al., 2016)</ref>. With a Gaussian latent noise variable, the KL-divergence term is instance-dependent, and thus the encoder must decide how much information to pass to the decoder. Even with training techniques such as annealing, the model often learns to ignore the encoder entirely. In our case, this does not oc- cur, since the KL divergence between a von-Mises Fisher with parameter κ and the uniform distribu- tion is independent of the mean f dir , allowing us to optimize κ separately using binary search. In prac- tice, we never observe issues with encoder collapse using standard gradient training.</p><p>We divide our experimental results into two parts. In Section 4.3, we evaluate the merits of the prototype- then-edit model as a generative modeling strategy, measuring its improvements on language modeling (perplexity) and generation quality (human evalua- tions of diversity and plausibility). In Section 4.4, we focus on the semantics learned by the model and its latent edit vector space. We demonstrate that it possesses interpretable semantics, enabling us to smoothly control the magnitude of edits, incremen- tally optimize sentences for target properties, and perform analogy-style sentence transformations.</p><p>5. SVAE: the sentence variational autoencoder of <ref type="bibr" target="#b3">Bowman et al. (2016)</ref>, sharing the same de- coder architecture as NEURALEDITOR. We compare the edit vector to differences of sen- tence vectors in the SVAE. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generative modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate perplexity on the Yelp review corpus (YELP, Yelp <ref type="formula" target="#formula_0">(2017)</ref>) and the One Billion Word Lan- guage Model Benchmark (BILLIONWORD, Chelba <ref type="formula" target="#formula_0">(2013)</ref>). For qualitative evaluations of generation quality and semantics, we focus on YELP as our pri- mary test case, as we found that human judgments of semantic similarity were much better calibrated in this focused setting. For both corpora, we used the named-entity rec- ognizer (NER) in spaCy <ref type="bibr">3</ref> to replace named entities with their NER categories. We replaced tokens out- side the top 10,000 most frequent tokens with an "out-of-vocabulary" token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approaches</head><p>Throughout our experiments, we compare the fol- lowing generative models:</p><p>1. NEURALEDITOR: the proposed approach.</p><p>2. NLM: a standard left-to-right neural language model generating from scratch. For fair com- parison, we use the exact same architecture as the decoder of NEURALEDITOR.</p><p>3. KN5: a standard 5-gram Kneser-Ney language model in <ref type="bibr">KenLM (Heafield et al., 2013</ref>).</p><p>Perplexity. We start by evaluating NEURALEDI- TOR's value as a language model, measured in terms of perplexity. For the NEURALEDITOR, we use the likelihood lower bound in Equation 3 where we sum over training set instances within Jaccard distance &lt; 0.5, and for the VAE term in NEURALEDITOR, we use the one-sample approximation to the lower bound used in <ref type="bibr" target="#b12">Kingma (2014)</ref> and <ref type="bibr" target="#b3">Bowman (2016)</ref>. Compared to NLM, our initial result is that NEU- RALEDITOR is able to drastically improve log- likelihood for a significant number of sentences in the test set ( <ref type="figure" target="#fig_0">Figure 2</ref>) when considering test sen- tences with at least one similar sentence in the train- ing set. However, it places lower log-likelihood and on sentences which are far from any prototypes, as it was not trained to make extremely large edits. Prox- imity to a prototype seems to be the chief deter- miner of NEURALEDITOR's performance. To eval- uate NEURALEDITOR's perplexity, we use smooth- ing with NLM to account for rare sentences not within our Jaccard distance threshold. <ref type="bibr">5</ref> We find NEURALEDITOR improves perplexity over NLM and KN5. <ref type="table" target="#tab_2">Table 1</ref> shows that this is the case for both YELP and the more general BILLIONWORD, which contains substantially fewer test-set sentences close to the training set. On YELP, we surpass even the best ensemble of NLM and KN5, while on BIL- LIONWORD we nearly match their performance.</p><p>Since NEURALEDITOR draws its strength from sentences in the training set, we also compared against a simpler alternative, in which we ensem- ble the NLM and MEMORIZATION (retrieval with- out edits). NEURALEDITOR performs dramatically better than this alternative.   Prototype x Revision x i had the fried whitefish taco which was decent, but i've had much better.</p><p>i had the &lt;unk&gt; and the fried carnitas tacos, it was pretty tasty, but i've had better. "hash browns" are unsea- soned, frozen potato shreds burnt to a crisp on the outside and mushy on the inside. the hash browns were crispy on the outside, but still the taste was missing.</p><p>i'm currently giving &lt;car- dinal&gt; stars for the service alone. Human evaluation. We now turn to human eval- uation of generation quality, focusing on grammat- icality and plausibility. <ref type="bibr">6</ref> We evaluate generations from NEURALEDITOR against a NLM with a tem- perature parameter on the per-token softmax <ref type="bibr">7</ref> , which is a popular technique for suppressing incoherent and ungrammatical sentences. Many NLM systems have noted a undesireable tradeoff between gram- maticality and diversity, where a temperature low enough to enforce grammaticality results in short and generic utterances ( . <ref type="figure" target="#fig_1">Figure 3</ref> illustrates that both the grammaticality and plausibility of NEURALEDITOR with a temper- ature of 1.0 is on par with the best tuned temperature for NLM, with a far higher diversity, as measured by unigram entropy. We also find that decreasing the temperature of the NEURALEDITOR can be used to slightly improve the grammaticality, without sub- stantially reducing the diversity of the generations.</p><p>A key advantage of edit-based models thus emerges: Prototypes sampled from the training set organically inject diversity into the generation pro- cess, even if the temperature of the decoder in the NEURALEDITOR is zero. Hence, we can keep the decoder at a very low temperature to maximize grammaticality and plausibility, without sacrificing sample diversity. In contrast, a zero temperature NLM would collapse to outputting one generic sen- tence. <ref type="bibr">6</ref> Human raters were asked, "How plausible is it for this sen- tence to appear in the corpus?" on a scale of 1-3. <ref type="bibr">7</ref> If si is the softmax logit for token wi and τ is a temperature parameter, the temperature-adjusted distribution is p(wi) ∝ exp(si/τ ).</p><p>This also suggests that the temperature parameter for the NEURALEDITOR captures a more natural no- tion of diversity -higher temperature encourages more aggressive extrapolation from the training set while lower temperatures favor more conservative mimicking. This is likely to be more useful than the tradeoff for generation-from-scratch, where the temperature also affects the quality of generations. In this section, we investigate learned semantics of the NEURALEDITOR, focusing on the two desider- ata discussed in Section 2: semantic smoothness, and consistent edit behavior.</p><p>In order to establish a baseline for these proper- ties, we consider existing sentence generation tech- niques which can sample semantically similar sen- tences. We are not aware of other approaches which attempt to learn a vector space for edits, but there are many approaches which learn a vector space for sentences. Of particular relevance is the sentence variational autoencoder (SVAE) which also imposes semantic structure onto a latent vector space, but uses the latent vector to represent the entire sen- tence, rather than just an edit. To use the SVAE to "edit" a target sentence into a semantically similar sentence, we perturb its underlying latent sentence vector and then decode the result back into a sen- tence -the same method used in <ref type="bibr" target="#b3">Bowman et al. (2016)</ref>. The neural editor frequently generates para- phrases and similar sentences while avoiding unrelated and degenerate ones. In contrast, the SVAE frequently generates identical and unrelated sentences and rarely generates paraphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head><p>Semantic smoothness. A good editing system should have fine-grained control over the semantics of a sentence: i.e., each edit should only alter the se- mantics of a sentence by a small and well-controlled amount. We call this property semantic smoothness.</p><p>To study smoothness, we first generate an "edit sequence" by randomly selecting a prototype sen- tence, and then repeatedly editing via the neural edi- tor (with edits drawn from the edit prior p(z)) to pro- duce a sequence of revisions. We then ask human annotators to rate the size of the semantic changes between revisions. An example is given in <ref type="table">Table 3</ref>.</p><p>For the SVAE baseline, we generate a similar se- quence of sentences by first encoding the prototype sentence, and then decoding after the addition of a random Gaussian with variance 0.4. 8 This process is repeated to produce a sequence of sentences which we can view as the SVAE equivalent of the edit se- quence. <ref type="figure" target="#fig_2">Figure 4</ref> shows that the neural editor frequently generates paraphrases despite being trained on lex- ical similarity, and only 1% of edits are unrelated from the prototype. In contrast, the SVAE often re- peats sentences exactly, and when it makes an edit it is equally likely to generate unrelated sentences. This suggests that the neural editor produces sub- stantially smoother sentence sequences with a sur- prisingly high frequency of paraphrases.</p><p>Qualitatively <ref type="table">(Table 3)</ref>, NEURALEDITOR seems to generate long, diverse sentences which smoothly change over time, while the SVAE biases towards short sentences with several semantic jumps, pre- sumably due to the difficulty of training a suffi- ciently informative SVAE encoder. RALEDITOR have the same average human similarity judge- ment between two successive sentences. This avoids situations where the SVAE produces completely unrelated sentence due to the perturbation size.</p><p>10 545 similarity assessments pairs were collected through Amazon Mechanical Turk following <ref type="bibr" target="#b0">Agirre (2014)</ref>, with the same scale and prompt. Similarity judgements were converted to descriptions by defining Paraphrase (5), Roughly Equivalent (4-3), Same Topic (2-1), Unrelated (0). NEURALEDITOR SVAE this food was amazing one of the best i've tried, service was fast and great.</p><p>this food was amazing one of the best i've tried, service was fast and great. this is the best food and the best service i've tried in &lt;gpe&gt;.</p><p>this place is a great place to go if you want a quick bite. some of the best &lt;norp&gt; food i've had in &lt;date&gt; i've lived in &lt;gpe&gt;. the food was good, but the service was terrible. i have to say this is the best &lt;norp&gt; food i've had in &lt;gpe&gt;.</p><p>this is the best &lt;norp&gt; food in &lt;gpe&gt;. best &lt;norp&gt; food i've had since moving to &lt;gpe&gt; &lt;date&gt;.</p><p>this place is a great place to go if you want to eat. this was some of the best &lt;norp&gt; food i've had in the &lt;gpe&gt;.</p><p>this is the best &lt;norp&gt; food in &lt;gpe&gt;. i've lived in &lt;gpe&gt; for &lt;date&gt; and every time we come in this is great the food was good, the service was great. i've lived in &lt;gpe&gt; for &lt;date&gt; and have enjoyed my haircut at &lt;gpe&gt; since &lt;date&gt;. the food was good, but the service was terrible. <ref type="table">Table 3</ref>: Example random walks from NEURALEDITOR, where the top sentence is the prototype.</p><p>Figure 5: Neural editors can shorten sentences (left), include common words (center, the word 'service') and rarer words (right 'pizza') while maintaining similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NEURALEDITOR SVAE</head><p>the coffee ice cream was one of the best i've ever tried. the coffee ice cream was one of the best i've ever tried. some of the best ice cream we've ever had! the &lt;unk&gt; was very good and the food was good. just had the best ice -cream i've ever had! the food was good, but not great. some of the best pizza i've ever tasted! the food was good, but not great. that was some of the best pizza i've had in the area. the food was good, but the service was n't bad. <ref type="table">Table 4</ref>: Examples of word inclusion trajectories for 'pizza'. The NeuralEditor produces smooth chains that lead to word inclusion, but the SVAE gets stuck on generic sentences.</p><p>Smoothly controlling sentences. We now show that we can selectively choose edits sampled from the neural editor to incrementally optimize a sen- tence towards desired attributes. This task serves as a useful measure of semantic coverage: if an edit model has high coverage of sentences that are se- mantically similar to a prototype, it should be able to satisfy the target attribute while deviating mini- mally from the prototype's original meaning.</p><p>get attribute. We repeat this process for a large num- ber of prototypes.</p><p>We use almost the same procedure for the SVAE, but instead of selecting by highest likelihood, we select the sequence whose endpoint has shortest la- tent vector distance from the prototype (as this is the SVAE's metric of semantic similarity).</p><p>We focus on controlling two simple attributes: compressing a sentence to below a desired length (e.g. 7 words), and inserting a target keyword into the sentence (e.g. "service" or "pizza").</p><p>Given a prototype sentence, we try to discover a semantically similar sentence satisfying the target attribute using the following procedure: First, we generate 1000 edit sequences using the procedure described earlier. Then, we select the sequence with highest likelihood whose endpoint possesses the tar- In <ref type="figure">Figure 5</ref>, we then aggregate the sentences from the collected edit sequences, and plot their seman- tic similarity to the prototype against their success in satisfying the target attribute. Not surprisingly, as target attribute satisfaction rises, semantic simi- larity drops. However, we also see that the neural editor sacrifices less semantic similarity to achieve the same level of attribute satisfaction as the SVAE. The SVAE is reasonable on tasks involving common words (such as the word service), but fails when the model is asked to generate rarer words such as pizza. Examples from these word inclusion problems show that the SVAE often becomes stuck generating short, generic sentences <ref type="table">(Table 4)</ref>.</p><p>Consistent edit behavior: sentence analogies. In the previous results, we showed that edit models learn to generate semantically similar sentences. We now assess whether the edit vector possesses glob- ally consistent semantics. Specifically, applying the same edit vector to different sentences should result in semantically analogous edits.</p><p>Formally, suppose we have two sentences, x 1 and x 2 , which are related by some underlying semantic relation r. Given a new sentence y 1 , we would like to find a y 2 such that the same relation r holds be- tween y 1 and y 2 .</p><p>Our approach is to estimate the edit vector be- tween x 1 and x 2 asˆzasˆ asˆz = f (x 1 , x 2 ) -the mode of our edit posterior q. We then apply this edit vector to y 1 using the neural editor to yieldˆyyieldˆ yieldˆy 2 = argmax x p edit (x | y 1 , ˆ z). Since it is difficult to outputˆyoutputˆ outputˆy 2 exactly matching y 2 , we take the top k candidate outputs of p edit (us- ing beam search) and evaluate whether the gold y 2 appears among the top k elements.</p><p>Evaluation for this task can be difficult, as two ar- bitrary sentences x 1 and x 2 are often not related by any well-defined relationship. However, prior work already provide well-established sets of word analo- gies ( <ref type="bibr" target="#b15">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013b</ref>). We leverage these to generate a new dataset of sen- tence analogies, using a simple strategy: given an analogous word pair (w 1 , w 2 ), we mine the Yelp corpus for sentence pairs (x 1 , x 2 ) such that x 1 is transformed into x 2 by inserting w 1 and removing w 2 (allowing for reordering and inclusion/exclusion of stop words).</p><p>For example, given the words (good, best), we mine the sentence pair x 1 ="this was a good restau- rant" and x 2 ="this was the best restaurant". Given a new sentence y 1 ="The cake was great", we ex- pect y 2 to be "The cake was the greatest".</p><p>For this task, we initially compared against the SVAE, but it had a top-k accuracy close to zero. Hence, we instead compare to the baseline of ran- domly sampling an edit vectorˆzvectorˆ vectorˆz ∼ p(z), instead usingˆzusingˆ usingˆz derived from f (x 1 , x 2 ). We can also com- pare to numbers on the original word analogy task, restricted to the relationships we find in the Yelp cor- pus although these numbers are for the easier task of computing word-level (not sentence) analogies.</p><p>Interestingly, the top-10 performance of our model in <ref type="table" target="#tab_5">Table 5</ref> is nearly as good as the perfor- mance of GloVe vectors on the simpler lexical anal- ogy task, despite the fact that the sentence predic- tion task is harder. In some categories the neural editor at top-10 actually performs better than word vectors, since the neural editor has an understanding of which words are likely to appear in the context of a Yelp review. Examples in <ref type="table">Table 6</ref> show the model is accurate and captures lexical analogies requiring word reorderings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work and discussion</head><p>Our work connects with a broad literature on neu- ral retrieval and attention-based generation meth- ods, semantically meaningful representations for sentences, and nonparametric statistics.</p><p>Neural language models ( <ref type="bibr" target="#b2">Bengio et al., 2003</ref>) based upon recurrent neural networks and sequence- to-sequence architectures <ref type="bibr" target="#b22">(Sutskever et al., 2014</ref>) have been widely used due to their flexibility and performance across a wide-range of NLP tasks <ref type="bibr" target="#b10">(Jurafsky and Martin, 2000;</ref><ref type="bibr" target="#b11">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b6">Hahn and Mani, 2000;</ref><ref type="bibr" target="#b19">Ritter et al., 2011</ref>). Our work is motivated by an emerging con- sensus that attention-based mechanisms ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> can substantially improve performance on various sequence to sequence tasks by captur- ing more information from the input sequence ( <ref type="bibr" target="#b26">Wu et al., 2016;</ref><ref type="bibr" target="#b24">Vaswani et al., 2017)</ref>. Our work ex- tends the applicability of attention mechanisms be- yond sequence to sequence tasks by deriving a train- ing method for models which attend to randomly sampled sentences.</p><p>There is a growing literature on applying re- trieval mechanisms to augment neural sequence-to- sequence models. For example, Song (2016) en- sembled a retrieval system and an NLM for dia- logue, using the NLM to transform the retrieved ut- terance, and <ref type="bibr" target="#b5">Gu (2017)</ref> used an off-the-shelf search engine system to retrieve and condition on train- ing set examples. Both of these approaches rely on a deterministic retrieval mechanism which se- lects a prototype x using some input c. In contrast, our work treats the prototype x as a latent vari-  = my son actually looks forward to going to the dentist! <ref type="table">Table 6</ref>: Examples of lexical analogies correctly answered by NEURALEDITOR. Sentence pairs generating the analogy relationship are shortened to only their lexical differences.</p><p>able, and marginalizes over all possible prototypes -a challenge which motivates our new lexical sim- ilarity training method in Section 3.1. Practically, marginalization over x makes our model attend to training examples based on similarity of output se- quences, while retrieval models attend to examples based on similarity of the input sequences.</p><p>In terms of generation techniques that capture semantics, the sentence variational autoencoder (SVAE) <ref type="bibr" target="#b3">(Bowman et al., 2016</ref>) is closest to our work in that it attempts to impose semantic structure on a latent vector space. However, the SVAE's latent vec- tor is meant to represent the entire sentence, whereas the neural editor's latent vector represents an edit. Our results from Section 4.4 suggest that local vari- ation over edits is easier to model than global varia- tion over sentences.</p><p>Our use of lexical similarity neighborhoods is comparable to context windows used in word vector training ( <ref type="bibr" target="#b15">Mikolov et al., 2013a</ref>). Proximity of words within text and lexical similarity both serve as a fil- ter which reveals semantics through distributional statistics of the corpus. More generally, results in manifold learning demonstrate that a weak metric such as lexical similarity can be used to extract semantic similarity through distributional statistics ( <ref type="bibr" target="#b23">Tenenbaum et al., 2000;</ref><ref type="bibr" target="#b7">Hashimoto et al., 2016)</ref>.</p><p>Prototype-then-edit is a semi-parametric ap- proach that remembers the entire training set and uses a neural editor to generalize meaningfully be- yond the training set. The training set provides a strong inductive bias -that the corpus can be char- acterized by prototypes surrounded by semantically similar sentences reachable by edits. Beyond im- provements on generation quality as measured by perplexity, the approach also reveals new semantic structures via the edit vector. A natural next step is to apply these ideas in the conditional setting for tasks such as dialogue generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility. Our code is made available at</head><p>https://github.com/kelvinguu/neural-editor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducible experiments are available at</head><p>https://worksheets.codalab.org/worksheets/ 0xa915ba2f8b664ddf8537c83bde80cc8c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Construction of the LSH</head><p>The LSH maps each sentence to other lexically sim- ilar sentences in the corpus, representing a graph over sentences. To speed up corpus construction, we apply breadth-first search (BFS) over the LSH sen- tence graph started at randomly selected seed sen- tences. We store the edges encountered during the BFS, and uniformly sample from this set to form the training set. The BFS ensures that every query to the LSH index returns a valid edit pair, at the cost of adding bias to our training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Neural editor architecture</head><p>Encoder. The prototype encoder is a 3-layer biL- STM. The inputs to each layer are the concatena- tion of the forward and backward hidden states of the previous layer, with the exception of the first layer, which takes word vectors as input. Word vec- tors were initialized with GloVe ( <ref type="bibr" target="#b18">Pennington et al., 2014</ref>).</p><p>Decoder. The decoder is a 3-layer LSTM with at- tention. At each time step, the hidden state of the top layer is used to compute attention over the top- layer hidden states of the prototype encoder. The re- sulting attention context vector is then concatenated with the decoder's top-layer hidden state and used to compute a softmax distribution over output tokens.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The NEURALEDITOR outperforms the NLM on examples similar to those in the training set (left panel, point size indicates number of training set examples with Jaccard distance &lt; 0.5). The N-gram baseline (right) shows no such behavior, with the NLM outperforming KN5 on most examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NEURALEDITOR provides plausibility and grammaticality on par with the best, temperature-tuned language model without any loss of diversity as a function of temperature. Results are based on 400 human evaluations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The neural editor frequently generates paraphrases and similar sentences while avoiding unrelated and degenerate ones. In contrast, the SVAE frequently generates identical and unrelated sentences and rarely generates paraphrases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 also</head><label>2</label><figDesc></figDesc><table>quali-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Perplexity of the NEURALEDITOR with the two VAE parameters κ outperform all methods on YELP and all non-ensemble methods on BILLIONWORD. i'm not sure what is prevent- ing me from giving it &lt;car- dinal&gt; stars, but i probably should. quick place to grab light and tasty teriyaki. NEURALEDITOR are substantially different from the original prototypes. this place is good and a quick place to grab a tasty sand- wich. sad part is we've been there before and its been good. i've been here several times and always have a good time.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Edited generations are substantially different from the sampled prototypes.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Edit vectors capture one-word sentence analogies with performance close to lexical analogies. 

Example 1 
Example 2 
Context i've had better service at &lt;org &gt;. my daughter actually looks forward to going to the dentist! 
Edit 
+ worst -worse 
+ his -her 
Result = best service i've had at &lt;org &gt;. 
</table></figure>

			<note place="foot" n="2"> In preliminary experiments, we found this to yield better results. Neural editor p edit (x | x , z): We implement our neural editor as a left-to-right sequence-to-sequence model with attention, where the prototype is the input sequence and the revised sentence is the output sequence. We employ an encoder-decoder architecture similar to Wu (2016), extending it to condition</note>

			<note place="foot" n="4"> We followed the procedure of Bowman (2016) with the modification that the KL penalty weight is annealed to 0.9 instead of 1.0 to avoid encoder degeneracy. 5 We smooth with NLM, as this corresponds to the model which does not condition on a prototype at all. Smoothing was done with linear interpolation, and weights were selected via a held-out validation set. 3 honnibal.github.io/spaCy</note>

			<note place="foot" n="8"> The variance was selected so that the SVAE and NEU</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Search engine guided non-parametric neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07267</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The challenges of automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Word embeddings as metric recovery in semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="273" to="286" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Writing research and the writer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Flower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1106" to="1113" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable modified kneser-ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Prentice Hall Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Auto-encoding variational Bayes. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">hlt-Naacl</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequenceto-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating high-quality and informative conversation responses with sequence-tosequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Two are better than one: An ensemble of retrievaland generation-based dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07149</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Simulation of the von mises fisher distribution. Communications in statistics-simulation and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<ptr target="https://www.yelp.com/dataset_challenge" />
	</analytic>
	<monogr>
		<title level="j">Yelp dataset challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
