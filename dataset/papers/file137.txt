Therefore, the selection of the optimal parameters is of critical importance to obtain a good 12 performance in handling learning task with SVMs.In this study, we mainly concentrate on the parameters optimization of SVMs, which has 14 gained great attentions in the past several years. The most popular and universal method is grid 15 search, which conducts an exhaustive search on the parameters space with the validation error 16 (such as 5-fold cross validation error) minimized. Obviously, although it can be easily parallelized 17 and seems safe [2], its computational cost scales exponentially with the number of parameters and 18 the number of the sampling points for each parameter [3]. Besides, the performance of grid search 19 is sensitive to the setting of the grid range and coarseness for each parameter, which are not easy 20 to set without prior knowledge. Instead of minimizing the validation error, another streams of 2 studies focused on minimizing the approximated bounds on generalization performance by 1 numerical optimization methods [4][5][6][7][8][9][10][11][12]. The numerical optimization methods are generally more 2 efficient than grid search for their fast convergence rate. However, this kind of methods can only 3 be suitable for the cases that error bounds are differentiable and continuous with respect to the 4 parameters in SVMs. It is also worth noting that the numerical optimization methods, such as 5 gradient descent, may get stuck in local optima and highly depend on the starting points.Furthermore, experimental evidence showed that several established bounds methods could not 7 compete with traditional 5-fold cross-validation method [6], which indicates inevitably gap 8 between the approximation bounds and real error [13]. Recently, evolutionary algorithms such as 9Genetic algorithm (GA), particle swarm optimization (PSO), ant colony optimization (ACO) and 10 simulated annealing algorithm (SA) have been employed to optimize the SVMs parameters for 11 their better global search abilities against numerical optimization methods [14][15][16][17][18][19]. However, GAs 12 and other evolutionary algorithms (EAs) are not guaranteed to find the global optimum solution to 13 a problem, though they are generally good at finding "acceptable good" or near-optimal solutions 14 to problems. Another drawback of EAs is that they are not well suited to perform finely tuned 15 search, but on the other hand, they are good at exploring the solution space since they search from 16 a set of designs and not from a single design.Memetic Algorithms (MAs), first coined by Moscato [20,21], have been regarded as an 18 promising framework that combines the Evolutionary Algorithms (EAs) with problem-specific 19 local searcher (LS), where the latter is often referred to as a meme defined as a unit of cultural 20 21 evolution that is capable of local refinements. From an optimization point of view, MAs are hybrid 3 EAs that combine global and local search by using an EA to perform exploration while the local 1 search method performs exploitation. This has the ability to exploit the complementary advantages 2 of EAs (generality, robustness, global search efficiency), and problem-specific local search 3 (exploiting application-specific problem structure, rapid convergence toward local minima) [22].Up to date, MAs have been recognized as a powerful algorithmic paradigm for evolutionary 5 computing in a wide variety of areas [23][24][25][26]. In particular, the relative advantage of MAs over 6 EAs is quite consistent on complex search spaces.Since the parameters space of SVMs is often considered complex, it is of interest to justify Furthermore, the problem of selecting promising individuals to experience local refinement is also 16 addressed and thus a novel probabilistic selection strategy is proposed to keep a balance between 17 exploration and exploitation. The performance of proposed PSO-PS based MA for parameters 18 optimization in SVMs is justified on several benchmarks against selected established counterparts.Experimental results and comparisons demonstrate the effectiveness of the proposed PSO-PS 20 21 based MA for parameters optimization of SVMs. 4 The rest of the study is organized as follows. Section 2 presents a brief review on SVMs.    x . The elegance of using the kernel function is 9 that one can deal with feature spaces of arbitrary dimensionality without having to compute the 10 map ()  x explicitly. Any function that satisfies Mercer"s condition [27] can be used as the kernel 11 function. The typical examples of kernel function are as follows:Linear kernel: ( , ) ,Polynomial kernel:Radial basis function (RBF) kernel:Sigmoid kernel:Where,  , r and d are kernel parameters. The kernel parameter should be carefully chosen as 17 it implicitly defines the structure of the high dimensional feature space ()  x and thus controls the 18 complexity of the final solution [28]. Generally, among these kernel functions, RBF kernel is 19 20 strongly recommended and widely used for its performance and complexity [2] and thus SVMs   6 with RBF kernel function is the one studied in this study.Overall, SVMs are a powerful classifier with strong theoretical foundations and excellent 2 generalization performance. Note that before implementing the SVMs with RBF kernel, there are 3 two parameters (penalty parameter C and RBF kernel parameter  ) have to set. Previous studies 4show that these two parameters play an important role on the success of SVMs. In this study, Memetic Algorithms (MAs), first coined by Moscato [20,21], have come to light as an union superior robustness across a wide range of problem domains [29,30]. However, according to theNo Free Lunch theory, the hybridization can be more complex and expensive to implement.Considering the simplicity in implementation of particle swarm optimizations (PSO) and its 18 proven performance, being able to produce good solutions at a low computational cost [16,[31][32][33]   Likewise, the fly velocity (position change) for particle i at iteration t can be described as . In this study, the position and velocity of one particle have only two dimensions 12 that denote the two parameters (C and  ) to be optimized in SVMs. A set of particlesis called a swarm (or population).Initially, the particles in the population are randomly generated in the solution space Where id x and id v are the position value and velocity in the th is used to constraint the velocity of the particle to avoid the particle 2 flying outside the search space.After the initialization of the population, the algorithm iteratively generates offspring using 4 PSO based operator and undergoes local refinement with pattern search, which is to be discussed 5 in the following sub-sections. 8Fitness kCVMRWhere, k is the size of folds. In this study, 5-fold cross validation is conducted, which is 10 widely used and suggested in [34].In the proposed PSO-PS based MA, PSO based operators are used to explore the search 13 space. Particle Swarm Optimization (PSO) [35] is a population-based meta-heuristic that simulates 14 social behavior such as birds flocking to a promising position to achieve precise objectives (e.g.,finding food) in a multi-dimensional space by interacting among them.To search for the optimal solution, each particle adjusts their flight trajectories by using the 17 following updating equations: is considered as the global best (gbest) position. Note  problems as well as unconstrained problems has been proven in [36]. It investigates nearest 11 neighbor parameter vectors of the current point, and tries to find a better move. If all neighbor 1 points fail to produce a decrease, then the search step is reduced. This search stops until the search 2 step gets sufficiently small, which ensuring the convergence to a local minimum. The pattern 3 search is based on a pattern P k that defines the neighborhood of current points. A well often used 4 pattern is five-point unit-size rood pattern which can be represented by the generating matrix P k in 5 Eq.(9).The procedure of pattern search is outlined in Fig.2. ∆ 0 denotes the default search step of PS, 8 ∆ is a search step, p k is a column of P k ,  denotes the neighborhood of the center point. The  An important issue in MAs to be addressed is which particles from the population should be 2 selected to undergo local improvement (see "Selection of particles to be refined" procedure in 3 Fig.1). This selection directly defines the balance of evolution (exploration) and the individual 4 learning (exploitation) under limited computational budget, which is of importance to the success 5 of MAs. In previous studies, a common way is to apply a local search to all newly generated 6 offspring, which suffers from much strong local exploitation. Another way is to apply local search 7to particles with a predefined probability p l , which seems too blind and the p l is not easy to define 8 [37]. Besides, as [38] suggests that those solutions that are in proximity of a promising basin of 9 attraction received an extended budget, the randomly selected particles may needs more cost to 10 reach the optimal. Hence, it seems a good choice that only the particles with the fitness larger than 11 a threshold v are selected for local improvement [39], while the threshold is problem specific and 12 some of the selected particles may be crowed in the same promising region.To this end, a probabilistic selection strategy of selecting non-crowed individuals to undergo Through initial experiments, the parameters in PSO and MA are set as follows. The acceleration 7 coefficients c 1 and c 2 are both equals to 2. For w, a time varying inertia weight linearly decreasing 8 from 1.2 to 0.8 is employed. The default search step (∆) in pattern search is set to 1. The radius of 9 the exploitation region (r) used in the probabilistic selection strategy equals to 2. To test the performance of the proposed PSO-PS based memetic algorithms, computational 12 13 experiments are carried out against some well-studied benchmarks. In this study, thirteen 15 commonly used benchmark datasets from IDA Benchmark Repository 1 are used. These datasets, 1 having been preprocessed by [40], consist of 100 random groups (20 in the case of image and 2 splice datasets) with training and testing sets (about 60%:40%) describing the binary classification 3 problem. The dataset in each group has already been normalized to zero mean and unit standard 4 deviation. For the dataset in each group, the training set is used to select the optimal parameters in 5 SVM based on 5-fold cross validation and establish the classifier, and then the test set is used to 6 assess the classifier with the optimal parameters.   16 MA2: PS is applied to each new particle with a predefined probability p l =0.1;MA3: PS is only applied to the two best fitness particles;18 MA4: PS is applied to the selected particles by probabilistic selection strategy proposed in 12 Table 2 describes the results obtained with the above algorithms. The column of "error"shows the mean and standard deviation of the error rates (%) for 30 times. The "#eva" gives the 14 mean and standard deviation of the numbers of fitness evaluations. The "Ave" row shows the From achieve the smallest error rates with a little more fitness evaluations than that of PSO but still MA4 with the proposed probabilistic selection strategy is effective and robust for solving 1 parameters optimization in SVMs modeling.In addition, the average computational time of each method is given in Furthermore, following the same experimental settings as in [12,13,40], the second 11 experiment will directly compare the performance of the best variant as MA4 of the proposed 12 PSO-PS based MA with established algorithms in previous studies.The experiment is conducted as follows. At first, on each of the first five groups of every 14 benchmark dataset, the parameters are optimized using the proposed MA4, and then the final  Table 4 and Fig.4 18show the comparison of mean testing error rates (%) and the standard deviations (%) obtained by 19 20 the proposed PSO-PS based MA and other algorithms proposed in previous studies [12,13,40]. For   The parameters of SVMs are of great importance to the success of the SVMs. To optimize the 16 parameters properly, this study proposed a PSO and pattern search (PS) based Memetic Algorithm.In the proposed PSO-PS based MA, PSO was used to explore the search space and detect the 18 potential regions with optimum solutions, while pattern search (PS) was used to conduct an 19 effective refinement on the potential regions obtained by PSO. To balance the exploration of PSO 20 and exploitation of PS, a probabilistic selection strategy was also introduced. The performance of 21 proposed PSO-PS based MA was confirmed through experiments on several benchmark datasets.The results can be summarized as: (1)    [40] RM [13] FE [13] L1-CB [12] L2-CB [12] Proposed MA
