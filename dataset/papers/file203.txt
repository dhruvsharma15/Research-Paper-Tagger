Here we would like to highlight those aspects of the LWN neurons that make them more similar in structure and function to the biological neurons as compared with the CWN neurons. Consider an axon of a source biological neuron connecting to the dendrite of the target neuron at a synapse. One of two types of neurotransmitter chemicals (either for excitatory or inhibitory receptors) is released from the axon's side of the synapse whenever the source neuron is activated [8]. These chemicals then bind with receptors on the dendrite-side of the synapse, resulting in an increase (in case of an excitatory receptor) or decrease (inhibitory receptor) in the electrical potential on the membrane of the target neuron. The electrical potential of that membrane is the sum of contributions due to the firings of all neurons that are connected through synapses to the target neuron. When the membrane's electrical potential reaches a threshold value, the target neuron fires. The highlight of the above narrative is the absence of multiplication operations and the presence of only two synaptic values, excitatory (similar to the +1 weight of LWN connections) and inhibitory(−1 weight) 1 . The connections-to-neurons ratio decreases with increase in the number of neurons in biological systems [9,10]. LWNs exhibit the same characteristic (Figure 2), but CWNs can not. The size of the Figure 1: Each CWN neuron is connected to all neurons in the preceding layer, whereas the LWN neurons have limited receptive fields. CWN weights have varying values (depicted by the thickness of the connections) and can be positive (solid lines) or negative (broken lines). The LWN weights have just two values, ±1.receptive field, N RF , of a biological neuron is the fan-in of that neuron. Studies of processing in the visual cortex of animals show that N RF varies among different types of neurons [11]. The LWN is similar in structure in that each LWN neuron is a specialist (as compared to the generalist neurons of the CWN) as LWN neuron specializes in a particular subset of inputs. The N RF for a conventional CWN is fixed for every neuron in every layer and is equal to the number of neurons in the preceding layer. For LWN, N RF is much smaller and varies with the number of neurons in a layer. LWN training is inspired by the synaptic pruning process in the biological brain [12]: start with plenty; prune the excess off later. This natural phenomenon prunes, for example, 25-50% synapses among humans as they approach adulthood, but does not reduce the number of neurons. For LWN, the training process prunes the initial count of the synapses (i.e. weights) by about 80-95%, which, in many cases, results in the elimination of some of the neurons as well.Although their weights are restricted to the set {0, ±1}, LWNs' thresholds have no such constraints. Their activation functions are bounded and odd, and are confined to hyperbolic tangent in this dis- cussion. With the help of these ingredients, we can create a one-dimensional bump of arbitrarily small height at an arbitrary location on the x-axis using expressions of the formwhere ρ, χ ∈ R. Summations of such bumps can be used to approximate arbitrary one-dimensional functions to any accuracy. We will now discuss the extension of this one-dimensional construction to denseness in C(R n ). We start by a theorem due to Khan [13]:Theorem 1 (Weightless Neural Network Existence Theorem) Summations of the form:: R → R is bounded and odd, ψ(.) = ασ(.), and α ∈ R\Q.  The Weightless Neural Network is, on one hand, simpler than CWN in having unit-valued input-and output-layer weights, but on the other hand more complex in having two types of hidden neurons. The two types differing only by an irrational multiplication factor, α, in their activation functions. Allowing the input-and output-layer weights to assume additional values of 0 and −1 does not harm the density result, but may result in networks that train quicker and are more compact. The choice of multiplication factor α is arbitrary, and can be chosen to be close to one. When simulated on a digital computer, that choice will become exactly one due to the limited precision of the computer [14], and the network will end up having only a single type of hidden neurons. This simplified configuration having I inputs and H hidden neurons can be written as:where a h , b hi ∈ {0, ±1}, c h ∈ R. This expression depicts a network having a solitary layer of hidden neurons. To form multi-hidden-layer networks, layers comprising neurons identical to those in the first hidden-layer were employed. Moreover, for our simulations, we used output neurons which were identical to the hidden neurons.Several approaches have been proposed for training neural networks with discrete weights. Hwang and Sung [4] take a trained CWN, discretize its weights to ternary {0, ±1} values, and retrain using backpropagation. They also restricted all signals to a depth of three bits. Mellempudi et al. [7] also start with a trained CWN and use a fine-grained ternarization method which exploits local correlations in the dynamic-range of the parameters to minimize the impact of discretization on the accuracy of the network. Li et al. [5] use the stochastic gradient descent method to train ternary-weight networks. They use ternary-weights during the forward and backward propagations, but not during the weight updates. Yin et al. [6] gradually discretize the weights to zero or powers of two by minimizing the Euclidean distance between conventional weights and their closest discrete value during backpropagation.We use the training heuristics proposed by Khan [3] augmented by a new step with the aim of minimizing the error: all examples all weights where o and t are the calculated and desired output vectors, w the value of an individual weight, and Q(·) the weight discretization function. Q(·) is differentiable and the zeros of (w − Q(w)) are {0, ±1}.The main point of the original heuristics was the sequential application of error-reduction and weight- discretization steps to the network in every training epoch. Both were based on steepest-descent, but weight-discretization was supplemented by an additional mechanism to take care of the weight-update paralysis. This paralysis was caused by the opposing weight-updates calculated by the error-reduction and weight-discretization steps. That additional mechanism -the black-hole mechanism -forced nearly- discrete weights to discrete values. The rate of weight-discretization and the radius of the black-hole grew as the error in the output of the network shrank. The black-hole mechanism worked well for shallow networks comprising hundreds of weights as discussed in [3], but failed to overcome the weight-update paralysis for deeper LWNs having thousands or millions of weights discussed in this paper. For such networks, we propose an additional mechanism that comes into play only when almost all weights are discrete. At that stage, all weights are rounded to their nearest discrete value from the set {0, ±1}. If that results in a network having acceptable test-data accuracy, training is concluded. Otherwise, the rounding step is rolled back and normal training is resumed with the pre-rounding weights. The resulting error plots for a typical LWN are shown in Figure 3.MNIST is a well-known data set of images of handwritten digits [15,16]. We used the version available through the TensorFlow machine learning library [17]. That version includes 55,000 images in the training set and 10,000 in the test set. All images have been normalized, centered and transformed to a 28 × 28 matrix of 32-bit floating-point numbers ranging from zero to one according to the gray-scale value of the associated pixel. The 28 × 28 matrices have been flattened to 784-element vectors. Image labels are 10-element vectors with only a single element in a vector equal to one, and the rest having a value of zero. We used this data set as is and did not try to help the network configuration or the training process with any information about the original, 2-D nature of the image vector. This is  because our interest is not to get the best possible result but to look at what the LWN learns with the simplest possible configuration. In this simplest configuration, all neurons in all layers are exactly the same and all layers are fully-connected to layer preceding them. The only factors that we varied were the number of hidden-layers and the number of neurons in each of those layers. The summary of our results for two-hidden-layer LWNs is shown in Table 1. For each configuration, the results for the top-two LWNs with the best test-data accuracies are shown. The test-data accuracies range from 93.8 to 97.0%. If we want to balance accuracy with size, we will choose the 784:256:128:10 LWN. It achieves an accuracy of 96.7% with 0.23 million weights which compares favorably with 97.0% for a much larger LWN having 1,152 extra neurons and 1.1 million additional weights. This result (Table 2) is similar to 96.95% reported for a 784:300:100:10 CWN and 97.05% reported for a 784:500:150:10 CWN [16]. We do not have much to say about the epochs required to achieve reasonable accuracy as that number is very much dependent on the training parameters. We did not make any effort to optimize the parameters for that purpose. Figure 3 shows the mean-squared error on training data, the ratio of misclassifications to the total number of test data examples, and the ratio of non-discrete weights to the total number of weights as a function of training epochs. That figure clearly reflects the very deliberate slowness of the weight-discretization processes as compared with the error minimization process. Our intention was to find the continuous-weight error minimum first and then look for the discrete-weight minimum in its immediate vicinity. The lack of smoothness in the training error and test misclassification curves is due to the weight-discretization corrections. The smoothness of the non-discrete weight curve indicates the slowness of discretization.The sparsity, N 0 , of the two-hidden-layer LWNs ranged from 79.5 to 96%, with the larger networks tending to have sparser weight matrices. The relationship of the dimensions of the hidden layers, H 1 and H 2 , and the number of non-zero weights, N ±1 , is shown in Figure 4 on a log-log-log scale. N ±1 seems to be mainly dependent on H 1 .The all-zero-rows column in Table 1 indicates that the LWN ignores some of the inputs when the number of neurons in the first hidden layer is inadequate. For the MNIST data set, the minimum layer size seems to be 256. This number is consistent with a minimum of 300 that was reported by LeCun et al. [18]. For the 784:32:16:10 LWNs, the ignored pixels were in the top five and bottom three rows, and four left-most and three right-most columns. Whereas for the larger 784:128:64:10 LWNs the ignored  pixels were in the top five and bottom row, and three left-most and two right-most columns. This indicates that the normalized MNIST images are slightly off-center and the LWNs are ignoring the nearly-white pixels around the digits due to their low information value.Due to its fully-connected structure, the weights-to-neurons ratio for CWN increases with the number of neurons. The LWN restricts this rise in complexity by limiting each neuron to a narrow receptive field. Figure 2 shows the weights-to-neurons ratio as a function of the number of neurons in the network.Variations in the average size of the receptive field of neurons in the first hidden-layer, N, as a function of the size of the hidden layer is shown in Figure 5. Here, we excluded the smaller networks, those with 30-32 neurons in the hidden-layer, from the linear-regression model. The LWN reduces the burden of processing on each neuron by reducing their N RF as more and more of them are added to the first hidden-layer.We emphasize here that the LWN has no way of knowing that the MNIST data set comprises 2-D images. Neither does the LWN architecture include any custom-designed, small-receptive-field convolution layers of the Convolution Neural Network (CNN) à la LeCun et al. [16]. It, however, tends to drop non-crucial inputs from the receptive-field of neurons by setting the corresponding weight to zero. This behavior is true for almost all neurons in all layers of the network. In contrast with the CNN, the size of the receptive field varies from neuron to neuron in an LWN and is not a parameter of the training process, but is arrived at naturally as a consequence of the training process. Receptive- fields are clearly present in biological systems for the processing of visual, aural and touch and possibly other stimuli. The design of CNN, for example, is inspired by those systems. There may be other data- processing situations where the receptive-fields may not be that obvious. The LWN can automatically discover and leverage them even in those situations.Some of the hidden neurons had all their weights set to zero as a consequence of the training regimen. The number of such dropped neurons, indicated by the number of all-zero rows in the weight matrices, is related to the mismatch between the number of neurons in consecutive layers. Table 1  Tables 1 also shows that, in 70% of the cases, a hidden-layer neuron is connected to at most two output neurons. This indicates the efficient distribution of the image recognition task among the hidden neurons. That is, the task is distributed in such a way that each neuron has a distinct and focused responsibility. Here the size of the hidden-layer does not seem to be much of a factor. In the most extreme case, there are 446 dropped neurons in the second hidden layer. Of the remaining 66, 61 are connected only to single output neuron, while the other five are connected to two of the output neurons. The N average RF for the output neurons was 7.1.Each LWN weight requires at most two binary-bits for storage. However, using variable-length coding (e. g. Huffman coding [19]), a weight matrix that is 90% sparse will require only 1.1 bits/weight. This   number approaches one as sparsity increases. Table 3 shows a comparison of storage requirements for various weight depths, but excludes the effect of thresholds on storage requirements. 32-and 8-bit thresholds for the 784:1024:512:10 configuration consume 48 and 12 kB, respectively.We have yet to try LWN training with 8-bit thresholds, but we expect it to work due to the results reported by Vanhoucke et al. [20].LWNs posses an efficient forward pass because multiplications in all neurons are trivial in nature as weights are restricted to the set {0, ±1}. Moreover, due to the high sparsity of weight matrices, even that trivial operation is not necessary most of the time. The thresholds are 32-bit floating-point numbers, but they are never part of a multiplication operation. Therefore, the LWN does not require the floating-point multiplication operation at all.We trained several LWNs with 4-16 hidden-layers of varying sizes to see if our training heuristics suffered from the vanishing (and/or exploding) gradient problem [21,22]. Results are shown in Table 4. We did these training runs using the same training parameter values (e.g. learning rate, momentum, discretization rates) as those of the 2-hidden-layer LWNs. Our focus was not on obtaining the best performing LWNs, but to see if we were able to train theses deeper LWNs at all. Surprisingly, these  training experiments did not present symptoms of the vanishing or exploding gradient. This is probably due to the restriction on the magnitude of the weights.After extensive simulations on the MNIST data, we looked at credit card fraud [23] and default [24] data to further validate the viability of the LWN. A key characteristic of these data sets is their class imbalance. This issue can be addressed by undersampling the majority class or oversampling the minority class. We tried oversampling in two different ways: simple repetition of the minority class examples and Synthetic Minority Oversampling Technique (SMOTE) [25]. For these simulations, we normalized the continuous features to zero-mean and unit-variance and clipped them to the range [−1, 1]. All binary features were mapped to {−1, 1}. We then split the data into training (70%) and testing (30%) sets using stratified sampling. We assigned separate outputs to each of the two classes. The higher of the two outputs was considered the winning class while testing. This methodology avoids the work required to find the most suitable output threshold (in case of a single output neuron) at which the classification switches from one class to the other. The credit card fraud data set contained 29 features and consisted of 284,807 transactions, out of which 0.17% were fraudulent. Our best performing network had an F1 score [26] of 0.83 with oversampled training data. Pozzolo et al. have reported G-mean scores of 0.944, 0.770, and 0.794 on this data set using Logit Boost, Random Forests, and Support Vector Machine, respectively [23]. It is difficult to compare these results with our G-mean score of 0.900 due to, among other things, differences in data pre-processing, training/testing data splits, and classification thresholds.The credit card defaults data set contained 23 features and consisted of 30,000 cases, out of which 22% were defaults. We replaced categorical features with separate features for each category. The three sampling techniques, in this case, resulted in very similar F1 scores (Table 6) due to the relatively lower imbalance among the two classes. Yeh et al. have reported 83% accuracy using the neural network classifier on this data set. It is not possible to compare this result with our result of 68.5% due to lack of adequate information about their training and testing processes. LWNs do not have weights in the conventional sense, just excitatory and inhibitory connections of unit strength. They can approximate any continuous function with any accuracy. They require mod- est storage and have a multiplication-free forward pass, rendering them suitable for deployment on inexpensive hardware. Their sparse weight matrices loosen the coupling among the layers, making the LWN more tolerant to failure of individual neurons. In a CWN, the learned information is distributed over all weights. In an LWN, the picture is less fuzzy and the localized nature of computation is much more obvious due to the presence of a large number of zero-valued weights. For image processing, a CWN does not scale well with increases in the resolution of the images due to CWN's fully-connected structure. An LWN, on the other hand, scales much better due to the sparsity of its weight matrices. The small magnitude of LWN weights should result in smooth mappings [27] and the small number of non-zero weights should result in low generalization error [28]. The LWN learning process automatically drops insignificant inputs, unnecessary weights, and un- needed hidden-neurons. That process is relatively complex and slow, but results in networks that are almost as accurate as the CWNs but have much lower information complexity. It is conjectured that those accuracies can be matched by using bigger networks. This can be understood by considering the limited number of angles at which an LWN neuron can draw classification boundaries as opposed to the CWN neuron that can draw those boundaries at an arbitrary angle. A superposition of two or more LWN neurons can, however, approximate those arbitrary boundaries, but the added complexity may not justify the minuscule improvement in approximation accuracy.At this time, the LWN neurons have unrestricted thresholds. We are exploring if the magnitude of these thresholds can be restricted to an arbitrary value [29], e.g. 1, which may lead to more efficient storage. In its current form, the LWN training process applies the same training operation to every weight irrespective of the value of the weight. We are trying to see if the training process can be made more efficient by sometimes ignoring those weights that have not deviated from a zero value for several epochs.
