A standard architecture of artificial neural network (ANN) is comprised of several layers where signal transformation flows from input to output, that is, in one direction. In the literature, this is often known as a feed-forward neural network [1]. Each layer of an ANN is comprised of a linear transform (LT) of an input vector, followed by a non-linear transform (NLT) to generate an output vector. The output vector of a layer is then used as an input vector to the next layer. A linear transform is represented by a weight matrix. A non-linear transform of an input vector is typically realized by a scalar- wise non-linear transform, known as an activation function. Note a standard ANN architecture in Figure 1. There exists a vast literature on ANN design and its functional approximation capabilities [2]- [4]. The contemporary research community is highly active in this area with a resurgence based on deep learning structures [5]- [8], extreme learning machines [9]- [11], confluence of deep learning and extreme learning [12], recurrent neural networks [13], residual networks [14], etc. There are many aspects in designing a structure of ANN for a practical application. For example, many layers are used in deep learning structures, such as in deep neural networks (DNN). It is often argued that the use of many layers provides varying abstraction of the input data and helps to generate informative feature vectors. Many techniques for parameter optimization in deep neural networks, in particular its weightThe structural architecture of an ANN, in particular the size of the network, matters in practice [25]. Researchers continue to investigate how many layers are needed, and how wide the network should be to achieve a reasonable performance. At this background, we find a lack of systematic design regarding how to choose the size of an ANN. In practice, the choice of size is ad-hoc, and in many cases, a choice is experimentally driven with high manual intervention and pain-staking tuning of parameters. For a given learning problem, examples of pertinent questions can be as follows.• How to choose number of layers in a network?• How to choose number of nodes in each and every layer?• How to guarantee that increase in size results in better (non-increasing) optimized cost for training data? • How to design with appropriate regularization of network parameters to avoid over-fitting to training data? That means, how to expect a good generalization in the sense of high quality test data performance? • Can we use random weight matrices to keep the number of parameters to learn in balance? • Can we reduce effort in tuning of parameters at the time of training? Alternatively, can we reduce influence of manual tuning on learning performance? • For a deep network, how do we show that approximation error in modeling an appropriate target function decreases with addition of each new layer? A deep network is comprised of many layers, where each layer has a finite number of nodes (each layer is not very wide). In this article, we design a training algorithm in a systematic manner to address these questions. A structure of ANN is decided in our systematic training by progressively adding nodes and layers with appropriate regularization constraints. We name the ANN as progressive learning network (PLN). It is well known that a joint optimization of ANN parameters is a non-convex problem. In our progressive learning approach, we use a layer-wise design principle. A new layer is added on an existing optimized network and each new layer is learned and optimized at a time with appropriate norm-based regulariza- tion. For learning of each layer, we have a convex optimization problem to solve. The training of PLN as a whole is greedy in nature and in general sub-optimal. Examples of existing greedy and/or layer-wise learning approaches can be found in [26]- [29]. Then, examples of norm-based regularization and other approaches such as softweights, dropout, can be found in [30]- [32]. using cross validation. To engineer a PLN, we use non-linear activation functions that hold progression property (PP). We define the PP such that an input vector passes through a non- linear transformation and the output vector is exactly equal to the input vector. This property will be formally stated later. We show that the rectifier linear unit (ReLU) function and some of its derivatives hold PP. Using PP, we design PLN in progression where nodes and layers are added sequentially, with regularization. This leads to a systematic design of a large neural network. We start with a single-layer network and then build a multi-layer network. In a PLN, we use the output of an optimal linear system as a partial input to the network at the very beginning. For a layer, the corresponding weight matrix is comprised of two parts: one part is optimized and the other part is a random matrix instance. The relevant optimization problem for each layer is convex. Due to the convexity, starting with a single-layer network, we show that addition of a new layer will lead to a reduced cost under certain technical conditions. That means, a two-layer network is better than a single-layer network, and further for multiple layers.In training phase, saturation trend in cost reduction helps to choose the number of nodes in each layer and the number of layers in a network.In a supervised learning problem, let (x, t) be a pair-wise form of data vector x that we observe and target vector t that we wish to infer. Let x ∈ R P and t ∈ R Q . A function f () acts as an inference functioñ t = f (x, θ) where θ are parameters of the function. In the training phase, we typically learn optimal parameters by minimizing a cost, for example usingis a non-linear function that takes a scalar argument and provides a scalar output. In the neural network literature, this function is often called activation function. For an input vector γ ∈ R N , the non-linear function g : R N → R N is a stack of g(.) functions such that each scalar component of input vector γ is treated independently, that is a scalar- wise use of g(.) function. Commonly used non-linear g(.) functions are step function, sigmoid, logistic regression, tan- hyperbolic, rectifier linear unit function, etc. Being non-linear, we expect that the g(.) function should followDefinition 1 (Progression Property). A non-linear g(.) func- tion holds the progression property (PP) if there are two known linear transformations V ∈ R M×N and U ∈ R N ×M such thatwhere p denotes p'th norm and E denotes the expectation operator. Then, in the testing phase, we use the optimal parameters learned from the training phase as de-facto. The expectation operation in the cost function is realized in practice as a sample average using training data. Denoting the j'th pair-wise data-and-target by (x (j) , t (j) ) and considering that there are a total J training instances, we use the cost asWe find that some g(.) functions, such as ReLU and leaky ReLU, hold the PP under certain technical conditions. The definition of ReLU function [33] isIf M = 2N , andthen ReLU holds PP. Here I N denotes identity matrix of size N . We use explicit notation V N to denote its dependency on dimension N . Next, the definition of leaky ReLU (LReLU) function [34] isθ where the constraintq ≤ ǫ acts as a regularization to avoid over-fitting to training data. Usually p and q are chosen as 2 (ℓ 2 norm). The alternative choice p and/or q be 1 enforces sparsity. Often a choice of ǫ is experimentally driven, for example by where 0 &lt; a &lt; 1 is a fixed scalar, and typically small. If and U U N = M = 2N , and where a, b &gt; 0 are fixed scalars, with a relation a &lt; b. If q ≤ ǫ. For p = 2 and q = 2, we can rewrite the optimization problem as a regularized least- squares problem (also known as Tikonov regularization) in unconstrained form as follows In pursuit of answering the questions raised in section I, we design a PLN that grows in size with appropriate constraints. Inclusion of a new node in a layer or inclusion of a new layer always results in non-increasing cost for training data. We provide appropriate regularization to parameters so that the PLN has a good generalization quality. Weight parameters in a large network is engineered by a mix of random weights and deterministic weights. For clarity, we develop and describe a single-layer PLN first, then a two-layer PLN and finally a multi-layer PLN.Here λ ls is an appropriate regularization parameter that de- pends on ǫ. The choice of λ ls requires a delicate tuning. Now, for the single layer PLN, we have the following relationNote that, once we set W 1 , we can generate z 1 = W 1 x and hence y 1 = g(z 1 ). Using training data, we then find an optimal output matrix for the single layer PLN and the corresponding optimal cost as followswhere α ≥ 1 is a chosen constant and Q q q = 2Q for q = 1, 2. We use y (j) 1Let us assume that a single layer PLN has n 1 ≥ 2Q nodes in its layer where a non-linear transformation takes place. The single layer PLN comprises of a weight matrix W 1 ∈ R n1×P , PP holding non-linear transformation g : R n1 → R n1 , and output matrix O 1 ∈ R Q×n1 . The signal transformation relations are:The parameters of the single layer PLN that we need to learn are W 1 and O 1 . We construct W 1 matrix by a combination of deterministic and random matrices, as follows notation to denote the corresponding y 1 for the input x (j) . The optimization problem of (9) is convex. We denote the output of optimized single-layer PLN by˜tby˜by˜t 1 = O ⋆ 1 y 1 . The architecture of single-layer PLN is shown in Figure 2. In the layer, nodes from (2Q + 1) to n 1 are random nodes. Here the term 'random nodes' means that the input to these nodes is generated via a linear transform where the linear transform is a random matrix instance. Note that we did not optimize W 1 and O 1 jointly, and hence the overall single-layer PLN is sub-optimal. and hence generate y 2 = g(z 2 ). Therefore we find the optimal output matrix for two-layer PLN and the optimal cost as followsWe use y P n 1 2 notation to denote the corresponding y 2 for the input x (j) . We denote the output of optimized two-layer PLN by˜tby˜ denote the optimized cost for the PLN with n 1 nodes by C ⋆ 1 (n 1 ). Then, by construction of the optimization problem of (9), we have C . The parameters to learn are W 2 and O 2 . We set the W 2 by a combination of deterministic and random matrices, asWe have described that a two-layer PLN is built on a single- layer PLN by adding a new layer. In this way, we can build a multi-layer PLN of l layers. We start with an optimized (l − 1) layer PLN and then, add the l'th layer. The architecture of an l layer PLN is shown in Figure 3 where we assume that the notation is clear from the context. For the l'th layer, we find the optimal output matrix and the optimal cost as followsis the optimal output matrix from the corresponding single- layer PLN. The matrix R 2 ∈ R (n2−2Q)×n1 is an instance of a random matrix, that is, nodes from (2Q + 1) to n 2 in the second layer are random nodes. Now, for the two-layer PLN, we have the relationTherefore, the total number of param- eters to learn is approximately l (Q × n l ) = Q × l n l . If Q ≪ n l , ∀l, then the number of parameters increases approximately in a linear scale with addition of a new layer. On the other hand, for a typical ANN, weight matrix at the l'th layer has a dimension n l × n l−1 . Hence, for such an ANN the total number of parameters to learn is approximately l (n l × n l−1 ). Assuming that for all layers we have similar number of nodes, that is ∀l, n l ≈ n, a PLN has O(Qln) parameters to learn, but ANN has O(ln 2 ) parameters to learn. Furthermore, if Q ≪ n and n is large, then O(Qln) can be roughly approximated as O(ln).results and show that these five parameters can be chosen in a less careful manner. In fact they are almost the same for several PLN structures used for varying applications.In this subsection we discuss the approximation error of a deep PLN that has many layers, and we assume that each layer has a finite number of nodes. Using PP, we note that the To design a PLN using training data, we start with a single- layer PLN where an important building block is a regularized least-squares. For the single layer PLN, we increase the num- ber of nodes in its layer. We add ∆ number of nodes (random nodes) in step-wise manner, until the cost improvement shows a saturation trend. Then, we design a two-layer PLN by adding a new layer as the second layer and increase its nodes in ∆ number as long as we get a tangible cost improvement. Next, we add a third layer to design a three-layer PLN and continue to add more layers. That is, layers are added and optimized one at a time. In this way we can design a deep network. If we have access to validation data, then we can test the cost improvement on the validation data instead of training data. Let us choose a performance cost as normalized-mean-error (NME) in dB, defined as follows NME = 10 log 10Et− ˜ t p p l = U Q , then, further addition of a new layer will not help to reduce the cost. Due to PP, the PLN is in a locally optimum point wherewhere˜twhere˜where˜t denotes predicted output. Let us denote the current NME by NME c after addition of a node/layer to an old net- work that has NME denoted by NME o . For the training data, PLN satisfies NME c ≤ NME o . We decide the progressive increase of nodes in a layer, or number of layers to increase depth of a network as long as the change in NME is more than a certain threshold, that is, We now discuss relations between a single-layer PLN and two standard learning methods: regularized least-squares and regularized ELM. We have discussed in section III-A that a single-layer PLN provides the same output from a regularized least-squares if the optimal output matrix of PLN O ⋆ NMEo−NMEc NMEo ≥ threshold. We choose two separate thresholds for adding nodes and adding layers. The threshold corresponding to addition of nodes is denoted by η n &gt; 0, and the threshold corresponding to addition of layers is denoted by η l &gt; 0. We set an upper limit on number of nodes in a layer, denoted by n max , to avoid much wide layers. We also set an upper limit on number of layers in a network, denoted by l max .We need to manually set the following parameters: λ ls for regularized least-squares, α for optimization of O l , ∀l, the threshold parameters η n and η l , maximum allowable number of nodes n max in each layer, and maximum allowable number of layers l max . Among these parameters, we tune λ ls more carefully to achieve a good regularized least-squares. We recall that PLN uses regularized least-squares in its first layer. We do not put much effort to tune the other five parameters: α, η n , η l , n max and l max . In section IV we report experimentalOn the other hand, if the output matrix is comprised of a zero matrix from the top and a regularized part from the bottom, where the zero matrix is of size Q × 2Q, then the output of single-layer PLN is equivalent to a regularized ELM. Overall, a single-layer PLN can be interpreted as a linear combination of regularized least-squares and regularized ELM. It is non- trivial to extend this interpretation for a multi-layer PLN.For the l'th layer we need to solve the optimization problem (13). While the optimization problem is convex, a practical problem is computational complexity for a large amount of data. Therefore, we use a computationally simple convex optimization method called alternating-direction-method-of- multipliers (ADMM) [35]. ADMM is an iterative algorithm, more familiar for distributed convex optimization problems [35]. Let us define new matrices , we can rewrite the optimization problem (13) for p = 2 in the following constrained least- squares problem where ǫ o α 1 q Q q and F denotes Frobenius norm. To solve the above problem using ADMM, we consider the following equivalent form of (15) where we drop the subscript l for notational clarity. Then, the ADMM iterations for solving the optimization problem would be as follows    are µ and an upper limit on iterations, denoted by k max . The choice of µ has a high influence on convergence rate and final solution.(17) where k denotes iteration index, µ &gt; 0 controls convergence rate of ADMM, and Λ stands for a Lagrange multiplier matrix. Noting that the two subproblems in (17) have closed-form solutions, the final algorithm can be written as , Remark 6. For q = 1 (ℓ 1 norm) and q = 2 (Frobenius norm), the projection in (18) has closed-form solutions. For q = 2 we have, and P Cq per- forms projection onto C q . As initial conditions for iterations, we set Q 0 and Λ 0 as zero matrices. The parameters to chooseIn this section, we show experimental results on various benchmark databases for two types of tasks: classification Further, we used a practical trick: the output corresponding to random nodes of each layer of PLN is scaled such that it has unit ℓ 2 norm. This is to ensure that the transformed signal does not grow arbitrarily large when it flows through layers. We conduct experiments using databases which are men- tioned in Table I and II for classification and regression tasks, respectively. These databases have been extensively used in relevant signal processing and machine learning applications [36]- [42]. For classification tasks, databases are mentioned in Table I -the 'vowel' database is for vowel recognition task (a speech recognition application) and all other databases are for image classification (computer vision applications). To give some examples, Caltech101 database contains 9144 images from 101 distinct object classes (including faces, flowers, cars, etc.) and one background class. The number of images in each class can vary from 31 to 800, and objects of same class have considerable variations in shape. Caltech101 is a database where achieving a higher classification accuracy is known to be challenging. In Table I, we mention instances of training data, instances of testing data, dimension of input data vector (P ), and number of classes (Q) for each database. We use the Q-dimensional target vector t in a classification task as a discrete variable with indexed representation of 1-out- of-Q-classes. A target variable (vector) instance has only one scalar component that is 1, and the other scalar components are 0. Table II informs databases for regression tasks, and shows instances of training data, instances of testing data, dimension of input data vector (P ), and target dimension (Q). For several databases in Tables I and Table II we do not have clear demarcation between training data and testing data. In those cases, we randomly partition the total dataset into two parts (training and testing datasets) and repeat experiments to reduce the effect of this randomness. This is denoted as the term 'random partition' in tables. If 'yes', then we do random partition; if 'no', then the databases already have clear partition. At this point, we discuss source of randomness in experiments. While we mentioned a type of randomness due to partition of a total database into training and testing datasets, another type of randomness arises in ELM and PLN due to the use of random weight matrices in their architecture. Considering both these types of randomness and to reduce their effects, we repeat experiments several times (50 times) and report average performance results (mean value) with standard deviations.Classification performance results are shown in Table III where we show training NME (NME for training data at the time of training), testing NME (NME for testing data after training), training time (calculated by Matlab tic-toc function at the time of training), and testing accuracy (classification performance for testing data). The most important performance measure is the testing accuracy. It can be seen that PLN provides better and/or competitive performance for several databases vis-a-vis regularized least-squares and regularized ELM. For Caltech101 database, PLN provides a significant performance improvement compared to ELM. Next we show corresponding parameters for the associated algorithms in Table IV. Parameters for regularized least-squares and regular- ized ELM are carefully chosen using extensive cross validation and manual tuning. We recall that regularized least-squares is used as a building block for PLN. Except for the part of parameters associated with regularized least-squares, only modest effort was put into choosing the other parameters of the PLN. In fact, we deliberately chose the same parameters for all databases to show that careful tuning of the parameters is not necessary. This helps to reduce manual effort. Next, we show regression performance results in Table V and note that PLN provides better and/or competitive performance. Table VI displays parameters for the associated algorithms and it shows that PLN indeed does not require high manual effort in tuning. While we argued that we wish to avoid such effort, a natural question is what happens if we manually tune parameters of PLN. Is it possible to improve performance? To investigate, we show performance of PLN with tuned parameters in Table VII for classification tasks on some randomly picked datasets. We did not exercise extensive tuning, but perform a limited tuning. Further we tuned some parameters among all parameters. The tuning is performed using intuition driven optimization and our gathered experience from previous experiments. Experimental results shown in Table VII confirm that further performance improvement of PLN is indeed possible by parameter tuning.     Figure 4. The instance of PLN has eight layers. The number of random nodes for each layer is shown in the first sub- figure. It is interesting to observe that the proposed learning algorithm helps to build up a self organizing architecture -some layers have more nodes  and some layers have less nodes. Natural questions include: what is the reason behind such an architecture? Which layer could have more nodes and which layer less nodes? These questions are non-trivial to answer. In the other sub-figures, note the stair-case type behavior where a sudden change occurs when a new layer is added. Addition of a new layer brings relatively high performance improvement as a sudden jump. The performance shows saturation trend when the number of nodes in each layer increases and the number of layers increases. While PLN training strategy ensures non-increasing NME for training dataset, it is interesting to see the testing dataset NME also shows a similar trend. Further, classification accuracy for both training and testing datasets show consistent improvement as the PLN size grows. It is natural to question why a sudden change in performance improvement occurs at addition of a new layer. At this point of time, a mathematically justified argument for this question is non-trivial. A qualitative argument would be that the addition of a new layer brings a richer feature representation than the previous layer. Further, we observe that the gap between training dataset performance and test dataset performance increases as the network grows in size. For a fixed amount of training dataset size, this observation is consistent with the knowledge that network generalization power diminishes as the number of parameters (to learn) grows with increase in network size.used to reproduce some experimental results reported in this article.We conclude that a large multi-layer ANN can be designed systematically where the number of layers and the number of nodes in each layer can be learned from training data. The learning process does not require a high manual intervention. Appropriate use of activation functions, regularization, convex optimization and random weights allows the learned network to show promising generalization properties. In the progressive strategy of growing the network size, addition of a new layer typically brings performance improvement with a sudden jump. This may be attributed to the common belief that non- linear transformations provide information rich feature vectors. In the development of PLN, we did not investigate a scope of using back propagation based learning approach for further training of PLN to design a more optimized network, but this can be investigated in future.
