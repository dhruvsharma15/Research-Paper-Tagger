It is generally believed that natural language understanding systems would benefit from incorporating common-sense knowledge about prototypical sequences of events and their participants. Early work focused on structured representa- tions of this knowledge (called scripts (Schank &amp; Abelson, 1977)) and manual construction of script knowledge bases. However, these approaches do not scale to complex do- mains (Mueller, 1998;Gordon, 2001). More recently, auto- matic induction of script knowledge from text have started to attract attention: these methods exploit either natural texts (Chambers &amp; Jurafsky, 2008;2009) or crowdsourced data ( Regneri et al., 2010), and, consequently, do not re- quire expensive expert annotation. Given a text corpus, they extract structured representations (i.e. graphs), for ex- ample chains (Chambers &amp; Jurafsky, 2008) or more gen- In order to get an intuition why the embedding approach may be attractive, consider a situation where a prototypi- cal ordering of events the bus disembarked passengers and the bus drove away needs to be predicted. An approach based on frequency of predicate pairs (Chambers &amp; Jurafsky, 2008), is unlikely to make a right prediction as driv- ing usually precedes disembarking. Similarly, an approach which treats the whole predicate-argument structure as an atomic unit ( Regneri et al., 2010) will probably fail as well, as such a sparse model is unlikely to be effectively learn- able even from large amounts of data. However, our em- bedding method would be expected to capture relevant fea- tures of the verb frames, namely, the transitive use for the predicate disembark and the effect of the particle away, and these features will then be used by the ranking component to make the correct prediction. In previous work on learning inference rules (Berant et al., 2011), it has been shown that enforcing transitivity con- straints on the inference rules results in significantly im- proved performance. The same is true for the event order- passengers bus Figure 1. Computation of an event representation (the bus disem- barked passengers).ing task, as scripts have largely linear structure, and ob- serving that a ≺ b and b ≺ c is likely to imply a ≺ c. In- terestingly, in our approach we implicitly learn the model which satisfies transitivity constraints, without the need for any explicit global optimization on a graph. The approach is evaluated on crowdsourced dataset of Reg- neri et al. (2010) and we demonstrate that using our model results in the 13.5% absolute improvement in F 1 on event ordering with respect to their graph induction method (84% vs. 71%).In this section we describe the model we use for computing event representations as well as the ranking component of our model.Learning and exploiting distributed word representations (i.e. vectors of real values, also known as embeddings) have been shown to be beneficial in many NLP applica- tions ( Bengio et al., 2001;Turian et al., 2010;Collobert et al., 2011). These representations encode semantic and syntactic properties of a word, and are normally learned in the language modeling setting (i.e. learned to be predictive of local word context), though they can also be specialized by learning in the context of other NLP applications such as PoS tagging or semantic role labeling (Collobert et al., 2011). More recently, the area of distributional composi- tional semantics have started to emerge (Baroni &amp; Zamparelli, 2011;Socher et al., 2012), they focus on induc- ing representations of phrases by learning a compositional model. Such a model would compute a representation of a phrase by starting with embeddings of individual words in the phrase, often this composition process is recursive and guided by some form of syntactic structure.In our work, we use a simple compositional model for rep- resenting semantics of a verb frame (i.e. the predicate and its arguments). The model is shown in Figure 1. Each word w i in the vocabulary is mapped to a real vector based on the corresponding lemma (the embedding function C). The hidden layer is computed by summing linearly transformed predicate and argument embeddings and passing it through the logistic sigmoid function. 1 We use different transfor- mation matrices for arguments and predicates, T and R, respectively. The event representation x is then obtained by applying another linear transform (matrix A) followed by another application of the sigmoid function.These event representations are learned in the context of 1 Only syntactic heads of arguments are used in this work. If an argument is a coffee maker, we will use only the word maker. event ranking: the transformation parameters as well as representations of words are forced to be predictive of the temporal order of events. However, one important charac- teristic of neural network embeddings is that they can be in- duced in a multitasking scenario, and consequently can be learned to be predictive of different types of contexts pro- viding a general framework for inducing different aspects of (semantic) properties of events, as well as exploiting the same representations in different applications. We evaluate our approach on crowdsourced data collected for script induction by Regneri et al. (2010), though, in principle, the method is applicable in arguably more gen- eral setting of Chambers &amp; Jurafsky (2008).The task of learning stereotyped order of events naturally corresponds to the standard ranking setting. Here, we as- sume that we are provided with sequences of events, and our goal is to capture this order. We discuss how we obtain this learning material in the next section. We learn a linear ranker (characterized by a vector w) which takes an event representation and returns a ranking score. Events are then ordered according to the score to yield the model predic- tion. Note that during the learning stage we estimate not only w but also the event representation parameters, i.e. matrices T , R and A, and the word embedding C. Note that by casting the event ordering task as a global rank- ing problem we ensure that the model implicitly exploits transitivity of the temporal relation, the property which is crucial for successful learning from finite amount of data, as we argued in the introduction and will confirm in our experiments.(called event sequence descriptions, ESDs) of various types of human activities (e.g., going to a restaurant, ironing clothes) using crowdsourcing (Amazon Mechanical Turk), this dataset was also complemented by descriptions pro- vided in the OMICS corpus (Gupta &amp; Kochenderfer, 2004). The datasets are fairly small, containing 30 ESDs per ac- tivity type in average (we will refer to different activities as scenarios), but the collection can easily be extended given the low cost of crowdsourcing. The ESDs are written in a bullet-point style and the annotators were asked to follow the temporal order in writing. Consider an example ESD for the scenario prepare coffee :{go to coffee maker} → {fill water in coffee maker} → {place the filter in holder} → {place coffee in filter} → {place holder in coffee maker} → {turn on coffee maker}We use an online ranking algorithm based on the Percep- tron Rank (PRank, (Crammer &amp; Singer, 2001)), or, more accurately, its large-margin extension. One crucial differ- ence though is that the error is computed not only with re- spect to w but also propagated back through the structure of the neural network. The learning procedure is sketched in Algorithm 1. Additionally, we use a Gaussian prior on weights, regularizing both the embedding parameters and the vector w. We initialize word representations using the Though individual ESDs may seem simple, the learning task is challenging because of the limited amount of train- ing data, variability in the used vocabulary, optionality of events (e.g., going to the coffee machine may not be men- tioned in a ESD), different granularity of events and vari- ability in the ordering (e.g., coffee may be put in a filter before placing it in a coffee maker).Unlike our work, Regneri et al. (2010) relies on WordNet to provide extra signal when using the Multiple Sequence Alignment (MSA) algorithm. As in their work, each de- scription was preprocessed to extract a predicate and heads of argument noun phrases to be used in the model. the introduction that inducing graph representations from scripts may not be an optimal strategy from the practical perspective.The methods are evaluated on human annotated scenario- specific tests: the goal is to classify event pairs as appearing in a given stereotypical order or not ( Regneri et al., 2010).The model was estimated as explained in Section 2.2 with the order of events in ESDs treated as gold standard. We used 4 held-out scenarios to choose model parameters, no scenario-specific tuning was performed, and the 10 test scripts were not used to perform model selection. When testing, we predicted that the event pair (e 1 ,e 2 ) is in the stereotypical order (e 1 ≺ e 2 ) if the ranking score for e 1 exceeded the ranking score for e 2Berant, Jonathan, Dagan, Ido, and Goldberger, Jacob. Global learning of typed entailment rules. In Proceed- ings of ACL, 2011.In our experiments, we compared our event embedding model (EE) against three baseline systems (BL , MSA) and BSMSA is the system of Regneri et al. (2010). BS is a a hierarchical Bayesian system of Frermann et al. (2014). BL chooses the order of events based on the preferred or- der of the corresponding verbs in the training set: (e 1 , e 2 ) is predicted to be in the stereotypical order if the number of times the corresponding verbs v 1 and v 2 appear in this or- der in the training ESDs exceeds the number of times they appear in the opposite order (not necessary at adjacent po- sitions); a coin is tossed to break ties (or if v 1 and v 2 are the same verb).Chambers, Nathanael and Jurafsky, Dan. We also compare to the version of our model which uses only verbs (EE verbs ). Note that EE verbs is conceptually very similar to BL, as it essentially induces an ordering over verbs. However, this ordering can benefit from the im- plicit transitivity assumption used in EE verbs (and EE), as we discussed in the introduction. The results are presented in Table 1.Frermann, Lea, Titov, Ivan, and Pinkal, Manfred. A hi- erarchical bayesian model for unsupervised induction of script knowledge. In EACL, Gothenberg, Sweden, 2014.Gordon, Andrew. Browsing image collections with repre- sentations of common-sense activities. JAIST, 52(11), 2001.The first observation is that the full model improves sub- stantially over the baseline and the previous methods (MSA and BS) (13.5% and 6.5% improvement over MSA and BS respectively in F1), this improvement is largely due to an increase in the recall but the precision is not negatively af- fected. We also observe a substantial improvement in all metrics from using transitivity, as seen by comparing the results of BL and EE verb (11.3% improvement in F1). This simple approach already outperforms the pipelined MSA system. These results seem to support our hypothesis in  3 The unseen event pairs are not coming from the same ESDs making the task harder: the events may not be in any temporal relation. This is also the reason for using the F1 score rather than the accuracy, both in Regneri et al. (2010)  
