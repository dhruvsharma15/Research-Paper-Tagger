With the renewed interest in neural networks, old problems re-emerge, specially if the solution is still open. That is the case with the so-called catastrophic forgetting or catas- trophic interference problem (McCloskey &amp; Cohen, 1989;Ratcliff, 1990). In essence, catastrophic forgetting corre- sponds to the tendency of a neural network to forget what it learned upon learning from new or different information. For instance, when a network is first trained to convergence on one task, and then trained on a second task, it forgets how to perform the first task.Storing previous information and using it to retrain the model was among the earliest attempts to overcome catas- trophic forgetting; a strategy named "rehearsal" (Robins, 1995). The use of memory modules in this context has been a subject of research until today ( Rebuffi et al., 2017;Lopez-Paz &amp; Ranzato, 2017). However, due to efficiency and capacity constrains, memory-free approaches were also introduced, starting with what was termed as "pseudo- rehearsal" (Robins, 1995). This approach has found some success in transfer learning situations where one needs to maintain a certain accuracy on the source task after learning the target task ( Jung et al., 2016;Li &amp; Hoiem, 2017). Within the pseudo-rehearsal category, we could also consider recent approaches that substitute the memory module by a gener- ative network (Venkatesan et al., 2017;Shin et al., 2017;Nguyen et al., 2017). Besides the difficulty of training a generative network for a sequence of tasks or certain types of data, both rehearsal and pseudo-rehearsal approaches imply some form of concurrent learning, that is, having to re-process 'old' instances for learning a new task.Overcoming catastrophic forgetting is an important step in the advancement towards more general artificial intel- ligence systems (Legg &amp; Hutter, 2007 The other popular strategy to overcome catastrophic forget- ting is to reduce representational overlap (French, 1991). This can be done at the output, intermediate, and also input levels (Gutsein &amp; Stump, 2015;He &amp; Jaeger, 2018). A clean way of doing that in a soft manner is through so-called "structural regularization" ( Zenke et al., 2017), either present in the loss function (Kirkpatrick et al., 2017;Zenke et al., 2017) or at a separate merging step ( Lee et al., 2017). With these strategies, one seeks to prevent major changes in the weights that were important for previous tasks. Dedicating specific sub-parts of the network for each task is another way of reducing representational overlap ( Rusu et al., 2016;Fernando et al., 2017;Yoon et al., 2018). The main trade-off in representational overlap is to effectively distribute the capacity of the network across tasks while maintaining important weights and reusing previous knowledge.In this paper, we propose a task-based hard attention mech- anism that maintains the information from previous tasks without affecting the learning of a new task. Concurrently to learning a task, we also learn almost-binary attention vectors through gated task embeddings, using backpropaga- tion and minibatch stochastic gradient descent (SGD). The attention vectors of previous tasks are used to define a mask and constrain the updates of the network's weights on cur- rent tasks. Since masks are almost binary, a portion of the weights remains static while the rest adapt to the new task. We call our approach hard attention to the task (HAT). We evaluate HAT in the context of image classification, using what we believe is a high-standard evaluation protocol: we consider random sequences of 8 publicly-available data sets representing different tasks, and compare with a dozen of recent competitive approaches. We show favorable results in 4 different experimental setups, cutting current rates by 45 to 80%. We also show robustness with respect to hyperpa- rameters and illustrate a number of monitoring capabilities. We make the code for all experiments publicly-available 1 . The primary observation that drives the proposed approach is that the task definition or, more pragmatically, its iden- tifier, is crucial for the operation of the network. Consider the task of discriminating between bird and dog images. When training the network to do so, it may learn some set of intermediate features. If the second task is to dis- criminate between brown and black animals using the same data (assuming it only contained birds and dogs that were either brown or black), the network may learn a new set of features, some of them with not much overlap with the first ones. Thus, if training data is the same in both tasks, one important difference should be the task description or identifier. Our intention is to learn to use the task identifier to condition every layer, and to later exploit this learned conditioning to prevent forgetting previous tasks.where σ(x) ∈ [0, 1] is a gate function and s is a positive scaling parameter. We use a sigmoid gate in our experiments, but note that other gating mechanisms could be used. All layers l = 1, . . . L − 1 operate equally except the last one, layer L, where a t L is binary hard-coded. The operation of layer L is equivalent to a multi-head output (Bakker &amp; Heskes, 2003), which is routinely employed in the context of catastrophic forgetting (for example Rusu et al., 2016;Li &amp; Hoiem, 2017;Nguyen et al., 2017).To condition to the current task t, we employ a layer-wise attention mechanism (Fig. 1, top). Given the output of the units 2 of layer l, h l , we element-wise multiply h l = a t l h l . However, an important difference with common attention 1 Code is attached in a ZIP file as Supplementary Material. 2 In the remaining of the paper, we will use 'units' to refer to both linear units (or fully-connected neurons) and convolutional filters. HAT can be extended to any parametric layer.The idea behind the gating mechanism of Eq. 1 is to form hard, possibly binary attention masks which, act as "in- hibitory synapses" (McCulloch &amp; Pitts, 1943), and can thus activate or deactivate the output of the units of every layer. In this way, and similar to PathNet (Fernando et al., 2017), we dynamically create and destroy paths across layers that can be later preserved when learning a new task. However, unlike PathNet, the paths in HAT are not based on modules, but on single units. Therefore, we do not need to pre-assign a module size nor to set a maximum number of modules per task. Given some network architecture, HAT learns and automatically dimensions individual-unit paths, which ultimately affect individual layer weights. Furthermore, in- stead of learning paths in a separate stage using genetic algorithms, HAT learns them together with the rest of the network, using backpropagation and SGD.train the embeddings e t To preserve the information learned in previous tasks upon learning a new task, we condition the gradients according to the cumulative attention from all the previous tasks. To obtain a cumulative attention vector, after learning task t and obtaining a t t l with backpropagation ( Fig. 1), we prefer a differentiable function. To construct a pseudo-step function that allows the gradient to flow, we use a sigmoid with a positive scaling parameter s (Eq. 1). This scaling is introduced to control the polarization, or 'hardness', of the pseudo-step function and the resulting output a l , we recursively computeusing element-wise maximum and the all-zero vector for a l . Our strategy is to anneal s during training, inducing a gradient flow, and set s = s max during testing, using s max 1 such that Eq. 1 approximates a unit step function. Notice that when s → ∞ we get a ≤0 l,i → {0, 1}, and that when s → 0 we get a t l . This preserves the attention values for units that were important for previous tasks, allowing them to condition the training of future tasks.l,i → 1/2. We will use the latter to start a training epoch with all network units being equally active, and progressively polarize them within the epoch.To condition the training of task t + 1, we modify the gra- dient g l,ij at layer l with the reverse of the minimum of the cumulative attention in the current and previous layers:During a training epoch, we incrementally linearly anneal the value of s bywhere the unit indices i and j correspond to the output (l) and input (l − 1) layers, respectively. In other words, we expand the vectors a ≤t ≤t l and a l−1 to match the dimen- sions of the gradient tensor of layer l, and then perform a element-wise minimum, subtraction, and multiplication ( Fig. 1, bottom). We do not compute any attention over the input data if this consists of complex signals like images or audio. However, in the case such data consisted of separate or independent features, one could also consider them as the output of some layer and apply the same methodology.where b = 1, . . . B is the batch index and B is the total number of batches in an epoch. The hyperparameter s max ≥ 1 controls the stability of the learned tasks or, in other words the plasticity of the network's units. If s max is close to 1, the gating mechanism operates like a regular sigmoid function, without particularly enforcing the binarization of a t Note that, with Eq. 2, we create masks to prevent large up- dates to the weights that were important for previous tasks. This is similar to the approach of PackNet (Mallya &amp; Lazebnik, 2017), which was made public during the development of HAT. In PackNet, after an heuristic selection and retrain- ing, a binary mask is found and later applied to freeze the corresponding network weights. In this regard, HAT differs from PackNet in three important aspects. Firstly, our mask is unit-based, with weight-based masks automatically de- rived from those. Therefore, HAT also stores and maintains a lightweight structure. Secondly, our mask is learned, in- stead of heuristically-or rule-driven. Therefore, HAT does not need to pre-assign compression ratios nor to determine parameter importance through a post-training step. Thirdly, our mask is not necessarily binary, allowing intermediate values between 0 and 1. This can be useful if we want to reuse weights for learning other tasks, at the expense of some forgetting, or we want to work in a more online mode, forgetting the oldest tasks to remember new ones.l . This provides plasticity to the units, with the model being able to forget previous tasks at the backpropagation stage (Sec. 2.3). If, alternatively, s max is a larger number, the gating mechanism starts operating as a unit step function. This provides stability with regard to previously learned tasks, preventing changes in the corresponding weights at the backpropagation stage.In preliminary analysis, we empirically observed that em- beddings e t l were not changing much, and that the magnitude of the gradient was weak on those weights. After some in- vestigation, we realized that the major part of the problem was due to the introduced annealing scheme (Eq. 3). To illustrate the effect of the annealing scheme on the gradients of e . If we do not perform any annealing and set s = 1, we obtain a cu- mulative gradient after one epoch that has a bell-like shape and spans the whole sigmoid range (Fig. 2). Contrastingly, if we set s = s max , we obtain a much larger magnitude, but in a much lower range (e tTo obtain a totally binary attention vector a t l , one could use a unit step function as gate. However, since we want to Fig. 2). The annealed version of s yields a distribution in-between, with a lower range than s = 1 and a lower magnitude than s = s max . A desirable situation would be to have a wide range, ide- ally spanning the range of s = 1, and a large cumulative magnitude, ideally proportional to the one in the active re- gion when s = s max . To achieve that, we apply a gradient compensation before updating e t l .Cumulative q l, iConstant s = 1 Constant s = s max Annealed s Compensated is the regularization term, and N l corresponds to the num- ber of units in layer l. Notice that Eq. 5 corresponds to a weighted and normalized L1 regularization over A t . Cumu- lative attentions over the past tasks A &lt;t define a weight for the current task, such that if a &lt;t t l,i → 1 then a Figure 2. Illustration of the effect that annealing s has on the gra- dient q of e t .l,i receives a weight close to 0 and vice versa. This excludes the units that were attended in previous tasks from regularization, unconstraining their reuse in the current task. The hyper- parameter c ≥ 0 controls the capacity spent on each task (Eq. 4). In a sense, it can be thought of as a compressibility constant, affecting the compactness of the learned models: the higher the c, the lower the number of active attention values a t l,i and the more sparse the resulting network is. We set c globally for all tasks and let HAT adapt to the best compression for each individual task.In essence, the idea of the embedding gradient compensa- tion is to remove the effects of the annealed sigmoid and to artificially impose the desired range and magnitude mo- tivated in the previous paragraph. To do so, we divide the gradient q l,i by the derivative of the annealed sigmoid, and multiply by the desired compensation,which, after operating, yieldsThe use of L1 regularization to promote network sparsity in the context of catastrophic forgetting has also been con- sidered by Yoon et al. (2018) with dynamically expandable networks (DEN), which were introduced while developing HAT. In DEN, plain L1 regularization is combined with a considerable set of heuristics such as L2-transfer, thresh- olding, and a measure of "semantic drift", and is applied to all network weights in the so-called "selective retraining" phase. In HAT, we use an attention-weighted L1 regulariza- tion over attention values, which is an independent part of the single training phase of the approach. Instead of consid- ering network weights, HAT focuses on unit attention.For numerical stability, we clamp |se In any case, however, q l,i → 0 when we hit those limits. That is, we are in the constant regions of the pseudo-step function. Notice also that, by Eq. 3, the minimum s is never equal to 0.We compare the proposed approach with the conceptually closest works, some of which appeared concurrently to the development of HAT. A more general overview of related work has been done in Sec. 1. A qualitative comparison with three of the most related strategies has been done along Sec. 2. A quantitative comparison with these and other approaches is done in Sec. 4 and Appendix C.It is important to realize that the hard attention values a t t l,i that are 'active', that is, a l,i → 1, directly deter- mine the units that will be dedicated to task t. There- fore, in order to have some model capacity reserved for future tasks, we promote sparsity on the set of attention vectorsTo do so, we add a regular- ization term to the loss function L that takes into account the set of cumulative attention vectors up to task t − 1,where c is the regularization constant, Both elastic weight consolidation (EWC; Kirkpatrick et al., 2017) and synaptic intelligence (SI; Zenke et al., 2017) ap- proaches add a 'soft' structural regularization term to the loss function in order to discourage changes to weights that are important for previous tasks. HAT uses a 'hard' struc- tural regularization, and does it both at the loss function and gradient magnitudes explicitly. EWC measures weights' importance after network training, while SI and HAT com- pute weights' importance concurrently to network training. EWC and SI use specific formulation while HAT learns attention masks. Incremental moment matching (IMM; Lee et al., 2017) is an evolution of EWC, performing a separate model-merging step after learning a new task.Progressive neural networks (PNNs; Rusu et al., 2016) dis- tribute the network weights in a column-wise fashion, pre-assigning a column width per task. They employ so-called adapters to reuse knowledge from previous columns/tasks, leading to a progressive increase of the number of weights assigned to future tasks. Instead of blindly pre-assigning column widths, HAT learns such 'widths' per layer, together with the network weights, and adapts them to the difficulty of the current task. PathNet ( Fernando et al., 2017) also pre-assigns some amount of network capacity per task but, in contrast to PNNs, avoids network columns and adapters. It uses an evolutionary approach to learn paths between a constant number of so-called modules (layer subsets) that interconnect between themselves. HAT does not maintain a population of solutions, entirely trains with backpropagation and SGD, and does not rely on a constant set of modules.Together with PNNs and PathNet, PackNet (Mallya &amp; Lazebnik, 2017) also employs a binary mask to constrain the network. However, such constrain is not based on columns nor layer modules, but on network weights. Therefore, it allows for a potentially better use of the network's capac- ity. PackNet is based on heuristic weight pruning, with pre-assigned pruning ratios. HAT also focuses on network weights, but uses unit-based masks to constrain those, which also results in a lightweight structure. It avoids any absolute or pre-assigned pruning ratio, although it uses the com- pressibility parameter c to influence the compactness of the learned models. Another difference between HAT and the previous three approaches is that it does not use purely bi- nary masks. Instead, the stability parameter s max controls the degree of binarization.2017), or two-task transfer learning setups where accuracy is measured on both source and target tasks (Li &amp; Hoiem, 2017). However, there are some limitations with these se- tups. Firstly, performing permutations of the MNIST data has been suggested to favor certain approaches, yielding mis- leading results 3 in the context of catastrophic forgetting ( Lee et al., 2017). Secondly, using only the MNIST data may not be very representative of modern computer vision tasks, nor particularly challenging ( Xiao et al., 2017). Thirdly, incrementally adding classes or groups of classes implies the assumption that all data comes from the same joint distri- bution, which is unrealistic for a real-world setting. Finally, evaluating catastrophic forgetting with only two tasks biases the conclusions towards transfer learning setups, and pre- vents the analysis of truly sequential learning with more than two tasks. In this paper, we consider the aforementioned MNIST and CIFAR setups (Sec. 4.2). Nonetheless, we pri- marily evaluate on a sequence of multiple tasks formed by different classification data sets (Sec. 4.1).To obtain a generic estimate, we weigh a number of tasks and uniformly randomize their order. After training task t, we compute the accuracies on all testing sets of tasks τ ≤ t. We repeat 10 times this sequential train/test procedure with 10 different seed numbers, which are also used in the rest of randomizations and initializations (see below). To compare between different task accuracies, and in order to obtain a general measurement of the amount of forgetting, we introduce the forgetting ratio Dynamically expandable networks (DEN; Yoon et al., 2018) also assign network capacity depending on the task at hand. However, they do so in a separate stage called "selective retraining". A complex mixture of heuristics and hyper- parameters is used to identify "drifting" units, which are duplicated and retrained in another stage. L1 regulariza- tion and L2-transfer are employed to condition learning, together with the corresponding regularization constants and an additional set of thresholds. HAT strives for simplic- ity, restricting the number of hyperparameters to two that have a straightforward conceptual interpretation. Instead of plain L1 regularization over network weights, HAT em- ploys an attention-weighted L1 regularization over attention masks. Attention masks are a lightweight structure that can be plugged in without the need of introducing important changes to a pre-existing network.R where A τ ≤t is the accuracy measured on task τ after se- quentially learning task t, A τ R is the accuracy of a random, frequency-based classifier solely trained on task τ , and A τ ≤t J is the accuracy measured on task τ after jointly learning t tasks in a multitask fashion. Note that ρ ≈ −1 and ρ ≈ 0 correspond to performances close to the ones of the random and multitask classifiers, respectively. To report a single number after learning t tasks, we take the average  Network -Unless stated otherwise, we employ an AlexNet-like architecture ( Krizhevsky et al., 2012) with 3 convolutional layers of 64, 128, and 256 filters with 4 × 4, 3 × 3, and 2 × 2 kernel sizes, respectively, plus two fully- connected layers of 2048 units each. We use rectified linear units as activations, and 2 × 2 max-pooling after the con- volutional layers. We also use a dropout of 0.2 for the first two layers and of 0.5 for the rest. A fully-connected layer with a softmax output is used as a final layer, together with categorical cross entropy loss. All layers are randomly initialized with Xavier uniform initialization (Glorot &amp; Bengio, 2010) except the embedding layers, for which we use a Gaussian distribution N (0, 1). Unless stated otherwise, our code uses PyTorch's defaults for version 0.2.0 ( Paszke et al., 2017). We adapt the same base architecture to all baseline approaches and match their number of parameters to 7.1 M.  Training -We train all models with backpropagation and plain SGD, using a learning rate of 0.05, and decaying it by a factor of 3 if there is no improvement in the validation loss for 5 consecutive epochs. We stop training when we reach a learning rate lower than 10 −4 or we have iterated over 200 epochs (we made sure that all considered approaches reached a stable solution before 200 epochs). Batch size is set to 64. All methods use the same task sequence, data split, batch shuffle, and weight initialization for a given seed.beyond a transfer learning setup. We find LFL extremely sensitive to the configuration of its hyperparameter, to the point that what is a good value for one seed, turns out to be a bad choice for another seed. Hence the poor average performance for 10 seeds. The highest standard deviations are obtained by LFL and PathNet (Table 1), which suggests a high sensitivity with respect to hyperparameters, initial- izations, or data sets. Another thing to note is that the IMM approaches only perform similarly or slightly better than the SGD-F reference. We believe this is due to both the different nature of the tasks' data and the consideration of more than two tasks, which complicates the choice of the mixing hyperparameter.We first look at the average forgetting ratio ρ ≤t after learning task t (Fig. 3). A first thing to note is that not all the consid- ered baselines perform better than the SGD references. That is the case of LWF and LFL. For LWF, we observe it is still competitive in the two-task setup for which it was designed, t = 2. However, its performance rapidly degrades for t &gt; 2, indicating that the approach has difficulties in extendingThe best performing baselines are EWC, PathNet, and PNN. PathNet and PNN present contrasting behaviors. Both, by construction, never forget; therefore, the important differ- ence is in their learning capability. PathNet starts by cor- rectly learning the first task and progressively exhibits dif- ficulties to do so for t ≥ 2. Contrastingly, PNNs exhibits difficulty in the first tasks and becomes better as t increases.These contrasting behaviors are due to the way the two ap- proaches allocate the network capacity. As mentioned, they cannot do it dynamically, and therefore need to pre-assign a number of network weights per task. When having more tasks but the same network capacity, this pre-assignment increasingly harms the performance of these baselines, low- ering the corresponding curves in Fig. 3. We now move to the HAT results. First of all, we observe that HAT consistently performs better than all considered baselines for all t ≥ 2 (Fig. 3). For the case of t = 2, it obtains an average forgetting ratio ρ ≤2 = −0.02, while the best baseline is EWC with ρ ≤2 = −0.08 (Table 1). For the case of t = 8, HAT obtains ρ ≤8 = −0.06, while the best baseline is PNN with ρ ≤8 = −0.11. This implies a reduction in forgetting of 75% for t = 2 and 45% for t = 8. Notice that the standard deviation of HAT is lower than the ones obtained by the big majority of the baselines (Table 1). This denotes a certain stability of HAT with respect to different task sequences, data sets, data splits, and network initializations. Given the slightly increasing tendency of PNN with t (Fig. 3), one could speculate that PNN would score above HAT for t &gt; 8. However, our empirical analyzes suggest that that is not the case (presumably due to the capacity pre-assignment and parameter increase problems underlined in Sec. 3 and above). In particular, we observe a gradual lowering of PathNet and PNN curves with increasing se- quences from t = 2 to 8. In addition, we observe PathNet and PNN obtaining worse performances than EWC in the case of t = 10 for the incremental class setup (see below and Appendix C.1). In general, none of the baseline meth- ods consistently outperforms the rest across setups and for all t, a situation that we do observe with HAT.To broaden the strength of our results, we additionally ex- periment with three common alternative setups. First, we consider an incremental class learning scenario, similar to Lopez-Paz &amp; Ranzato (2017), using class subsets of both CIFAR10 and CIFAR100 data. In this setup, the best base- line after t ≥ 3 is EWC, with ρ ≤10 = −0.18. HAT scores ρ ≤10 = −0.09 (55% forgetting reduction). Next, we con- sider the permuted MNIST sequence of tasks ( Srivastava et al., 2013). In this setup, the best result we could find in the literature was from SI, with A ≤10 = 97.1%. HAT scores A ≤10 = 98.6% (52% error rate reduction). Finally, we also consider the split MNIST task of Lee et al. (2017). In this setup, the best result from the literature corresponds to the conceptor-aided backpropagation approach (He &amp; Jaeger, 2018), with A ≤2 = 94.9%. HAT scores A ≤2 = 99.0% (80% error rate reduction). The detail for all these setups and results can be found in Appendix C.In any machine learning algorithm, it is important to assess the sensitivity with respect to the hyperparameters. HAT has two: the stability parameter s max and the compressibil- ity parameter c (Secs. 2.4 and 2.6). A low s max provides plasticity to the units and capacity of adaptation, but the network may easily forget what it learned. A high s max prevents forgetting, but the network may have difficulties in adapting to new tasks. A low c allows to use almost all of the network's capacity for a given task, potentially spending too much in the current task. A high c forces it to learn a very compact model, at the expense of not reaching the accuracy that the original network could have reached. We empirically found good operation ranges s max ∈ [25,800] and c ∈ [0.1, 2.5]. As we can see, any variation within these ranges results in reasonable performance (Fig. 4). Unless stated otherwise, we use s max = 400 and c = 0.75.It is interesting to note that the hard attention mechanism in- troduced in Sec. 2 offers a number of possibilities to monitor the behavior of our models. For instance, by computing the conditioning mask in Eq. 2 from the hard attention vectors a ≤t l , we can assess which weights obtain a high attention value, binarize it, and compute an estimate of the instanta- neous network capacity usage (Fig. 5). We may also inform ourselves of the amount of active weights per layer and task (Appendix B.2). Another facet we can monitor is the weight reuse across tasks. By a similar procedure, comparing the conditioning masks between tasks t i and t j , j &gt; i, we can asses the percentage of weights of task t i that are later reused in task t j (Fig. 6).Another by-product of hard attention masks is that we can   the task (Appendix B.3). Comparing these numbers with the compression rates used by PackNet (25 or 50%), we see that HAT generally uses a much more compact model. Compar- ing with DEN on the specific MNIST and CIFAR100 tasks (18 and 52%), we observe that HAT compresses to 1 and 21%, respectively. Interestingly, and in contrast to these and the majority of network pruning approaches, HAT learns to prune network weights through backpropagation and SGD, and at the same time as the network weights themselves.  use them to assess which of the network's weights are impor- tant, and then prune the most irrelevant ones ( LeCun et al., 1990). This way, we can compress the network for further deployment in low-resource devices or time-constrained environments (cf. Han et al., 2016). If we want to focus on such compression task, we can set c to a higher value than the one used for catastrophic forgetting and start with a positive random initialization of the embeddings e l . The former will promote more compression while the latter will ensure we start learning the model by putting attention to all weights in the first epochs (full capacity). We empirically found that using c = 1.5 and U(0, 2) yields a reasonable trade-off between accuracy and compression for a single task (Fig. 7). With that, we can compress the network to sizes between 1 and 21% of its original size, depending on We introduce HAT, a hard attention mechanism that, by focusing on a task embedding, is able to protect the infor- mation of previous tasks while learning new tasks. This hard attention mechanism is lightweight, in the sense that it adds a small fraction of weights to the base network, and is trained together with the main model, with negligi- ble overhead using backpropagation and vanilla SGD. We demonstrate the effectiveness of the approach to control catastrophic forgetting in the image classification context by running a series of experiments with multiple data sets and state-of-the-art approaches. HAT has only two hyperparame- ters, which intuitively refer to the stability and compactness of the learned knowledge, and whose tuning we demonstrate is not crucial for obtaining good performance. In addition, HAT offers the possibility to monitor the used network ca- pacity across tasks and layers, the unit reuse across tasks, and the compressibility of a model trained for a given task. We hope that our approach may be also useful in online learning or network compression contexts, and that the hard attention mechanism presented here may also find some applicability beyond the catastrophic forgetting problem.The data sets used in our experiments are summarized in Table 2. The MNIST data set ( LeCun et al., 1998) comprises 28 × 28 monochromatic images of handwritten digits. Fashion-MNIST ( Xiao et al., 2017) comprises gray-scale images of the same size from Zalando's articles 4 . The German traffic sign data set (TrafficSigns; Stallkamp et al., 2011) contains traffic sign images. We used the version of the data set from the Udacity self-driving car github repository 5 . The NotMNIST data set (Bulatov, 2011) comprises glyphs extracted from publicly available fonts, making a similar data set to MNIST; we just need to resize the images 6 . The SVHN data set (Netzer et al., 2011) comprises digits cropped from house numbers in Google Street View images. The FaceScrub data set (Ng &amp; Winkler, 2014) is widely used in face recognition tasks (KemelmacherShlizerman et al., 2016). Because some of the images listed in the original data set were not hosted anymore on the corresponding Internet domains, we use a version of the data set stored on the MegaFace challenge website 7 (Kemelmacher- Shlizerman et al., 2016), from which we select the first 100 people with the most appearances 8 . The CIFAR10 and CIFAR100 data sets contain 32 × 32 color images (Krizhevsky, 2009).To match the image input shape required in our experiments, some of the images in the corresponding data sets need to be resized (FaceScrub, TrafficSigns, and NotMNIST) or padded with zeros (MNIST and FashionMNIST). In addition, for the data sets comprising monochromatic images, we replicate the image across all RGB channels. Note that we do not perform any sort of data augmentation; we just adapt the inputs. We provide the necessary code to perform such adaptations in the links listed above. We report all forgetting ratios ρ ≤t for t = 1 to 8 in Table 3. A total of 10 runs with 10 different seeds are performed and the averages and standard deviations are taken.In Fig. 8 we show an example of layer capacity monitoring as the sequence of tasks evolves. As mentioned in the main paper, we can compute a percent of active weights for a given layer and task.The final results of the network compression experiment reported in the main paper (after reaching convergence) are available in Table 4. We run HAT on isolated tasks with c = 1.5 and uniform embedding initialization U(0, 2). Table 3. Average forgetting ratio ρ ≤t for the considered approaches (10 runs, standard deviation into parenthesis).  To have an idea of the training time for each of the considered approaches, we report some reference values in Table 5. We see that HAT is also quite competitive in this aspect. As an additional experiment to complement our evaluation, we consider the incremental CIFAR setup, following a similar approach as Lopez-Paz &amp; Ranzato (2017). We divide both CIFAR10 and CIFAR100 data sets into consecutive-class subsets and use them as tasks, presented in random order according to the seed. We take groups of 2 classes for CIFAR10 and 20 classes for CIFAR100, yielding a total of 10 tasks. We decide to take groups of 2 and 20 classes in order to have a similar number of training instances per task. The rest of the procedure is as in the main paper. The most important results are summarized there. The complete numbers are depicted in Fig. 9 and reported in Table 6. A common experiment is the one proposed by Srivastava et al. (2013), and later employed to evaluate catastrophic forgetting by Goodfellow et al. (2014). It consists of taking random permutations of the pixels in the MNIST data set as tasks. Typically, the average accuracy after sequentially training on 10 MNIST permutations is reported. To match the different number of Table 6. Average forgetting ratio ρ ≤t for the incremental CIFAR task (10 runs, standard deviation into parenthesis).  parameters used in the literature, we consider a small, medium, and a large network based on a two-layer fully-connected architecture as Zenke et al. (2017), with 100, 500, and 2000 hidden units, respectively. For the large network we set dropout probabilities as Kirkpatrick et al. (2017). We use s max = 200 and c = 0.5 for the small network, and s max = 400 and c = 0.5 for the medium and large networks. The results are available in Table 7.Another popular experiment is to split the MNIST data set into tasks and report the average accuracy after learning them one after the other. We follow Lee et al. (2017) by splitting the data set using labels 0-4 and 5-9 as tasks and running the experiment 10 times. We also match the base network architecture to the one used by Lee et al. (2017). We train HAT for 50 epochs with c = 0.1. Results are reported in Table 8. In preliminary experiments we observed that dropout could increase accuracy by some percentage. However, to keep the same configuration as in the cited reference, we finally did not use it. In this section, we want to mention a number of alternatives we experimented with during the development of HAT. The purpose of the section is not the report a formal set of results, but to inform the reader about potential different choices when implementing HAT, or variations of it, and to give an intuition on the outcome of some of such choices.When we realized that the embedding weights e t l were not changing much and that their gradients were small compared to the rest of the network due to the introduced annealing of s, we initially tackled the issue by using a different learning rate for the embeddings. With that, we empirically found that factors of 10-50 times the original learning rate were able to tackle the issue, leading to performances that were almost as good as the final ones reported in the main paper. However, the use of a different learning rate introduced an additional parameter that we could not conceptually relate to catastrophic forgetting and that could have been tricky to tune for a generic setting.l start with a value of 1, which has the effect of distributing the attention over all units for more time at the beginning of training. Using values of k 1 ∈ [1,6] and k 2 ∈ [0.5, 2] yielded competitive results, yet worse than the ones using N (0, 1). Our intuition is that a uniform initialization like U(0, 2) is better for a purely compressive approach, as used in the last experiment of the main paper.We initially experimented with a normalized L1 regularizationResults were a small percentage lower than the ones with the attention-weighted regularization of the main paper. We also exchanged the previous L1 regularization with the L2-based regularizationWith that, we observed similar accuracies as the L1 regularization, but under different values for the hyperparameter c.As mentioned in the main paper, no attention mask is used for the input (that is, there is no a t 0 ). We find this is a good strategy for a general image classification problem and for first-layer convolutional filters in particular. However, if the input consists of independent, isolated features, one may think of putting hard attention to the input as a kind of supervised feature selection process. We performed a number of experiments using only fully-connected layers and the MNIST data as above, and introduced additional hard attention vectors a t 0 that directly multiplied the input of the network. The results suggested that it could potentially be a viable option for feature selection and data compression (Fig. 10).After writing a first version of the paper, we realized that the idea of a binary mask that affects a given unit could be potentially traced back to the "inhibitory synapses" of McCulloch &amp; Pitts (1943). This idea of inhibitory synapses is quite unconventional and rarely seen today (Wang &amp; Raj, 2017) and, to the best of our knowledge, no specific way for learning
