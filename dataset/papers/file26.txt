The application of deep learning ( LeCun et al., 2015) has in recent years lead to a dramatic boost in performance in many areas such as computer vision, speech recognition or natural language processing. Despite this huge empirical success, the theoretical understanding of deep learning is still limited. In this paper we address the non-convex opti- mization problem of training a feedforward neural network. This problem turns out to be very difficult as there can be exponentially many distinct local minima (Auer et al., 1996;Safran &amp; Shamir, 2016). It has been shown that the training of a network with a single neuron with a variety of activation functions turns out to be NP-hard (Sima, 2002).In practice local search techniques like stochastic gradient descent or variants are used for training deep neural net- works. Surprisingly, it has been observed ( Dauphin et al., 2014;Goodfellow et al., 2015) that in the training of state- of-the-art feedforward neural networks with sparse con- nectivity like convolutional neural networks ( LeCun et al., 1990;Krizhevsky et al., 2012) or fully connected ones On the theoretical side there have been several interesting developments recently, see e.g. (Brutzkus &amp; Globerson, 2017;Lee et al., 2016;Poggio &amp; Liao, 2017;Rister &amp; Rubin, 2017;Soudry &amp; Hoffer, 2017;Zhou &amp; Feng, 2017). For some class of networks one can show that one can train them globally optimal efficiently. However, it turns out that these approaches are either not practical ( Janzamin et al., 2016;Haeffele &amp; Vidal, 2015;Soltanolkotabi, 2017) as they require e.g. knowledge about the data generating measure, or they modify the neural network structure and objective ( Gautier et al., 2016). One class of networks which are simpler to analyze are deep linear networks for which it has been shown that every lo- cal minimum is a global minimum (Baldi &amp; Hornik, 1988;Kawaguchi, 2016). While this is a highly non-trivial result as the optimization problem is non-convex, deep linear networks are not interesting in practice as one efficiently just learns a linear function. In order to characterize the loss surface for general networks, an interesting approach has been taken by (Choromanska et al., 2015a). By ran- domizing the nonlinear part of a feedforward network with ReLU activation function and making some additional simplifying assumptions, they can relate it to a certain spin glass model which one can analyze. In this model the objective of local minima is close to the global optimum and the number of bad local minima decreases quickly with the distance to the global optimum. This is a very interesting result but is based on a number of unrealistic assumptions ( Choromanska et al., 2015b). It has recently been shown (Kawaguchi, 2016) that if some of these assumptions are dropped one basically recovers the result of the linear case, but the model is still unrealistic.In this paper we analyze the case of overspecified neural networks, that is the network is larger than what is required to achieve minimum training error. Under overspecifica- tion (Safran &amp; Shamir, 2016) have recently analyzed under which conditions it is possible to generate an initialization so that it is in principle possible to reach the global opti- mum with descent methods. However, they can only deal with one hidden layer networks and have to make strong assumptions on the data such as linear independence or cluster structure. In this paper overspecification means that there exists a very wide layer, where the number of hidden units is larger than the number of training points. For this case, we can show that a large class of local minima is glob- ally optimal. In fact, we will argue that almost every critical point is globally optimal. Our results generalize previous work of (Yu &amp; Chen, 1995), who have analyzed a similar setting for one hidden layer networks, to networks of arbi- trary depth. Moreover, it extends results of (Gori &amp; Tesi, 1992;Frasconi et al., 1997) who have shown that for cer- tain deep feedforward neural networks almost all local min- ima are globally optimal whenever the training data is lin- early independent. While it is clear that our assumption on the number of hidden units is quite strong, there are sev- eral recent neural network structures which contain a quite wide hidden layer relative to the number of training points e.g. in ( Lin et al., 2016) they have 50,000 training samples and the network has one hidden layer with 10,000 hidden units and ( Ba &amp; Caruana, 2014) have 1.1 million training samples and a layer with 400,000 hidden units. We refer to (Ciresan et al., 2010;Neyshabur et al., 2015;Vincent et al., 2010;Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples. We conjecture that for these kind of wide networks it still holds that almost all local minima are globally optimal. The reason is that one can expect linear separability of the training data in the wide layer. We provide supporting evidence for this conjecture by showing that basically every critical point for which the training data is linearly separable in the wide layer is glob- ally optimal. Moreover, we want to emphasize that all of our results hold for neural networks used in practice. There are no simplifying assumptions as in previous work. rameters of the network. In this paper, [a] denotes the set of integers {1, 2, . . . , a} and [a, b] the set of integers from a to b. The activation function σ : R → R is assumed at least to be continuously differentiable, that is σ ∈ C 1 (R). In this paper, we assume that all the functions are applied componentwise. Let f k , g k : R d → R n k be the mappings from the input space to the feature space at layer k, which are defined asT ∈ R N ×n k be the matrices that store the feature vectors of layer k after and before ap- plying the activation function. One can easily check thatIn this paper we analyze the behavior of the loss of the network without any form of regularization, that is the final objective Φ : P → R of the network is defined aswhere l : R → R is assumed to be a continuously differ- entiable loss function, that is l ∈ C 1 (R). The prototype loss which we consider in this paper is the squared loss, l(α) = α 2 , which is one of the standard loss functions in the neural network literature. We assume throughout this paper that the minimum of (1) is attained.The idea of backpropagation is the core of our theoretical analysis. Lemma 2.1 below shows well-known relations for feed-forward neural networks, which are used throughout the paper. The derivative of the loss w.r.t. the value of unit j at layer k evaluated at a single training sample x i is denoted as δ kj (x i ) =We are mainly concerned with multi-class problems but our results also apply to multivariate regression problems. Let N be the number of training samples and denote byN ×m the input resp. output matrix for the training data (x i , y i ) ∂g kj (xi) . We arrange these vectors for all training samples into a single matrix ∆ k , defined as, where d is the input dimension and m the number of classes. We consider fully-connected feedfor- ward networks with L layers, indexed from 0, 1, 2, . . . , L, which correspond to the input layer, 1st hidden layer, etc, and output layer. The network structure is determined by the weight matrices (W k )In the following we use the Hadamard product •, which forwhere n k is the number of hid- den units of layer k (for consistency, we set n 0 = d, n L = m), and the bias vectors (b k )We denote by P = W × B the space of all possible pa-The Loss Surface of Deep and Wide Neural NetworksFor every k ∈ [L − 1], the chain rule yields for everyNote that Lemma 2.1 does not apply to non-differentiable activation functions like the ReLU function, σ ReLU (x) = max{0, x}. However, it is known that one can approxi- mate this activation function arbitrarily well by a smooth function e.g.We first discuss some prior work and present then our main result together with extensive discussion. For improved readability we postpone the proof of the main result to the next section which contains several intermediate results which are of independent interest.and henceOur work can be seen as a generalization of the work of (Gori &amp; Tesi, 1992;Yu &amp; Chen, 1995). While ( Yu &amp; Chen, 1995) has shown that for a one-hidden layer network, that if n 1 = N − 1, then every local minimum is a global minimum, the work of (Gori &amp; Tesi, 1992) con- sidered also multi-layer networks. For the convenience of the reader, we first restate Theorem 1 of (Gori &amp; Tesi, 1992) using our previously introduced notation. The crit- ical points of a continuously differentiable function f : R d → R are the points where the gradient vanishes, that is ∇f (x) = 0. Note that this is a necessary condition for a local minimum. &amp; Tesi, 1992) Let Φ : P → R be defined as in (1) with least squares loss l(a) = a 2 . As-to be continuously differentiable with strictly positive derivative andThe Loss Surface of Deep and Wide Neural Networksis real-analytic on (−1, ∞) and the composition with the exponential function is real-analytic, we get that σ 3 is a real-analytic function.While this result is already for general multi-layer net- works, the condition "[X, 1 N ] T ∆ 1 = 0 implies ∆ 1 = 0" is the main caveat. It is already noted in (Gori &amp; Tesi, 1992), that "it is quite hard to understand its practical meaning" as it requires prior knowledge of ∆ 1 at every critical point. Note that this is almost impossible as ∆ 1 depends on all the weights of the network. For a particular case, when the training samples (biases added) are linearly indepen- dent, i.e. rank([X, 1 N ]) = N , the condition holds auto- matically. This case is discussed in the following Theorem 3.4, where we consider a more general class of loss and activation functions.Finally, we note that σ 1 ,σ 2 , σ 3 are strictly monotonically increasing. Since σ 1 ,σ 2 are bounded, they both satisfy As- sumption 3.2. For σ 3 , we note that 1 + e αt ≤ 2e αt for t ≥ 0, and thus it holds for every t ≥ 0 thatand with log(1+x) ≤ x for x &gt; −1 it holds log(1+e αt ) ≤ e αt for every t ∈ R. In particular Krantz &amp; Parks, 2002). All results in this section are proven under the following assumptions on the loss/activation function and training data.2. σ is analytic on R, strictly monotonically increasing andThe conditions on l are satisfied for any twice continu- ously differentiable convex loss function. A typical ex- ample is the squared loss l(a) = a 2 or the Pseudo- Huber loss (Hartley &amp; Zisserman, 2004) given as l δ (a) = 2δ2 ( 1 + a 2 /δ 2 − 1) which approximates a 2 for small a and is linear with slope 2δ for large a. But also non-convex loss functions satisfy this requirement, for instance:For small a, this curve approximates a 2 , whereas for large a the asymptotic value is − log(δ).These conditions are not always necessary to prove some of the intermediate results presented below, but we decided to provide the proof under the above strong assumptions for better readability. For instance, all of our results also hold for strictly monotonically decreasing activation functions. Note that the above conditions are not restrictive as many standard activation functions satisfy them.2. Corrupted-Gaussian:This function computes the negative log-likehood of a gaussian mixture model. 3. Cauchy: l(a) = δ 2 log(1 + a 2 /δ 2 ) for δ = 0. This curve approximates a 2 for small a and the value of δ determines for what range of a this approximation is close.We refer to (Hartley &amp; Zisserman, 2004) (p.617-p.619) for more examples and discussion on robust loss functions.1+t is real-analytic on R + = {t ∈ R | t ≥ 0}. The exponential function is analytic with values in (0, ∞). As composition of real-analytic function is real- analytic (see Prop 1.4.2 in (Krantz &amp; Parks, 2002)), we get that σ 1 and σ 2 are real-analytic. Similarly, since log(1 + t)As a motivation for our main result, we first analyze the case when the training samples are linearly independent, which requires N ≤ d+1. It can be seen as a generalization of Corollary 1 in (Gori &amp; Tesi, 1992). Theorem 3.4 Let Φ : P → R be defined as in (1)  We prove in this section a similar guarantee in our main Theorem 3.8 by implicitly transporting this condition to some higher layer. A similar guarantee has been proven by (Yu &amp; Chen, 1995) for a single hidden layer network, whereas we consider general multi-layer networks. The main ingredient of the proof of our main result is the ob- servation in the following lemma.Proof: The proof is based on induction. At a critical point it holds is a global minimum.Proof: By Lemma 2.1 it holds thatTheorem 3.4 implies that the weight matrices of potential saddle points or suboptimal local minima need to have low rank for one particular layer. Note however that the set of low rank weight matrices in W has measure zero. At the moment we cannot prove that suboptimal low rank lo- cal minima cannot exist. However, it seems implausible that such suboptimal low rank local minima exist as ev- ery neighborhood of such points contains full rank matrices which increase the expressiveness of the network. Thus it should be possible to use this degree of freedom to further reduce the loss, which contradicts the definition of a local minimum. Thus we conjecture that all local minima are indeed globally optimal. The first condition of Lemma 3.5 can be seen as a gener- alization of the requirement of linearly independent train- ing inputs in Theorem 3.4 to a condition of linear inde- pendence of the feature vectors at a hidden layer. Lemma 3.5 suggests that if we want to make statements about the global optimality of critical points, it is sufficient to know when and which critical points fulfill these conditions. The third condition is trivially satisfied by a critical point and the requirement of full column rank of the weight matrices is similar to Theorem 3.4. However, the first one may not be fulfilled since rank([F k , 1 N ]) is dependent not only on the weights but also on the architecture. The main difficulty of the proof of our following main theorem is to prove that this first condition holds under the rather simple require- ment that n k ≥ N − 1 for a subset of all critical points.First of all we note that the full column rank condition of (W l ) L But before we state the theorem we have to discuss a par- ticular notion of non-degenerate critical point.The Hessian w.r.t. a subset of vari- ables S ⊆ {x 1 , . . . , x n } is denoted as ∇ l=k+2 in Theorem 3.4, and 3.8 implicitly requires that n k+1 ≥ n k+2 ≥ . . . ≥ n L . This means the network needs to have a pyramidal structure from layer k + 2 to L. It is interesting to note that most modern neural network archi- tectures have a pyramidal structure from some layer, typi- cally the first hidden layer, on. Thus this is not a restrictive requirement. Indeed, one can even argue that Theorem 3.8 gives an implicit justification as it hints on the fact that such networks are easy to train if one layer is sufficiently wide.We use this to introduce a slightly more general notion of non-degenerate critical point.Note that Theorem 3.8 does not require fully non- degenerate critical points but non-degeneracy is only needed for some subset of variables that includes layer k + 1. As a consequence of Theorem 3.8, we get directly a stronger result for non-degenerate local minima. • x is non-degenerate for a subset of variablesProof: The Hessian at a non-degenerate local minimum is positive definite and every principal submatrix of a positive definite matrix is again positive definite, in particular for the subset of variablesNote that a non-degenerate critical point might not be non- degenerate for a subset of variables, and vice versa, if it is non-degenerate on a subset of variables it does not neces- sarily imply non-degeneracy on the whole set. For instance, Let us discuss the implications of these results. First, note that Theorem 3.8 is slightly weaker than Theorem 3.4 as it requires also non-degeneracy wrt to a set of variables in- cluding layer k + 1. Moreover, similar to Theorem 3.4 it does not exclude the possibility of suboptimal local min- ima of low rank in the layers "above" layer k + 1. On the other hand it makes also very strong statements. In fact, if n k ≥ N − 1 for some k ∈ [L − 1] then even degenerate saddle points/local maxima are excluded as long as they are non-degenerate with respect to any subset of parame- ters of upper layers that include layer k + 1 and the rank condition holds. Thus given that the weight matrices of the upper layers have full column rank , there is not much room left for degenerate saddle points/local maxima. Moreover, for a one-hidden-layer network for which n 1 ≥ N − 1, every non-degenerate critical point with respect to the out- put layer parameters is a global minimum, as the full rank condition is not active for one-hidden layer networks.Concerning the non-degeneracy condition of main Theo- rem 3.8, one might ask how likely it is to encounter degen- erate points of a smooth function. This is answered by an application of Sard's/Morse theorem in (Milnor, 1965).  Note that the theorem would still hold if one would draw w uniformly at random from the set {z ∈ R d | 2 ≤ ǫ} for any ǫ &gt; 0. Thus almost every linear perturbation f ′ of a function f will lead to the fact all of its critical points are non-degenerate. Thus, this result indicates that exact de- generate points might be rare. Note however that in prac- tice the Hessian at critical points can be close to singular (at least up to numerical precision), which might affect the training of neural networks negatively ( Sagun et al., 2016).As we argued for Theorem 3.4 our main Theorem 3.8 does not exclude the possibility of suboptimal degenerate local minima or suboptimal local minima of low rank. However, we conjecture that the second case cannot happen as ev- ery neighborhood of the local minima contains full rank matrices which increase the expressiveness of the network and this additional flexibility can be used to reduce the loss which contradicts the definition of a local minimum.Proof: Any linear function is real analytic and the set of real analytic functions is closed under addition, multiplication and composition, see e.g. Prop. 2.2.2 and Prop. 2.2.8 in (Krantz &amp; Parks, 2002). As we assume that the activation function is real analytic, we get that all the output functions of the neural network f k are real analytic functions of the parameters as compositions of real analytic functions. ✷The concept of real analytic functions is important in our proofs as these functions can never be "constant" in a set of the parameter space which has positive measure unless they are constant everywhere. This is captured by the following lemma.As mentioned in the introduction the condition n k ≥ N −1 looks at first sight very strong. However, as mentioned in the introduction, in practice often networks are used where one hidden layer is rather wide, that is n k is on the order of N (typically it is the first layer of the network). As the con- dition of Theorem 3.8 is sufficient and not necessary, one can expect out of continuity reasons that the loss surface of networks where the condition is approximately true, is still rather well behaved, in the sense that still most local min- ima are indeed globally optimal and the suboptimal ones are not far away from the globally optimal ones.n | f (x) = 0} has Lebesgue measure zero.In the next lemma we show that there exist network param- eters such that rank([F k , 1 N ]) = N holds if n k ≥ N − 1. Note that this is only possible due to the fact that one uses non-linear activation functions. For deep linear networks, it is not possible for F k to achieve maximum rank if the layers below it are not sufficiently wide. To see this, one considersFor better readability, we first prove our main Theorem 3.8 for a special case where I is the whole set of upper layers, i.e. I = {k + 1, . . . , L} , and then show how to extend the proof to the general case where I ⊆ {k + 1, . . . , L} . Our proof strategy is as follows. We first show that the output of each layer are real analytic functions of network parameters. Then we prove that there exists a set of pa- rameters such that rank([F k , 1 N ]) = N. Using proper- ties of real analytic functions, we conclude that the set of parameters where rank([F k , 1 N ]) &lt; N has measure zero. Then with the non-degeneracy condition, we can ap- ply the implicit-function theorem to conclude that even if rank([F k , 1 N ]) = N is not true at a critical point, then still in any neighborhood of it there exists a point where the conditions of Lemma 3.5 are true and the loss is minimal. By continuity of Φ, this implies that the loss must also be minimal at the critical point. k for a linear network, then rank(F k ) ≤ min{rank(F k−1 ), rank(W k )} + 1 since the addition of a rank-one term does not increase the rank of a matrix by more than one. By using induction, one getsThe existence of network parameters where rank([F k , 1 N ]) = N together with the previous lemma will then be used to show that the set of network parameters where rank([F k , 1 N ]) &lt; N has measure zero.Proof: We first show by induction that there always ex- ists a set of parameters (W l , b l )The set of (W 1 , b 1 ) that makes F 1 to have distinct rows is character- ized byWe introduce some notation frequently used in the proofs.Note, that σ is strictly monotonic and thus bijective on its domain. Thus this is equivalent to T W T Moreover, since σ is strictly monotonically increasing it holds for every β ∈ R, σ(β) &gt; µ. Pick some β ∈ R. For α ∈ R, we define w j = −αa, v j = αzLet us denote the first column of W 1 by a, then the exis- tence of a for which j a + β for every j ∈ [N − 1]. Note that the matrix A changes as we vary α. Thus, we consider a family of matrices A(α) defined aswould imply the result. Note that by assumption x i = x j for all i = j. Then the set {a ∈ R d | a T (x i − x j ) = 0} is a hyperplane, which has measure zero and thus the set where condition (2) fails corresponds to the union ofhy- perplanes which again has measure zero. Thus there always exists a vector a such that condition (2) is satisfied and thus there exists (W 1 , b 1 ) such that the rows of F 1 are distinct. Now, assume that F p−1 has distinct rows for some p ≥ 1, then by the same argument as above we need to constructLetˆELetˆ LetˆE(α) be a modified matrix where one subtracts every row i by row (i − 1) of E(α), in particular, letBy construction f p−1 (x i ) = f p−1 (x j ) and thus with the same argument as above we can choose W p such that this condition holds. As a result, there exists a set of parametersk−1 l=1 so that F k−1 has distinct rows. Now, given that F k−1 has distinct rows, we show how to construct (W k , b k ) in such a way that [F k , 1 N ] ∈ R N ×(n k +1) has full row rank. Since n k ≥ N − 1, it is sufficient to make the first N − 1 columns of F k together with the all-ones vector become linearly inde- pendent. In particular, let F k = [A, B] where A ∈ R N ×(N −1) and B ∈ R N ×(n k −N +1) be the matrices con- taining outputs of the first (N − 1) hidden units and last (n k − N + 1) hidden units of layer k respectively. LetWe do not show the values of other entries as what matters is that the limit, lim α→+∞ˆE α→+∞ˆ α→+∞ˆE(α), is an upper triangular matrix.As mentioned above, we just need to show there exists (w j , v j ) Thus, the determinant is equal to the product of its diago- nal entries which is non-zero. Note that the determinant ofˆE ofˆ ofˆE(α) is the same as that of E(α) as subtraction of some row from some other row does not change the determinant, and thus we get that lim α→+∞ E(α) has full rank N . As thesatisfying potentially after reordering w.l.o.g. z 1 &lt; z 2 &lt; . . . &lt; z N By the discussion above such a vector always exists since the complementary set is con- tained in i =j {a ∈ R n k−1 | i − z j , a = 0} which has measure zero.determinant of E(α) is a polynomial of its entries and thus continuous in α, there exists α 0 ∈ R s.t. for every α ≥ α 0 it holds rank(E(α)) = rank([1 N , A(α)]) = N. Moreover, since A is chosen as the first (N − 1) columns of F k , one can always choose the weights of the first (N − 1) hidden units of layer k so that rank([F k , 1 N ]) = N .We first prove the result for the case where σ is bounded. Since σ is bounded and strictly monotonically increasing, there exist two finite values γ, µ ∈ R with µ &lt; γ s.t.In the case where the activation function fulfills |σ(t)| ≤ ρ 1 e ρ2t for t &lt; 0 and |σ(t)| ≤ ρ 3 t + ρ 4 for t ≥ 0 we consider directly the determinant of the matrix E(α). In particular, let us pick some β ∈ R such that σ(β) = 0. We consider the family of matrices A(α) defined asNote that the all-ones vector is now where S N is the set of all N ! permutations of the set {1, . . . , N } and we used the fact that the last column of E(α) is equal to the all ones vector. Define the permuta- tion γ as γ(j) = j for j ∈ [N ]. Then we haveNote that with Lemma 4.1 the output F k of layer k is an analytic func- tion of the network parameters on P. The set of low rank matrices E k can be characterized by a system of equations such that theThe idea now is to show that N −1 j=1 E(α) π(j)j goes to zero for every permutation π = γ as α goes to infinity. And since the whole summation goes to zero while σ(β) = 0, the determinant would be non-zero as desired. With that, we first note that for any permutation π = γ there has to be at least one component π(j) where π(j) &gt; j, in which case,T a &lt; 0 and thus for sufficiently large α, it holds αδ j + β &lt; 0. Thus determinants of all N × N submatri- ces of E k are zero. As the determinant is a polynomial in the entries of the matrix and thus an analytic function of the entries and composition of analytic functions are again analytic, we conclude that each determinant is an analytic function of the network parameters of the first k layers. By Lemma 4.3 there exists at least one set of network parame- ters of the first k layers such that one of these determinant functions is not identically zero and thus by Lemma 4.2 the set of network parameters where this determinant is zero has measure zero. But as all submatrices need to have low rank in order that rank([F k , 1 N ]) &lt; N , it follows that the set of network parameters whereWe conclude that for n k ≥ N − 1 even if there are net- work parameters such that rank([F k , 1 N ]) &lt; N , then every neighborhood of these parameters contains network param-If π(j) = j then E(α) π(j)j = σ(β). In cases where π(j) &lt; j(j = N ) it holds that δ j = (z j − z π(j) ) T a &gt; 0 and thus for sufficiently large α, it holds αδ j + β &gt; 0 and we have So far, we have shown that |E(α) π(j)j | can always be upper-bounded by an exponential function resp. affine function of α when π(j) &gt; j resp. π(j) &lt; j or it is just a constant when π(j) = j. The above observations imply that there exist positive constants P, Q, R, S, T such that it holds for every π ∈ S N \ {γ} ,, ǫ has positive Lebesgue mea- sure while S has measure zero due to Lemma 4.4. Thus, for everyThe final proof of our main Theorem 3.8 is heavily based on the implicit function theorem, see e.g. (Marsden, 1974).As α → ∞ the upper bound goes to zero. As there are only finitely many such terms, we get lim Theorem 4.6 Let Ψ :and thus with the same argument as before we can argue that there exists a finite α 0 for which E(α) has full rank.Now we combine the previous lemma with Lemma 4.2 to conclude the following.is non-singular at (u 0 , v 0 ), then there is an open ball B(u 0 , ǫ) for some ǫ &gt; 0 and a unique function α :˜ v has full column rank. Assume (˜ u, ˜ v) corresponds to the following representation With all the intermediate results proven above, we are fi- nally ready for the proof of the main result.Proof of Theorem 3.8 for case I = {k + 1, . . . , L} Let us divide the set of all parameters of the network into two subsets where one corresponds to all param- eters of all layers up to k, for that we denote u = [vec(W 1 ) T , bWe obtain the followingT , and the other corre- sponds to the remaining parameters, for that we denote, which is the gradient mapping of Φ w.r.t. all parameters of the upper layers from (k + 1) to L. Since the gradient vanishes at a critical point, it holds thatNote that this construction can be done for anyAs the crit- ical point is assumed to be non-degenerate with respect to v, it holds thatTherefore, Ψ and (u * , v * ) satisfy the conditions of the implicit function the- orem 4.6. Thus there exists an open ball B(u * , δ 1 ) ⊂ R r=1 be a strictly monotonically decreasing sequence such that γ 1 = δ 3 and lim r→∞ γ r = 0. By Corol- lary 4.5 and the previous argument, we can choose for any γ r &gt; 0 a point˜upoint˜ point˜u r ∈ B(u * , γ r ) such that˜vthat˜ that˜v r = α(˜ u r ) has full rank and Φ(˜ u r , ˜ v r ) = p * . Moreover, as lim r→∞ γ r = 0, it follows that lim r→∞˜ur→∞˜ r→∞˜u r = u * and as α is a continu- ous function, it holds with˜vwith˜ with˜v r = α(˜ u r ) that lim r→∞˜vr→∞˜ r→∞˜v r = lim r→∞ α(˜ u r ) = α(lim r→∞˜ur→∞˜ r→∞˜u r ) = α(u * ) = v * . Thus we get lim r→∞ (˜ u r , ˜ v r ) = (u * , v * ) and as Φ is a continuous function it holds s for some δ 1 &gt; 0 and a continuously differentiable function α : B(u * , δ 1 ) → R t such thatas Φ attains the global minimum for the whole sequence (˜ u r , ˜ v r )., that is the weight matrices of the "upper" layers have full col- umn rank. Note that (W * l ) L l=k+2 corresponds to the weight matrix part of v * where one leaves out WIn the general case I ⊆ {k + 1, . . . , L}, the previous proof can be easily adapted. The idea is that we fix all layers in {k + 1, . . . , L} \ I. In particular, let * k+1 . Thus there exists a sufficiently small ǫ such that for any v ∈ B(v * , ǫ),l=k+2 of v has full column rank. In particular, this, combined with the continuity of α, implies that for a potentially smaller 0 &lt; δ 2 ≤ δ 1 , it holds for all u ∈ B(u * , δ 2 ) thatand that the weight matrix part (W l )has full column rank. Now, by Corollary 4.5 for any 0 &lt; δ 3 ≤ δ 2 there exists a ˜ u ∈ B(u * , δ 3 ) such that the generated output matrix˜Fmatrix˜ matrix˜F k at layer k of the corresponding network parameters of˜uof˜ of˜u satis-The only difference is that all the layers from {k + 1, . . . , L} \ I are hold fixed. They are not con- tained in the arguments of Ψ, thus will not be involved in our perturbation analysis. In this way, the full rank prop- erty of the weight matrices of these layers are preserved, which is needed to obtain the global minimum. We have seen that n k ≥ N − 1 is a sufficient condi- tion which leads to a rather simple structure of the critical points, in the sense that all local minima which have full rank in the layers k + 2 to L and for which the Hessian is non-degenerate on any subset of upper layers that in- cludes layer k + 1 are automatically globally optimal. This suggests that suboptimal locally optimal points are either completely absent or relatively rare. We have motivated before that networks with a certain wide layer are used in practice, which shows that the condition n k ≥ N − 1 is not completely unrealistic. On the other hand we want to discuss in this section how it could be potentially relaxed.The following result will provide some intuition about the case n k &lt; N − 1, but will not be as strong as our main result 3.8 which makes statements about a large class of critical points. The main idea is that with the condition n k ≥ N − 1 the data is linearly separable at layer k. As modern neural networks are expressive enough to repre- sent any function, see (Zhang et al., 2017) for an interest- ing discussion on this, one can expect that in some layer the training data becomes linearly separable. We prove that any critical point, for which the "learned" network outputs at any layer are linearly separable (see Definition 5.1) is a global minimum of the training error. In this section, we use a slightly different loss function than in the previous section. The reason is that the standard least squares loss is not necessarily small when the data is lin- early separable. Let C 1 , . . . , C m denote m classes. We consider the objective function Φ : P → R from (1) which is similar to the truncated squared loss (also called squared hinge loss) used in the SVM for binary classifica- tion. Since σ and l are continuously differentiable, all the results from Lemma 2.1 still hold.Our main result in this section is stated as follows. where the loss function now takes the new form1. Every critical point of Φ for which the feature vectors contained in the rows of F k are linearly separable and all the weight matrices (W l ) L l=k+2 have full column rank is a global minimum.where l 1 , l 2 penalize the deviation from the label encoding for the true class resp. wrong classes. We assume that the minimum of Φ is attained over P. Note that Φ is bounded from below by zero as l 1 and l 2 are non-negative loss func- tions. The results of this section are made under the follow- ing assumptions on the activation and loss function. On the other hand,where the last inequality is implied by (6) as W L has full column rank n L = m. Since the above product of matrices is a non-zero matrix, there must exist a non-zero column, say p ∈ [n L−1 ], then
