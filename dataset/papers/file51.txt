Conventional sub-word based automatic speech recognition (ASR) typically involves three models -acoustic model (AM), pronuncia- tion model and decision tree (PMT), and language model (LM) [1]. The AM computes the probability pAM(A|s) of the acoustics A given the sub-word units s. The PMT models the probability pPMT(w|s) of the word sequence w given the sub-word unit se- quence s. The LM acts as a prior pLM(w) on the word sequence w. Hence, finding the most likely sequence of words given the acoustics A becomes a maximum aposteriori optimization problem over the following probability density function: pASR(w|A) ∝ pLM(w)p(A|w)= pLM(w) pAM(A|s, w)pPMT(s|w)s ≈ pLM(w) pAM(A|s)pPMT(s|w) .s While both the AM and LM in modern ASR systems use deep neural networks (DNNs) and their variants [2], the PMT is usually based onThe authors thank Ewout Van Den Berg and Andrew Rosenberg of IBM, and Florian Metze of Carnegie Mellon University for useful discussions. decision trees and finite state transducers. Training the AM requires alignments between the acoustics and sub-word units, and several iterations of model training and re-alignment. Recent end-to-end (E2E) models have obviated the need for aligning the sub-word units to the acoustics. Popular E2E models include recurrent neural net- works (RNNs) trained with the connectionist temporal classification (CTC) loss function [3][4][5][6][7][8][9][10] and the attention-based encoder-decoder RNNs [11][12][13][14][15].These approaches are not truly E2E because they still use sub-word units, and hence require a decoder and separately- trained LM to perform well.In contrast, the recently-proposed direct acoustics-to-word (A2W) models [16,17] train a single RNN to directly optimize pASR(w|A). This eliminates the need for sub-word units, pronun- ciation model, decision tree, decoder, and externally-trained LM, which significantly simplifies the training and decoding process. However, prior research on A2W models has shown that such mod- els require several orders of magnitude more training data when compared with conventional sub-word based models. This is be- cause the A2W models need sufficient acoustic training examples per word to train well. For example, [16] used more than 125,000 hours of speech to train an A2W model with a vocabulary of nearly 100,000 words that matched the performance of a state-of-the-art CD state-based CTC model. Our prior work [17] explored A2W models for the well-known English Switchboard task and presented a few initialization techniques to effectively train such models with only 2000 hours of data. However, we still observed a gap of around 3-4% absolute in WER between the Switchboard phone CTC and the A2W models on the Hub5-2000 evaluation set.This paper further improves the state-of-the-art in A2W mod- els for English conversational speech recognition. We present a training recipe that achieves WER of 8.8%/13.9% on the Switch- board/CallHome subsets of the Hub5-2000 evaluation set, compared to our previous best result of 13.0%/18.8% [17]. These new re- sults are at par with several state-of-the-art models that use sub-word units, a decoder, and a LM. We quantify the gains made by each in- gredient of our training recipe and conclude that model initialization, training data order, and regularization are the most important factors.Next, we turn our attention to the issue of data sparsity while training A2W models. The conventional solution to this problem uses a sub-word unit-based model that needs a decoder and LM dur- ing testing. As an alternative, we propose the spell and recognize (SAR) CTC model that learns to first spell the word into its charac- ter sequence and then recognize it. Not only does this model retain all advantages of a direct A2W model, it also provides rich hypothe- ses to the user which are readable especially in the case of unseen or rarely-seen words. We illustrate the benefits of this model for out-of-vocabulary (OOV) words.The next section discusses the baseline A2W model [17]. Sec- tion 3 discusses the proposed training recipe, an analysis of the im- pact of individual ingredients, and the results. Section 4 presents our SAR model that jointly models words and characters. The paper concludes in Section 5 with a summary of findings. Table 1. This table shows the Switchboard/CallHome WERs for our baseline 300/2000-hour A2W and phone CTC models from [17].Conventional losses used for training neural networks, such as cross- entropy, require a one-to-one mapping (or alignment) between the rows of the T ×D input feature vector matrix A and length-L output label sequence y. The connectionist temporal classification (CTC) loss relaxes this requirement by considering all possible alignments. It introduces a special blank symbol φ that expands the length-L tar- get label sequence y to multiple length-T label sequences˜ysequences˜ sequences˜y contain- ing φ, such that˜ythat˜ that˜y maps to y after removal of all repeating symbols and φ. ˜ ywhere the set Ω(y), is the set of CTC paths for y, and at/yt denote the t th elements of the sequences. A forward-backward algorithm efficiently computes the above loss function and its gradient, which is then back-propagated through the neural network [3]. The next section describes the baseline A2W model [17].Our prior experience with training A2W models led us to conclude that model initialization and regularization are important aspects of training such models. One key reason for this is the fact that A2W models attempt to solve the difficult problem of directly recogniz- ing words from acoustics with a single neural network. Hence, our previously-proposed strategy of initializing the A2W BLSTM with the phone CTC BLSTM and the final linear layer with word em- beddings gave WER gains. In this work, we started by exploring several other strategies in the same spirit. All our new experiments were conducted in PyTorch [20] with the following changes com- pared to [17]:• We included delta and delta-delta coefficients because they slightly improved the WER. Hence, the total acoustic vector was of size 340 after stacking+decimation and appending the 100-dimensional i-vectors.We used two standard training data sets for our experiments. The "300-hour" set contained 262 hours of segmented speech from the Switchboard-1 audio with transcripts provided by Mississippi State University. The "2000-hour" set contained an additional 1698 hours from the Fisher data collection and 15 hours from CallHome audio. We extracted 40-dimensional logMel filterbank features over 25 ms frames every 10 ms from the input speech signal. We used stacking+decimation [6], where we stacked two successive frames and dropped every alternate frame during training. This resulted in a stream of 80-dimensional acoustic feature vectors at half the frame rate of the original stream. The baseline models also used 100-dimensional i-vectors [18] for each speaker, resulting in 180- dimensional acoustic feature vectors.The baseline A2W model consisted of a 5-layer bidirectional LSTM (BLSTM) RNN with a 180-dimensional input and 320- dimensional hidden layers in the forward and backward directions. We picked words with at least 5 occurrences in the training data in the vocabulary. This resulted in a 10,000-word output layer for the 300-hour A2W system and a 25,000-output layer for the 2000-hour system. As noted in [17], initialization is crucial to training an A2W model. Thus, we initialized the A2W BLSTM with the BLSTM from a trained phone CTC model, and the final linear layer using word embeddings trained using GloVe [19]. Table 1 gives the WERs of the baseline A2W and phone CTC models reported in [17] on the Hub5-2000 Switchboard (SWB) and CallHome (CH) test sets. We performed the decoding of the A2W models via simple peak-picking over the output word posterior dis- tribution, and removing repetitions and blank symbols. The phone CTC model used a full decoding graph and a LM. We observe that the 2000-hour A2W model lags behind the phone CTC model by 3.4%/2.8% absolute WER on SWB/CH, and the gap is much bigger for the 300-hour models. We next discuss our new training recipe.• We initialized all matrices to samples of a uniform distribu- tion over (− where is the inverse of the √ fan-in or the size of the input vector [21]. This takes the dimensionality of the input vectors at each layer in account, which improved convergence compared to our old strategy of using = 0.01.• In place of new-bob annealing, we kept a fixed learning rate for the first 10 epochs and decayed it by √ 0.5 every epoch.Training data order is an important consideration for sequence-to- sequence models such as E2E ASR systems because such models operate on the entire input and output sequences. All training se- quences have to be padded to the length of the longest sequence in the batch in order to do GPU tensor operations. Random sequence order during batch creation is not memory-efficient because batches will contain a larger range of sequence lengths, which will lead to more wasteful padding on average. Hence, sequences have to be sorted before batch creation. We compare the impact of sorting input acoustic sequences in or- der of ascending and descending length in Table 2. Our results show that ascending order gives significantly better WER than sorting in descending order. The intuition behind this result is that shorter se- quences are easier to train on initially, which enables the network to reach a better point in the parameter space. This can be regarded as an instance of curriculum learning [22].We also experimented with Nesterov momentum-based stochastic gradient descent (SGD), which has been shown to give better con- vergence compared to simple SGD on several tasks. We use the  [17] 20.8 30.4  where vn is the velocity or a running weighted-sum of the gradient of the loss function f . The constant ρ is usually set to 0.9 and λ is the learning rate, set to 0.01 in our experiments. We also experimented with a dropout of 0.25 in order to prevent over-fitting. Table 2 shows that both momentum and dropout improve the WER.We also note that our A2W model uses a vocabulary of 25,000 words which has an OOV rate of 0.5%/0.8% on the SWB/CH test sets. All the other models used much bigger vocabularies and hence did not suffer from OOV-induced errors.In contrast with phone or character-based CTC models, A2W models have a large output size equal to the size of the vocabulary. Prior research [23] has shown that decomposing the output linear layer of size V × D into two layers of sizes V × d and d × D with d &lt; D speeds-up model training due to reduced number of parameters. We experimented with a projection layer of size 256 and found that it speeds-up training by a factor of 1. 2x and also slightly improves the WER, which we attribute to a reduction in over-fitting.We finally initialized our model with the phone CTC BLSTM which gave improvements in our previous work [17]. As expected, this initialization lowered the WER, despite the presence of all the above useful strategies. With dropout in place, we trained a bigger 6-layer model with 512-dimensional BLSTM and saw slight gains in WER.In order to understand the impact of individual components of our recipe, we conducted an ablation study on our best 300-hour A2W model. We removed each component of the recipe while keeping others fixed, trained a model, and decoded the test sets. Figure 1 shows the results of this experiment. We observed that changing the training data order from ascending to descending order of length re- sulted in the biggest drop in performance. The second biggest factor was dropout -excluding it leads to over-training because the heldout loss rises after epoch 10. Choosing a smaller 5-layer model instead of 6-layer led to the next largest drop in WER. Finally, as expected, excluding the projection layer had the least impact on WER.We initialized the 2000-hour A2W model with the best 300-hour A2W model and used the same recipe for training. Table 3 shows the WER of the resulting system along with our previous best A2W WER and several other published results. We obtained a significant improvement of 4.2%/4.9% absolute WER compared to our previous result. We also see that our direct A2W is at par with most hybrid CD state-based and E2E models, while utilizing no decoder or LM. As noted in [18], the CallHome test set is more challenging than Switchboard because 36 out of 40 speakers in the latter appear in the training set. The results on the CallHome test set are especially good, where our A2W model matches the best result obtained using a hybrid BLSTM [18] that used exactly the same acoustic features 1 . Fig. 1. This figure shows the results of the ablation experiment on our best 300-hour A2W model. The numbers in parentheses in the legend give the SWB/CH WERs.Despite strong results, the A2W model does not give any mean- ingful output to the user in case of OOV words but simply emits an "UNK" tag. This is not a big problem for the Switchboard task be- cause the OOV rates for the SWB/CH test sets are 0.5%/0.8% on the 25,000 word vocabulary. But other tasks might be affected by the limited vocabulary. As a solution, the next section discusses our joint word-character model that aims to provide the user with a richer output that is especially useful for unseen or rarely-seen words.The advantage of the A2W model is that it directly emits word hy- potheses by forward-passing acoustic features through a RNN with- out needing a decoder or externally-trained LM. However, its vocab- ulary is fixed and OOV words cannot be recognized by the system. Furthermore, words infrequently seen in the training data are not rec- ognized well by the network due to insufficient training examples. Prior approaches to dealing with the above limitations completely rely on sub-word units. This includes work on character models [9,27] and N-grams [28], RNN-Transducer [29], RNN-Aligner [30].In contrast, our approach is to have the best of both worlds by combining the ease of decoding a A2W model with the flexibility of recognizing unseen/rarely-seen words with a character-based model. One natural candidate is a multi-task learning (MTL) model contain- ing a shared lower network, and two output networks corresponding to the two tasks -recognizing words and characters. However, such an MTL network is not suitable for our purpose because the recog- nized word and character sequences for an input speech utterance are not guaranteed to be synchronized in time. This is because the CTC loss does not impose time-alignment on the output sequence.The proposed spell and recognize (SAR) model circumvents this alignment problem by presenting training examples that contain both words and characters. This allows us to continue to leverage an A2W framework without resorting to more complex graph-based decoding methodologies employed in, for example, word-fragment based sys- tems [31][32][33]. Consider the output word sequence "THE CAT IS BLACK" The SAR model uses the following target sequence: a higher OOV rate than the 2000-hour set and also contains several rare words. We trained a 6-layer BLSTM with joint word and charac- ter targets after preparing the output training sequences as described in the previous section. We initialized the SAR BLSTM using the A2W BLSTM. The training recipe was the same as for the A2W model presented previously. The SAR model permits three decodes:• Word: Use only word predictions, similar to the A2W model.• Characters: Use only character predictions, and combine them into words using the word-begin characters.• Switched: Use the character predictions up to the previous word when the model predicts an "UNK" symbol, and use the word prediction everywhere else. We observe that the SAR model gives comparable performance to the baseline A2W model, but additionally gives meaningful output for OOVs, as illustrated by the following test set examples: where lowercase alphabets are the character targets, and b-/e- denote special prefixes for word beginning and end. Hence, the model is trained to first spell the word and then recognize it. In contrast with a MTL model, the SAR model has a single softmax over words+characters in the output layer.The words in bold are OOVs. We observe that the SAR model emits the UNK tag in these cases, but the characters preceding it con- tain the spelling of the word. In some cases, this spelling is incorrect, e.g. "SCHOLARLY → COLARLY", but still is more meaningful to the user than the UNK tag. Future research will try to fix these errors using data-driven methods.We experimented with two character sets for the SAR model. The first one is the simple character set consisting of a total of 41 sym- bols -alphabets a-z, digits 0-9, whitespace , and other punctuations. The second character set is the one used in [5], and includes separate character variants depending on position in a word -beginning, mid- dle, and end. It also includes special symbols for repeated characters, e.g. a separate symbol for ll. The intuition behind this character set is that its symbols capture more context as compared to the simple set, and also disambiguate legitimate character repetitions from double peaks emitted by the CTC model. We observed that the performance with the latter character set is slightly better than using simple char- acters. Hence, we present results only for this case.We restricted ourselves to the 300-hour set for experiments on the SAR model because it uses a 10,000 word vocabulary, leading to Conventional wisdom and prior research suggests that direct acoustic- to-word (A2W) models require orders of magnitude more data than sub-word unit-based models to perform competitively. This paper presents a recipe to train a A2W model on the 2000-hour Switch- board+Fisher data set that performs at-par with several state-of- the-art hybrid and end-to-end models using sub-word units. We conclude that data order, model initialization, and regularization are crucial to obtaining a competitive A2W model with a WER of 8.8%/13.9% on the Switchboard/CallHome subsets of the Hub5- 2000 test set. Next, we present a spell and recognize (SAR) model that learns to first spell a word and then recognize it. The pro- posed SAR model gives a rich and readable output to the user while maintaining the training/decoding simplicity and performance of a A2W model. We show some examples illustrating the SAR model's benefit for utterances containing OOV words.
