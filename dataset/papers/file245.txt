Autism Spectrum Disorders (ASDs) are defined as a range of developmental disability conditions that effect at some degree the social interaction and com- munication abilities of patients. Prevalence of ASD is reported to be 1 in 88 individuals [4]. ASD is generally characterized by restricted, repetitive, and stereotyped patterns of behavior in patients. Stereotypical Motor Movements (SMMs) in autism (such as body rocking, mouthing, and complex hand move- ments) [10] can significantly restrict the learning and social interactions. Further, SMMs frequently increase in occasion of emotional or sensory overload, which may lead to autistic meltdown events. Alleviating SMMs is thus one primary target of interventions on ASD, which requires accurate tools for recognizing and quantifying SMM patterns. In order to guide behavioral interventions [7] and possibly prevent SMM insurgence, it is worthwhile to consider the limita- tions of traditional methods for measuring SMM [7], e.g., paper-and-pencil rating scales, direct behavioral observation, and video-based coding. Measures by wire- less accelerometer sensing technology and machine learning techniques provide an automatic, time efficient, and accurate measure of SMM [2, 3, 5-8, 15-19, 22].Research by Goodwin et al. [7] showed the potential of automatic SMM detec- tion in real-life settings and the limits towards developing robust and adaptive real-time algorithms.As in many other signal processing applications, SMM detection is com- monly based on extracting ad-hoc ("handcrafted") features from the accelerom- eter signals. So far, a wide variety of feature extraction methods have been used in the literature. Generally, two types of features are extracted from the ac- celerometer signal [9]: i) time domain features; ii) frequency domain features. For time domain features, statistical features such as mean, standard deviation, zero-crossing, energy, and correlation are extracted from overlapping windows of signal. In frequency domain features, the discrete Fourier transform is used to estimate the power of different frequency bands. Recently, the Stockwell trans- form [20] has been proposed for feature extraction from inertial 3-axis accelerom- eter in order to provide better time-frequency resolution for non-stationary sig- nals [7]. Despite its popularity in movement analysis, manual feature extraction suffers from two main limitations [14]: a) the feature extraction phase is mainly based on generic researchers' domain knowledge rather than encoding movement information. Thus, characteristics of atypical movements may be missed, with- out coping with intra-subject and inter-subject variation; b) feature extraction is a computationally intensive step in the processing pipeline and such compu- tational cost limits the applicability of atypical movement detection in real-time scenarios.To overcome such limitations, we propose to use the deep learning paradigm in order to learn discriminating features for SMM pattern recognition. In par- ticular, we introduce a Convolutional Neural Network (CNN) deep learning model [13] to bypass the commonly used feature extraction procedure. The CNN can be employed to transform the multi-channel accelerometer signal into a re- duced set of features, and then an SVM classifier is used to classify the new representation of signal into SMM and no-SMM classes. The CNN architec- ture is shown to be an effective machine learning technique for a wide range of problems such as object recognition [12,21], speech processing [1], and affect recognition [14]. We hypothesize that the feature learning and transfer learning capabilities of CNN provide more accurate SMM detectors, as well as a platform for learning more robust representation of inertial signals, thus giving the capa- bility of effectively transforming this learned representation to a new dataset, which is essential in longitudinal studies. To the best of the authors' knowledge, the CNN model has not been applied in SMM detection applications so far.Let S i i i x , S y , S z ∈ R n×d be n samples of recorded signal by ith ∈ {1, 2, ..., s} accelerometer sensor with d sampling rate at x,y, and z directions, respectively. Assume Y n×1 ∈ {−1, 1} be the corresponding label vector for the recorded data where −1 and 1 represent no-SMM and SMM classes, respectively. Then let X ∈ R n×c×d be a 3-dimensional tensor matrix constructed by concatenating the signal of s accelerometer sensors along each sensor-direction dimension (see Figure 1), where c = s * 3 (3 is the number of directions, i.e. x,y, and z). With this simple encoding, we consider each direction of a sensor as an input data channel.where d is the length of input signal, c is the number of input channels, and F is the number of filters.Convolutional neural networks (CNNs) benefit from invariant local receptive fields, shared weights, and spatio-temporal sub-sampling features to provide ro- bustness over shift and distortion of the input space [13]. A typical CNN has a hierarchical architecture that alternates convolutional and pooling layers in or- der to summarize large input spaces with spatio-temporal relations into a lower dimensional feature space. A 1-dimensional convolutional layer C i contains a set of F filters, i.e. receptive fields,which learn different patterns on a time window of time-series, where f represents the size of each filter. The aim of the training phase is indeed to learn these filters from the input data. Each filter is convolved sequentially with the input signal across channels. Then, the output of the convolution operator is passed through an activation function to compute the feature maps M ∈ R n×F ×d . Generally, a rec- tified linear unit (ReLU) is used as an activation function in a deep architecture, where ReLU (a) = max{0, a}. To reduce the sensitivity of the output to shifts and distortions, feature maps are fed to an additional layer, called pooling layer, which performs a local averaging or sub-sampling. In fact, a pooling layer re- duces the resolution of a feature map by factor of 1 k where k is the stride size, i.e. the stride between successive pooling regions. Max-pooling and average-pooling are two commonly used pooling functions which compute maximum or average among values in a pooling window, respectively. This aggregation is separately performed inside each feature map and provides X ∈ R n×F × d k as the output of the pooling layer. X can be used as an input to another convolutional layer C i+1 in a multi-layer architecture. Figure 1 illustrates the basic architecture of one layer of a 1-dimensional convolutional layer.Here we consider a three-layer CNN to transform the time-series of multiple accelerometer sensors into a new feature space. The SMM-detector architec- ture is shown in Figure 2. Three convolutional layers C 1 , C 2 , C 3 have 4, 4, and 8 filters respectively, each with a length of 9 time-points (0.1 second); i.e.,. . , 8} . Each convolutional layer is followed by an average-pooling layer. The length of the pooling window and the pooling stride are fixed to 3 (p = 3) and 2, re- spectively. Pooling stride of 2 reduces the length of feature maps by factor of 0.5. The output of the third convolutional layer is connected to a flattening layer to provide the learned feature vector. In the learning phase, a fully-connected layer with 8 neurons connected to a softmax layer is employed for classification. To configure and train CNN networks, we applied the Deeppy library 1 , which provides a GPU based infrastructure for computation [11]. We used a dataset of accelerometer signals, collected from 6 subjects with autism in a longitudinal study [7] 2 . The data were collected in laboratory and classroom environments; the subjects wore three 3-axis wireless accelerometer sensors and engaged in SMMs (body rocking, hand flapping, or simultaneous body rocking and hand flapping) and non-SMM behaviors. The sensors were worn on the left wrist and right wrist using wristbands, and on the torso using a thin strip of comfortable fabric tied around the chest. To annotate the data, subject activ- ities were recorded with a video camera and analyzed by an expert. The first data collection (here called Study1 ), were recorded by MITes sensors at 60Hz sampling frequency [3]. The second dataset (Study2 ) was collected on the same subjects three years later by Wockets sensors with sampling frequency of 90Hz. To equalize sampling frequencies between two datasets, Study1 data are resam- pled to 90Hz with a linear interpolation. To remove DC component from signal, a 0.1Hz cut-off high pass filter is applied. Then, similar to [7], the signal is seg- mented to 1-second long (i.e. 90 time-points) using a sliding window. The sliding window is moved along the time dimension with 10 time-steps resulting in 0.87 overlap between consecutive windows. Considering the data are collected using 3 sensors, X is a n × 9 × 90 matrix. Due to the skewness of classes, same as [7], the training data are balanced to the number of samples in the minority class. After constructing the input matrix X, Zero Component Analysis (ZCA) is used to normalize the input data.To investigate the effect of feature learning and transfer learning via CNN in SMM detection, we conducted four experiments. In all experiments, the leave- one-subject-out scheme is used for model evaluation. For the sake of fair compar- ison with [7], Support Vector Machine (SVM) is used for classifying the learned features to target classes.Experiment 1: The aim of this experiment is to evaluate a baseline for the effect of feature extraction and feature learning on the classification performance. Therefore, without any feature extraction, samples of raw data are used as the input to the SVM classifier for SMM detection. In this case, all data channels of each sample in X are collapsed into a vector and a n × 810 (810 = 9 × 90) input matrix is constructed. We will refer to this experiment as "Raw". Experiment 2: In this setting, we replicated the third experiment in [7] using exactly the same implementation provided by the authors. All extracted features mentioned in [7] including time, frequency, and Stockwell transform features are used for the classification. We will refer to this experiment as "Goodwin et al.".The main aim of this experiment is to investigate the superi- ority of learning robust features over handcrafted features in the across-subject classification setting. To this end, the proposed CNN architecture is used to learn a middle representation of the accelerometer signals, i.e. to learn the features. In the training phase, one layer of 8 hidden neurons followed by two softmax neurons (since it is a binary classification problem) are attached densely to the last layer of CNN. All parameters of CNN, i.e. weights and biases, are initial- ized by drawing small random numbers from normal distribution. The stochastic gradient descent with momentum (fixed to 0.9) is used for training the CNN. Then an SVM classifier is used to classify the new learned feature space to the target labels. All these steps performed on the training set to ensure unbiased error estimation. Due to the random initialization of weights and employing stochastic gradient descent algorithm for optimization, results can be different from one run to another. Therefore, we repeated the whole procedure of learning and classification 15 times, and the errorbars are reported. This experiment is performed separately on Study1 and Study2 data and is referred as "CNN". Experiment 4: In this experiment, we investigate the possibility of transferring learned knowledge from one dataset to another. To this end, we firstly trained the CNN on one dataset, e.g., Study1, and then we used the learned parameters, i.e. filters and weights, for initializing the parameters of CNN in another dataset, e.g., Study2. In fact, we tried to transfer the learned representation from one study to another in a longitudinal study. We refer to this experiment as "Transferred- CNN". Table 1: F1-score of four experiments on 6 subjects of Study1 and Study2 datasets. Table 1 and Figure 3 summarize the results of four experiments. Due to the highly unbalanced sample in the test set, the evaluation is performed by computing F1- scores. The bar diagrams are representing the F1-score of four different methods on Study1 and Study2 datasets. The x-axis represents the subjects' ID (S1- S6) and the mean results over all subjects (M). In the case of the first and the second experiments, the result of experiments is deterministic in the leave- one-subject-out scenario. Therefore, no errorbar is reported. Examination of the mean performances on two datasets highlights the following remarks:1. The higher classification performance achieved by handcrafted and learned features with respect to the classification on the raw data illustrates the importance of feature extraction/learning for SMM detection. 2. Comparison between results achieved by Goodwin et al. and CNN/transferred- CNN demonstrates the efficacy of feature learning over the manual feature extraction in SMM detection. To better illustrate the superiority of learned features over handcrafted features, Figure 4 shows the distribution of SMM and no-SMM samples in 2-dimensional PCA space. Samples of two classes are less overlapped in the case of learned features compared to handcrafted features. 3. Finally, our results show that transferring the learned knowledge from one dataset to another, by pre-initializing CNN, can improve the classification performance in longitudinal studies. We proposed an original application of deep learning for SMM detection in ASD subjects using accelerometer sensors. To the best of our knowledge, this is the first effort toward applying deep learning paradigm for detecting SMMs in autism. Our experimental results showed that convolutional neural network outperforms the traditional classification on the handcrafted features. This ob- servation supports our initial hypotheses about effectiveness of embedded feature learning and transfer learning capabilities of deep framework in providing more accurate SMM detection systems. As future work, we plan to use the speed and adaptability power of the proposed framework in a real-time scenario.
