A number of studies that use deep learning approaches have claimed state-of-the-art performances in a considerable number of tasks such as image classification [9], natural language processing [15], speech recognition [5], and text classification [18]. These deep learn- ing models employ the conventional softmax function as the classi- fication layer.However, there have been several studies [2,3,12] on using a classification function other than Softmax, and this study is yet another addition to those.In this paper, we introduce the use of rectified linear units (ReLU) at the classification layer of a deep learning model. This approach is the novelty presented in this study, i.e. ReLU is conventionally used as an activation function for the hidden layers in a deep neural network. We accomplish this by taking the activation of the penul- timate layer in a neural network, then use it to learn the weight parameters of the ReLU classification layer through backpropaga- tion.We demonstrate and compare the predictive performance of DL-ReLU models with DL-Softmax models on MNIST [10], Fashion- MNIST [17], and Wisconsin Diagnostic Breast Cancer (WDBC) [16] classification. We use the Adam [8] optimization algorithm for learn- ing the network weight parameters.Cancer (WDBC). The WDBC dataset [16] consists of features which were computed from a digi- tized image of a fine needle aspirate (FNA) of a breast mass. There are 569 data points in this dataset: (1) 212 -Malignant, and (2) 357 -Benign.We normalized the dataset features using Eq. 1,where X represents the dataset features, µ represents the mean value for each dataset feature x (i) , and σ represents the correspond- ing standard deviation. This normalization technique was imple- mented using the StandardScaler [11] of scikit-learn.For the case of MNIST and Fashion-MNIST, we employed Princi- pal Component Analysis (PCA) for dimensionality reduction. That is, to select the representative features of image data. We accom- plished this by using the PCA [11] of scikit-learn.We implemented a feed-forward neural network (FFNN) and a con- volutional neural network (CNN), both of which had two different classification functions, i.e. (1) softmax, and (2) ReLU.2.4.1 Softmax. Deep learning solutions to classification prob- lems usually employ the softmax function as their classification function (last layer). The softmax function specifies a discrete prob- ability distribution for K classes, denoted by K k =1 p k . If we take x as the activation at the penultimate layer of a neural network, and θ as its weight parameters at the softmax layer, we have o as the input to the softmax layer, prediction units (see Eq. 6). The θ parameters are then learned by backpropagating the gradients from the ReLU classifier. To accom- plish this, we differentiate the ReLU-based cross-entropy function (see Eq. 7) w.r.t. the activation of the penultimate layer,Let the input x be replaced the penultimate activation output h,Consequently, we haveThe backpropagation algorithm (see Eq. 8) is the same as the conventional softmax-based deep neural network.Hence, the predicted class would bê yAlgorithm 1 shows the rudimentary gradient-descent algorithm for a DL-ReLU model.ReLU is an activation func- tion introduced by [6], which has strong biological and mathemati- cal underpinning. In 2011, it was demonstrated to further improve training of deep neural networks. It works by thresholding values at 0, i.e. f (x) = max(0, x). Simply put, it outputs 0 when x &lt; 0, and conversely, it outputs a linear function when x ≥ 0 (refer to Figure  1 for visual representation).Algorithm 1: Mini-batch stochastic gradient descent training of neural network with the rectified linear unit (ReLU) as its classification function.Any standard gradient-based learning algorithm may be used. We used adaptive momentum estimation (Adam) in our experiments.In some experiments, we found the DL-ReLU models perform on par with the softmax-based models. We propose to use ReLU not only as an activation function in each hidden layer of a neural network, but also as the classification function at the last layer of a network.Hence, the predicted class for ReLU classifier would bê y,To evaluate the performance of the DL-ReLU models, we employ the following metrics:  The table for describing classification performance.2.4.3 Deep Learning using ReLU. ReLU is conventionally used as an activation function for neural networks, with softmax being their classification function. Then, such networks use the softmax cross-entropy function to learn the weight parameters θ of the neural network. In this paper, we still implemented the mentioned loss function, but with the distinction of using the ReLU for theAll experiments in this study were conducted on a laptop computer with Intel Core(TM) i5-6300HQ CPU @ 2.30GHz x 4, 16GB of DDR3 RAM, and NVIDIA GeForce GTX 960M 4GB DDR5 GPU. Table 1 shows the architecture of the VGG-like CNN (from Deep Learning using Rectified Linear Units (ReLU) , ,Keras [4]) used in the experiments. The last layer, dense_2, used the softmax classifier and ReLU classifier in the experiments. The Softmax-and ReLU-based models had the same hyper- parameters, and it may be seen on the Jupyter Notebook found in the project repository: https://github.com/AFAgarap/relu-classifier.  Table 2 shows the architecture of the feed-forward neural net- work used in the experiments. The last layer, dense_6, used the softmax classifier and ReLU classifier in the experiments.  In training a VGG-like CNN [4] for MNIST classification, we found the results described in Table 4.We implemented both CNN and FFNN defined in Tables 1 and 2 on a normalized, and PCA-reduced features, i.e. from 28 × 28 (784) dimensions down to 16 × 16 (256) dimensions.In training a FFNN with two hidden layers for MNIST classifica- tion, we found the results described in Table 3.Despite the fact that the Softmax-based FFNN had a slightly higher test accuracy than the ReLU-based FFNN, both models had 0.98 for their F1-score. These results imply that the FFNN-ReLU is on par with the conventional FFNN-Softmax. Figures 2 and 3 show the predictive performance of both models for MNIST classification on its 10 classes. Values of correct pre- diction in the matrices seem to be balanced, as in some classes,   The CNN-ReLU was outperformed by the CNN-Softmax since it converged slower, as the training accuracies in cross validation were inspected (see Table 5). However, despite its slower convergence, it was able to achieve a test accuracy higher than 90%. Granted, it is lower than the test accuracy of CNN-Softmax by ≈ 4%, but further optimization may be done on the CNN-ReLU to achieve an on-par performance with the CNN-Softmax.   We implemented both CNN and FFNN defined in Tables 1 and 2 on a normalized, and PCA-reduced features, i.e. from 28 × 28 (784) dimensions down to 16 × 16 (256) dimensions. The dimensionality reduction for MNIST was the same for Fashion-MNIST for fair comparison. Though this discretion may be challenged for further investigation.In training a FFNN with two hidden layers for Fashion-MNIST classification, we found the results described in Table 6.Despite the fact that the Softmax-based FFNN had a slightly higher test accuracy than the ReLU-based FFNN, both models had 0.89 for their F1-score. These results imply that the FFNN-ReLU is on par with the conventional FFNN-Softmax.    7 show the predictive performance of both models for Fashion-MNIST classification on its 10 classes. Values of correct prediction in the matrices seem to be balanced, as in some classes, the ReLU-based FFNN outperformed the Softmax-based FFNN, and vice-versa.In training a VGG-like CNN [4] for Fashion-MNIST classification, we found the results described in Table 7.Similar to the findings in MNIST classification, the CNN-ReLU was outperformed by the CNN-Softmax since it converged slower, as the training accuracies in cross validation were inspected (see Table 8). Despite its slightly lower test accuracy, the CNN-ReLU had the same F1-score of 0.86 with CNN-Softmax -also similar to the findings in MNIST classification. Figures 8 and 9 show the predictive performance of both models for Fashion-MNIST classification on its 10 classes. Contrary to the findings of MNIST classification, CNN-ReLU had the most number of correct predictions per class. Conversely, with its faster conver- gence, CNN-Softmax had the higher cumulative correct predictions per class.     We implemented FFNN defined in Table 2, but with hidden layers having 64 neurons followed by 32 neurons instead of two hidden layers both having 512 neurons. For the WDBC classfication, we only normalized the dataset features. PCA dimensionality reduction might not prove to be prolific since WDBC has only 30 features.In training the FFNN with two hidden layers of [64,32] neurons, we found the results described in Table 9. Figures 10 and 11 show the predictive performance of both mod- els for WDBC classification on binary classification. The confusion matrices show that the FFNN-Softmax had more false negatives than FFNN-ReLU. Conversely, FFNN-ReLU had more false positives than FFNN-Softmax.The relatively unfavorable findings on DL-ReLU models is most probably due to the dying neurons problem in ReLU. That is, no gradients flow backward through the neurons, and so, the neurons become stuck, then eventually "die". In effect, this impedes the Deep Learning using Rectified Linear Units (ReLU) , , learning progress of a neural network. This problem is addressed in subsequent improvements on ReLU (e.g. [13]). Aside from such drawback, it may be stated that DL-ReLU models are still compa- rable to, if not better than, the conventional Softmax-based DL models. This is supported by the findings in DNN-ReLU for image classification using MNIST and Fashion-MNIST.Future work may be done on thorough investigation of DL-ReLU models through numerical inspection of gradients during backprop- agation, i.e. compare the gradients in DL-ReLU models with the gradients in DL-Softmax models. Furthermore, ReLU variants may be brought into the table for additional comparison.
