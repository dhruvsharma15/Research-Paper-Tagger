The core idea of hierarchical reinforcement learning is to break down the reinforcement learning problem into sub- tasks through a hierarchy of abstractions. In terms of Markov Decision Processes (MDP), a well studied framework in re- inforcement learning literature (Sutton and Barto 1998), one way of looking at the full reinforcement learning problem is to assume that the agent is in one state of the MDP at each time step. The agent then performs one of several possible primitive actions which along with the current state decides the next state. However, for large problems, this can lead to too much granularity: when the agent has to decide on each• Identifying bottlenecks in the state space, where the state space is partitioned into sets. The transitions between two sets of states that are rare introduce bottleneck states at the respective points of such transitions. Policies to reach such bottleneck states are cached as options -for example, (McGovern and Barto 2001).• Using the structure present in a factored state representa- tion to identify sequences of actions that cause what are otherwise infrequent changes in the state variables: these sequences are cached away as options (Hengst 2004).• * -Equal Contribution• Obtaining a graphical representation of an agent's inter- action with its environment and using betweenness cen- trality measures to identify subtasks (S ¸ ims¸ekims¸ek and Barreto 2009).• † -Work done while authors were at IIT Madras• Using clustering methods (spectral or otherwise) to sepa- rate out different strongly connected components of the MDP and identifying bottlenecks that connect different clusters (Menache, Mannor, and Shimkin 2002).There are certain deficiencies with the above methods, even though they have had varying degrees of success. Bot- tleneck based approaches do not have a natural way of iden- tifying the part of the state space where options are applica- ble without external knowledge about the problem domain. Spectral methods need some form of regularization in order to prevent unequal splits that might lead to arbitrary split- ting of the state space. There have also been attempts to learn skill acquisition in robotics. ( Konidaris et al. 2011) attempt to discover skills for a robot given abstractions of the surroundings it has to operate with. (Ranchod, Rosman, and Konidaris 2015) try to use inverse reinforcement learn- ing for skill discovery with reward segmentation. However, the above two works assume the knowledge of the abstrac- tions of the world and demonstrations of expert trajectories respectively while our goal is to acquire skills and discover the abstractions without any prior knowledge.We present a framework that detects well-connected or meta-stable regions of the state space from an MDP model estimated from trajectories. We use PCCA+, a spec- tral clustering algorithm from conformal dynamics (Weber, Rungsarityotin, and Schliep 2004) that not only partitions the MDP into different regions but also returns the connec- tivity information between the regions, unlike other cluster- ing approaches used earlier for option discovery. This helps us to build an abstraction of the MDP, where we call the regions identified by PCCA+ as abstract states. Then we define options that take an agent from one abstract state to another connected abstract state. Since PCCA+ also re- turns the membership function between states and abstract states, we propose a very efficient way of constructing op- tion policies directly from the membership function, which is to perform hill climbing at the granular states on the mem- bership function of the destination abstract state, to yield the option policy without further learning. Since the abstract states are aligned with the underlying structure of the MDP the same option policies could be efficiently reused across multiple tasks in the same MDP. Once we have these op- tions, we could use standard reinforcement learning algo- rithms to learn a policy over these subtasks to solve a given task. Specifically, we use SMDP Q-Learning (Sutton, Pre- cup, and Singh 1999) and Intra Option Q-Learning (Sutton, Precup, and Singh ) for our experiments. Note that our ap- proach works well even without the exact model of the full MDP, by being able to work on approximate estimate of the model using sample trajectories, as demonstrated by the ex- periments.Finally, we present our attempt to extend our pipeline to large state spaces where spectral methods on the original state space are infeasible. We therefore propose to operate the PCCA+ pipeline on an aggregated state space. The orig- inal state space is aggregated through clustering on the rep- resentation of the state space learned using deep neural net- works. Specifically, we try to learn a representation that cap- tures spatio-temporal structure of the state-space, for which we borrow the framework of ( Oh et al. 2015) which uses a Convolutional-LSTM Action Conditional Video Prediction Network to predict the next frames of the game conditioned on the trajectory so far. We use the Arcade Learning Envi- ronment (ALE) platform ( Bellemare et al. 2012) which pro- vides a simulator for Atari 2600 games. We present our re- sults on the Atari 2600 game Seaquest which is a relatively complex game and requires abstract moves like filling up oxygen, evading bullets, shooting enemies.We summarize our key contributions in this paper below:• A novel automated skill acquisition pipeline to operate without prior knowledge or abstractions of the world. The pipeline uses a spectral clustering algorithm from confor- mal dynamics -PCCA+, which builds an abstraction of the MDP by segmenting the MDP into different regions called abstract states and also provides the membership function between the MDP states and abstract states.• An elegant way of composing options (sequence of ac- tions to move from one abstract state to another) from the membership function so obtained by doing hill climbing on the membership function of the target abstract state at the granular states belonging to the current abstract state.• Extension of the pipeline to complex tasks with large state spaces, for which we propose to run the pipeline on an aggregated state space due to infeasibility to operate on the original state space of the MDP. To aggregate the state space, we learn a representation capturing spatio-temporal aspects of the state space using a deep action-conditional video prediction network.Our approach can be considered as an attempt to address automated option discovery in RL using spatio-temporal clustering: Spatial because the abstract states are segment- ing the state space into abstractions (clusters); and Temporal since the transition structure of the MDP is used to discover these abstractions.Markov Decision Process (MDP) is a widely used frame- work on which reinforcement learning algorithms are used to learn control policies. A finite discrete MDP is formalised as M = (S, A, P, R) with S with a finite discrete state space S, finite action space A, a distribution over the next states P (s, a, s ) when an action a is performed at state s, and a corresponding reward R(s, a, s ) specifying a scalar cost or reward associated with that transition. A policy is a sequence of actions involved in performing a task on the MDP, while value functions are a measure of the expected long term re- turn in following a policy in the MDP.The option framework is one of the formalisations used to represent hierarchies in RL (Sutton, Precup, and Singh 1999). Formally, an option is a tuple O = (I, µ, β) here:• I is the initiation set: a set of states from which the action can be activated• µ is a policy function where µ(s, a) represents the pref- erence value given to action a in state s following option O.• β is the termination function: When an agent enters a state s while following option O, the option could be termi- nated with a probability β(s).When the agent is in state s where it can start an option, it can choose from all the options O for which s ∈ I(O) and all primitive actions that can be taken in s. This choice is dictated by the policy guiding the agent. on the transformed basis, which are identified as vertices of a simplex in the R k subspace (the Spectral Gap method is used to estimate the number of clusters k). For the first or- der perturbation, the simplex is just a linear transformation around the origin and to find the simplex vertices, one needs to find the k points which form a convex hull such that the deviation of all the points from this hull is minimized. This is achieved by finding the data point which is located farthest from the origin and iteratively identify data points which are located farthest from the hyperplane fit to the current set of vertices. Refer to Algorithm 1 for details.An MDP appended with a set of options is a Semi-Markov Decision Process (SMDP). In an SMDP, at each time step, the agent selects an option which can be initiated at that state and follows the corresponding option policy until termina- tion. In SMDP theory, the effects of an option are modelled using r o s and p o ss which represent the total return and the probability of terminating at state s for an option o initi- ated at state s. Each option is viewed as an indivisible unit and its structure not utilized. The update rule of SMDP Q- learning (Sutton, Precup, and Singh 1999) is given bywhere Q is the action/option value function.A major drawback of SMDP learning methods is that the option needs to be completely executed upto termination be- fore learning about its outcome. At any point of time, we would potentially be learning about the value function of only option or a primitive action. This slows down learning significantly for large problems of the scale of Atari 2600 games. (Sutton, Precup, and Singh ) propose to use all op- tions that are consistent with an observed trajectory and have their values updated efficiently. This can help learn the val- ues of certain options without even executing those options. Let o be an option chosen at state s t at time t which termi- nates at time t+τ . Instead of using a single training example to update Q(s t , o) as in SMDP methods, the Markov nature of the option o is exploited and each of the transitions in the τ time steps are valid training examples. The Q-value of op- tions o consistent with actions taken during those time steps are also updated allowing for efficient off-policy learning.Figure 1: Simplex First order and Higher order Perturba- tion: Shows the visualization of the first order perturbation assumption to identify the simplex vertices on data points which have higher order perturbations. The first case shows data points which satisfy the first order assumption and hence the simplex fits perfectly without any noise. In the second case, the simplex vertices obtained through the lin- ear transformation are only able to capture the structure with some noise and these deviations could be understood as the effect of the higher order perturbations present.Compute n (number of vertices) eigenvalues of L in de- scending order 3: Choose first k eigenvalues for whichPerron Cluster Analysis (PCCA+)Given an algebraic representation of the graph represent- ing an MDP, we want to find suitable abstractions aligned to the underlying structure. We use a spectral clustering al- gorithm to do this. Central to the idea of spectral clustering is the graph Laplacian which is obtained from the similar- ity graph. In this approach, the spectra of the Laplacian L (derived from the adjacency matrix S) is constructed and the best transformation of the spectra is found such that the transformed basis aligns itself with the clusters of data points in the eigenspace. A projection method described in (Weber, Rungsarityotin, and Schliep 2004) is used to find the mem- bership of each of the states to a set of k special points lying  place of other clustering algorithms. This is inpired by (We- ber, Rungsarityotin, and Schliep 2004) who use PCCA+ to detect the conformal states of a dynamical system by op- erating on the transition structure. On providing the graph (transition structure) of the MDP to PCCA+, we derive the abstraction of the MDP where the simplex vertices identi- fied by PCCA+ are the abstract states. We also get a mem- bership function from PCCA+, χ, which defines the degree of membership of each state s to an abstract state S. We de- scribe how this information can used to compose options in the next section.Composing Options from PCCA+ Comparison to other clustering algorithms We are in particular concerned with dynamical systems where there are no a priori assumptions on the size and the relative place- ments of the metastable states. So it becomes essential for the clustering algorithm to work in a generalized setting as far as possible. We tested different spectral clustering algo- rithms on different classes of problems. PCCA+ was found to give better results (Fig 2) in capturing the structure (topol- ogy). Normalized Cut (NCut) Algorithms by (Shi and Malik 2000),(Ng and others 2002) and PCCA+ (Weber, Rungsar- ityotin, and Schliep 2004) have deep similarities, but the main difference behind the usage of PCCA+ is the change of point of view for identification of metastable states from crisp clustering to that of relaxed almost invariant sets. Both the methods are similar till the identification of the eigen- vector matrix as points in n dimensional subspace but after this, (Shi and Malik 2000),(Ng and others 2002) use stan- dard clustering algorithms like K-Means while PCCA+ uses a mapping to the simplex structure. Since the bottlenecks occur rather rarely, it is shown by (Weber, Rungsarityotin, and Schliep 2004) that in general for such cases, soft meth- ods outperform crisp clustering. Although the other spectral algorithms result in good clusters for the simple room-in-a- room domain, they give poor results on more topologically complex spaces and need some form of regularization to work well in such settings (Fig 2). PCCA+ has an intrin- sic regularization mechanism by which it is able to cluster complex spaces neatly.Given the membership functions and the abstractions dis- covered by PCCA+, we provide a very elegant way to com- pose option policies to move from one abstract state to an- other. The transition to another abstract state is done simply by following the positive gradient (hill climbing) of the mem- bership value to the destination abstract state. This navi- gates the agent through states whose membership functions to the target abstract state progressively increases. Specifically, consider that we have N states s 1 , s 2 , s 3 , · · · , s N and k abstract states S 1 , S 2 , · · · , S k . Typically, N k. Let χ ij denote the membership of state s i to the abstract state S j . s i is said to belong to abstract state S a , where a = argmax j χ ij . Thus, any option between abstract states S i and S j will start at a state in S i and end at a state in S j . The connectivity information between two abstract states (S i , S j ) is given by the (i, j) th entry of χ T Lχ where L is the Laplacian corresponding to the MDP transition matrix. The diagonal entries provide the relative connectivity information within a cluster. We generate an option for every pair of connected abstract states. For an option from S i to S j :• The initiation set I represents states that belong to S.• The option policy µ(s, a) that takes the agent from ab- stract state S i to S j is a stochastic gradient function given by:where Spatial Abstraction using PCCA+ The transition struc- ture of the MDP could be arbitrarily complex and hence, to identify abstractions that are well aligned to the state space transition structure, we use PCCA+ to abstract the MDP in• Finally, termination condition β is a function which as- signs the probability of termination of the current option at a state s. It could also be viewed as the probability of a state s being decision epoch given the current option be- ing executed. For an option taking the agent from abstract state S i to S j , we define β as follows:We offer an intuitive explanation for the choice of the option policy µ and option termination condition β. χ Sj Si (s, a) is the expected increase in the membership func- tion to abstract state S j on taking action a at state s, while χ Sj (s) is the membership function to S j at current state s. The probability of an action a involved in taking the agent from state s ∈ S i to an abstract state S j must proportional to the expected increase in the membership function to the abstract state S j if positive and 0 if the expected increase is negative. The choice for the termination condition also has a simple heuristic: When transitioning from S i to S j , the agent would encounter bottleneck state(s) s * which would approximately satisfy χ Si (s) = χ Sj (s), and hence be suit- able for the termination of the option. Till then, the member- ship value of S i would be higher than S j (with the difference gradually reducing along the option trajectory), and hence the probability to terminate would be proportional to how much of a bottleneck the current state s is. Even though we do not look for bottleneck states directly in our approach un- like (McGovern and Barto 2001), the termination condition we propose naturally captures transitioning through bottle- neck states. Estimate model on microstate space from sample tra- jectories 6: Operate PCCA+ on estimated model to derive ab- stract macrostates and memberships 7: Discover options from the abstract macrostates and memberships 8:Augment agent with new options 9:Update value functions and behavioral policy using Intra-Option Q Learning with DQN 10: end while sufficient portion of the state space is seen for learning a good enough aggregate space for PCCA+ to operate on.The previous section discussed how to discover options given that abstractions are obtained from the transition struc- ture of the MDP. However, an online agent has no prior knowledge of the model and has to learn these abstractions from scratch. We present Algorithms 2 and 3 respectively for an online agent to perform Option Discovery using Spatio- Temporal Clustering (ODSTC) for two cases: i) Small state space task ii) Large State Space Task (where it is infeasible to do model estimation on the original state space).For every sampled trajectory, we maintain transition counts φ Estimate model from sample trajectories s U (s,a,s ) . We then use an SMDP value learning algo- rithm to find the optimal policy over the constructed options.The underlying transition structure φ 4:Operate PCCA+ on estimated model to derive ab- stract states and memberships 5:Discover options from the abstract states and mem- berships 6:Augment agent with new options 7:Update value functions and behavioral policy using SMDP Q Learning 8: end whileHere are two caveats in Algorithm 3:• It might be too expensive to perform PCCA+ for large scale problems after every episode and hence we could update the skills or options from PCCA+ after a fixed number of episodes depending on the problem complex- ity.• Though for small scale problems, the first initial explo- ration could come from random policies, this wouldn't work for large problems like Seaquest, where an in- formed exploration with a partially trained Deep Q Net- work (DQN) ( Mnih et al. 2015) is necessary to ensure ss encodes only the topological properties of the state space. There are other kinds of structures in the environments which we would like an autonomous agent to discover. For example, a monkey climbing a tree to pick up a fruit abstracts the task as to reach the branch of the tree nearest to the fruit. These structures are functional in nature, depending on the functional prop- erties of the task. We use reward counts for each transition to encode the functional properties in the transition struc- ture. The idea is that the spatio-temporal abstractions should degenerate in places where there is a spike in the reward dis- tribution, so that our agent interprets a state of high reward (as goal states typically are) as a different abstract state, and naturally composes options that would lead it to that state. Let ss | where v is the regularization constant that balances the relative weight of the underlying reward and transition distribution. We use an exponential weighting for rewards to ensure that the ad- jacency function has a very low value at the spike points where we want to abstraction to degenerate. It also returns a  For the current task, let us assume that the goal of the agent is to start from the tile marked S and reach the goal tile marked G. The agent can either move a step in the direction towards north, east, west or south. PCCA+ discovers three abstract states without incorporat- ing the reward structure, each corresponding to one room in the 3-room world. On incorporation of the reward structure, there would be 4 abstract states, with the additional one cor- responding to the goal state alone. The doorway could be seen as bottleneck states at which the membership functions lead to termination conditions of options exiting one abstract state (current room) and entering another abstract (destina- tion room). The navigation task could simply be abstracted as exiting the current room by moving to the doorway and entering the correct room to reach the goal state in it. We visualize the membership function identified by PCCA+ on the 3-room world in Fig 3b without reward structure incor- poration. It is clear that each abstract state can be identi- fied with one of the three rooms in the world and the MDP transition structure has been neatly segmented by PCCA+. Fig 3c shows the MDP segmentation and the option policies discovered without and with the reward structure incorpo- rated. For the first case (left), without the reward structure, there are only 3 abstract states corresponding to each room and hence the option policies discovered would be naviga- tion from one room to another. The figure shows stochastic option policies to navigate to room 3 from the other two rooms. In the second case (right), with the reward (goal) considered in the transition structure, 4 abstract states are identified, with the new one lonely being the goal state in the bottom right corner as indicated. The figure shows the (stochastic) option policies in getting to room 3 from rooms 1 and 2, and in getting from the group of granular states other than the goal state in room 3 to goal state. We hence clearly see the advantage of discovering these structural (from tran- sition structure) and functional (from reward structure) ab- stractions since the solving the task now is as simple as plan- ning to move from one abstract state to another instead of trying to learn the optimal policy at every granular state in the MDP. Seaquest: Atari 2600 games have been one of the widely used benchmarks for reinforcement learning algorithms on large state spaces ( Mnih et al. 2015), (Mnih et al. 2016), etc. Due to infeasibility in applying the PCCA+ framework directly on the exponential state space, we perform a state space aggregation by using K-Means clustering. However, aggregating directly on the input pixel space (210 × 160 × 3) in Seaquest is infeasible and no latent concepts (spatial and temporal) are captured in such a space. We therefore adopt the Action Conditional Video Prediction Network of ( Oh et al. 2015) for learning a latent space that captures the spatio-temporal concepts. The overall pipeline is described in Algorithm 3. Fig 4 helps to appreciate the semantics of an example option generated in Seaquest. We plot the agent's membership in the abstract state corresponding to particular option for a randomly chosen episode's trajectory. The skill learnt here corresponds to resurfacing to replenish oxygen in Seaquest.In this paper, we have presented a new framework for auto- mated skill acquisition in reinforcement learning that builds an abstraction over the MDP well aligned to the transition structure and discovers skills or options that enable move- ment across these abstract states. The framework works without an exact model of the MDP and the option policies are obtained in a very elegant way by performing hill climb- ing on the membership function of the abstract states. These options can also be efficiently reused across multiple tasks sharing a common transition structure. We also discuss an approach to scaling the pipeline to a larger state spaces by performing state aggregation through clustering on the rep- resentation discovered by an unsupervised model prediction network.
