We begin by describing our problem space and then detail our policy architecture, demonstrating how we can train it using policy gradient methods in an actor-critic framework. We demonstrate the effectiveness of our policy on various tasks with up to one million actions, but with the intent that our approach could scale well beyond millions of actions.Advanced AI systems will likely need to reason with a large number of possible actions at every step. Recommender systems used in large systems such as YouTube and Ama- zon must reason about hundreds of millions of items every second, and control systems for large industrial processes may have millions of possible actions that can be applied at every time step. All of these systems are fundamentallyWe consider a Markov Decision Process (MDP) where A is the set of discrete actions, S is the set of discrete states, P : S × A × S → R is the transition probability distribution, R : S × A → R is the reward function, and γ ∈ [0, 1] is a discount factor for future rewards. Each action a ∈ A corresponds to an n-dimensional vector, such that a ∈ R n . This vector provides information related to the action. In the same manner, each state s ∈ S is a vector s ∈ R m .*Equal contribution.The return of an episode in the MDP is the discounted sum of rewards received by the agent during that episode: R t = T i=t γ i−t r(s i , a i ). The goal of RL is to learn a policy π : S → A which maximizes the expected return over all episodes, E[R 1 ]. The state-action value function Q π (s, a) = E[R 1 | s 1 = s, a 1 = a, π] is the expected re-turn starting from a given state s and taking an action a, following π thereafter. Q π can be expressed in a recursive manner using the Bellman equation:In this paper, both Q and π are approximated by parametrized functions.There are two primary families of policies often used in RL systems: value-based, and actor-based policies.For value-based policies, the policy's decisions are directly conditioned on the value function. One of the more com- mon examples is a policy that is greedy relative to the value function:a∈AIn the common case that the value function is a parame- terized function which takes both state and action as input, | A | evaluations are necessary to choose an action. This quickly becomes intractable, especially if the parameter- ized function is costly to evaluate, as is the case with deep neural networks. This approach does, however, have the desirable property of being capable of generalizing over actions when using a smooth function approximator. If a i and a j are similar, learning about a i will also inform us about a j . Not only does this make learning more efficient, it also allows value-based policies to use the action features to reason about previously unseen actions. Unfortunately, execution complexity grows linearly with | A | which ren- ders this approach intractable when the number of actions grows significantly. In a standard actor-critic approach, the policy is explicitly defined by a parameterized actor function: π θ : S → A.In practice π θ is often a classifier-like function approxima- tor, which scale linearly in relation to the number of ac- tions. However, actor-based architectures avoid the com- putational cost of evaluating a likely costly Q-function on every action in the arg max in Equation (1). Nevertheless, actor-based approaches do not generalize over the action space as naturally as value-based approaches, and cannot extend to previously unseen actions.We propose a new policy architecture which we call the Wolpertinger architecture. This architecture avoids the heavy cost of evaluating all actions while retaining general- ization over actions. This policy builds upon the actor-critic (Sutton &amp; Barto, 1998) framework. We define both an effi- cient action-generating actor, and utilize the critic to refine our actor's choices for the full policy. We use multi-layer neural networks as function approximators for both our ac- tor and critic functions. We train this policy using Deep Deterministic Policy Gradient ( Lillicrap et al., 2015).The Wolpertinger policy's algorithm is described fully in Algorithm 1 and illustrated in Figure 1. We will detail these in the following sections.State s previously received from environment.Apply a to environment; receive r, s . Sub-linear complexity relative to the action space and an ability to generalize over actions are both necessary to handle the tasks we interest ourselves with. Current ap- proaches are not able to provide both of these, which moti- vates the approach described in this paper.Our architecture reasons over actions within a continuous space R n , and then maps this output to the discrete action set A. We will first define:f θ π is a function parametrized by θ π , mapping from the state representation space R m to the action representation space R n . This function provides a proto-action in R n for a given state, which will likely not be a valid action, i.e. it is likely thatâthatˆthatâ / ∈ A. Therefore, we need to be able to map fromâfromˆfromâ to an element in A. We can do this with:As we demonstrate in Section 7, this second pass makes our algorithm significantly more robust to imperfections in the choice of action representation, and is essential in mak- ing our system learn in certain domains. The size of the generated action set, k, is task specific, and allows for an explicit trade-off between policy quality and speed.g k is a k-nearest-neighbor mapping from a continuous space to a discrete set 1 . It returns the k actions in A that are closest tô a by L 2 distance. In the exact case, this lookup is of the same complexity as the arg max in the value-function derived policies described in Section 3, but each step of evaluation is an L 2 distance instead of a full value-function evaluation. This task has been extensively studied in the approximate nearest neighbor literature, and the lookup can be performed in an approximate manner in logarithmic time (Muja &amp; Lowe, 2014). This step is de- scribed by the bottom half of Figure 1, where we can see the actor network producing a proto-action, and the k-nearest neighbors being chosen from the action embedding.Although the architecture of our policy is not fully differ- entiable, we argue that we can nevertheless train our policy by following the policy gradient of f θ π . We will first con- sider the training of a simpler policy, one defined only as˜π as˜ as˜π θ = g • f θ π . In this initial case we can consider that the policy is f θ π and that the effects of g are a deterministic aspect of the environment. This allows us to maintain a standard policy gradient approach to train f θ π on its outputâ outputˆoutputâ, effectively interpreting the effects of g as environmen- tal dynamics. Similarly, the arg max operation in Equation (2) can be seen as introducing a non-stationary aspect to the environmental dynamics.The training algorithm's goal is to find a parameterized policy π θ * which maximizes its expected return over the episode's length. To do this, we find a parametrization θ * of our policy which maximizes its expected return over an episode:Depending on how well the action representation is struc- tured, actions with a low Q-value may occasionally sit clos- est tô a even in a part of the space where most actions have a high Q-value. Additionally, certain actions may be near each other in the action embedding space, but in certain states they must be distinguished as one has a particularly low long-term value relative to its neighbors. In both of these cases, simply selecting the closest element tô a from the set of actions generated previously is not ideal.We perform this optimization using Deep Deterministic Policy Gradient (DDPG) ( Lillicrap et al., 2015) to train both f θ π and Q θ Q . DDPG draws from two stability- inducing aspects of Deep Q-Networks ( Mnih et al., 2015) to extend Deterministic Policy Gradient ( Silver et al., 2014) to neural network function approximators by introducing a replay buffer (Lin, 1992) and target networks. DPG is sim- ilar to work introduced by NFQCA (Hafner &amp; Riedmiller, 2011) and leverages the gradient-update originally intro- duced by ADHDP ( Prokhorov et al., 1997).To avoid picking these outlier actions, and to generally im- prove the finally emitted action, the second phase of the algorithm, which is described by the top part of Figure 1, refines the choice of action by selecting the highest-scoring action according to Q θ Q :The goal of these algorithms is to perform policy iteration by alternatively performing policy evaluation on the cur- rent policy with Q-learning, and then improving upon the current policy by following the policy gradient.This equation is described more explicitly in Algorithm 1.It introduces π θ which is the full Wolpertinger policy. The parameter θ represents both the parameters of the action generation element in θ π and of the critic in θThe critic is trained from samples stored in a replay buffer ( Mnih et al., 2015). Actions stored in the replay buffer are generated by π θ π , but the policy gradient ∇ a Q θ Q (s, a) is taken atâatˆatâ = f θ π (s). This allows the learning algorithm to leverage the otherwise ignored information of which action was actually executed for training the critic, while taking the policy gradient at the actual output of f θ π . The target action in the Q-update is generated by the full policy π θ and not simply f θ π .A detailed description of the algorithm is available in the 1 For k = 1 this is a simple nearest neighbor lookup.supplementary material.nary decisions to act. This learns a factorized value func- tion from which a greedy policy can be derived for each subspace. This amounts to performing log(| A |) binary op- erations on each action-selection step.Time-complexity of the above algorithm scales linearly in the number of selected actions, k. We will see that in prac- tice though, increasing k beyond a certain limit does not provide increased performance. There is a diminishing re- turns aspect to our approach that provides significant per- formance gains for the initial increases in k, but quickly renders additional performance gains marginal.Consider the following simplified scenario. For a random proto-actionâactionˆactionâ, each nearby action has a probability p of being a bad or broken action with a low value of Q(s, ˆ a)−c. The values of the remaining actions are uniformly drawn from the interval, where b ≤ c. The probability distribution for the value of a chosen action is therefore the mixture of these two distributions.A similar approach was proposed which leverages Error- Correcting Output Code classifiers (ECOCs) (Dietterich &amp; Bakiri, 1995) to factorize the policy's action space and al- low for parallel training of a sub-policy for each action sub- space ( Dulac-Arnold et al., 2012) . In the ECOC-based ap- proach case, a policy is learned through Rollouts Classifi- cation Policy Iteration (Lagoudakis &amp; Parr, 2003), and the policy is defined as a multi-class ECOC classifier. Thus, the policy directly predicts a binary action code, and then a nearest-neighbor lookup is performed according to Ham- ming distance.Lemma 1. Denote the closest k actions as integers {1, . . . , k}. Then in the scenario as described above, the expected value of the maximum of the k closest actions isBoth these approaches effectively factorize the action space into log(| A |) binary subspaces, and then reason about these subspaces independently. These approaches can scale to very large action spaces, however, they require a binary code representation of each action, which is difficult to de- sign properly. Additionally, the generalized value-function approach uses a Linear Program and explicitly stores the value function per state, which prevents it from generaliz- ing over a continuous state space. The ECOC-based ap- proach only defines an action producing policy and does not allow for refinement with a Q-function.The highest value an action can have is Q(s, ˆ a) + b. The best action within the k-sized set is thus, in expectation,1−p smaller than this value. The first term is in O(p k ) and decreases exponentially with k. The second term is in O( 1 k+1 ). Both terms decrease a relatively large amount for each additional action while k is small, but the marginal returns quickly diminish as k grows larger. This property is also observable in experiments in Section 7, notably in Figures 6 &amp; 7. Using 5% or 10% of the maximal number of actions the performance is similar to when the full action set is used. Using the remaining ac- tions would result in relatively small performance benefits while increasing computational time by an order of magni- tude.These approaches cannot naturally deal with discrete ac- tions that have associated continuous representations. The closest approach in the literature uses a continuous-action policy gradient method to learn a policy in a continuous action space, and then apply the nearest discrete action (Van Hasselt et al., 2009). This is in principle similar to our approach, but was only tested on small problems with a uni-dimensional continuous action space (at most 21 dis- crete actions) and a low-dimensional observation space. In such small discrete action spaces, selecting the nearest dis- crete action may be sufficient, but we show in Section 7 that a more complex action-selection scheme is necessary to scale to larger domains.The proof to Lemma 1 is available in the supplementary material.There has been limited attention in the literature with re- gards to large discrete action spaces within RL. Most prior work has been concentrated on factorizing the action space into binary subspaces. Generalized value functions were proposed in the form of H-value functions (Pazis &amp; Parr, 2011), which allow for a policy to evaluate log(| A |) bi- Recent work extends Deep Q-Networks to 'unbounded' ac- tion spaces (He et al., 2015), effectively generating action representations for any action the environment provides, and picking the action that provides the highest Q. How- ever, in this setup, the environment only ever provides a small (2-4) number of actions that need to be evaluated, hence they do not have to explicitly pick an action from a large set.This policy architecture has also been leveraged by the au- thors for learning to attend to actions in MDPs which take in multiple actions at each state (Slate MDPs) ( Sunehag et al., 2015).We evaluate the Wolpertinger agent on three environment classes: Discretized Continuous Control, Multi-Step Plan- ning, and Recommender Systems. These are outlined be- low:sible n-length action sequences. We have 2 base actions: {down, right}. This means that environments with a plan of length n have 2 n actions in total, for n = 20 we have 2 20 ≈ 1e6 actions.To evaluate how the agent's performance and learning speed relate to the number of discrete actions we use the MuJoCo (Todorov et  This environment demonstrates our agent's abilities with very large number of actions that are more difficult to dis- cern from their representation, and have less obvious con- tinuity with regards to their effect on the environment com- pared to the MuJoCo tasks. We represent each action with the concatenation of each step of the plan. There are two possible steps which we represent as either {0, 1} or {1, 0}. This means that a full plan will be a vector of concatenated steps, with a total length of 2n. This representation was chosen arbitrarily, but we show that our algorithm is never- theless able to reason well with it.In cart-pole swing-up, the agent must balance a pole at- tached to a cart by applying force to the cart. The pole and cart start in a random downward position, and a reward of +1 is received if the pole is within 5 degrees of vertical and the cart is in the middle 10% of the track, otherwise a re- ward of zero is received. The current state is the position and velocity of the cart and pole as well as the length of the pole. The environment is reset after 500 steps.We use this environment as a demonstration both that our agent is able to reason with both a small and large num- ber of actions efficiently, especially when the action repre- sentation is well-formed. In these tasks, actions are repre- sented by the force to be applied on each dimension. In the cart-pole case, this is along a single dimension, so actions are represented by a single number.To demonstrate how the agent would perform on a real- world large action space problem we constructed a sim- ulated recommendation system utilizing data from a live large-scale recommendation engine. This environment is characterized by a set of items to recommend, which cor- respond to the action set A and a transition probability ma- trix W , such that W i,j defines the probability that a user will accept recommendation j given that the last item they accepted was item i. Each item also has a reward r asso- ciated with it if accepted by the user. The current state is the item the user is currently consuming, and the previously recommended items do not affect the current transition.Choosing amongst all possible n-step plans is a general large action problem. For example, if an environment has i actions available at each time step and an agent needs to plan n time steps into the future then the number of actions i n is quickly intractable for arg max-based approaches. We implement a version of this task on a puddle world envi- ronment, which is a grid world with four cell types: empty, puddle, start or goal. The agent consistently starts in the start square, and a reward of -1 is given for visiting an empty square, a reward of -3 is given for visiting a puddle square, and a reward of 250 is given and the episode ends if on a goal cell. The agent observes a fixed-size square window surrounding its current position.At each time-step, the agent presents an item i to the user with action A i . The recommended item is then either ac- cepted by the user (according to the transition probability matrix) or the user selects a random item instead. If the presented item is accepted then the episode ends with prob- ability 0.1, if the item is not accepted then the episode ends with probability 0.2. This has the effect of simulating user patience -the user is more likely to finish their session if they have to search for an item rather than selecting a rec- ommendation. After each episode the environment is reset by selecting a random item as the initial environment state.The goal of the agent is to find the shortest path to the goal that trades off the cost of puddles with distance traveled. The goal is always placed in the bottom right hand cor- ner of the environment and the base actions are restricted to moving right or down to guarantee goal discovery with random exploration. The action set is the set of all pos- For each environment, we vary the number of nearest neighbors k from k = 1, which effectively ignores the re- ranking step described in Section 4.2, to k = | A |, which effectively ignores the action generation step described in Section 4.1. For k = 1, we demonstrate the performance of the nearest-neighbor element of our policy g • f θ π . This is the fastest policy configuration, but as we see in the sec- tion, is not always sufficiently expressive. For k = | A |, we demonstrate the performance of a policy that is greedy relative to Q, always choosing the true maximizing action from A. This gives us an upper bound on performance, but we will soon see that this approach is often computation- ally intractable. Intermediate values of k are evaluated to demonstrate the performance gains of partial re-ranking.We also evaluate the performance in terms of training time and average reward for full nearest-neighbor search, and three approximate nearest neighbor configurations. We use FLANN (Muja &amp; Lowe, 2014) with three settings we refer to as 'Slow', 'Medium' and 'Fast'. 'Slow' uses a hierarchi- cal k-means tree with a branching factor of 16, which corre- sponds to 99% retrieval accuracy on the recommender task. 'Medium' corresponds to a randomized K-d tree where 39 nearest neighbors at the leaf nodes are checked. This cor- responds to a 90% retrieval accuracy in the recommender task. 'Fast' corresponds to a randomized K-d tree with 1 nearest neighbor at the leaf node checked. This corre- sponds to a 70% retrieval accuracy in the recommender task. These settings were obtained with FLANN's auto- tune mechanism. Figure 3 shows performance as a function of wall-time on the cart-pole task. It presents the performance of agents with varying neighbor sizes and FLANN settings after the same number of seconds of training. Agents with k = 1 are able to achieve convergence after 150,000 seconds whereas k = 5, 000 (0.5% of actions) trains much more slowly.  In this section we analyze results from our experiments with the environments described above.2.4 8.5 23 0.5% -5, 0000.6 0.6 0.7 0.7The cart-pole task was generated with a discretization of one million actions. On this task, our algorithm is able to find optimal policies. We have a video available of our final policy with one million actions, k = 1, and 'fast' FLANN lookup here: http://goo.gl/3YFyAE. We visualize performance of our agent on a one million action cart-pole task with k = 1 and k = 0.5% in Figure  2, using an exact lookup. In the relatively simple cart-pole task the k = 1 agent is able to converge to a good policy. However, for k = 0.5%, which equates to 5,000 actions, training has failed to attain more than 100,000 steps in the same amount of time. Table 1 display the median steps per second for the train- ing algorithm. We can see that FLANN is only helpful for k = 1 lookups. Once k = 5, 000, all the computation time is spent on evaluating Q instead of finding nearest neighbors. FLANN performance impacts nearest-neighbor lookup negatively for all settings except 'fast' as we are looking for a nearest neighbor in a single dimension. We will see in the next section that for more action dimensions this is no longer true.Cartpole, 1e6 actions, exact lookup. We ran our system on a fixed Puddle World map of size 50×50. In our setup the system dynamics are deterministic, our main goal being to show that our agent is able to find appropriate actions amongst a very large set (up to more than one million). To begin with we note that in the simple case with two actions, n = 1 in Figure (4) it is difficult to find a stable policy. We believe that this is due to a large number of states producing the same observation, which makes a high-frequency policy more difficult to learn. As the plans get longer, the policies get significantly better. The best possible score, without puddles, is 150 (50+50 steps of -1, and a final score of 250).  Step Figure 4. Agent performance for various lengths of plan, a plan of n = 20 corresponds to 2 20 = 1, 048, 576 actions. The agent is able to learn faster with longer plan lengths. k = 1 and 'slow' FLANN settings are used.Puddle World with the number of neighbors k = 1 and k = 52428, or 5% of actions. In this figure k = | A | is absent as it failed to arrive to the first evaluation step. We can see that in this task we are finding a near optimal policy while never using the arg max pass of the policy. We see that even our most lossy FLANN setting with no re-ranking converges to an optimal policy in this task. As a large num- ber of actions are equivalent in value, it is not surprising that even a very lossy approximate nearest neighbor search returns sufficiently pertinent actions for the task. Experi- ments on the recommender system in Section 8.3 show that this is not always this case. Experiments were run on 3 different recommender tasks involving 49 elements, 835 elements, and 13,138 elements. These tasks' dynamics are quite irregular, with certain ac- tions being good in many states, and certain states requiring a specific action rarely used elsewhere. This has the effect of rendering agents with k = 1 quite poor at this task. Ad- ditionally, although initial exploration methods were purely uniform random with an epsilon probability, to better simu- late the reality of the running system -where state transi- tions are also heavily guided by user choice -we restricted our epsilon exploration to a likely subset of good actions provided to us by the simulator. This subset is only used to guide exploration; at each step the policy must still choose amongst the full set of actions if not exploring. Learning with uniform exploration converges, but in the larger tasks performance is typically 50% of that with guided explo- ration.Recommender, 835 actions. Puddle World, n=20    We note that the agent using all actions (in yellow) does not train as many steps due to slow training speed. It is training approximately 15 times slower in wall-time than the 1% agent. Figure (8) shows performance for varying FLANN settings on this task with a fixed k at 5% of actions. We can quickly see both that lower-recall settings significantly impact the performance on this task.Results on the 49-element task with both a 200-    dimensional and a 20-dimensional representation are pre- sented in Figure 9 using a fixed 'slow' setting of FLANN and varying values of k. We can observe that when using a small number of actions, a more compact representation of the action space can be beneficial for stabilizing conver- gence. In this paper we introduce a new policy architecture able to efficiently learn and act in large discrete action spaces. We describe how this architecture can be trained using DDPG and demonstrate good performance on a series of tasks with a range from tens to one million discrete actions. Results on this series of tasks suggests that our approach can scale to real-world MDPs with large number of actions, but exploration will remain an issue if the agent needs to learn from scratch. Fortunately this is generally not the case, and either a domain-specific system provides a good starting state and action distribution, or the system's dy- namics constrain transitions to a reasonable subset of ac- tions for a given states.Architectures of this type give the policy the ability to gen- eralize over the set of actions with sub-linear complexity relative to the number of actions. We demonstrate how con- sidering only a subset of the full set of actions is sufficient in many tasks and provides significant speedups. Addition- ally, we demonstrate that an approximate approach to the nearest-neighbor lookup can be achieved while often im- pacting performance only slightly.Future work in this direction would allow the action repre- sentations to be learned during training, thus allowing for actions poorly placed in the embedding space to be moved to more appropriate parts of the space. We also intend to in- vestigate the application of these methods to a wider range of real-world control problems. The critic is trained from samples stored in a replay buffer. These samples are generated on lines 9 and 10 of Algorithm 2. The action a t is sampled from the full Wolpertinger pol- icy π θ on line 9. This action is then applied on the environ- ment on line 10 and the resulting reward and subsequent state are stored along with the applied action in the replay buffer on line 11.Pazis, Jason and Parr, Ron. Generalized value functions for large action sets. In Proceedings of the 28th Interna- tional Conference on Machine Learning (ICML-11), pp. 1185-1192, 2011.On line 12, a random transition is sampled from the replay buffer, and line 13 performs Q-learning by applying a Bell- man backup on Q θ Q , using the target network's weights for the target Q. Note the target action is generated by the full policy π θ and not simply f θ π .The actor is then trained on line 15 by following the policy gradient:Prokhorov, Danil V, Wunsch, Donald C, et al. Adaptive critic designs. Neural Networks, IEEE Transactions on, 8(5):997-1007, 1997.  for x &gt; 1 .Initialize a random process N for action exploration 7:Receive initial observation state s 1for t = 1, T do 9:Select action a t = π θ (s t ) according to the current policy and exploration method 10:Execute action a t and observe reward r t and new state s t+1If we select k such actions, the CDF of the maximum of these actions equals the product of the individual CDFs, because the probability that the maximum value is smaller that some given x is equal to the probability that all of the values is smaller than x, so that the cumulative distribution function for 11:Store transition (s t , a t , r t , s t+1 ) in B Update the critic by minimizing the loss:i∈{1,...,k}Update the actor using the sampled gradient:Update the target networks:where the last step is due to the assumption that the dis- tribution is equal for all k closest actions (it is straightfor- ward to extend this result by making other assumptions, e.g., about how the distribution depends on distance to the selected action). The CDF of the maximum is therefore given by Now we can determine the desired expected value as Actions stored in the replay buffer are generated by π θ π , but the policy gradient ∇ ˆ a Q θ Q (s, ˆ a) is taken atâatˆatâ = f θ π (s). This allows the learning algorithm to leverage otherwise ignored information of which action was actually executed for training the critic, while taking the policy gradient at the actual output of f θ π .  
