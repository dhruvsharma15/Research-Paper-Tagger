The accumulation of errors in the iterated case deteriorates the accuracy of the prediction, and the direct strategy requires a sharply increased computational expense, especially in the case of a longer prediction horizon. Based upon these two mainstream modeling strategies, there has been a revival of interest in developing novel multi-step-ahead prediction strategies such as the DirRec strategy [4], the multiple-input multiple-output (MIMO) strategy [5] and multiple-input several multiple-outputs (MISMO) strategy [6,7] (DIRMO in [8]). A review and comparative study on strategies for multi-step-ahead time series has indicated that the multiple-output strategies, including MIMO and MISMO (DIRMO), are the best performing approaches, which has been validated with NN5 forecasting competition data [8].This study focuses on a MISMO strategy, improving it in terms of modeling flexibility and more accurate prediction. MISMO is achieved by transforming the original H -step-ahead prediction task into n H s = subtasks, yielding n sub-models with a fixed number of multiple outputs, s , for each, where 1 s H ≤ ≤ . The value of s is chosen through cross-validation, analyzing the performance of the MISMO model for different values of s on the learning set, and then using the best value to estimate the outputs [6,7]. In the special case where s H = , MISMO can be regarded as MIMO, and and prediction, most models continue to be limited to one-step-ahead prediction rather than multi-step-ahead prediction, primarily because of the challenges arising from increased uncertainty from longer prediction horizons. The most common modeling strategies for multi-step-ahead time series prediction rely either on iterated or direct strategies [1][2][3]. An iterated strategy first constructs a one-step-ahead prediction model, and then uses the predicted values as known data to Manuscript received September 12, 2012. This work was supported in part by the Natural Science Foundation of China under Grant No. 70771042, the Fundamental Research Funds for the Central Universities (2012QN208-HUST), the MOE (Ministry of Education in China) Project of Humanities and Social Science (Project No.13YJA630002), and a grant from the Modern Information Management Research Center at Huazhong University of Science and Technology.Yukun Bao (corresponding author), is with school of management, Huazhong University of Science and Technology, Wuhan, P.R.China (Tel: 86-27-87558579; fax: 86-27-87556437; e-mail: yukunbao@hust.edu.cn).Tao Xiong, is with school of management, Huazhong University of Science and Technology, Wuhan, P.R.China (e-mail: taoxiong@hust.edu.cn).Zhongyi Hu, is with school of management, Huazhong University of Science and Technology, Wuhan, P.R.China (e-mail: huzhyi21@hust.edu.cn). when 1 s = , MISMO is the same as a direct strategy. The superior out-of-sample performance of the MISMO strategy presented in [7] depends on the trade-off between heterogeneity and computational complexity. The MISMO strategy exploits the heterogeneity across subtasks efficiently, with less computational complexity than the direct prediction technique. An important feature of the MISMO strategy is that the original prediction modeling task is divided into several subtasks with an equal number of outputs (prediction horizons). Handling indivisibility in the problem is an explicit difficulty with the method. In addition, an underlying problem arises from the crisp divides on the prediction horizons with equal size s that induce a latent separation of dependencies among the steps in the different sub models. This problem may prevent the MISMO strategy from considering complex dependencies between prediction steps within different sub models and consequently reduce the prediction accuracy. Thus a strategy with more flexibility and a mechanism to self-adaptively determine the prediction horizon divides while modeling is appealing. This study proposes an improved PSO-MISMO modeling strategy for multi-step-ahead prediction that incorporates a heuristic based on particle swarm optimization (PSO) into the MISMO modeling process to self-adaptively determine the number of sub-models with varying prediction horizons.For the purpose of justification, this study compares the rank of the proposed modeling strategy with the four well-established strategies (i.e., iterated, direct, MIMO and MISMO) with neural networks (NNs). In addition, as a straightforward alternative, binary genetic algorithm-based MISMO modeling strategy (GA-MISMO) is selected as counterpart against the proposed PSO-MISMO and compared as well. Both simulated (i.e., Logistic and Mackey-Glass time series) and real (i.e., NN3 competition data) datasets are used for the comparisons.The paper is structured as follows. Section II provides a brief review of the multiple-output modeling strategies, including MIMO and MISMO, with limited details. Section III describes the basic concepts of FNN for multi-step-ahead prediction and the implementation of the FNN-based prediction model in the current study. The concept of the binary PSO is explained in Section IV. In Section V, details of the proposed PSO-MISMO strategy are presented. Section VI details the experimental setting on data, accuracy measure, genetic algorithm as alternative, and experimental procedure. Experimental results are discussed in Section VII, and concluding remarks are provided in Section VIII.The limitation of MIMO lies in forcing all the prediction horizons to be predicted with the same model structure (for instance, the same inputs ( ) 1 , ,), resulting in a reduction of the modeling effort, but likely to bias the returned model results.A solution to the shortcomings of MIMO was proposed in [6,7], where the constraint of the fixed modeling structure is relaxed by tuning an integer parameter s , which calibrates the dimensionality of the outputs on the basis of a validation criterion. For a given s , MISMO generates n H s = sub-models, which may have different inputs from each other but a fixed size of outputs at s , resulting in slightly more modeling flexibility. The MISMO modeling strategy learns n multiple-output sub-models as follows:where d is the maximum embedding order and ε is a zero-mean vector of size s .After the learning process, the estimations of the H next values are returned by the following:Multi-step-ahead prediction can be described as an estimation of , ( 1, 2, , ), where H is an integer greater than one, given the current and previous observation , ( 1, 2, , ) t t N ϕ =. In this section, two competing modeling strategies, MIMO and MISMO, with multiple-input multiple-output structures for multi-step-ahead prediction are described, to enable understanding of the proposed PSO-MISMO strategy in Section V.MIMO was first proposed in [5] and characterized as a multiple-input multiple-output approach, where the predicted value is not a scalar quantity but a vector of future valuesThe MISMO approach highlights the trade-off between the property of preserving the stochastic dependency among the predicted values and the flexibility of the modeling procedure [8]. However, the increased flexibility of the MISMO strategy requires an additional parameter s, which gives rise to several issues in itself. An explicit issue is handling indivisibility when computing n H s = , where all the parameters should be integers. Furthermore, the number of outputs of every sub-model derived from MISMO is fixed at s . This induces a latent separation of dependencies among the steps that may prevent the MISMO strategy from considering complex dependencies between prediction steps within different sub-models and consequently reduce the prediction accuracy. Further study has been solicited to determine the divides of prediction horizons adaptively while modeling rather than fixing them at s . where d is the maximum embedding order and ε is a vector noise term of zero mean and nondiagonal covariance. After the learning process, the estimations of the H next values are returned according to the following equation:Although many types of neural networks (NNs) have been proposed, the most popular one for time series prediction is the feed-forward NNs (FNNs). Because the focus of this study is the evaluation of the proposed modeling strategy relative to the other competitors for multi-step-ahead prediction, a simple three-layer FNN is implemented to model the selected strategies, capable of creating a common and reliable benchmark. In addition, the proposed modeling strategy and the four well-established strategies can all be implemented in the FNN because of the flexible structure of the FNN. Thus, the standard three-layer FNN with nodes in adjacent layers fully connected is used in this study. In this section, a short introduction to feed-forward neural networks for multi-step-ahead prediction is provided, and then the implementation of the FNN-based prediction model in the current study is described.A. FNN for Multi-Step-Ahead Prediction Fig. 1 shows an example of a typical three-layer FNN model with four input nodes, four hidden nodes (one hidden layer), and two output neurons used for multi-step-ahead prediction. The input nodes are the previous lagged observations while the outputs provide the forecast for the future values in a multi-step-ahead fashion. Hidden nodes with appropriate nonlinear activation functions are used to process the information received by the input nodes. Bias (or intercept) terms are used both in the hidden and the output layers. To facilitate the understanding of the FNN for multi-step-ahead time series prediction, the FNN with the MIMO modeling strategy will be used as an example in this section.   .In the current study, the mean square error (MSE) is used as the index of multi-step-ahead prediction performance to be minimized, defined as follows:Following the procedure from [9], the design tasks for the FNN-based prediction model can be roughly divided into four parts: data preprocessing, FNN designing, FNN implementation and validation. The implementation of the FNN-based model for multi-step-ahead prediction is presented step-by-step below: Data preprocessing. Normalization is a standard requirement for time series modeling and prediction. Thus, the data sets are first scaled by linear transference to map onto a range of [0,1]. After the linear transference, deseasonalization and detrending are performed when necessary. Deseasonalization is performed with the most recent X-12-ARIMA seasonal adjustment procedure. Detrending is performed by fitting a polynomial time trend to the data, and then subtracting the estimated trend from the series when trends are detected by the Mann-Kendall test. FNN designing. Selecting an appropriate architecture for the FNN is normally the first step when designing an FNN-based prediction system. The standard three-layer FNN with fully connected nodes in adjacent layers is used in this study. The number of input nodes, hidden nodes, output nodes, and the type of activation functions are defined in the present study as follows. 1) Selecting the number of input nodes: The number of input nodes is determined by the input selection. The filter method, which selects a set of inputs by optimizing a criterion over different combinations of inputs by means of a search algorithm, is employed for input selection in the current study. The filter method requires the setting of two elements: the criterion, i.e., the statistic which estimates the quality of the selected variables, and the search algorithm, which describes the policy used to explore the input space. With respect to the criterion, the partial mutual information [10] is used for the models using the iterated or direct strategies, and an extension of the Delta test [6] is used for the models with MIMO, MISMO, GA-MISMO, or PSO-MISMO strategies. With respect to the search algorithm, a forward-backward selection method that offers the flexibility to reconsider input variables previously discarded and to discard input variables previously selected is used. The maximum embedding order, d , is set to 15.2) Selecting the number of hidden nodes: The number of the hidden nodes cannot be determined in advance. Thus empirical experimentations are needed to determine this number [11]. Because of the small sample size of many of the series, the experimentation is limited to five possible values of hidden nodes, i.e., 2, 4, 6, 8, or 10. The best number of hidden nodes is determined by using the original Akaike's information criterion (AIC).3) Selecting the number of output nodes: The number of output nodes is determined by the modeling strategy. Most of the modeling strategies found in the literature can be classified into two categories, single-output structure and multiple-output structure. In the single-output structure strategies, such as the iterated and direct strategies, multiple inputs map to a single output. The MIMO, MISMO, GA-MISMO, and PSO-MISMO strategies are all based on the multiple-output structure that maps multiple inputs to multiple outputs. Therefore, for the iterated and direct strategies, only one output node is used for the FNN model. When employing the MIMO strategy, the number of output nodes is equal to the number of prediction horizons. In the MISMO strategy, the original H -step-ahead prediction tasks are separated into n H s = subtasks, resulting in n sub-models with fixed multiple outputs, s , for each, where 1 s H ≤ ≤ . Unlike the MISMO strategy, which uses a fixed number of outputs s for each sub-model, the GA-MISMO and PSO-MISMO strategies self-adaptively determines the number of sub-models with varying number of outputs, as described in Section V. 4) Selecting the type of activation functions: A sigmoid transfer function is used at each hidden node and a linear transfer function is used at each output node.  FNN implementation. The Levenberg and Marquardt algorithm (LMA) provided by the MATLAB (Version 2009b) NN toolkit is used for the training. Node biases are applied at the hidden and output layers. For the stopping criterion, the number of learning epochs is chosen as 1000, having no prior knowledge of the appropriate value. To determine the optimal parameters for the FNN, the common practice of fivefold cross-validation is used.  Validation: For each series of the simulated and real dataset, the last 18 observations are separated for ex-ante performance assessment (out-of-sample), statistically independent from the parameter estimation process. The remainder of the data (in-sample) is used for parameter estimation and the FNN modeling.differences between the binary PSO and the original PSO. The first difference is the representation of the particle. In the binary PSO, every particle is characterized by a binary solution representation (Note that the representation is primarily related to a coding issue in the proposed PSO-MISMO strategy). The other difference is that the velocity of a particle in the binary PSO is a probability vector, where each probability element determines the likelihood of that binary variable having the value of one. Fig. 2 lists the pseudocode algorithm for the basic binary PSO.Initialize a population of particles with random positions and velocities, throughout the input space.While not sufficiently good performance or maximum number of iterations do for each particleUpdate velocity i V according to equation (8).Update position i P according to equation (11). end end while Particle swarm optimization (PSO) [12] is a population-based self-adaptive search algorithm that exploits a population of individuals to probe promising regions of the search space. In the PSO, the population is referred to as a swarm that consists of a number of particles. Each particle represents a potential solution of the optimization task and has a position represented by a vector. Each particle moves to a new position according to both local information (the particle memory) and global information (the knowledge of all the particles). Thus, the PSO has the ability to converge to local and/or global optimal solutions in a small number of generations. In this section, a brief introduction to a variant of the original PSO, i.e., binary PSO, is presented, to understand the mechanism of the proposed PSO-MISMO strategy applied in this study.The PSO was initially designed to solve continuous optimization problems. Since the introduction of PSO, successful applications to several optimization problems [13][14][15][16][17] have demonstrated its potential. However, the major obstacle of applying a PSO successfully is its continuous nature. The first variant of the PSO algorithm for solving problems with binary-value solution elements (binary PSO) [18] was also developed by the creator of the original PSO. There are two keyThe binary PSO updates the velocity according to equation (8) as follows: are random real numbers uniformly distributed between 0 and 1. w , the inertia weight, a user-specified parameter controls the momentum of the particle. A larger inertia weight pushes towards global exploration while a smaller inertia weight helps in fine-tuning the current search area. The following weighting function is usually utilized:where max w and min w are initial weight and final weight, respectively, T is the maximum number of allowable iterations and t is the current iteration.The velocity is constrained to the interval [0,1] by using the following sigmoid transformation:where , is the probability that bit is equal to 1. To avoid approaching 0 or 1, a constant is often set atEach particle, at each time step, changes its current position according to equation (10), based on equation (12), as follows:4 such that , then the following relationships arewhere is a random real number uniformly distributed between 0 and 1. To address the limitations of MISMO mentioned first in Section I and illustrated further in Section II.B, an improved binary PSO-based MISMO is developed that self-adaptively determines the number of sub-models with varying prediction horizons. The proposed strategy is abbreviated PSO-MISMO. Fig. 3 depicts the flowchart of the proposed PSO-MISMO modeling strategy. As shown in Fig. 3, the detailed PSO-MISMO algorithm consists of three major operations: initialization, evaluation, and update. Before describing details of these operations, coding issues will be addressed. The overall learning process in Fig. 3  observations for which the next observations are to be predicted. To avoid the restriction associated with adopting a fixed in the original MISMO method, a flexible vector 0 or 1; if the value of a variable is 0, then the original task will not be divided by this segmentation point. If the value of a variable is 1, then the prediction task will be divided by this segmentation point. Fig. 4 illustrates the solution representation.is used, where is the number of prediction     The prediction horizons of these prediction sub-models (or the number of outputs of the three FNN models) are 3, 5, and 2, respectively. This difference, or heterogeneity, is the highlight of the proposed strategy. For clarity, the flow of this example is illustrated in Fig. 5. Initialization. All the particles are randomly generated with 0 or 1 using a 50% probability. All the velocity components are assigned the initial value of 0. Concerning the selection of parameters (i.e., inertial weight w , coefficients 1 c and 2 c , swarm size I , and number of iterations T ) in binary PSO, it is yet another challenging model selection task [19]. Fortunately, several empirical and theoretical studies have been performed about the parameters of PSO from which valuable information can be obtained [20][21][22][23]. In this study, the parameters are determined according to the recommendations in these studies and selected based on the prediction performance and computational time in a trial-error fashion. Table I    Update. In the current generation, once the fitness values of all the particles in the swarm are calculated, the pbest of each particle, and the gbest of the swarm are obtained. Then, all the particles' velocities and positions are updated according to equations (8) and (12) The NN3 competition was organized in 2007, targeting computational-intelligence forecasting approaches. The competition dataset of 111 monthly time series drawn from homogeneous population of real business time series is used for evaluation 2 .corresponding -for particle P* ; otherwise, go back to the previous step.To evaluate the performances of the proposed PSO-MISMO and the counterparts in terms of the forecast accuracy, two simulated time series, i.e., Logistic and Mackey-Glass time series, and a real world dataset, i.e., NN3 competition dataset, are used in this present study.Logistic and Mackey-Glass time series are recognized as benchmark time series that have been commonly used and reported by a number of studies related to time series modeling and forecasting [24][25][26].The Logistic map is a demographic model that was popularized by May [27] as an example of a simple non-linear system that exhibits complex, chaotic behavior. It is drawn from equation (13).Mackey-Glass time series, and 111 NN3 time series are used for evaluating the performances of the proposed PSO-MISMO and the counterparts in this study. Each series is split into an estimation sample and a hold-out sample. The last 18 observations are saved for evaluating and comparing the out-of-sample forecast performances of the various multi-step-ahead prediction strategies. All performance comparisons are based on these 18 × 20 out-of-sample points for Logistic and Mackey-Glass datasets and 18 × 111 out-of-sample points for NN3 datasets.For each prediction horizon h , three alternative forecast accuracy measures are considered: the mean absolute percentage error (MAPE), the symmetric mean absolute percentage error (SMAPE), and the mean absolute scaled error (MASE). The definitions of each are as follows: The Mackey-Glass time series is approximated from the differential equation (14) (see [28]). To further assess the performance of the proposed PSO-MISMO modeling strategy for multi-step-ahead prediction, we compare the experimental results of the PSO-MISMO with those produced by other discrete evolutionary algorithms, such as genetic algorithm (GA) (for detailed introduction to GA, please refer to [29][30][31]). This subsection briefly presents the implementation of GA-MISMO, a straightforward alternative against PSO-MISMO. Fig. 7 depicts the flowchart of the GA-MISMO modeling strategy. As shown in Fig. 7, the detailed GA-MISMO algorithm consists of three major parts: initialization, evaluation, and operatation. Before describing details of these parts, coding issues will be addressed. The overall learning process in Fig. 7 is elaborated step by step below.  Coding. The chromosome design in GA-MISMO is the same with the particle design in PSO-MISMO, as was presented in Apply GA operators: selection, crossover, and mutationStep 3： OperatationOutput and or 1: if the value of a variable is 0, then the original task will not be divided by this segmentation point; if the value of a variable is 1, then the prediction task will be divided by this segmentation point. The coding issue in GA-MISMO is the same as PSO-MISMO and will not be presented in details here to save space.  Initialization. All the chromosomes are randomly generated with 0 or 1 using a 50% probability. In setting the parameters of GA, it is required to do extensive simulations to find suitable values for various parameters. In this study, GA's parameters are determined through preliminary simulation and selected according to the recommendations in [32][33][34] in a trial-error fashion. Table III summarizes the final parameter of GA.  Evaluation. The evaluation in GA-MISMO is the same as  Operatation. Designing the operators for selection, crossover, and mutation is a major issue in a GA implementation and is always done ad hoc. In each generation, after the fitness values of all the chromosomes in the same population are calculated, the selection operator that chooses chromosomes from the current generation's population for inclusion in the next generation's population is executed. There are many ways for choosing parents [35]. In this study, the roulette method is used for Logistic time series and top percent method is adopted for Mackey-Glass and NN3 time series through preliminary simulation in a trial-error fashion. After selection of parents, the crossover operator should be applied to produce two offspring by exchanging some genetic information between the parents. Crossover occurs during evolution according to the crossover probability. The crossover probability is how often a crossover will be performed. In this study, the probability of crossover is set to 0.9, as shown in Table III; two-point operator is used. This process of selection and reproduction is repeated until the number of offspring becomes equal to the number of eliminated chromosomes; by adding the offspring to population, its size becomes equal to the initial size [34]. The mutation is the third operator, which is applied on the population (parents and their offspring) excluding the best chromosome. A simple mutation operator that selects a few percent of the genes of the population randomly and flips their values from zero to one and vice versa is used in this study [36]. In this study, the probability of mutation is set to 0.02, as shown in  Fig. 8 shows the experimental procedure using the simulated and real time series. Each series is first split into the estimation sample and the hold-out sample. Then, the input selection and model selection for each series are determined using the filter method, the Levenberg and Marquardt algorithm, and fivefold cross-validation with iterated, direct, MIMO, MISMO, GA-MISMO, and PSO-MISMO strategies. Finally the models are tested with the hold-out samples and the MAPE h , SMAPE h and MASE h are computed for each prediction horizon h (in this case =1,2, h … ,18 ) over datasets (i.e., Logistic, Mackey-Glass, and NN3 datasets). The modeling process for each series is repeated ten times. Upon the termination of this loop, performance of the examined models with selected strategies at each prediction horizon and dataset is judged in terms of the mean, averaged by ten, of the MAPE h , SMAPE h and MASE h . Analysis of variance (ANOVA) test procedures are used to determine if the means of performance measures are significantly different among the three models for each prediction horizon and datasets. If so, Tukey's honesty significant difference (HSD) tests [37] are then used to furtherThe prediction performances of all six modeling strategies examined in terms of the three accuracy measures (MAPE, SMAPE, and MASE) and average rank for three datasets are shown in Table IV. The columns labeled as 'Average 1-h ' show the average accuracy measures over the prediction horizon 1 to h . The last column shows the average ranking for each model over all prediction horizons of the out-of-sample prediction performance. For each column of Table IV, the entry with the smallest value is set in boldface and marked with an asterisk, and the entry with second smallest value is set in boldface type.The results in Table IV lead to the following conclusions:  The rankings from best to worst are: PSO-MISMO and GA-MISMO are almost a tie, then MISMO, MIMO, direct, and iterated strategies, regardless of the accuracy measures considered. Thus our findings are robust to the choice of accuracy measures.  The modeling strategies based on the multiple-output structure (i.e., PSO-MISMO, GA-MISMO, MISMO, and MIMO strategies) outperform those based on the single-output structure (i.e., iterated and direct strategies), which is in agreement with [7].  Despite the popularity of the iterated strategy in multi-step-ahead prediction literature, it is consistently the worst performing strategy for prediction.  Comparing the iterated strategy with the direct strategy, the direct strategy is better regardless of the accuracy measures considered, which demonstrates that the accumulation of errors in the case of iterated strategy drastically deteriorates the accuracy of the prediction.  The MIMO strategy consistently achieves more accurate forecasts than either the iterated or the direct strategies for all prediction horizons. It is conceivable that the reason for the superiority of the MIMO strategy is that it preserves, among the predicted values, the stochastic dependency characterizing the time series.  The MISMO strategy seems to produce forecasts which are more accurate than those of the MIMO strategy (though only marginally). It is conceivable that the reason for the superiority of the MISMO strategy is that it trades off the property of preserving the stochastic dependency between future values with a greater flexibility of the predictor.  When comparing the results of two heuristic-based modeling strategies (i.e., PSO-MISMO and GA-MISMO) with those of MISMO strategy, the heuristic-based strategies are generally better, which may indicate that the fixed number of outputs of every sub-model derived from the MISMO strategy tends to prevent the MISMO strategy from considering complex dependencies between prediction steps within different sub models and consequently reduces the prediction accuracy.   Table V shows the results of the multiple comparison tests for three datasets. For each accuracy measure and prediction horizon, the strategies are rank ordered from 1 (the best) to 6 (the worst). Several observations can be made from Table V:  For each accuracy measure and dataset, PSO-MISMO and GA-MISMO significantly outperforms MISMO for the overwhelming majority of prediction horizons.  Considering the two heuristic-based modeling strategies, we can see that, whatever the dataset used, whatever the accuracy measure considered, and whatever the prediction horizon examined, the difference in prediction performance between PSO-MISMO and GA-MISMO is not significant at the 0.05 level.  As far as the comparison MISMO vs. MIMO is concerned, the difference in prediction performance is not significant at the 0.05 level, even with a few exceptions  Generally, the strategies based on the single-output structure perform significantly worse than those based on the multiple-output structure.  Concerning the current two leading strategies, the direct strategy significantly outperforms the iterated strategy for the majority of prediction horizons.  The iterated strategy performs the poorest at 95% statistical confidence level in most cases, even with some exceptions.report only the results for randomly selected three time series from Logistic, Mackey-Glass, and NN3 datasets, respectively. and the No.55 series from NN3 dataset. In addition, in the following experiment, the swarm/population size is fixed to 20 for both PSO-MISMO and GA-MISMO strategies, and generation number is set to 100, as described in Section V. Each strategy is run ten times over each of these time series, in an attempt to eliminate the influence of a lucky initial solution. All the numerical experiments are performed on a personal computer, Inter(R) Core(TM) 2 Duo CPU 2.50 GHz, 1.87-GB memory, and MATLAB environment (Version R2009b). As was mentioned above, the learning process is repeated 10 times for both PSO-MISMO and GA-MISMO for each time series. Thus, typical convergences of the best and average fitness values as a function of generations for both PSO-MISMO and GA-MISMO on three aforementioned time series are shown in Fig. 9(a)-(c), respectively. As it can be seen, in a typical PSO-MISMO optimization process the best fitness value decreases rapidly and converges at about 48 generations for No.1 Logistic time series, 65 generations for No.18 Mackey-Glass time series, and 59 generations for No. 55 time series in NN3 datasets; whereas for GA-MISMO it takes about 63 generations for No.1 Logistic time series, 77 generations for No.18 Mackey-Glass time series, and 68 generations for No. 55 time series in NN3 datasets. To show how the evolution process is going on for both PSO-MISMO and GA-MISMO, the convergence of the average fitness values are also shown in Fig.  9. Looking at Fig. 9, it is clear that PSO-MISMO seems to perform better than GA-MISMO. Thus, for the present problem the performance of the PSO-MISMO is better than GA-MISMO from an evolutionary point of view. Up to now, we have compared the proposed PSO-MISMO with other five competitors (i.e., iterated, direct, MIMO, MISMO, and GA-MISMO) in terms of prediction accuracy. In this subsection, the convergence and computational time of the two heuristic-based prediction strategies (i.e., PSO-MISMO and GA-MISMO) are further examined.It should be noted that although we have used a number of time series for each datasets, i.e., Logistic, Mackey-Glass, and NN3 datasets, general results do not change much within the time series for each datasets. Therefore, to save space, we Multi-step-ahead time series prediction has usually proved to be an intractable task due to the growing amount of uncertainties arising from various sources. For instance, an accumulation of errors and lack of information make multi-step-ahead prediction more difficult. Thus, modeling strategies for multi-step-ahead prediction are major research topics with significant practical implications. The contribution of this study is an extension to a well-established MISMO modeling strategy by incorporating a heuristic based on binary particle swarm optimization into the MISMO modeling process to self-adaptively determine the number of sub-models with varying prediction horizons, and conduct a large scale comparative study with neural networks for validation. Quantitative and comprehensive assessments are performed with the simulated and real time series on the basis of the prediction accuracy, convergence, and computational time.Experimental results and comparisons demonstrate the superiority of the proposed PSO-MISMO modeling strategy for multi-step-ahead time series prediction. The second test performed is designed to measure the average computational time needed per time series from all the datasets within these two strategies. The result is shown in Table VI, from which we can see that the PSO-MISMO needs generally less time than GA-MISMO.  
