• Computing methodologies → Supervised learning by clas- sification; Support vector machines; Neural networks; KEYWORDS artificial intelligence; artificial neural networks; classification; im- age classification; machine learning; mnist dataset; softmax; super- vised learning; support vector machine MNIST [10] is an established standard handwritten digit classifica- tion dataset that is widely used for benchmarking deep learning models. It is a 10-class classification problem having 60,000 training examples, and 10,000 test cases -all in grayscale. However, it is argued that the MNIST dataset is "too easy" and "overused", and "it can not represent modern CV [Computer Vision] tasks" [15]. Hence, [13] proposed the Fashion-MNIST dataset. The said dataset consists of Zalando's article images having the same distribution, the same number of classes, and the same color profile as MNIST. A number of studies involving deep learning approaches have claimed state-of-the-art performances in a considerable number of tasks. These include, but are not limited to, image classification [9], natural language processing [12], speech recognition [4], and text classification [14]. The models used in the said tasks employ the softmax function at the classification layer.However, there have been studies [2,3,11] conducted that takes a look at an alternative to softmax function for classification -the support vector machine (SVM). The aforementioned studies have Dataset MNIST Fashion-MNIST Training 60,000 10,000 Testing 60,000 10,000Both datasets were used as they were, with no preprocessing such as normalization or dimensionality reduction.The support vector machine (SVM) was developed by Vapnik [5] for binary classification. Its objective is to find the optimal hyperplane f (w, x) = w · x + b to separate two classes in a given dataset, with features x ∈ R m .SVM learns the parameters w by solving an optimization problem (Eq. 1).where w T w is the Manhattan norm (also known as L1 norm), C is the penalty parameter (may be an arbitrary value or a selected value using hyper-parameter tuning), y ′ is the actual label, and w T x + b is the predictor function. Eq. 1 is known as L1-SVM, with the standard hinge loss. Its differentiable counterpart, L2-SVM (Eq. 2), provides more stable results [11]. where ∥w∥ 2 is the Euclidean norm (also known as L2 norm), with the squared hinge loss.Convolutional Neural Network (CNN) is a class of deep feed-forward artificial neural networks which is commonly used in computer vision problems such as image classification. The distinction of CNN from a "plain" multilayer perceptron (MLP) network is its usage of convolutional layers, pooling, and non-linearities such as tanh, siдmoid, and ReLU.The convolutional layer (denoted by CONV) consists of a filter, for instance, 5 × 5 × 1 (5 pixels for width and height, and 1 because the images are in grayscale). Intuitively speaking, the CONV layer is used to "slide" through the width and height of an input image, and compute the dot product of the input's region and the weight learning parameters. This in turn will produce a 2-dimensional activation map that consists of responses of the filter at given regions.Consequently, the pooling layer (denoted by POOL) reduces the size of input images as per the results of a CONV filter. As a result, the number of parameters within the model is also reduced -called down-sampling.Lastly, an activation function is used for introducing non-linearities in the computation. Without such, the model will only learn linear mappings. The commonly-used activation function these days is the ReLU function [6] (see Figure 1). ReLU is commonly-used over tanh and siдmoid for it was found out that it greatly accelerates the convergence of stochastic gradient descent compared the other two functions [9]. Furthermore, compared to the extensive computa- tion required by tanh and siдmoid, ReLU is implemented by simply thresholding matrix values at zero (see Eq. 3). At the 10 th layer of the CNN, instead of the conventional softmax function with the cross entropy function (for computing loss), the L2-SVM is implemented. That is, the output shall be translated to the following case y ∈ {−1, +1}, and the loss is computed by Eq. 2. The weight parameters are then learned using Adam [8].There were two parts in the experiments for this study: (1) training phase, and (2) test case. The CNN-SVM and CNN-Softmax models were used on both MNIST and Fashion-MNIST.Only the training accuracy, training loss, and test accuracy were considered in this study.The code implementation may be found at https://github.com/AFAgarap/cnn- svm. All experiments in this study were conducted on a laptop com- puter with Intel Core(TM) i5-6300HQ CPU @ 2.30GHz x 4, 16GB of DDR3 RAM, and NVIDIA GeForce GTX 960M 4GB DDR5 GPU. In this paper, we implement a base CNN model with the following architecture:( The hyper-parameters listed in Table 2 were manually assigned, and were used for the experiments in both MNIST and Fashion- MNIST.     Table 3: Test accuracy of CNN-Softmax and CNN-SVM on im- age classification using MNIST [10] and Fashion-MNIST [13].  Figure 4 shows the training accuracy of CNN-Softmax and CNN- SVM on image classification using MNIST, while Figure 5 shows their training loss. At 10,000 steps, the CNN-Softmax model was able to finish its training in 4 minutes and 47 seconds, while the CNN-SVM model was able to finish its training in 4 minutes and 29 seconds. The CNN-Softmax model had an average training accuracy of 94% and an average training loss of 0.259750089, while the CNN- SVM model had an average training accuracy of 90.15% and an average training loss of 0.793701683.After 10,000 training steps, both models were tested on the test cases of each dataset. As shown in Table 1 10,000 test cases each. Table 3 shows the test accuracies of CNN- Softmax and CNN-SVM on image classification using MNIST [10] and Fashion-MNIST [13]. The test accuracy on the MNIST dataset does not corroborate the findings in [11], as it was CNN-Softmax which had a better classification accuracy than CNN-SVM. This result may be attrib- uted to the fact the there were no data pre-processing than on the MNIST dataset. Furthermore, [11] had a relatively more sophisti- cated model and methodology than the simple procedure done in this study. On the other hand, the test accuracy of the CNN-Softmax model matches the findings in [15], as both methodology did not involve data preprocessing of the Fashion-MNIST.The results of this study warrants an improvement on its method- ology to further validate its review on the proposed CNN-SVM of [11]. Despite its contradiction to the findings in [11], quantitatively speaking, the test accuracies of CNN-Softmax and CNN-SVM are almost the same with the related study. It is hypothesized that with data preprocessing and a relatively more sophisticated base CNN model, the results in [11] shall be reproduced.An expression of gratitude to Yann LeCun, Corinna Cortes, and Christopher J.C. Burges for the MNIST dataset [10], and to Han Xiao, Kashif Rasul, and Roland Vollgraf for the Fashion-MNIST dataset [13].
