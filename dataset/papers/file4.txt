Existing theory mainly provides guarantees for multi- task learning (MTL) of homogeneous tasks, such as pure regression or classification tasks (Baxter, 2000;Ben-David and Schuller, 2003). These guar- antees, however, do not hold for the heterogeneous Work done during a visit at the University of Copenhagen.Our limited understanding of multi-task learning is also a practical problem: With hundreds of potential sharing structures, an exhaustive exploration of the search space of multi-task learning for specific prob- lems is infeasible. Existing work ( Kumar and Daumé III, 2012;Maurer et al., 2013) uses sparsity to share task predictors, but has only been applied to homoge- neous tasks. Previous work in multi-task learning of heterogeneous tasks only considers at most a couple of architectures for sharing (Søgaard and Goldberg, 2016;Peng and Dredze, 2016;Martínez Alonso and Plank, 2017). In contrast, we present a framework that unifies such different approaches by introducing trainable parameters for the components that differ- entiate multi-task learning approaches. We build on recent work trying to learn where to split merged networks ( Misra et al., 2016), as well as work try- ing to learn how best to combine private and shared subspaces ( Bousmalis et al., 2016;Liu et al., 2017). Figure 1: A SLUICE NETWORK with one main task A and one auxiliary task B. It consists of a shared input layer (shown left), two task-specific output lay- ers (right), and three hidden layers per task, each partitioned into two subspaces. α parameters control which subspaces are shared between main and aux- iliary task, while β parameters control which layer outputs are used for prediction.Contributions Our architecture (shown in Figure  1) is empirically justified and deals with the dirtiness ( Jalali et al., 2010) of loosely related tasks. It goes significantly beyond previous work, learning both what layers to share, and which parts of those layers to share, as well as using skip connections to learn a mixture model at the architecture's outer layer. We show that it is a generalization of various multi-task learning algorithms such as hard parameter sharing (Caruana, 1998), low supervision (Søgaard and Goldberg, 2016), and cross-stitch networks ( Misra et al., 2016), as well as transfer learning algorithms such as frustratingly easy domain adaptation (Daumé III, 2007). Moreover, we study what task properties pre- dict gains, and what properties correlate with learning certain types of sharing, as well as the inductive bias of the resulting architecture.layer associating the elements of an input sequence, in our case English words, with vector representa- tions via word and character embeddings. The two sequences of vectors are then passed on to their re- spective inner recurrent layers. Each layer is divided into subspaces, e.g., for A into G A,1,1 and G A,1,2 , which allow the network to learn task-specific and shared representations, if beneficial. The output of the inner layer of network A is then passed to its sec- ond layer, as well as to the second layer of network B. This traffic of information is mediated by a set of parameters α in a way such that the second layer of each network receives a weighted combination of the output of the two inner layers. The subspaces have different weights. Importantly, these weights are trainable and allow the model to learn whether to share, whether to restrict sharing to a shared sub- space, etc. Finally, a weighted combination of the outputs of the outer recurrent layers G ·,3,· as well as the weighted outputs of the inner layers are mediated through β parameters, which reflect a mixture over the representations at various depths of the network. In sum, sluice networks have the capacity to learn what layers and subspaces should be shared, as well as at what layers the network has learned the best representations of the input sequences.We introduce a novel architecture for multi-task learn- ing, which we refer to as a SLUICE NETWORK, sketched in Figure 1 for the case of two tasks. The network learns to share parameters between deep recurrent neural networks (RNNs) (Hochreiter and Schmidhuber, 1997). The recurrent networks could easily be replaced with multi-layered perceptrons or convolutional neural networks for other applications.The two networks A and B share an embeddingWe cast learning what to share as a matrix regularization problem, following (Jacob et al., 2009;Yang and Hospedales, 2017). As- sume M different tasks that are loosely related, with M potentially non-overlapping datasets D 1 , . . . , D M . Each task is associated with a deep neural network with K layers L 1 , . . . L K . We assume that all the deep networks have the same hyper-parameters at the outset. With loosely related tasks, one task may be better modeled with one hidden layer; another one with two (Søgaard and Goldberg, 2016); some may share many parameters, while others mostly rely on task-specific representations. Our architecture is flex- ible enough to learn this by allocating appropriate subspaces and mediating weights starting from the union of the a priori task networks.Let W ∈ R M ×D be a matrix in which each row i corresponds to a model θ i with D parameters. The loss that sluice networks minimize, with a penalty term Ω, is then as follows:The loss functions L i are cross-entropy functions of the form − y p(y) log q(y) where y i are the la- bels of task i. Note that sluice networks are not restricted to tasks with the same loss functions, but could also be applied to jointly learn regression and classification tasks. The weights λ i determine the importance of the different tasks during training. In all our experiments, we use the same weight for all tasks.We explicitly add inductive bias to the model via the regularizer Ω, which we describe in Equation 4. However, our model also implicitly learns regular- ization through multi-task learning (Caruana, 1993) mediated by the α weights, while the β weights are used to learn the parameters of the mixture functions f (·), as detailed in the following.designates the stacking of two vectors a, b ∈ R D to a matrix M ∈ R 2×D .Extending the α-layers to include subspaces, for 2 tasks and 2 subspaces, we obtain an α matrix ∈ R 4×4 that not only controls the interaction between the layers of both tasks, but also between their subspaces:(3) where h A 1 ,k is the output of the first subspace of the k-th layer of task A and h A 1 ,k is the linear com- bination for the first subspace of task A. The input to the k + 1-th layer of task A is then the concatenation of both subspace outputs: Recall that our architecture is partly motivated by the observation that for loosely related tasks, only certain features in specific layers should be shared, while many of the layers and subspaces may remain more task-specific (Søgaard and Goldberg, 2016). We want to learn what to share while inducing models for the different tasks. For simplicity, we ignore subspaces at first and assume only two tasks A and B. The outputs h A,k,t and h B,k,t of the k-th layer for time step t for task A and B respectively interact through what Misra et al. (2016) refer to as cross- stitch units α (see Figure 1). Omitting t for simplicity, the output of the α layers is:. Different α weights correspond to different matrix regularizers Ω, including several ones that have been proposed previously for multi-task learning. We re- view those in Section 3. For now just observe that if all α-values are set to 0.25 (or any other constant), we obtain hard parameter sharing (Caruana, 1993), which is equivalent to a heavy L 0 -regularizer.Adding Inductive Bias Naturally, we can also add inductive bias to sluice networks by partially con- straining the regularizer or adding to the learned penalty. Inspired by work on shared-space com- ponent analysis ( Salzmann et al., 2010), we add a penalty to enforce a division of labor and discourage redundancy between shared and task-specific sub- spaces. While the networks can theoretically learn such a separation, an explicit constraint empirically leads to better results and enables the sluice net- works to take better advantage of subspace-specific α-values. This is modeled by an orthogonality con- straint ( Bousmalis et al., 2016) between the layer- wise subspaces of each model:where h A,k is a linear combination of the outputs that is fed to the k+1-th layer of task A, and a , bwhere M is the number of tasks, K is the number of layers, · 2 F is the squared Frobenius norm, and G m,k,1 and G k,2,m are the first and second subspace respectively in the k-th layer of m-th task model.Many tasks have an implicit hierarchy that informs their interaction. Rather than predefining it (Søgaard and Goldberg, 2016;Hashimoto et al., 2017), we enable our model to learn hierarchical relations by associating different tasks with different layers if this is beneficial for learning. Inspired by advances in residual learning ( He et al., 2016), we employ skip-connections from each layer, controlled using β parameters. This layer acts as a mixture model, returning a mixture of expert predictions:is equivalent to a mean-constrained 0 -regularizerIf the sum of weighted losses are smaller than 1, the loss with penalty is always the highest when all parameters are shared.a weighted sum over the 2 norms of the groups, often used to enforce subspace sharing ( Zhou et al., 2010;´ Swirszcz and Lozano, 2012). Our architecture learns a 1 // 2 group lasso over the two subspaces (with the same degrees of freedom), when all α A,B and α B,A -values are set to 0. When the outer layer α-values are not shared, we get block communication between the networks. where h A,k is the output of layer k of model A, while h A,t is the linear combination of all layer out- puts of model A that is fed into the final softmax layer.Complexity Our model only adds a minimal num- ber of additional parameters compared to single-task models of the same architecture. In our experiments, we add α parameters between all task networks. As such, they scale linearly with the number of layers and quadratically with the number of tasks and sub- spaces, while β parameters scale linearly with the number of tasks and the number of layers. For a sluice network with M tasks, K layers per task, and 2 subspaces per layer, we thus obtain 4KM 2 addi- tional α parameters and KM β parameters. Training sluice networks is not much slower than training hard parameter sharing networks, with only an 5-7% in- crease in training time.The ap- proach to domain adaptation in (Daumé III, 2007), which relies on a shared and a private space for each task or domain, can be encoded in sluice networks by setting all α A,B -and α B,A -weights associated with G i,k,1 to 0, while setting all α A,B -weights associated with G i,k,2 to α B,B , and α B,A -weights associated with G i,k,2 to α A,A . Note that Daumé III (2007) dis- cusses three subspaces. We obtain this space if we only share one half of the second subspaces across the two networks.Low Supervision Søgaard and Goldberg (2016) propose a model where only the inner layers of two deep recurrent works are shared. This is ob- tained using heavy mean-constrained L 0 regular- ization over the first layer L i,1 , e.g., Ω(W ) = K i ||L i,1 || 0 with i λ i L(i) &lt; 1, while for the aux- iliary task, only the first layer β parameter is set to 1.The architecture is very flexible and can be seen as a generalization over several existing algorithms for transfer and multi-task learning, including (Caruana, 1998;Daumé III, 2007;Søgaard and Goldberg, 2016;Misra et al., 2016). We show how to derive each of these below.Hard Parameter Sharing in the two networks ap- pears if all α values are set to the same constant (Caruana, 1998;Collobert and Weston, 2008). ThisCross-Stitch Networks Misra et al. (2016) intro- duce cross-stitch networks that have α values control the flow between layers of two convolutional neural networks. Their model corresponds to setting the α- values associated with G i,j,1 be identical to those for G i,j,2 , and by letting all but the β-value associated with the outer layer be 0.In our experiments, we include hard parameter sharing, low supervision, and cross-stitch networks as baselines. We do not report results for group lasso and frustratingly easy domain adaptation, which were consistently inferior on development data by some margin.  Data We want to experiment with multiple loosely related NLP tasks, but also study performance across domains to make sure our architecture is not prone to overfitting. As testbed for our experiments, we there- fore choose the OntoNotes 5. We present experiments with the English portions of datasets, for which we show statistics in Table 1. 1 building block of our model. The BiLSTM consists of 3 layers with a hidden dimension of 100. At every time step, the model receives as input the concate- nation between the 64-dimensional embedding of a word and its character-level embedding produced by a Bi-LSTM over 100-dimensional character em- beddings. Both word and character embeddings are randomly initialized. The output layer is an MLP with a dimensionality of 100. We initialize α pa- rameters with a bias towards one source subspace for each direction and initialize β parameters with a bias towards the last layer 3 . We have found it most effective to apply the orthogonality constraint only to the weights associated with the LSTM inputs.Tasks In multi-task learning, one task is usually considered the main task, while other tasks are used as auxiliary tasks to improve performance on the main task. As main tasks, we use chunking (CHUNK), named entity recognition (NER), and a simplified version of semantic role labeling (SRL) where we only identify headwords 2 , and pair them with part-of-speech tagging (POS) as an auxiliary task, following (Søgaard and Goldberg, 2016). Ex- ample annotations for each task can be found in Table  2.Training and Evaluation We train our models with stochastic gradient descent (SGD), an initial learning rate of 0.1, and learning rate decay 4 . During training, we randomly and uniformly sample from the data for each task. We perform early stopping with patience of 2 based on the main task and hyperpa- rameter optimization on the in-domain development data of the newswire domain. We use the same hy- perparameters for all comparison models across all domains. We train our models on each domain and evaluate them both on the in-domain test set ( Table  3, top) as well as on the test sets of all other domains (Table 3, bottom) to evaluate their out-of-domain generalization ability 5 .Baseline Models As baselines, we compare against i) a single-task model only trained on chunk- ing; ii) the low supervision model (Søgaard and Goldberg, 2016), which predicts the auxiliary taskModel We use a state-of-the-art BiLSTM-based sequence labeling model ( Plank et al., 2016) as the 1 Note that not all sentences are annotated with all tasks. 2 We do this to keep pre-processing for SRL minimal. 3 We experimented with different initializations for α and β parameters and found these to work best. 4 We use SGD as Søgaard and Goldberg (2016) also employed SGD. Adam yielded similar performance differences. 5 Due to this set-up, our results are not directly comparable to the results in Søgaard and Goldberg (2016)   at the first layer; iii) an MTL model based on hard parameter sharing (Caruana, 1993); and iv) cross-stitch networks ( Misra et al., 2016). We com- pare these against our complete sluice network with subspace constraints and learned α and β param- eters. We implement all models in DyNet (Neu- big et al., 2017) and make our code available at http:anonymized.com.Our assumption is that MTL particularly helps to generalize to data whose distribution differs from the one seen during training. We thus first investigate how well sluice networks perform on in-domain and out-of-domain test data compared to state-of-the-art multi-task learning models by evaluating all models on chunking with POS tagging as auxiliary task.is the same as during training. On out-of-domain data, hard parameter sharing performs better, demonstrat- ing the regularizing effect of MTL, which improves the model's generalization ability.Results We show results on in-domain and out-of- domain tests sets in Table 3. On average, sluice networks significantly outperform all other model architectures on both in-domain and out-of-domain data. Single task models and hard parameter shar- ing achieve the lowest results. We see that single task learning is comparatively most useful in the in- domain setting, where the distribution of the test data Both models are consistently outperformed by low supervision, which is only slightly better than hard pa- rameter sharing in the out-of-domain setting. Cross- stitch networks provide another significant perfor- mance improvement. Finally, sluice networks per- form best for all domains, except for the telephone conversation (tc) domain, where they are outper- formed by cross-stitch networks. The performance boost is particularly significant for the out-of-domain setting, where sluice networks add more than 1 point in performance compared to hard parameter sharing and almost .5 compared to the strongest baseline on average, demonstrating that sluice networks are par- ticularly useful to help a model generalize better.In summary, this shows that our proposed model for learning which parts of multi-task models to share, with a small set of additional parameters to learn, can achieve significant and consistent improvements over strong baseline methods.   learning for NLP employs two tasks, typically pairing a main task with an auxiliary task. Existing studies ( Bingel and Søgaard, 2017;Martínez Alonso and Plank, 2017) also only evaluate pair-wise interac- tions between tasks. ( Hashimoto et al., 2017) is the only model we are aware of that learns from multiple stand-alone NLP tasks, but uses a task-specific archi- tecture to do so, while our model can be applied to any task combination. We use one sluice network to jointly learn our four tasks on the newswire domain and show results comparing it to the baseline models in Table 5 6 .Performance across Tasks We now compare sluice nets across different combinations of main and auxiliary tasks. In particular, we evaluate them on NER with POS tagging as auxiliary task and sim- plified semantic role labeling with POS tagging as auxiliary task. We show results in Table 4. Sluice networks outperform the comparison models for both tasks on in-domain test data and successfully general- ize to out-of-domain test data on average. They yield the best performance on 5 out of 7 domains and 4 out of 7 domains for NER and semantic role labeling.Discussion For MTL with four tasks, the low-level POS tagging and simplified SRL tasks are the only ones that benefit from hard parameter sharing. This is consistent with results in Table 3 and previous work (Søgaard and Goldberg, 2016). These results highlight that hard parameter sharing by itself is not sufficient for doing effective multi-task learning with semantic tasks. We rather require task-specific layers that can be used to transform the shared, low-level representation into a form that is able to capture more fine-grained task-specific knowledge.Performance with all Tasks To the best of our knowledge, most of the existing work in multi-task  With sluice networks, we are able to outperform the single task models for all tasks except chunking in the all-tasks setting. Generally, MTL with more than two stand-alone tasks is little explored in NLP and the best choices for hyperparameters such as the sampling ratio, when to start, and stop training for each task and whether to freeze or continue training already learned parameters remain to be discovered.Task Properties and Performance Bingel and Sø- gaard (2017) correlate meta-characteristics of task pairs and gains from hard parameter sharing across a large set of NLP task pairs. Inspired by this study, we correlate various meta-characteristics with error reductions and α, β values in sluice networks, as well as in hard parameter sharing. Most importantly, we find that a) multi-task learning gains, also in sluice networks, are higher when there is less training data, and b) sluice networks learn to share more when there is more variance in the training data (cross-task αs are higher, intra-task αs lower). Generally, α val- ues at the inner layers correlate more highly with meta-characteristics than α values at the outer layers.Overall, we find that learnable α parameters are preferable over constant α parameters. Learned β parameters marginally outperform skip-connections in the hard parameter sharing setting, while skip- connections are competitive with learned β values in the learned α setting. In addition, modeling sub- spaces explicitly helps for almost all domains. To our knowledge, this is the first time that subspaces within individual LSTM layers have been shown to be beneficial 7 . Being able to effectively partition LSTM weights opens the way to research in inducing more structured neural network representations that encode task-specific priors. Finally, concatenation of layer outputs is a viable form to share information across layers as has also been demonstrated by recent models such as DenseNet ( Huang et al., 2017). Figure 2 presents the final α weights in the sluice networks for Chunking, NER, and SRL, trained with newswire as training data. We see that a) for the low-level simplified SRL, there is more sharing at inner layers, which is in line with (Søgaard and Goldberg, 2016), while Chunking and NER also rely on the outer layer, and b) more infor- mation is shared from the more complex target tasks than vice versa.Ablation Analysis Different types of sharing may be more important than others. In order to analyze this, we perform an ablation analysis in Table 6. We investigate the impact of i) the α parameters; ii) the β parameters; and iii) the division into subspaces with an orthogonality penalty. We also evaluate whether concatenation of the outputs of each layer is a reason- able alternative to our mixture model.Inspecting the β values for the all-tasks sluice net in Table 5, we find that all tasks place little emphasis on the first layer, but prefer to aggregate their representations in different later layers of the model: The more semantic NER and chunking tasks use the second and third layer to a similar extent, while for POS tagging and simplified SRL the representation of one of the two later layers dominates the prediction.Ability to Fit Noise Sluice networks can learn to disregard sharing completely, so we expect them to be as good as single-task networks to fit random noise, potentially even better. We verify this by computing a learning curve for random relabelings of 200 sen- tences annotated with syntactic chunking brackets, as well as 100 gold standard POS-annotated sentences. Figure 3 shows that hard parameter sharing, while learning faster because of the smoother loss surface in multi-task learning, is a good regularizer, confirm- ing the findings in (Søgaard and Goldberg, 2016). The sluice network is even better at fitting noise than the single-task models. We believe the ability to fit noise in practical settings (on datasets of average size, with average hyper-parameters) is more informative than Rademacher complexities ( Zhang et al., 2017) and we argue that this result suggests introducing additional inductive bias may be beneficial. In the context of deep neural networks, multi-task learning is often done with hard or soft parameter sharing of hidden layers. Hard parameter sharing was introduced by Caruana (1993). There, all hidden layers are shared between tasks which are projected into output layers specific to different tasks. This multi-task learning approach is easy to implement, reduces overfitting, but is only guaranteed to work for (certain types of) closely related tasks (Baxter, 2000;Maurer, 2007). Peng and Dredze (2016) apply a variation of hard parameter sharing to multi-domain multi-task se- quence tagging with a shared CRF layer and domain- specific projection layers. Yang et al. (2016) also use hard parameter sharing to jointly learn different sequence-tagging tasks (NER, POS tagging, Chunk- ing) across languages. They also use word and char- acter embeddings and share character embeddings in their model. Martínez Alonso and Plank (2017) explore a similar set-up, but sharing is limited to the initial layer. In all three papers, the amount of sharing between the networks is fixed in advance.In soft parameter sharing, on the other hand, each task has separate parameters and separate hidden lay- ers, as in our architecture, but the loss at the outer layer is regularized by the current distance between the models. In ( Duong et al., 2015), for example, the loss is regularized by the L 2 distance between (selec- tive parts of) the main and auxiliary models. Other regularization schemes used in multi-task learning include the 1 // 2 group lasso ( Argyriou et al., 2008) and the trace norm (Ji and Ye, 2009). regularized with orthogonality and similarity con- straints for domain adaptation for computer vision. Liu et al. (2017) use a similar technique for sentiment analysis.In contrast to all the work mentioned above, we do not limit ourselves to a predefined way of sharing, but let the model learn which parts of the network to share using latent variables, the weights of which are learned in an end-to-end fashion.The work most related to ours is ( Misra et al., 2016), who also look into learning what to share in multi-task learning. However, they only consider a very small class of the architectures that are learn- able in sluice networks. Specifically, they restrict themselves to learning split architectures. In such architectures, two n-layer networks share the inner- most k layers with 0 ≤ k ≤ n, and they learn k with a mechanism that is very similar to our α-values. Our work can be seen as a generalization of ( Misra et al., 2016), including a more in-depth analysis of augmented works.Selective Sharing Kumar and Daumé III (2012) and Maurer et al. (2013) enable selective sharing by allowing task predictors to select from sparse pa- rameter bases for homogeneous tasks. Several au- thors have discussed which parts of the model to share for heterogeneous tasks. Søgaard and Goldberg (2016) perform experiments on which hidden layers to share in the context of hard parameter shar- ing with deep recurrent neural networks for sequence tagging. They show that low-level tasks, i.e. easy natural language processing tasks typically used for preprocessing such as part-of-speech tagging and named entity recognition, should be supervised at lower layers when used as auxiliary tasks.Another line of work looks into separating the learned space into a private (i.e. task-specific) and shared space ( Salzmann et al., 2010;Virtanen et al., 2011) to more explicitly capture the difference be- tween task-specific and cross-task features. To en- force such behavior, constraints are enforced to pre- vent the models from duplicating information. Bousmalis et al. (2016) use shared and private encoders
