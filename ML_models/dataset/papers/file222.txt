Deep learning methods have shown great promise in a variety of artificial intelligence tasks such as image classification ( Krizhevsky et al., 2012;Simonyan &amp; Zisserman, 2014), semantic segmen- tation ( Noh et al., 2015;Shelhamer et al., 2016;Ronneberger et al., 2015), and natural image gen- eration ( Goodfellow et al., 2014;Kingma &amp; Welling, 2014;. Some key network layers, such as convolutional layers ( LeCun et al., 1998), pooling layers, fully connected layers and deconvolutional layers, have been frequently used to create deep models for different tasks. De- convolutional layers, also known as transposed convolutional layers (Vedaldi &amp; Lenc, 2015), are initially proposed in ( Zeiler et al., 2010;. They have been primarily used in deep models that require up-sampling of feature maps, such as generative models (Radford et al., 2015;Makhzani &amp; Frey, 2015;Rezende et al., 2014) and encoder-decoder architectures (Ronneberger et al., 2015;Noh et al., 2015). Although deconvolutional layers are capable of producing larger feature maps from smaller ones, they suffer from the problem of checkerboard artifacts ( Odena et al., 2016). This greatly limits deep model's capabilities in generating photo-realistic images and producing smooth outputs on semantic segmentation. To date, very little efforts have been devoted to improving the deconvolution operation.In this work, we propose a simple, efficient, yet effective method, known as the pixel deconvolutional layer (PixelDCL), to address the checkerboard problem suffered by deconvolution operations. Our method is motivated from a fresh interpretation of deconvolution operations, which clearly pinpoints the root of checkerboard artifacts. That is, the up-sampled feature map generated by deconvolution can be considered as the result of periodical shuffling of multiple intermediate feature maps com- puted from the input feature map by independent convolutions. As a result, adjacent pixels on the output feature map are not directly related, leading to the checkerboard artifacts. To overcome this problem, we propose the pixel deconvolutional operation to be used in PixelDCL. In this new layer, the intermediate feature maps are generated sequentially so that feature maps generated in a later stage are required to depend on previously generated ones. In this way, direct relationships among adjacent pixels on the output feature map have been established. Sequential generation of intermedi- ate feature maps in PixelDCL may result in slight decrease in computational efficiency, but we show that this can be largely overcome by an implementation trick. Experimental results on semantic segmentation (samples in Figure 1) and image generation tasks demonstrate that the proposed Pix- elDCL can effectively overcome the checkerboard problem and improve predictive and generative performance.Our work is related to the pixel recurrent neural networks (PixelRNNs) (  and PixelCNNs (van den Reed et al., 2017), which are generative models that con- sider the relationship among units on the same feature map. They belong to a more general class of autoregressive methods for probability density estimation (Germain et al., 2015;Larochelle &amp; Murray, 2011). By using masked convolutions in training, the training time of PixelRNNs and PixelCNNs is comparable to that of other generative models such as generative adversarial networks (GANs) ( Goodfellow et al., 2014;Reed et al., 2016) and variational auto- encoders (VAEs) (Kingma &amp; Welling, 2014;Johnson et al., 2016). However, the prediction time of PixelRNNs or PixelCNNs is very slow since it has to generate images pixel by pixel. In contrast, our PixelDCL can be used to replace any deconvolutional layer in a plug-and-play manner, and the slight decrease in efficiency can be largely overcome by an implementation trick.We introduce deconvolutional layers and analyze the cause of checkerboard artifacts in this section. We then propose the pixel deconvolutional layers and the implementation trick to improve efficiency.Deconvolutional networks and deconvolutional layers are proposed in ( Zeiler et al., 2010;. They have been widely used in deep models for applications such as semantic segmentation (Noh et al., 2015) and generative models (Kingma &amp; Welling, 2014;Goodfellow et al., 2014;). Many encoder-decoder architectures use deconvolutional layers in decoders for up-sampling. One way of understanding deconvolutional operations is that the up-sampled output feature map is obtained by periodical shuffling of multiple intermediate feature maps obtained by applying multiple convolutional operations on the input feature maps ( Shi et al., 2016). This interpretation of deconvolution in 1D and 2D is illustrated in Figures 2 and 3, respectively.It is clear from these illustrations that standard deconvolutional operation can be decomposed into several convolutional operations depending on the up-sampling factor. In the following, we assume the up-sampling factor is two, though deconvolution operations can be applied to more generic settings. Formally, given an input feature map F in , a deconvolutional layer can be used to generate an up-sampled output F out as follows:where denotes the convolutional operation and ⊕ denotes the periodical shuffling and combina- tion operation as in Figure 3,  This implies that the values of adjacent pixels can be significantly different from each other, result- ing in the problem of checkerboard artifacts (Odena et al., 2016) as illustrated in Figure 4. One way to alleviate checkerboard artifacts is to apply post-processing such as smoothing ( Li et al., 2001), but this adds additional complexity to the network and makes the entire network not fully trainable.In this work, we propose the pixel deconvolutional operation to add direct dependencies among intermediate feature maps, thereby making the values of adjacent pixels close to each other and effectively solving the checkerboard artifact problem. In addition, our pixel deconvolutional layers can be easily used to replace any deconvolutional layers without compromising the fully trainable capability.To solve the checkerboard problem in deconvolutional layers, we propose the pixel deconvolutional layers ( where [·, ·] denotes the juxtaposition of feature maps. Note that in Eqn. 2, k i denotes a set of kernels as it involves convolution with the juxtaposition of multiple feature maps. Since the intermediate feature maps in Eqn. 2 depend on both the input feature map and the previously generated ones, we term it input pixel deconvolutional layer (iPixelDCL). Through this process, pixels on output feature maps will be conditioned not only on input feature maps but also on adjacent pixels. Since there are direct relationships among intermediate feature maps and adjacent pixels, iPixelDCL is expected to solve the checkerboard problem to some extent. Note that the relationships among intermediate feature maps can be very flexible. The intermediate feature maps generated later on can rely on part or all of previously generated intermediate feature maps. This depends on the design of pixel dependencies in final output feature maps. Figure 5 illustrates a specific design of sequential dependencies among intermediate feature maps.In iPixelDCL, we add dependencies among generated intermediate feature maps, thereby making adjacent pixels on final output feature maps directly related to each other. In this process, the in- formation of the input feature map is repeatedly used when generating intermediate feature maps. When generating the intermediate feature maps, information from both the input feature map and previous intermediate feature maps is used. Since previous intermediate feature maps already con- tain information of the input feature map, the dependencies on the input feature map can be removed. Removing such dependencies for some intermediate feature maps can not only improve the compu- tational efficiency but also reduce the number of trainable parameters in deep models.In this simplified pixel deconvolutional layer, only the first intermediate feature map will depend on the input feature map. The intermediate feature maps generated afterwards will only depend on previously generated intermediate feature maps. This will simplify the dependencies among pixels on final output feature map. In this work, we use PixelDCL to denote this simplified design. Our experimental results show that PixelDCL yields better performance than iPixelDCL and regular deconvolution. Compared to Eqn. 2, F out in PixelDCL is obtained as follows:PixelDCL is illustrated in Figure 5 by removing the connections denoted with dash lines. When analyzing the relationships of pixels on output feature maps, it is clear that each pixel will still rely on adjacent pixels. Therefore, the checkerboard problem can be solved with even better computational efficiency. Meanwhile, our experimental results demonstrate that the performance of models with these simplified dependencies is even better than that with complete connections. This demonstrates that repeated dependencies on the input may not be necessary.Pixel deconvolutional layers can be applied to replace any deconvolutional layers in various mod- els involving up-sampling operations such as U-Net ( Ronneberger et al., 2015), VAEs (Kingma &amp; Welling, 2014) and GANs ( Goodfellow et al., 2014). By replacing deconvolutional layers with pixel deconvolutional layers, deconvolutional networks become pixel deconvolutional networks (PixelDCN). In U-Net for semantic segmentation, pixel deconvolutional layers can be used to up- sample from low-resolution feature maps to high-resolution ones. In VAEs, they can be applied in decoders for image reconstruction. The generator networks in GANs typically use deep model (Rad- ford et al., 2015) and thus can employ pixel deconvolutional layers to generate large images. In our Figure 6: An efficient implementation of the pixel deconvolutional layer. In this layer, a 4×4 feature map is up-sampled to a 8×8 feature map. The purple feature map is generated through a 3×3 convolutional operation from the input feature map (step 1). After that, another 3×3 convolutional operation is applied on the purple feature map to produce the orange feature map (step 2). The purple and orange feature maps are dilated and added together to form a larger feature map (step 3). Since there is no relationship between the last two intermediate feature maps, we can apply a masked 3×3 convolutional operation, instead of two separate 3×3 convolutional operations (step 4). Finally, the two large feature maps are combined to generate the final output feature map (step 5).experiments, we evaluate pixel deconvolutional layers in U-Net and VAEs. The results show that the performance of pixel deconvolutional layers outperforms deconvolutional layers in these networks.In practice, the most frequently used up-sampling operation is to increase the height and width of input feature maps by a factor of two, e.g., from 2×2 to 4×4. In this case, the pixels on output feature maps can be divided into four groups as in Eqn. 1. The dependencies can be defined as in Figure 5. When implementing pixel deconvolutional layers, we design a simplified version to reduce sequential dependencies for better parallel computation and training efficiency as illustrated in Figure 6. Experimental Setup: We use the PASCAL 2012 segmentation dataset ( Everingham et al., 2010) and MSCOCO 2015 detection dataset ( Lin et al., 2014) to evaluate the proposed pixel deconvo- lutional methods in semantic segmentation tasks. For both datasets, the images are resized to 256×256×3 for batch training. Our models directly predict the label for each pixel without any post-processing. Here we examine our models in two ways: training from scratch and fine-tuning from state-of-art model such as DeepLab-ResNet.For the training from scratch experiments, we use the U-Net architecture (Ronneberger et al., 2015) as our base model as it has been successfully applied in various image segmentation tasks. The network consists of four blocks in the encoder path and four corresponding blocks in the decoder path. Within each decoder block, there is a deconvolutional layer followed by two convolutional layers. The final output layer is adjusted based on the number of classes in the dataset. The PASCAL 2012 segmentation dataset has 21 classes while the MSCOCO 2015 detection dataset has 81 classes. As the MSCOCO 2015 detection dataset has more classes than the PASCAL 2012 segmentation dataset, the number of feature maps in each layer for this dataset is doubled to accommodate more output channels. The baseline U-Net model employs deconvolutional layers within the decoder path to up-sample the feature maps. We replace the deconvolutional layers with our proposed pixel deconvolutional layers (iPixelDCL) and their simplified version (PixelDCL) while keeping all other variables unchanged. The kernel size in DCL is 6×6, which has the same number of parameters as iPixelDCL with 4 sets of 3×3 kernels, and more parameters than PixelDCL with 2 sets of 3×3 and 1 set of 2×2 kernels. This will enable us to evaluate the new pixel deconvolutional layers against the regular deconvolutional layers while controlling all other factors.For the fine-tuning experiments, we fine-tune our models based on the architecture of DeepLab- ResNet ( Chen et al., 2016). The DeepLab-ResNet model is fine-tuned from ResNet101 ( He et al., 2016) and also use external data for training. The strategy of using external training data and fine- tuning from classic ResNet101 greatly boosts the performance of the model on both accuracy and mean IOU. The output of DeepLab-ResNet is eight times smaller than the input image on the height and width dimensions. In order to recover the original dimensions, we add three up-sampling blocks, each of which up-samples the feature maps by a factor of 2. For each up-sampling block, there is a deconvolutional layer followed by a convolutional layer. By employing the same strategy, we replace the deconvolutional layer by PixelDCL and iPixelDCL using kernels of the same size as in the training from scratch experiments.Analysis of Results: Some sample segmentation results of U-Net using deconvolutional layers (DCL), iPixelDCL, and PixelDCL on the PASCAL 2012 segmentation dataset and the MSCOCO 2015 detection dataset are given in Figures 7 and 8, respectively. We can see that U-Net models using iPixelDCL and PixelDCL can better capture the local information of images than the same base model using regular deconvolutional layers. By using pixel deconvolutional layers, more spacial features such as edges and shapes are considered when predicting the labels of adjacent pixels.Moreover, the semantic segmentation results demonstrate that the proposed models tend to produce smoother outputs than the model using deconvolution. We also observe that, when the training epoch is small (e.g., 50 epochs), the model that employs PixelDCL has better segmentation outputs than the model using iPixelDCL. When the training epoch is large enough (e.g., 100 epochs), they have similar performance, though PixelDCL still outperforms iPixelDCL in most cases. This indicates that PixelDCL is more efficient and effective, since it has much fewer parameters to learn. Table 1 shows the evaluation results in terms of pixel accuracy and mean IOU on the two datasets. The U-Net models using iPixelDCL and PixelDCL yield better performance than the same base  model using regular deconvolution. The model using PixelDCL slightly outperforms the model us- ing iPixelDCL. For the models fine-tuned from Deeplab-ResNet, the models using iPixelDCL and PixelDCL have better performance than the model using DCL, with iPixelDCL performs the best. In semantic segmentation, mean IOU is a more accuracy evaluation measure than pixel accuracy (Everingham et al., 2010). The models using pixel deconvolution have better evaluation results on mean IOU than the base model using deconvolution.Experimental Setup: The dataset used for image generation is the celebFaces attributes (CelebA) dataset ( Liu et al., 2015). To avoid the influence of background, the images have been preprocessed so that only facial information is retained. The image generation task is to reconstruct the faces excluding backgrounds in training images. The size of images is 64 × 64 × 3. We use the standard variational auto-encoder (VAE) (Kingma &amp; Welling, 2014) as our base model for image generation. The decoder part in standard VAE employs deconvolutional layers for up-sampling. We apply our proposed PixelDCL to replace deconvolutional layers in decoder while keeping all other components the same. The kernel size in DCL is 6×6, which has more parameters than PixelDCL with 2 sets of 3×3 and 1 set of 2×2 kernels.Analysis of Results: Figure 9 shows the generated faces using VAEs with regular deconvolution (baseline) and PixelDCL in decoders. Some images generated by the baseline model suffer from  apparent checkerboard artifacts, while none is found on the images generated by the model with PixelDCL. This demonstrates that the proposed pixel deconvolutional layers are able to establish direct relationships among adjacent pixels on generated feature maps and images, thereby effectively overcoming the checkerboard problem. Our results demonstrate that PixelDCL is very useful for generative models since it can consider local spatial information and produce photo-realistic images without the checkerboard problem. Table 2 shows the comparison of the training and prediction time of the U-Net models using DCL, iPixelDCL, and PixelDCL for up-sampling. We can see that the U-Net models using iPixelDCL and PixelDCL take slightly more time during training and prediction than the model using DCL, since the intermediate feature maps are generated sequentially. The model using PixelDCL is more efficient due to reduced dependencies and efficient implementation discussed in Section 2.3. Overall, the increase in training and prediction time is not dramatic, and thus we do not expect this to be a major bottleneck of the proposed methods.In this work, we propose pixel deconvolutional layers that can solve the checkerboard problem in deconvolutional layers. The checkerboard problem is caused by the fact that there is no direct rela- tionship among intermediate feature maps generated in deconvolutional layers. PixelDCL proposed here try to add direct dependencies among these generated intermediate feature maps. PixelDCL generates intermediate feature maps sequentially so that the intermediate feature maps generated in a later stage are required to depend on previously generated ones. The establishment of dependencies in PixelDCL can ensure adjacent pixels on output feature maps are directly related. Experimental results on semantic segmentation and image generation tasks show that PixelDCL is effective in overcoming the checkerboard artifacts. Results on semantic segmentation also show that PixelDCL is able to consider local spatial features such as edges and shapes, leading to better segmentation results. In the future, we plan to employ our PixelDCL in a broader class of models, such as the generative adversarial networks (GANs).
