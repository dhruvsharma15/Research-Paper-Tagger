The task of automatically describing videos containing rich and open-domain activities poses an important chal- lenges for computer vision and machine learning research. It also has a variety of practical applications. For example, However, if a video is poorly tagged, its utility is dramati- cally diminished [24]. Automatic video description gener- ation has the potential to help improve indexing and search quality for online videos. In conjunction with speech syn- thesis technology, annotating video with natural language descriptions also has the potential to benefit the visually im- paired.While image description generation is already consid- ered a very challenging task, the automatic generation of video description carries additional difficulties. Simply dealing with the sheer quantity of information contained in video data is one such challenge. Moreover, video descrip- tion involves generating a sentence to characterize a video clip lasting typically 5 to 10 seconds, or 120 to 240 frames. Often such clips contain complex interactions of actors and objects that evolve over time. All together it amounts to a vast quantity of information, and attempting to represent this information using a single, temporally collapsed feature representation is likely to be prone to clutter, with tempo- rally distinct events and objects being potentially fused in- coherently. It is therefore important that an automatic video description generator exploit the temporal structure under- lying video.We argue that there are two categories of temporal struc- ture present in video: (1) local structure and (2) global struc- ture. Local temporal structure refers to the fine-grained mo- tion information that characterizes punctuated actions such as "answering the telephone" or "standing up". Actions such as these are relatively localized in time, evolving over only a few consecutive frames. On the other hand, when we refer to global temporal structure in video, we refer to the sequence in which objects, actions, scenes and people, etc. appear in a video. Video description may well be termed video summarization, because we typically look for a sin- gle sentence to summarize what can be a rather elaborate sequence of events. Just as good image descriptions often focus on the more salient parts of the image for description, we argue that good video description systems should selec- tively focus on the most salient features of a video sequence.Recently, Venugopalan et al. [41] used a so-called encoder-decoder neural network framework [9] to automat- ically generate the description of a video clip. They ex- tracted appearance features from each frame of an input video clip using a previously trained convolutional neural network [22]. The features from all the frames, or subsam- pled frames, were then collapsed via simple averaging to result in a single vector representation of the entire video clip. Due to this indiscriminate averaging of all the frames, this approach risks ignoring much of the temporal structure underlying the video clip. For instance, it is not possible to tell the order of the appearances of two objects from the collapsed features.In this paper, we introduce a temporal attention mech- anism to exploit global temporal structure. We also aug- ment the appearance features with action features that en- code local temporal structure. Our action features are de- rived from a spatio-temporal convolutional neural network (3-D CNN) [39,19,16]. The temporal attention mechanism is based on a recently proposed soft-alignment method [1] which was used successfully in the context of machine translation. While generating a description, the temporal at- tention mechanism selectively focuses on a small subset of frames, making it possible for the generator to describe only the objects and/or activities in that subset (see Fig. 1 for the graphical illustration). Our 3-D CNN, on the other hand, starts from both temporally and spatially local motion de- scriptors of video and hierarchically extracts more abstract action-related features. These features preserve and empha- size important local structure embedded in video for use by the description generator.We evaluate the effectiveness of the proposed mecha- nisms for exploiting temporal structure on the most widely used open-domain video description dataset, called the Youtube2Text dataset [7], which consists of 1,970 video clips with multiple descriptions per video. We also test the proposed approaches on a much larger, and more recently proposed, dataset based on the descriptive video service (DVS) tracks in DVD movies [38], which contains 49,000 video clips.Our work makes the following contributions: 1) We pro- pose the use of a novel 3-D CNN-RNN encoder-decoder architecture which captures local spatio-temporal informa- tion. We find that despite the promising results generated by both prior work and our own here using static frame CNN- RNN video description methods, our experiments suggest that it is indeed important to exploit local temporal struc- ture when generating a description of video. 2) We pro- pose the use of an attention mechanism within a CNN- RNN encoder-decoder framework for video description and we demonstrate through our experiments that it allows fea- tures obtained through the global analysis of static frames throughout the video to be used more effectively for video description generation. Furthermore, 3) we observe that the improvements brought by exploiting global and local tem- poral information are complimentary, with the best perfor- mance achieved when both the temporal attention mecha- nism and the 3-D CNN are used together.In this section, we describe a general approach, based purely on neural networks to generate video descriptions. This approach is based on the encoder-decoder frame- work [9], which has been successfully used in machine translation [33,9,1] as well as image caption genera- tion [20,12,42,44,18].The encoder-decoder framework consists of two neu- ral networks; the encoder and the decoder. The encoder network φ encodes the input x into a continuous-space representation which may be a variable-sized set V = {v 1 , . . . , v n } of continuous vectors:The architecture choice for the encoder φ depends on the type of input. For example, in the case of machine transla- tion, it is natural to use a recurrent neural network (RNN) for the encoder, since the input is a variable-length sequence of symbols [33,9]. With an image as input, a convolutional neural network (CNN) is another good alternative [44].The decoder network generates the corresponding out- put y from the encoder representation V . As was the case with the encoder, the decoder's architecture must be chosen according to the type of the output. When the output is a natural language sentence, which is the case in automatic video description, an RNN is a method of choice.The decoder RNN ψ runs sequentially over the output sequence. In brief, to generate an output y, at each step t the RNN updates its internal state h t based on its previous internal state h t−1 as well as the previous output y t−1 and the encoder representation V , and then outputs a symbol y t :open [41] and closed [12] domains. Among these recently successful applications of the RNN in natural language gen- eration, it is noticeable that most of them [33,9,1,42,44], if not all, used long short-term memory (LSTM) units [14] or their variant, gated recurrent units (GRU) [9]. In this pa- per, we also use a variant of the LSTM units, introduced in [45], as the decoder.The LSTM decoder maintains an internal memory state c t in addition to the usual hidden state h t of an RNN (see Eq. (1)). The hidden state h t is the memory state c t modu- lated by an output gate:where is an element-wise multiplication. The output gate o t is computed by where for now we simply note as ψ the function updating the RNN's internal state and computing its output. The RNN is run recursively until the end-of-sequence symbol is generated, i.e., y t = In the remaining of this section, we detail choices for the encoder and decoder for a basic automatic video description system, taken from [41] and on which our work builds.Deep convolutional neural networks (CNNs) have re- cently been successful at large-scale object recognition [22,34]. Beyond the object recognition task itself, CNNs trained for object recognition have been found to be useful in a va- riety of other computer vision tasks such as object local- ization and detection (see, e.g., [29]). This has opened a door to a flood of computer vision systems that exploit rep- resentations from upper or intermediate layers of a CNN as generic high-level features for vision. For instance, the ac- tivation of the last fully-connected layer can be used as a fixed-size vector representation [20], or the feature map of the last convolutional layer can be used as a set of spatial feature vectors [44].In the case where the input is a video clip, an image- trained CNN can be used for each frame separately, result- ing in a single vector representation v i of the i-th frame. This is the approach proposed by [41], which used the con- volutional neural network from [22]. In our work here, we will also consider using the CNN from [34], which has demonstrated higher performance for object recognition.where σ is the element-wise logistic sigmoid function and ϕ t is a time-dependent transformation function on the en- coder features. W o , U o , A o and b o are, in order, the weight matrices for the input, the previous hidden state, the con- text from the encoder and the bias. E is a word embedding matrix, and we denote by E [y t−1 ] an embedding vector of word y t−1 .The memory state c t is computed as a weighted sum be- tween the previous memory state c t−1 and the new memory content update˜cupdate˜ update˜c t :where the coefficients -called forget and input gates respec- tively -are given byThe updated memory content˜ccontent˜ content˜c t also depends on the current input y t−1 , previous hidden state h t−1 and the features from the encoder representation ϕ t (V ):Once the new hidden state h t is computed, a probability distribution over the set of possible words is obtained using a single hidden layer neural networkAs discussed earlier, it is natural to use a recurrent neu- ral network (RNN) as a decoder when the output is a natural language sentence. This has been empirically confirmed in the contexts of machine translation [33,9,1], image cap- tion generation [42,44] and video description generation in where W p , U p , b p , d are the parameters of this network, [. . . ] denotes vector concatenation. The softmax function allows us to interpret p t as the probabilities of the distribu- tion p(y t | y &lt;t , V ) over words.At a higher level, the LSTM decoder can be written down asIt is then trivial to generate a sentence from the LSTM decoder. For instance, one can recursively evaluate ψ and sample from the returned p(y t | . . . ) until the sampled y t is the end-of-sequence symbol. One can also approximately find the sentence with the highest probability by using a simple beam search [33].In [41], Venugopalan et al. used this type of LSTM de- coder for automatic video description generation. However, in their work the feature transformation function ϕ t con- sisted in a simple averaging, i.e., where the v i 's are the elements of the set V returned by the CNN encoder from Sec. 2.2. This averaging effectively collapses all the frames, indiscriminate of their temporal re- lationships, leading to the loss of temporal structure under- lying the input video.In this section, we delve into the main contributions of this paper and propose an approach for exploiting both the local and global temporal structure in automatic video de- scription.Our 3-D CNN architecture is composed of three 3-D convolutional layer, each followed by rectified linear activa- tions (ReLU) and local max-pooling. From the activation of the last 3-D convolution+ReLU+pooling layer, which pre- serves the temporal arrangement of the input video and ab- stracts the local motion features, we can obtain a set of tem- poral feature vectors by max-pooling along the spatial di- mensions (width and height) to get feature vectors that each summarize the content over short frame sequences within the video. Finally, these feature vectors are combined, by concatenation, with the image features extracted from sin- gle frames taken at similar positions across the video. Fig. 5 illustrates the complete architecture of the described 3-D CNN. Similarly to the object recognition trained CNN (see Sec. 2.2), the 3-D CNN is pre-train on activity recognition datasets.A Temporal Attention MechanismWe propose to model the local temporal structure of videos at the level of the temporal features V = {v 1 , . . . , v n } that are extracted by the encoder. Specifi- cally, we propose to use a spatio-temporal convolutional neural network (3-D CNN) which has recently been demon- strated to capture well the temporal dynamics in video clips [39,19].We use a 3-D CNN to build the higher-level represen- tations that preserve and summarize the local motion de- scriptors of short frame sequences. This is done by first dividing the input video clip into a 3-D spatio-temporal grid of 16 × 12 × 2 (width × height × timesteps) cuboids. Each cuboid is represented by concatenating the histograms of oriented gradients, oriented flow and motion boundary (HoG, HoF, MbH) [10,43] with 33 bins. This transforma- tion is done in order to make sure that local temporal struc- ture (motion features) are well extracted and to reduce the computation of the subsequence 3-D CNN.The 3-D CNN features of the previous section allows us to better represent short-duration actions in a subset of con- secutive frames. However, representing a complete video by averaging these local temporal features as in Eq. 4 would jeopardize the model's ability to exploit the video's global temporal structure.Our approach to exploiting such non-local temporal structure is to let the decoder selectively focus on only a small subset of frames at a time. By considering subsets of frames in sequence, the model can exploit the temporal or- dering of objects and actions across the entire video clip and avoid conflating temporally disparate events. Our approach also has the potential of allowing the model to focus on key elements of the video that may have short duration. Meth- ods that collapse the temporal structure risk overwhelming these short duration elements.Specifically, we propose to adapt the recently pro- posed soft attention mechanism from [1], which allows the decoder to weight each temporal feature vector V = {v 1 , . . . , v n }. This approach has been used successfully by Xu et al. [44] for exploiting spatial structure underlying an image. Here, we thus adapt it to exploit the temporal struc- ture of video instead.Instead of a simple averaging strategy (as shown in Eq. (4)), we take the dynamic weighted sum of the temporal feature vectors such that structure, if there is useful temporal structure in the data. Later in Sec. 5, we empirically show that this is indeed the case. See Fig. 3 for the graphical illustration of the temporal attention mechanism. n 4. Related Work(t) i = 1 and α i 's are computed at each time step t inside the LSTM decoder (see Sec. 2.3). We refer to α (t) i as the attention weights at time t.The attention weight α (t) i reflects the relevance of the i-th temporal feature in the input video given all the previ- ously generated words, i.e., y 1 , . . . y t−1 . Hence, we design a function that takes as input the previous hidden state h t−1 of the LSTM decoder, which summarizes all the previously generated words, and the feature vector of the i-th temporal feature and returns the unnormalized relevance score e (t) i :where w, W a , U a and b a are the parameters that are esti- mated together with all the other parameters of the encoder and decoder networks. Once the relevance scores efor all the frames i = 1, . . . , n are computed, we normalize them to obtain the αWe refer to the attention mechanism as this whole process of computing the unnormalized relevance scores and nor- malizing them to obtain the attention weights. The attention mechanism allows the decoder to selec- tively focus on only a subset of frames by increasing the attention weights of the corresponding temporal feature. However, we do not explicitly force this type of selective attention to happen. Rather, this inclusion of the atten- tion mechanism enables the decoder to exploit the temporal Video description generation has been investigated and studied in other work, such as [21,2,28]. Most of these examples have, however, constrained the domain of videos as well as the activities and objects embedded in the video clips. Furthermore, they tend to rely on hand-crafted vi- sual representations of the video, to which template-based or shallow statistical machine translation approaches were applied. In contrast, the approach we take and propose in this paper aims at open-domain video description gen- eration with deep trainable models starting from low-level video representations, including raw pixel intensities (see Sec. 2.2) and local motion features (see Sec. 3.1).In this sense, the approach we use here is more closely related to the recently introduced static image caption gen- eration approaches based mainly on neural networks [20,12,42,44,18]. A neural approach to static image caption generation has recently been applied to video description generation by Venugopalan et al. [41]. However, their di- rect adaptation of the underlying static image caption gener- ation mechanism to the videos is limited by the fact that the model tends to ignore the temporal structure of the under- lying video. Such structure has demonstrated to be helpful in the context of event and action classification [35,13,6], and is explored in this paper. Other recent work [27] has explored the use of DVS annotated video for video descrip- tion research and has underscored the observation that DVS descriptions are typically much more relevant and accurate descriptions of the visual content of a video compared to movie scripts. They present results using both DVS and script based annotations as well as cooking activities.While other work has explored 3-D Deep Networks for video [36,16,18,30] our particular approach differs in a number of ways from prior work in that it is based on CNNs as opposed to other 3-D deep architectures and we focus on pre-training the model on a number of widely used action recognition datasets. In contrast to other 3-D CNN for- mulations, the input to our 3-D CNN consists of features derived from a number of state of the art image descrip- tors. Our model is also fully 3-D in that we model en- tire volumes across a video clip. In this paper, we use a state-of-the-art static convolutional neural network (CNN) and a novel spatio-temporal 3-D CNN to model input video clips. This way of modeling video using feedforward con- volutional neural networks, has become increasingly pop- ular recently [41,30,39]. However, there has also been a stream of research on using recurrent neural networks (RNN) for modeling video clips. For instance, in [32] We test the proposed approaches on two video- description corpora: Youtube2Text [7] and DVS [38]. Im- plementations are available at https://github.com/ yaoli/arctic-capgen-vid.Video Preprocessing To reduce the computational and memory requirement, we only consider the first 240 frames of each video 3 For appearance features, (trained) 2- D GoogLeNet [34] CNN is used to extract fixed-length representation (with the help of the popular implemen- tation in Caffe [17]). Features are extracted from the pool5/7x7 s1 layer. We select 26 equally-spaced frames out of the first 240 from each video and feed them into the CNN to obtain a 1024 dimensional frame-wise feature vec- tor. We also apply the spatio-temporal 3-D CNN (trained as described in Sec. 5.2) in order to extract local motion Youtube2Text The Youtube2Text video corpus [7] is well suited for training and evaluating an automatic video de- scription generation model. The dataset has 1,970 video clips with multiple natural language descriptions for each video clip. In total, the dataset consists of approximately 80,000 video / description pairs, with the vocabulary of approximately 16,000 unique words. The dataset is open- domain and covers a wide range of topics including sports, animals and music. Following [41], we split the dataset into a training set of 1,200 video clips, a validation set of 100 clips and a test set consisting of the remaining clips.DVS The DVS dataset was recently introduced in [38] with a much larger number of video clips and accompa- nying descriptions than the existing video/description cor- pora such as Youtube2Text. It contains video clips extracted from 92 DVD movies along with semi-automatically tran- scribed descriptive video service (DVS) narrations. The dataset consists of 49,000 video clips covering a wide vari- ety of situations. We follow the standard split of the dataset into a training set of 39,000 clips, a validation set of 5,000 clips and a test set of 5,000 clips, as suggested by [38].We test four different model variations for video description generation based on the underlying encoder- decoder framework, with results presented in Table 1. Enc- Dec (Basic) denotes a baseline incorporating neither local nor global temporal structure. Is it based on an encoder using the 2-D GoogLeNet CNN [34] as discussed in Sec- tion 2.2 and the LSTM-based decoder outlined in Section 2.3. Enc-Dec + Local incorporates local temporal struc- ture via the integration of our proposed 3-D CNN features (as outlined in Section 3.1) with the 2-D GoogLeNet CNN features as described above. Enc-Dec + Global adds the temporal attention mechanism of Section 3.2. Finally, Enc- Dec + Local + Global incorporates both the 3-D CNN and the temporal attention mechanism into the model. All mod- els otherwise use the same number of temporal features v i . These experiments will allow us to investigate whether the contributions from the proposed approaches are com- plimentary and can be combined to further improve perfor- mance. Training For all video description generation models, we estimated the parameters by maximizing the log-likelihood:N tn temporal structure (the fourth row in Table 1). We observed this gain consistently across both datasets as well as using all four automatic evaluation metrics.where there are N training video-description pairs (x n , y n ), and each description y n is t n words long. We used Adadelta [46] with the gradient computed by the backpropagation algorithm. We optimized the hyper- parameters (e.g. number of LSTM units and the word em- bedding dimensionality) using random search to maximize the log-probability of the validation set. 5 Training contin- ued until the validation log-probability stopped increasing for 5,000 updates. As mentioned earlier in Sec. 3.1, the 3- D CNN was trained on activity recognition datasets. Due to space limitation, details regarding the training and eval- uation of the 3-D CNN on activity recognition datasets are provided in the Supplementary Material.Evaluation We report the performance of our proposed method using test set perplexity and three model-free au- tomatic evaluation metrics. These are BLEU [25], ME- TEOR [11] and CIDEr [40]. We use the evaluation script prepared and introduced in [8].In the first block of Table 1, we present the performance of the four different variants of the model using all four met- rics: BLEU, METEOR, CIDEr and perplexity. Subsequent lines in the table give comparisons with prior work. The first three rows (Enc-Dec (Basic), +Local and +Global), show that it is generally beneficial to exploit some type of tempo- ral structure underlying the video. Although this benefit is most evident with perplexity (especially with the temporal attention mechanism exploiting global temporal structure), we observe a similar trend with the other model-free metrics and across both Youtube2Text and DVS datasets.We observe, however, that the biggest gain can be achieved by letting the model exploit both local and global Although the model-free evaluation metrics such as the ones we used in this paper (BLEU, METEOR, CIDEr) were designed to reflect the agreement level between reference and generated descriptions, it is not intuitively clear how well those numbers (see Table 1) reflect the quality of the actual generated descriptions. Therefore, we present some of the video clips and their corresponding descriptions, both generated and reference, from the test set of each dataset. Unless otherwise labeled, the visualizations in this section are from the best model which exploits both global and local temporal structure (the fourth row of Table 1).In Fig. 4, two video clips from the test set of Youtube2Text are shown. We can clearly see that the gen- erated descriptions correspond well with the video clips. In  Fig. 4, we show also two sample video clips from the DVS dataset. Clearly, the model does not perform as well on the DVS dataset as it did on Youtube2Text, which was already evident from the quantitative analysis in Sec. 5.3. However, we still observe that the model often focuses correctly on a subset of frames according to the word to be generated. For instance, in the left pane, when the model is about to generate the second "SOMEONE", it focuses mostly on the first frame. Also, on the right panel, the model correctly at- tends to the second frame when the word "types" is about to be generated. As for the 3-D CNN local temporal features, we see that they allowed to correctly identify the action as "frying", as opposed to simply "cooking".More samples of the video clips and the gener- ated/reference descriptions can be found in the Supplemen- tary Material, including visualizations from the global tem- poral attention model alone (see the third row in Table 1). In this work, we address the challenging problem of pro- ducing natural language descriptions of videos. We iden- tify and underscore the importance of capturing both lo- for the frame when the corresponding word (color-coded) was generated. From the top left panel, we can see that when the word "road" is about to be generated, the model focuses highly on the third frame where the road is clearly visible. Similarly, on the bottom left panel, we can see that the model attends to the second frame when it was about to generate the word "Someone". The bottom row includes alternate descriptions generated by the other model variations.cal and global temporal structure in addition to frame-wise appearance information. To this end, we propose a novel 3-D convolutional neural network that is designed to cap- ture local fine-grained motion information from consecutive frames. In order to capture global temporal structure, we propose the use of a temporal attentional mechanism that learns the ability to focus on subsets of frames. Finally, the two proposed approaches fit naturally together into an encoder-decoder neural video caption generator. and Flickr [15]. We intend to more fully explore this direc- tion in future work. Figure 5. Illustration of the spatio-temporal convolutional neural network (3-D CNN). This network is trained for activity recogni- tion. Then, only the convolutional layers are involved when generating video descriptions.The 3-D CNN architecture is specified in Figure 5. Our model is composed of three 3-D convolutional layers, using 3×3×3 kernels. The number of output features after the different convolutions is given in Figure 5. Each convolutional layer is followed by a rectified linear activations (ReLU) and local max-pooling. After the convolutions, a fully-connected layer (dimension 2500, with ReLU activation) is applied, followed by a Softmax layer. A dropout of 0.5 is applied on those last two layers.Multitask learning is used to train the model on three human activity recognition datasets: UCF101 [31] with 13320 Youtube videos and 101 various human activity classes, HMDB51 [23] with 3700 videos and 51 various human activity classes, and a random subset of Sports-1M dataset [18] using 50,000 videos that have 487 sports labels.We trained the 3-D CNN using stochastic gradient descent with a momentum of 0.7. Learning rate is initially set to 0.1 and then is decreased, using the following scheme 0.05, 0.02, 0.01, each time the validation cost stagnate. At each iteration a minibatch of size 48 is constructed by sampling uniformly all 3 human activity datasets. We perturb each video along three axes to encourage the model to learn invariant feature representation, we take random crops of size 15×15×120 cuboids out of the original 20×20×120 cuboids. Video are also randomly flipped.Despite our interest in video-description, we validate that our model obtains reasonable performances on the activity recognition task using HMDB51 (split 1) and UCF101. On HMDB (split 1) our model achieves an accuracy of 52.3%, our result is 3% lower than the best motion-based single model (temporal-based CNN of [30]). On UCF-101, our 3-D CNN obtain an accuracy of 76.49%. While the temporal-based CNN [30] achieves 83.7%, our model outperforms other single 3-D convolution based approaches such as C3D (72.29%) [39] and slow-fusion convnet (65.4%) [19].Hyperparameters reflects the learning capacity of the models. We have made sure each type of models have been sufficient explored in their hyperparameters. The model selections on both Youtube2Text and DVS are performed by random search [4]. There are four types of models being trained:• Basic Enc-Dec• Basic + Global (Temporal Attention)• Basic + Local + Global For each of four types of models, we performed 50 experiments with random search on the critical hyperparameters. Each of the 50 experiments is associated with a specific hyperparameter setup. And the 50 setups are shared cross all four types of models. The critial hyperparameters experimented are:• the dimensionality of word embedding, in the range of [100,1000] • the dimensionality of LSTM hidden/memory states, in the range of [100,3000] • dropout, either use or not used, decided at random.The same procedure is used on both Youtube2Text and DVS datasets.  We illustrate the caption generation process of the proposed soft-attentional models trained with Basic+Global v.s. trained with Basic+Local+Global, with a dynamic α on frames for each word in the generated caption.The bar chart shows the magnitude of α. The generated caption is shown on the left. Each generated word corresponds to an α vector, show in the same row. Each bar corresponds to a particular frame on the very top of the figure, organized sequentially. Within the same row, the height of the bar shows the importance of its corresponding frame in generating that word. 20 frames are shown for better visibility.      Model shifts its attention across frames to generate a caption. The bar char shows the magitude of α, sum to 1 row-wise, the higher the bar, the bigger the magnitude. 3DConv att generates a more faithful description with a much richer content than Figure 10. It even learns to generate a rare work "teasing".This section illustrates on DVS, a much more challenging dataset. See the following figures for detailed explaination of soft-attention applied on videos with different properties. Figure 12. Model type: Basic + Global. The model tends to produce a smooth distribution in α row-wise, due to the uniformity of the scene with a slowly changing continuous shot.    Model type: Basic + Global. The model seems to focus on the second shot of the scene at the beginning, yet the part of the generated caption "out of the car" distributes a decent amount of its attention on the first scene as well. This may due to the fact that the memory of decoding LSTM already contains the information of almost the entire scene (two shots).  Figure 16. The model focuses on the car in the second shot when generating "sit", "back seat". When generating two "SOMEONE", it divides its attentio among two shots. Figure 18. Model type: Basic + Global. The description is argubly not very accurate Figure 19. Model type: Basic + Local + Global. With the help of additional features, the model successfully describes the cell phone and the room, a much faithful description than Figure 18 
