Inspired by the computational efficiency of human brain in processing unstructured data, neural networks have been explored since 1940s for a wide variety of data analytics applications. The latest generation of Deep Neural networks (DNNs) have achieved impressive successes rivaling typical human performance, thanks to their ability to capture hidden features from unstructured data using multiple layers of neu- rons [1]. However, as the number of layers (depth) of the networks increase, DNN training becomes computationally intense and time consuming due to the physically separated execution and memory units in conventional von Neumann machines. This has motivated the exploration of non-von Neumann architectures with closely integrated processing units and local memory elements in dense cross bar arrays with memristive devices [2].It has been recently proposed that DNNs can be imple- mented by 2D cross bar arrays of resistive processing units (RPUs) that can store multiple analog states and adjust its conductivity with simple voltage pulses [3]. These RPU de- vices when implemented in a cross bar array can accelerate DNN training if all the weights in the array can be updated in parallel. In this scheme, vector cross product operations of O(N 2 ) complexity, required for the back-propagation algo- rithm for network training, can be implemented with simple AND operation of stochastic bit streams representing the neuronal signals with a complexity of O(1). However, in order to maintain high accuracies, a stringent set of specifications have to be satisfied by memristive RPU devices, one of which is a resolution of 1000 conductance levels within a dynamic range of 10.Several memristive devices have been explored for realizing cross bar arrays for neuromorphic systems [2]- [5]. Recently, Phase Change Memory (PCM) arrays have been used to store synaptic weights of a 3-layer neural network for hand- written digit classification achieving an accuracy of 82.9% in the MNIST database [6]. Numerical simulation studies based on experimentally observed programming characteristics of Pr 1−x Ca x MnO 3 (PCMO) synapses suggest that MNIST recognition accuracies exceeding 90% is achievable [7]. Linear and symmetric conductance response are observed to improve accuracy; hence, several strategies have been proposed to compensate for the non linear and asymmetric conductance response of typical memristive devices [8]- [10]. It has been projected that NVM based DNNs can provide 25× speed- up and up to 3000× improvement in power compared to GPU based implementations [11]. Therefore, highly efficient neuromorphic systems can be developed if memristive device characteristics can be improved and algorithms co-optimized to account for their non-ideal limitations.Here, we first use stochastic weight updates to train a 4-layer deep network with double precision floating-point weights. We then study the performance of an equivalent network which uses a cross bar architecture with typical memristive devices with limited dynamic range, resolution, and conductance vari- ability [12]. To the best of our knowledge, this paper presents the first study of noise resilience for inference using stochastic learning of DNNs with non-ideal memristive devices. The main insight from this study is that the key device parameter to optimize the learning efficiency of DNNs is the variability in its programming characteristics. DNNs with such memristive synapses, even with dynamic range as low as 15 and resolution of 32 levels, when trained based on stochastic updates can achieve close to the floating point base-line accuracies (within 3%) in the benchmark hand-written digit recognition task. Furthermore, the degradation in performance of stochastic memristive DNNs when used as inference engines with noise corrupted data is better than that of the software baseline.This paper is organized as follows: We first discuss the fundamental basics of DNN training and methods to accelerate training using stochastic weight updates. We then describe the network used for stochastic learning for handwritten digit classification and the memristive model for crossbar compat- ible implementation. Finally, we compare the performance of stochastic learning for this network with the software baseline and demonstrate the superior noise-tolerance characteristics of stochastic DNN based inference engines. DNN training involves two steps -forward pass to calculate the activation functions and backward pass for calculating the weight update required for all the synapses in the network. During forward pass, the input y to neuron j in layer (k +1) is determined based on the outputs of the neurons in the previous layer and the strength of synapses between the two layers, according to the relation:to alter the state of the RPU. To determine the product of x i and δ j in equation 2, their equivalent stochastic bit streams are fed through the row and column in the cross bar; the RPU conductance will change depending on the coincidence of these two stochastic pulse streams. The stochastic weight update can be represented aswhere f (x) is a non-linear transformation function such as sinh(x), tanh(x) or ReLU(x) = xH(x), with H(x) denoting the Heaviside step function. During back propagation, the error δ at layer (k + 1) is fed back to determine the error in the previous layer and determine the weight update asAll these steps are of O(N 2 ) complexity, and network train- ing becomes highly time consuming when implemented on von Neumann machines with physically separated memory and computational units. DNN training can be accelerated using a cross bar array with memristive devices at the cross point representing the synaptic weights and neuronal computational circuits at the periphery [3].Operations in equations 1 and 2 can also be parallelized by leveraging the fact that programming pulses applied at the periphery can be used to read and program all the cells of the array in parallel, if the currents involved are sufficiently small. While the forward and backward pass is implemented as a parallel read operation, the weight update operation can be implemented using the idea that coincidence detection (AND operation) of stochastic streams representing real numbers is equivalent to the multiplication operation.In the stochastic computing framework [13], [14], a number x ∈ [0,1] can be represented as a Bernoulli sequence X = [x 1 , x 2 , x 3 , . . . , x N ] such that the binary random variable x i has a probability P (x i = 1) = x , and N is the length of the Bernoulli sequence [15]. Here, BL refers to the bit length of the Bernoulli sequence and B is the minimum conductance resolution as a result of one pulse pair overlap. High-level design studies in [3] suggest that in order to maintain network accuracies that are close to the ideal software performance, these devices should have a resolution of at least 1000 programmable levels within a dynamic range of 10, which is a stringent requirement to realize in nanoscale devices.In this study, we study the performance characteristics of DNNs implemented using models of memristive devices that are more representative of experimental devices today [12]. Some of these non-idealities include limited dynamic range, resolution, and variability in conductance levels attained during programming. We study the performance characteristics of DNNs trained using stochastic methods, and compare their performance when used as inference engines for noise- corrupted data. Our studies reveal that the stochastic learning method helps to mitigate some of the non-ideal effects of the variability that is inherent to nanoscale devices. As the length of the Bernoulli sequence increases, the error in the estimated average decreases. Thus, multiplication oper- ation can be implemented efficiently using simple logic gates or coincidence detection operation. In the resistive processing units (RPUs) based implemen- tation of DNNs [3], parallel weight update is achieved by applying programming pulses with amplitude ±V s /2 on the cross-bar wires, where V s is the minimum amplitude necessary A 4−layer network with 784 − 256 − 128 − 10 fully connected neurons is used in our study for hand written digit classification (Fig. 1). Images from the MNIST database is used for training and testing the networks. The input images are pre-processed by mean normalization, and the network is trained by minimizing the multi-class cross-entropy objective function with sigmoid activation function for the hidden layers and softmax function for the output layer. The weights are updated after every image (batch size of one) and a variable learning rate scheme is employed for learning. We refer to the network which uses stochastic pulses for forward pass, back propagation and weight update as 'stochastic DNN'. Deterministic analog unipolar Memristor -2 PCM synapse? The base line network response of the deep learning network with floating-point synaptic weight resolution is shown in Fig.  3. Each epoch of training consists of presentation of all the 60, 000 images in the MNIST training set. For the stochastic DNN, the training error is a function of the number of bits (BL) in the stochastic code. After 30 epochs, the maximum test accuracy of the baseline-floating point DNN is 98.04% while that of the stochastic DNN with BL = 100 bits is 98.07%. The stochastic DNN test accuracy drops to 97.48% if BL = 10 is used. In the following sections, BL = 10 bits is used for simulations involving stochastic DNN due to its comparable accuracy with deterministic DNN, but 10× improvement in throughput and learning acceleration.Cons: periodic RESET pulseFig. 2: Left: Unidirectional weight update scheme with 2 memristive devices per synapse in the cross bar. The synaptic weight is G ef f = G + − G − and devices are selectively programmed to increase or decrease G ef f ; Right: Illustration of the memristive programming -starting from initial con- ductance G i , the final state G f is determined by the pulse overlap (2 here), and a zero mean gaussian noise with standard deviation σ representing programming noise. In (a), the final state under-estimates the required conductance change, while (b) illustrates an over-estimate. The ratio σ/B determines the impact of programming noise; B denotes the resolution of the conductance levels. In order to represent both positive and negative synaptic weights, two devices are used for each synapse [16]. The effective synaptic weight is the difference of the two device conductances, G ef f = G + − G − as shown in Fig 2. Most memristive devices exhibit incremental programming only in one direction; hence we assume a unidirectional device programming scheme, where the conductance of the device always increments in the positive direction. To increase (de- crease) G ef f , G + (G − ) device is selectively programmed [17]. Due to the limited conductance resolution and on-off ratio, the device can saturate at its maximum conductance state, preventing further weight updates and learning. In order to avoid this and facilitate continuous learning, the devices are periodically reset after every 15 image is presented to the network. In our implementation, the memristive conductance response is assumed to be linear with an on-off ratio of 15 and have a resolution of 32 conductance states. First, we study the performance of stochastic DNN for handwritten digit classi- fication as a function of the programming variability of the memristive device. We show that close to base-line accuracies can be maintained, even if the standard deviation (σ) in the programmed distribution is one-third of the separation in the levels (B) of the device. We then analyze the noise resilience characteristics of stochastic inference engines with devices which have close to ideal conductance variability. We then performed numerical simulations incorporating the characteristics of the memristor devices for stochastic updates. Using memristive synapses with minimum programming vari- ability (σ = 0.1B), the stochastic DNN test accuracy is 95.31%, which is 2.7% lesser than the baseline. The maximum test accuracy for stochastic DNNs as a function of the pro- gramming variability of memristive devices is shown in Fig.  4. There is negligible drop in test accuracy even at σ = 0.3B, though the performance falls quickly as the programming variability increases further.We now study the performance of stochastic DNNs that use close to ideal memristive synaptic weights with σ = 0.1B when used as inference engines with noise corrupted test data. The network is trained using BL = 10, but we study the inference accuracy for BL = 10 and BL = 100. Zero mean Gaussian noise with variances of σ We demonstrate highly noise-resilient deep neural networks with memristive devices at the synapse, trained using stochas- tic updates. Even with limited on-off ratio and dynamic range of the device, the performance of the memristive network is within 3% to that of the base-line floating point simulation. For efficient implementation of on-chip machine learning, algorithms that could circumvent the non-ideal characteristics of memristive devices have to be designed. The detrimental impact due to the non-linearities of nanoscale devices can be minimized when used in DNNs that use stochastic codes for data encoding and signal transmission in inference engines, especially for noisy inputs. This research was supported in part by the National Science Foundation grant 1710009 and CISCO Systems Inc.respective non-noise test accuracy, the stochastic network with BL = 10 has higher noise-resilience as its accuracy drops by 12.23% while that of the baseline floating point network drops by 14.35%. The stochastic network's degradation can be minimized further if more bits are used for stochastic encoding during inference, even though the learning was performed with BL = 10. With BL = 100, the stochastic network accuracy drops only by 9.2%. Hence, stochastic weight updates can compensate for noise in the input set as well as the variability introduced by nanoscale devices. 
