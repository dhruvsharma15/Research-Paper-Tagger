Learning quickly is a hallmark of human intelligence, whether it involves recognizing objects from a few exam- ples or quickly learning new skills after just minutes of experience. Our artificial agents should be able to do the same, learning and adapting quickly from only a few exam- ples, and continuing to adapt as more data becomes avail- able. This kind of fast and flexible learning is challenging, since the agent must integrate its prior experience with a small amount of new information, while avoiding overfit- ting to the new data. Furthermore, the form of prior ex- perience and new data will depend on the task. As such, for the greatest applicability, the mechanism for learning to learn (or meta-learning) should be general to the task and In this work, we propose a meta-learning algorithm that is general and model-agnostic, in the sense that it can be directly applied to any learning problem and model that is trained with a gradient descent procedure. Our focus is on deep neural network models, but we illustrate how our approach can easily handle different architectures and different problem settings, including classification, regres- sion, and policy gradient reinforcement learning, with min- imal modification. In meta-learning, the goal of the trained model is to quickly learn a new task from a small amount of new data, and the model is trained by the meta-learner to be able to learn on a large number of different tasks.The key idea underlying our method is to train the model's initial parameters such that the model has maximal perfor- mance on a new task after the parameters have been up- dated through one or more gradient steps computed with a small amount of data from that new task. Unlike prior meta-learning methods that learn an update function or learning rule (Schmidhuber, 1987;Bengio et al., 1992;Andrychowicz et al., 2016;Ravi &amp; Larochelle, 2017), our algorithm does not expand the number of learned param- eters nor place constraints on the model architecture (e.g. by requiring a recurrent model ( Santoro et al., 2016) or a Siamese network (Koch, 2015)), and it can be readily com- bined with fully connected, convolutional, or recurrent neu- ral networks. It can also be used with a variety of loss func- tions, including differentiable supervised losses and non- differentiable reinforcement learning objectives.The process of training a model's parameters such that a few gradient steps, or even a single gradient step, can pro- duce good results on a new task can be viewed from a fea- ture learning standpoint as building an internal representa- tion that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynami- cal systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss.The primary contribution of this work is a simple model- and task-agnostic algorithm for meta-learning that trains a model's parameters such that a small number of gradi- ent updates will lead to fast learning on a new task. We demonstrate the algorithm on different model types, includ- ing fully connected and convolutional networks, and in sev- eral distinct domains, including few-shot regression, image classification, and reinforcement learning. Our evaluation shows that our meta-learning algorithm compares favor- ably to state-of-the-art one-shot learning methods designed specifically for supervised classification, while using fewer parameters, but that it can also be readily applied to regres- sion and can accelerate reinforcement learning in the pres- ence of task variability, substantially outperforming direct pretraining as initialization. Figure 1. Diagram of our model-agnostic meta-learning algo- rithm (MAML), which optimizes for a representation θ that can quickly adapt to new tasks.We aim to train models that can achieve rapid adaptation, a problem setting that is often formalized as few-shot learn- ing. In this section, we will define the problem setup and present the general form of our algorithm.The goal of few-shot meta-learning is to train a model that can quickly adapt to a new task using only a few datapoints and training iterations. To accomplish this, the model or learner is trained during a meta-learning phase on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or trials.In effect, the meta-learning problem treats entire tasks as training examples. In this section, we formalize this meta- learning problem setting in a general manner, including brief examples of different learning domains. We will dis- cuss two different learning domains in detail in Section 3.In our meta-learning scenario, we consider a distribution over tasks p(T ) that we want our model to be able to adapt to. In the K-shot learning setting, the model is trained to learn a new task T i drawn from p(T ) from only K samples drawn from q i and feedback L Ti generated by T i . During meta-training, a task T i is sampled from p(T ), the model is trained with K samples and feedback from the corre- sponding loss L Ti from T i , and then tested on new samples from T i . The model f is then improved by considering how the test error on new data from q i changes with respect to the parameters. In effect, the test error on sampled tasks T i serves as the training error of the meta-learning process. At the end of meta-training, new tasks are sampled from p(T ), and meta-performance is measured by the model's perfor- mance after learning from K samples. Generally, tasks used for meta-testing are held out during meta-training.We consider a model, denoted f , that maps observa- tions x to outputs a. During meta-learning, the model is trained to be able to adapt to a large or infinite num- ber of tasks. Since we would like to apply our frame- work to a variety of learning problems, from classifica- tion to reinforcement learning, we introduce a generic notion of a learning task below. Formally, each taskconsists of a loss function L, a distribution over initial ob- servations q(x 1 ), a transition distribution q(x t+1 |x t , a t ), and an episode length H. In i.i.d. supervised learning prob- lems, the length H = 1. The model may generate samples of length H by choosing an output a t at each time t. The loss L(x 1 , a 1 , . . . , x H , a H ) → R, provides task-specific feedback, which might be in the form of a misclassification loss or a cost function in a Markov decision process.In contrast to prior work, which has sought to train re- current neural networks that ingest entire datasets (Santoro et al., 2016;Duan et al., 2016b) or feature embed- dings that can be combined with nonparametric methods at test time ( Vinyals et al., 2016;Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferrable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in p(T ), rather than a single individual task. How can we en- courage the emergence of such general-purpose representa- tions? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learn- ing rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from p(T ), without overfit- ting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from p(T ), when altered in the direction of the gradient of that loss (see Figure 1). WeRequire: p(T ): distribution over tasks Require: α, β: step size hyperparameters 1: randomly initialize θ 2: while not done do products, which is supported by standard deep learning li- braries such as TensorFlow ( Abadi et al., 2016). In our experiments, we also include a comparison to dropping this backward pass and using a first-order approximation, which we discuss in Section 5.2.3:for all T i do 3. Species of MAMLEvaluate ∇ θ L Ti (f θ ) with respect to K examples 6:Compute adapted parameters with gradient de- scent: θend for 8:In this section, we discuss specific instantiations of our meta-learning algorithm for supervised learning and rein- forcement learning. The domains differ in the form of loss function and in how data is generated by the task and pre- sented to the model, but the same basic adaptation mecha- nism can be applied in both cases. make no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector θ, and that the loss function is smooth enough in θ that we can use gradient-based learning techniques.Formally, we consider a model represented by a parametrized function f θ with parameters θ. When adapt- ing to a new task T i , the model's parameters θ become θ i . In our method, the updated parameter vector θ i is computed using one or more gradient descent updates on task T i . For example, when using one gradient update, θThe step size α may be fixed as a hyperparameter or meta- learned. For simplicity of notation, we will consider one gradient update for the rest of this section, but using multi- ple gradient updates is a straightforward extension.Few-shot learning is well-studied in the domain of super- vised tasks, where the goal is to learn a new function from only a few input/output pairs for that task, using prior data from similar tasks for meta-learning. For example, the goal might be to classify images of a Segway after seeing only one or a few examples of a Segway, with a model that has previously seen many other types of objects. Likewise, in few-shot regression, the goal is to predict the outputs of a continuous-valued function from only a few datapoints sampled from that function, after training on many func- tions with similar statistical properties.The model parameters are trained by optimizing for the per- formance of f θ i with respect to θ across tasks sampled from p(T ). More concretely, the meta-objective is as follows:To formalize the supervised regression and classification problems in the context of the meta-learning definitions in Section 2.1, we can define the horizon H = 1 and drop the timestep subscript on x t , since the model accepts a single input and produces a single output, rather than a sequence of inputs and outputs. The task T i generates K i.i.d. ob- servations x from q i , and the task loss is represented by the error between the model's output for x and the correspond- ing target values y for that observation and task.Note that the meta-optimization is performed over the model parameters θ, whereas the objective is computed us- ing the updated model parameters θ . In effect, our pro- posed method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task.Two common loss functions used for supervised classifica- tion and regression are cross-entropy and mean-squared er- ror (MSE), which we will describe below; though, other su- pervised loss functions may be used as well. For regression tasks using mean-squared error, the loss takes the form:The meta-optimization across tasks is performed via stochastic gradient descent (SGD), such that the model pa- rameters θ are updated as follows:where x (j) , y (j) are an input/output pair sampled from task T i . In K-shot regression tasks, K input/output pairs are provided for learning for each task. Ti∼p(T ) where β is the meta step size. The full algorithm, in the general case, is outlined in Algorithm 1.Similarly, for discrete classification tasks with a cross- entropy loss, the loss takes the form:The MAML meta-gradient update involves a gradient through a gradient. Computationally, this requires an addi- tional backward pass through f to compute Hessian-vectorRequire: p(T ): distribution over tasks Require: α, β: step size hyperparameters 1: randomly initialize θ 2: while not done do 3: Sample batch of tasks Ti ∼ p(T ) 4:for all Ti do 5:SampleEvaluate ∇ θ LT i (f θ ) using D and LT i in Equation (2) or (3) 7:Compute adapted parameters with gradient descent:Sample datapoints DRequire: p(T ): distribution over tasks Require: α, β: step size hyperparameters 1: randomly initialize θ 2: while not done do 3: Sample batch of tasks Ti ∼ p(T ) 4:for all Ti do 5:SampleCompute adapted parameters with gradient descent:end for 10:Updateend for 10:Updateand LT i in Equation 2 or 3 11: end while and LT i in Equation 4 11: end while According to the conventional terminology, K-shot classi- fication tasks use K input/output pairs from each class, for a total of N K data points for N -way classification. Given a distribution over tasks p(T i ), these loss functions can be di- rectly inserted into the equations in Section 2.2 to perform meta-learning, as detailed in Algorithm 2.In reinforcement learning (RL), the goal of few-shot meta- learning is to enable an agent to quickly acquire a policy for a new test task using only a small amount of experience in the test setting. A new task might involve achieving a new goal or succeeding on a previously trained goal in a new environment. For example, an agent might learn to quickly figure out how to navigate mazes so that, when faced with a new maze, it can determine how to reliably reach the exit with only a few samples. In this section, we will discuss how MAML can be applied to meta-learning for RL.Since the expected reward is generally not differentiable due to unknown dynamics, we use policy gradient meth- ods to estimate the gradient both for the model gradient update(s) and the meta-optimization. Since policy gradi- ents are an on-policy algorithm, each additional gradient step during the adaptation of f θ requires new samples from the current policy f θ i . We detail the algorithm in Algo- rithm 3. This algorithm has the same structure as Algo- rithm 2, with the principal difference being that steps 5 and 8 require sampling trajectories from the environment cor- responding to task T i . Practical implementations of this method may also use a variety of improvements recently proposed for policy gradient algorithms, including state or action-dependent baselines and trust regions ( Schulman et al., 2015).Each RL task T i contains an initial state distribution q i (x 1 ) and a transition distribution q i (x t+1 |x t , a t ), and the loss L Ti corresponds to the (negative) reward function R. The entire task is therefore a Markov decision process (MDP) with horizon H, where the learner is allowed to query a limited number of sample trajectories for few-shot learn- ing. Any aspect of the MDP may change across tasks in p(T ). The model being learned, f θ , is a policy that maps from states x t to a distribution over actions a t at each timestep t ∈ {1, ..., H}. The loss for task T i and model f φ takes the form.t=1In K-shot reinforcement learning, K rollouts from f θ and task T i , (x 1 , a 1 , ...x H ), and the corresponding rewards R(x t , a t ), may be used for adaptation on a new task T i .The method that we propose in this paper addresses the general problem of meta-learning (Thrun &amp; Pratt, 1998;Schmidhuber, 1987;Naik &amp; Mammone, 1992), which in- cludes few-shot learning. A popular approach for meta- learning is to train a meta-learner that learns how to up- date the parameters of the learner's model ( Bengio et al., 1992;Schmidhuber, 1992;Bengio et al., 1990). This ap- proach has been applied to learning to optimize deep net- works ( Hochreiter et al., 2001;Andrychowicz et al., 2016;Li &amp; Malik, 2017), as well as for learning dynamically changing recurrent networks ( Ha et al., 2017). One recent approach learns both the weight initialization and the opti- mizer, for few-shot image recognition (Ravi &amp; Larochelle, 2017). Unlike these methods, the MAML learner's weights are updated using the gradient, rather than a learned update; our method does not introduce additional parameters for meta-learning nor require a particular learner architecture.Few-shot learning methods have also been developed for specific tasks such as generative modeling (Edwards &amp; Storkey, 2017;Rezende et al., 2016) and image recogni- tion ( Vinyals et al., 2016). One successful approach for few-shot classification is to learn to compare new exam- ples in a learned metric space using e.g. Siamese net- works (Koch, 2015) or recurrence with attention mech- anisms ( Vinyals et al., 2016;Shyam et al., 2017;Snell et al., 2017). These approaches have generated some of the most successful results, but are difficult to directly extend to other problems, such as reinforcement learning. Our method, in contrast, is agnostic to the form of the model and to the particular learning task.model learned with MAML continue to improve with addi- tional gradient updates and/or examples?All of the meta-learning problems that we consider require some amount of adaptation to new tasks at test-time. When possible, we compare our results to an oracle that receives the identity of the task (which is a problem-dependent rep- resentation) as an additional input, as an upper bound on the performance of the model. All of the experiments were performed using TensorFlow ( Abadi et al., 2016), which al- lows for automatic differentiation through the gradient up- date(s) during meta-learning. The code is available online 1 .Another approach to meta-learning is to train memory- augmented models on many tasks, where the recurrent learner is trained to adapt to new tasks as it is rolled out. Such networks have been applied to few-shot image recog- nition ( Santoro et al., 2016;Munkhdalai &amp; Yu, 2017) and learning "fast" reinforcement learning agents (Duan et al., 2016b; Wang et al., 2016). Our experiments show that our method outperforms the recurrent approach on few- shot classification. Furthermore, unlike these methods, our approach simply provides a good weight initialization and uses the same gradient descent update for both the learner and meta-update. As a result, it is straightforward to fine- tune the learner for additional gradient steps.Our approach is also related to methods for initialization of deep networks. In computer vision, models pretrained on large-scale image classification have been shown to learn effective features for a range of problems ( Donahue et al., 2014). In contrast, our method explicitly optimizes the model for fast adaptability, allowing it to adapt to new tasks with only a few examples. Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters. A number of prior works have ex- plored sensitivity in deep networks, often in the context of initialization ( Saxe et al., 2014;Kirkpatrick et al., 2016). Most of these works have considered good random initial- izations, though a number of papers have addressed data- dependent initializers ( Krähenbühl et al., 2016;Salimans &amp; Kingma, 2016), including learned initializations (Husken &amp; Goerick, 2000;Maclaurin et al., 2015). In contrast, our method explicitly trains the parameters for sensitivity on a given task distribution, allowing for extremely efficient adaptation for problems such as K-shot learning and rapid reinforcement learning in only one or a few gradient steps.We start with a simple regression problem that illustrates the basic principles of MAML. To evaluate performance, we fine- tune a single meta-learned model on varying numbers of K examples, and compare performance to two baselines: (a) pretraining on all of the tasks, which entails training a net- work to regress to random sinusoid functions and then, at test-time, fine-tuning with gradient descent on the K pro- vided points, using an automatically tuned step size, and (b) an oracle which receives the true amplitude and phase as input. In Appendix C, we show comparisons to addi- tional multi-task and adaptation methods.The goal of our experimental evaluation is to answer the following questions: (1) Can MAML enable fast learning of new tasks? (2) Can MAML be used for meta-learning in multiple different domains, including supervised regres- sion, classification, and reinforcement learning? (3) Can aWe evaluate performance by fine-tuning the model learned by MAML and the pretrained model on K = {5, 10, 20} datapoints. During fine-tuning, each gradient step is com- puted using the same K datapoints. The qualitative results, shown in Figure 2 and further expanded on in Appendix B show that the learned model is able to quickly adapt with only 5 datapoints, shown as purple triangles, whereas the model that is pretrained using standard supervised learning on all tasks is unable to adequately adapt with so few dat- apoints without catastrophic overfitting. Crucially, when the K datapoints are all in one half of the input range, the Figure 2. Few-shot adaptation for the simple regression task. Left: Note that MAML is able to estimate parts of the curve where there are no datapoints, indicating that the model has learned about the periodic structure of sine waves. Right: Fine-tuning of a model pretrained on the same distribution of tasks without MAML, with a tuned step size. Due to the often contradictory outputs on the pre-training tasks, this model is unable to recover a suitable representation and fails to extrapolate from the small number of test-time samples. Note that MAML continues to improve with additional gradient steps without overfitting to the extremely small dataset during meta-testing, achieving a loss that is substantially lower than the baseline fine-tuning approach.We follow the experimental protocol proposed by Vinyals et al. (2016), which involves fast learning of N -way clas- sification with 1 or 5 shots. The problem of N -way classi- fication is set up as follows: select N unseen classes, pro- vide the model with K different instances of each of the N classes, and evaluate the model's ability to classify new in- stances within the N classes. For Omniglot, we randomly select 1200 characters for training, irrespective of alphabet, and use the remaining for testing. The Omniglot dataset is augmented with rotations by multiples of 90 degrees, as proposed by Santoro et al. (2016).model trained with MAML can still infer the amplitude and phase in the other half of the range, demonstrating that the MAML trained model f has learned to model the periodic nature of the sine wave. Furthermore, we observe both in the qualitative and quantitative results (Figure 3 and Ap- pendix B) that the model learned with MAML continues to improve with additional gradient steps, despite being trained for maximal performance after one gradient step. This improvement suggests that MAML optimizes the pa- rameters such that they lie in a region that is amenable to fast adaptation and is sensitive to loss functions from p(T ), as discussed in Section 2.2, rather than overfitting to pa- rameters θ that only improve after one step.To evaluate MAML in comparison to prior meta-learning and few-shot learning algorithms, we applied our method to few-shot image recognition on the Omniglot (Lake et al., 2011) and MiniImagenet datasets. The Omniglot dataset consists of 20 instances of 1623 characters from 50 dif- ferent alphabets. Each instance was drawn by a different person. The MiniImagenet dataset was proposed by Ravi &amp; Larochelle (2017), and involves 64 training classes, 12 validation classes, and 24 test classes. The Omniglot and MiniImagenet image recognition tasks are the most com- mon recently used few-shot learning benchmarks ( Vinyals et al., 2016;Santoro et al., 2016;Ravi &amp; Larochelle, 2017).Our model follows the same architecture as the embedding function used by Vinyals et al. (2016), which has 4 mod- ules with a 3 × 3 convolutions and 64 filters, followed by batch normalization (Ioffe &amp; Szegedy, 2015), a ReLU non- linearity, and 2 × 2 max-pooling. The Omniglot images are downsampled to 28 × 28, so the dimensionality of the last hidden layer is 64. As in the baseline classifier used by Vinyals et al. (2016), the last layer is fed into a soft- max. For Omniglot, we used strided convolutions instead of max-pooling. For MiniImagenet, we used 32 filters per layer to reduce overfitting, as done by (Ravi &amp; Larochelle, 2017). In order to also provide a fair comparison against memory-augmented neural networks ( Santoro et al., 2016) and to test the flexibility of MAML, we also provide re- sults for a non-convolutional network. For this, we use a network with 4 hidden layers with sizes 256, 128, 64, 64, each including batch normalization and ReLU nonlineari- ties, followed by a linear layer and softmax. For all models, the loss function is the cross-entropy error between the pre- dicted and true class. Additional hyperparameter details are included in Appendix A.1.We present the results in Table 1. The convolutional model learned by MAML compares well to the state-of-the-art re- sults on this task, narrowly outperforming the prior meth- ods. Some of these existing methods, such as matching networks, Siamese networks, and memory models are de- signed with few-shot classification in mind, and are not readily applicable to domains such as reinforcement learn- ing. Additionally, the model learned with MAML uses Table 1. Few-shot classification on held-out Omniglot characters (top) and the MiniImagenet test set (bottom). MAML achieves results that are comparable to or outperform state-of-the-art convolutional and recurrent models. Siamese nets, matching nets, and the memory module approaches are all specific to classification, and are not directly applicable to regression or RL scenarios. The ± shows 95% confidence intervals over tasks. Note that the Omniglot results may not be strictly comparable since the train/test splits used in the prior work were not available. The MiniImagenet evaluation of baseline methods and matching networks is from Ravi &amp; Larochelle (2017).5-way Accuracy 20-way Accuracy Omniglot ( Lake et al., 2011) 1-shot 5-shot 1-shot 5-shot MANN, no conv ( Santoro et al., 2016) 82.8% 94.9% - - MAML, no conv (ours) 89.7 ± 1.1% 97.5 ± 0.6% - - Siamese nets (Koch, 2015) 97.3% 98.4% 88.2% 97.0% matching nets ( Vinyals et al., 2016) 98.1% 98.9% 93.8% 98.5% neural statistician (Edwards &amp; Storkey, 2017) 98.1% 99.5% 93.2% 98.1% memory mod. ( Kaiser et al., 2017) 98.4% 99.6% 95.0% 98.6% MAML (ours)98.7 ± 0.4% 99.9 ± 0.1% 95.8 ± 0.3% 98.9 ± 0.2%5-way Accuracy MiniImagenet (Ravi &amp; Larochelle, 2017) 1-shot 5-shot fine-tuning baseline 28.86 ± 0.54% 49.79 ± 0.79% nearest neighbor baseline 41.08 ± 0.70% 51.04 ± 0.65% matching nets ( Vinyals et al., 2016) 43.56 ± 0.84% 55.31 ± 0.73% meta-learner LSTM (Ravi &amp; Larochelle, 2017) 43.44 ± 0.77% 60.60 ± 0.71% MAML, first order approx. (ours)48.07 ± 1.75% 63.15 ± 0.91% MAML (ours)48.70 ± 1.84% 63.11 ± 0.92%fewer overall parameters compared to matching networks and the meta-learner LSTM, since the algorithm does not introduce any additional parameters beyond the weights of the classifier itself. Compared to these prior methods, memory-augmented neural networks ( Santoro et al., 2016) specifically, and recurrent meta-learning models in gen- eral, represent a more broadly applicable class of meth- ods that, like MAML, can be used for other tasks such as reinforcement learning (Duan et al., 2016b;Wang et al., 2016). However, as shown in the comparison, MAML sig- nificantly outperforms memory-augmented networks and the meta-learner LSTM on 5-way Omniglot and MiniIm- agenet classification, both in the 1-shot and 5-shot case.A significant computational expense in MAML comes from the use of second derivatives when backpropagat- ing the meta-gradient through the gradient operator in the meta-objective (see Equation (1)). On MiniImagenet, we show a comparison to a first-order approximation of MAML, where these second derivatives are omitted. Note that the resulting method still computes the meta-gradient at the post-update parameter values θ Figure 4. Top: quantitative results from 2D navigation task, Bot- tom: qualitative comparison between model learned with MAML and with fine-tuning from a pretrained network.mance of the first-order approximation. This approxima- tion removes the need for computing Hessian-vector prod- ucts in an additional backward pass, which we found led to roughly 33% speed-up in network computation.i , which provides for effective meta-learning. Surprisingly however, the perfor- mance of this method is nearly the same as that obtained with full second derivatives, suggesting that most of the improvement in MAML comes from the gradients of the objective at the post-update parameter values, rather than the second order updates from differentiating through the gradient update. Past work has observed that ReLU neu- ral networks are locally almost linear ( Goodfellow et al., 2015), which suggests that second derivatives may be close to zero in most cases, partially explaining the good perfor- To evaluate MAML on reinforcement learning problems, we constructed several sets of tasks based off of the sim- ulated continuous control environments in the rllab bench- mark suite ( Duan et al., 2016a). We discuss the individual domains below. In all of the domains, the model trained by MAML is a neural network policy with two hidden lay- ers of size 100, with ReLU nonlinearities. The gradient updates are computed using vanilla policy gradient (RE- INFORCE) (Williams, 1992), and we use trust-region pol- icy optimization (TRPO) as the meta-optimizer ( Schulman et al., 2015). In order to avoid computing third derivatives, Figure 5. Reinforcement learning results for the half-cheetah and ant locomotion tasks, with the tasks shown on the far right. Each gradient step requires additional samples from the environment, unlike the supervised learning tasks. The results show that MAML can adapt to new goal velocities and directions substantially faster than conventional pretraining or random initialization, achieving good performs in just two or three gradient steps. We exclude the goal velocity, random baseline curves, since the returns are much worse (&lt; −200 for cheetah and &lt; −25 for ant).we use finite differences to compute the Hessian-vector products for TRPO. For both learning and meta-learning updates, we use the standard linear feature baseline pro- posed by Duan et al. (2016a), which is fitted separately at each iteration for each sampled task in the batch. We com- pare to three baseline models: (a) pretraining one policy on all of the tasks and then fine-tuning, (b) training a policy from randomly initialized weights, and (c) an oracle policy which receives the parameters of the task as input, which for the tasks below corresponds to a goal position, goal di- rection, or goal velocity for the agent. The baseline models of (a) and (b) are fine-tuned with gradient descent with a manually tuned step size. Videos of the learned policies can be viewed at sites.google.com/view/maml 2D Navigation. In our first meta-RL experiment, we study a set of tasks where a point agent must move to different goal positions in 2D, randomly chosen for each task within a unit square. The observation is the current 2D position, and actions correspond to velocity commands clipped to be in the range [−0.1, 0.1]. The reward is the negative squared distance to the goal, and episodes terminate when the agent is within 0.01 of the goal or at the horizon of H = 100. The policy was trained with MAML to maximize performance after 1 policy gradient update using 20 trajectories. Ad- ditional hyperparameter settings for this problem and the following RL problems are in Appendix A.2. In our evalu- ation, we compare adaptation to a new task with up to 4 gra- dient updates, each with 40 samples. The results in Figure 4 show the adaptation performance of models that are initial- ized with MAML, conventional pretraining on the same set of tasks, random initialization, and an oracle policy that receives the goal position as input. The results show that MAML can learn a model that adapts much more quickly in a single gradient update, and furthermore continues to improve with additional updates. the negative absolute value between the current velocity of the agent and a goal, which is chosen uniformly at random between 0.0 and 2.0 for the cheetah and between 0.0 and 3.0 for the ant. In the goal direction experiments, the re- ward is the magnitude of the velocity in either the forward or backward direction, chosen at random for each task in p(T ). The horizon is H = 200, with 20 rollouts per gradi- ent step for all problems except the ant forward/backward task, which used 40 rollouts per step. The results in Fig- ure 5 show that MAML learns a model that can quickly adapt its velocity and direction with even just a single gra- dient update, and continues to improve with more gradi- ent steps. The results also show that, on these challenging tasks, the MAML initialization substantially outperforms random initialization and pretraining. In fact, pretraining is in some cases worse than random initialization, a fact observed in prior RL work ( Parisotto et al., 2016).We introduced a meta-learning method based on learning easily adaptable model parameters through gradient de- scent. Our approach has a number of benefits. It is simple and does not introduce any learned parameters for meta- learning. It can be combined with any model representation that is amenable to gradient-based training, and any differ- entiable objective, including classification, regression, and reinforcement learning. Lastly, since our method merely produces a weight initialization, adaptation can be per- formed with any amount of data and any number of gra- dient steps, though we demonstrate state-of-the-art results on classification with only one or five examples per class. We also show that our method can adapt an RL agent using policy gradients and a very modest amount of experience.Locomotion. To study how well MAML can scale to more complex deep RL problems, we also study adaptation on high-dimensional locomotion tasks with the MuJoCo sim- ulator ( Todorov et al., 2012). The tasks require two sim- ulated robots -a planar cheetah and a 3D quadruped (the "ant") -to run in a particular direction or at a particular velocity. In the goal velocity experiments, the reward is Reusing knowledge from past tasks may be a crucial in- gredient in making high-capacity scalable models, such as deep neural networks, amenable to fast training with small datasets. We believe that this work is one step toward a sim- ple and general-purpose meta-learning technique that can be applied to any problem and any model. Further research in this area can make multitask initialization a standard in- gredient in deep learning and reinforcement learning.In this section, we provide additional details of the experi- mental set-up and hyperparameters.For N-way, K-shot classification, each gradient is com- puted using a batch size of N K examples. For Omniglot, the 5-way convolutional and non-convolutional MAML models were each trained with 1 gradient step with step size α = 0.4 and a meta batch-size of 32 tasks. The network was evaluated using 3 gradient steps with the same step size α = 0.4. The 20-way convolutional MAML model was trained and evaluated with 5 gradient steps with step size α = 0.1. During training, the meta batch-size was set to 16 tasks. For MiniImagenet, both models were trained using 5 gradient steps of size α = 0.01, and evaluated using 10 gradient steps at test time. Following Ravi &amp; Larochelle (2017), 15 examples per class were used for evaluating the post-update meta-gradient. We used a meta batch-size of 4 and 2 tasks for 1-shot and 5-shot training respectively. All models were trained for 60000 iterations on a single NVIDIA Pascal Titan X GPU.The pretraining baseline in the main text trained a single network on all tasks, which we referred to as "pretraining on all tasks". To evaluate the model, as with MAML, we fine-tuned this model on each test task using K examples. In the domains that we study, different tasks involve dif- ferent output values for the same input. As a result, by pre-training on all tasks, the model would learn to output the average output for a particular input value. In some in- stances, this model may learn very little about the actual domain, and instead learn about the range of the output space.We experimented with a multi-task method to provide a point of comparison, where instead of averaging in the out- put space, we averaged in the parameter space. To achieve averaging in parameter space, we sequentially trained 500 separate models on 500 tasks drawn from p(T ). Each model was initialized randomly and trained on a large amount of data from its assigned task. We then took the average parameter vector across models and fine-tuned on 5 datapoints with a tuned step size. All of our experiments for this method were on the sinusoid task because of com- putational requirements. The error of the individual regres- sors was low: less than 0.02 on their respective sine waves.In all reinforcement learning experiments, the MAML pol- icy was trained using a single gradient step with α = 0.1. During evaluation, we found that halving the learning rate after the first gradient step produced superior performance. Thus, the step size during adaptation was set to α = 0.1 for the first step, and α = 0.05 for all future steps. The step sizes for the baseline methods were manually tuned for each domain. In the 2D navigation, we used a meta batch size of 20; in the locomotion problems, we used a meta batch size of 40 tasks. The MAML models were trained for up to 500 meta-iterations, and the model with the best average return during training was used for evaluation. For the ant goal velocity task, we added a positive reward bonus at each timestep to prevent the ant from ending the episode.We tried three variants of this set-up. During training of the individual regressors, we tried using one of the fol- lowing: no regularization, standard 2 weight decay, and 2 weight regularization to the mean parameter vector thus far of the trained regressors. The latter two variants en- courage the individual models to find parsimonious solu- tions. When using regularization, we set the magnitude of the regularization to be as high as possible without signif- icantly deterring performance. In our results, we refer to this approach as "multi-task". As seen in the results in Ta- ble 2, we find averaging in the parameter space (multi-task) performed worse than averaging in the output space (pre- training on all tasks). This suggests that it is difficult to find parsimonious solutions to multiple tasks when training on tasks separately, and that MAML is learning a solution that is more sophisticated than the mean optimal parameter vector.In Figure 6, we show the full quantitative results of the MAML model trained on 10-shot learning and evaluated on 5-shot, 10-shot, and 20-shot. In Figure 7, we show the qualitative performance of MAML and the pretrained base- line on randomly sampled sinusoids.In this section, we include more thorough evaluations of our approach, including additional multi-task baselines and a comparison representative of the approach of Rei (2015).Rei (2015) developed a method which learns a context vec- tor that can be adapted online, with an application to re- current language models. The parameters in this context vector are learned and adapted in the same way as the pa- rameters in the MAML model. To provide a comparison to using such a context vector for meta-learning problems, we concatenated a set of free parameters z to the input x, and only allowed the gradient steps to modify z, rather than modifying the model parameters θ, as in MAML. For im- Figure 6. Quantitative sinusoid regression results showing test-time learning curves with varying numbers of K test-time samples. Each gradient step is computed using the same K examples. Note that MAML continues to improve with additional gradient steps without overfitting to the extremely small dataset during meta-testing, and achieves a loss that is substantially lower than the baseline fine-tuning approach.  Tables 3, 4, and 5. Learning an adaptable con- text vector performed well on the toy pointmass problem, but sub-par on more difficult problems, likely due to a less flexible meta-optimization.  1-shot 5-shot context vector 94.9 ± 0.9% 97.7 ± 0.3% MAML 98.7 ± 0.4% 99.9 ± 0.1% 
