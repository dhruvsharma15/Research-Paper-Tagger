Those who cannot learn from history are doomed to repeat it.Deep neural networks (DNNs) have shown to be very powerful methods and thus have attracted great attention from the computer vision community. However, their adoption and application are somewhat hampered by their high complexity and in par- ticular the many choices of hyperparameter values (e.g., learning rate, network architec- tures, activation functions) one must take care of when applying DNN to a new dataset or problem. Finding appropriate values for the hyperparameters, although essential for achieving good performance, is usually time consuming and difficult. Further, complex, state-of-the-art deep learning models (e.g., Residual Networks [1]) may take up to sev- eral days for training, making the traditional hyperparameter tunning approaches such as grid-search impractical. This aroused great interest in developing efficient and sys- tematic hyperparameter optimization approaches [2][3][4][5][6][7][8]. But, even those methods need more than hundred hyperparameter evaluations to find near-optimal hyperparameter values and thus remain impractical for DNN models requiring very long training time.A promising approach to address the challenging and highly complex hyperparam- eter optimization problems is to transfer the knowledge from well-tuned hyperparam- eters of a DNN evaluated on a source dataset to the hyperparameter optimization for evaluation on a new dataset. However, the optimal hyperparameter values for different datasets can vary greatly in terms of scale and location. This makes knowledge transfer for hyperparameter optimization a difficult problem that only few research works have explored.Feurer et al. [9] address the issue by extracting task and dataset features as metafea- tures and use them to initialize the hyperparameter optimization methods. Bardenet et al. [10] describe an approach based on surrogate ranking and techniques for collabo- rative tunning that constructs a common performance surface of the model. In similar fashion, Yogatama and Mann [11] propose to use the deviations from per-dataset mean as a common representation of the model's performance on two datasets.Those proposed methods use hand-crafted features for knowledge transfer between hyperparameter optimizations on different datasets. However, deep learning models are evidence that hand-designed features are typically inferior to learned features. With this in mind, we explore a new direction and propose a hyperparameter transfer learning method that has the following two unique features:• First, it utilizes surrogates to efficiently model the distribution of the validation error given a hyperparameter values and a dataset.• Second, it employs a small neural network to parameterize a knowledge transfer function that maps hyperparameter values from a source dataset to similarly per- forming hyperparameters values on a target dataset.These unique features enable the proposed method to efficiently optimize the hyperpa- rameters on a dataset of interest while using as much as five times less evaluations, as demonstrated by extensive experiments on three image classification datasets. We name our method as Hyperparameter Transfer using Surrogates or HTS for short.All previous approaches to knowledge transfer for hyperparameter optimization meth- ods depend on hand-crafted features to capture the dataset specific properties. In the fol- lowing, we briefly describe such methods and compare them with our proposed method. Yogatama and Mann [11] describe a Bayesian optimization method that maps the validation errors for a set of hyperparameter values on multiple datasets to a common space by scaling the errors with the per-dataset mean and standard deviation. Their approach is based on the assumption that different datasets produce similar validation errors aside from the location and the scale of the error. We argue that the relationship between the validation errors on different datasets is much more complex and cannot be captured solely by the per-dataset deviations. In contrast, our proposed method provides a transfer function that maps the validation errors on different datasets and thus can be used by any hyperparameter optimization method.Feurer et al. [9] use dataset metafeatures to compute a dataset distance metric which is then used to transfer high-performing hyperparameter values from "close" datasets. The proposed method is complex and relies on as many as 46 metafeatures. Each metafeature carries an assumption about the datasets properties, which may not hold in practice. Different from their method, our proposed method learns the dataset properties directly from the performance of the model through training a small neural network.Both of the above methods employ hand-crafted features to transfer the knowledge gained from hyperparameter optimization on a source dataset to a target dataset. The HTS method, learns a function that is able to map hyperparameter values from a source dataset to hyperparameter values offering comparable performance on a target dataset. To the best of our knowledge, HTS is the first method to directly learn the parameters of the transfer function that maps hyperparameters across different datasets.We now describe our method -Hyperparameter Transfer using Surrogates (HTS) -for knowledge transfer between hyperparameter optimizations on different datasets.Consider a deep learning algorithm A with k configurable hyperparameters collec- tively denoted asAssume the values of the hyperparameters are constrained to the domain X . Hyperparameter optimization aims to find the best hyper- parameter configuration x * to minimize the validation error of the algorithm A which is trained with x * as its hyperparameter values. The training and evaluation of A using the hyperparameter set x is considered as evaluation of an expensive function f : X → R that maps a hyperparameter set to a validation error. We search for x * by minimizing the function f with respect to x. We focus on learning to transfer the hyperparameter sets that perform well when A is applied on a source dataset D S to hyperparameter sets that perform well when the same algorithm A is applied on a target dataset D T .We define Ω S as the set of n pairs of hyperparameter configurations and the corre- sponding validation errors,, of the algorithm A on a source dataset D S . Similarly, we define Ω T as the set of (x, f T (x)) pairs, where f T (x) is the validation error of the same algorithm A evaluated on another dataset D T (we call this dataset as the target dataset).Our goal is to use the best hyperparameter sets from D S to obtain the lowest val- idation errors on D T as well. However, the same hyperparameter sets that yielded the lowest validation errors on D S will not necessarily give the lowest validation errors on D T due to the domain shift between the two datasets. To address this problem, we propose a hyperparameter transfer learning algorithm that automatically adapts the hyperparameter values from one dataset to another. More concretely, we first define a hyperparameter transfer function g : X → X that maps the hyperparameter configura- tion x to another onê x such that the validation error on the target dataset has a high positive correlation with the validation error on the source dataset. We parameterize the function g by a small neural network with learnable parameters θ that can be estimated through minimizing the following objective function:A problem with the above (straightforward) approach is the insufficiency of training data for optimizing the neural network g(θ) since both Ω S and Ω T have a small number of elements and populating them with new pairs is very expensive. We circumvent the problem by substituting f S and f T in Equation (1) with their surrogate models S S and S T (we use RBF to build the surrogates and details are deferred to the supplementary material). We fit S S using Ω S and before fitting S T we populate Ω T by evaluating m Latin hypercube samples (LHS) of X .Another practical problem is the difficulty of backpropagation through the correlation of the two surrogate functions. For this, we construct two arrays of 10,000 LHS samples of X and sort one array according to the value of the sample when evaluated on S S and the other array when evaluated on S T . As a consequence, the S S and S T values of the two sorted arrays has high positive correlation so they are apt substitute for the correlation function. Now, the objective function can be simplified to a function that aims to reduce the mean squared distance of hyperparameter values with a same rank in these two arrays. Accordingly, the neural network training set M contains LHS samples evaluated on S S as inputs and the LHS samples with same ranked S T values as desired outputs.Inputs: ΩS {Set of n pairs of hyperparameters and their evaluation on the source dataset DS} m {Number of Latin hypercube samples (LHS) from X to initialize ΩT }; r {Transfer step-size}; tmax {Number of evaluations allowed on the target dataset DT }; Output: ΩT {Set of tmax pairs of hyperparameters and their evaluations on the target dataset};g {Function that maps x s.t. fT (g(x)) has high positive correlation with fS(x)} 1: Use ΩS to approximate the evaluation of a hyperparameter set on the source dataset, fS, with a surrogate model SS., where xi are m LHS samples from X . 3: Set t = m 4: while t ≤ tmax do 5:Use ΩT to approximate the evaluation of a hyperparameter set on the target dataset, fT , with a surrogate model ST . 6:Set M as a training set containing LHS samples of X evaluated on SS as inputs and LHS samples with same ranked ST as desired outputs.Learn a function g by training a neural network on M. 8:After convergence of the neural network, sample r hyperparameter sets from ΩS with lowest validation errors and set. 9:Set t = t + r 10: end while 11: Return the set ΩT and the latest function g.After convergence of the neural network, 2 we use g to map the r top performing hy- perparameter sets from Ω S and evaluate them using f T .the cycle until we exhaust the available budget of hyperparameter evaluations on D T . A formal algorithm description is given in Algorithm 1. We give implementation details, discussions on method variations, and analysis of m and r in the supplementary. Hyperparameter evaluationsMean best error % (N = 5)Hyperparameter evaluationsMean best error % (N = 5) Fig. 1. Validation error curves for optimizing 8 hyperparameters of a CNN network on two CIFAR-10 and SVHN. Left, we compare the optimization progress on CIFAR-10 when not using a source dataset, when using MNIST as a source dataset, when using SVHN as a source dataset, and finally when using the top-performing SVHN hyperparameter values but without using the mapping function g to adapt them to CIFAR-10, i.e. with linear mapping. Right, we compare the optimization progress on SVHN when not using a source dataset, when using MNIST as a source dataset, and when mapping the top-performing MNIST hyperparameters without adaptation.We designed two experiments to demonstrate the efficiency of the HTS method compared against a baseline method that does not use knowledge transfer. As a baseline we choose the recently proposed HORD method [7] since it was shown to obtain state- of-the-art performance on most hyperparameter optimization tasks.We only compare to this baseline method since we cannot compare to [9] as their method is designed to transfer hyperparameters only of a subset of the same dataset. We also cannot compare to [11] as their work deals with transfer between general purpose datasets of small size (&lt; 5,000 samples). Further, they have not made their method's code public so due to time constraints we leave the reimplementation and subsequent comparison of their method for future work.In both experiments, we optimize 8 hyperparameters of a convolutional neural net- work (CNN). We use stochastic gradient descent algorithm to train the network, and optimize its learning rate and momentum. We also optimize the number of nodes in fully-connected layers and the dropout rate. We apply the CNN network on three com- puter vision benchmark datasets: MNIST, SVHN, and CIFAR-10. We give details of the CNN, the hyperparameters with their ranges, and datasets details in the supplementary.In the first experiment, we apply the CNN network on the CIFAR-10 dataset and optimize its hyperparameters. We compare the optimization progress on CIFAR-10 when not using a source dataset (denoted as "Without transfer") against the optimiza- tion progress when using MNIST as a source dataset and against when using SVHN as a source dataset. Since MNIST contains grayscale images of digits, transferring the hyperparameter configurations to the task of classifying color images of the everyday objects found in CIFAR-10 requires a complex mapping function. Further, simple CNN networks typically achieve a validation error of around 2% on the relatively easy task of classifying MNIST digits, but when applied to the CIFAR-10 dataset they achieve an error of around 37%. Similarly, SVHN contains color images of house numbers, so here simple CNN typically achieves an error of around 16%. This means, the hyper- parameter mapping function of HTS should also learn how to deal with the different scale of the validation error. Nonetheless, HTS successfully maps the best found hy- perparameters from SVHN to CIFAR-10 and reaches a validation error using five times less evaluations than the baseline method. The progress is slightly less when using MNIST as source dataset since its task is much easier than SVHN or CIFAR-10. Next, we demonstrate the ability of the hyperparameter mapping function to adapt the source hyperparameters to appropriate target hyperparameters. For this, we perform an opti- mization where we sort the hyperparameters according to the validation error on the source dataset and then, in that order, directly evaluate them on the target dataset. Here we use SVHN as a source dataset and denote the optimization as "SVHN-linear". As expected, this sort of optimization starts with low error, but fails to make progress since its not able to properly adapt the hyperparameters (e.g., number of nodes) to a more complex dataset such as CIFAR-10.In the second experiment, we apply the same CNN network on the SVHN dataset and optimize the same hyperparameters. We compare the optimization progress with- out knowledge transfer against the optimization progress with knowledge transfer from optimization on MNIST. Similar to experiment 1, the HTS method achieves low valida- tion errors while using as much as three times less evaluations than the baseline method. We again demonstrate the effectiveness of the transfer function as it can map hyperpa- rameter values that achieve much lower validation error than the optimization where we directly use the source hyperparameter values.We illustrate each optimization with Figure 1 by reporting the mean best validation error per hyperparameter evaluation over five trials using different random seeds.We presented HTS, a method for transferring hyperparameter configurations between datasets. The proposed method efficiently learns to map the top-performing hyperpa- rameter configurations on a source dataset to hyperparameter configurations with com- parable relative performance on a target dataset. The resulting transfer function can be used to transfer the top-performing configurations, but it can also be used to initialize any hyperparameter optimization method and significantly speed up its convergence. In the future, we plan to evaluate our method on transferring hyperparameter configura- tions between a broader set of datasets.It is important for the initial surrogate S T to be a close approximation of f T . Thus, one might want to use higher number of initial samples m, with the expense of spending valuable f T evaluations. Further, if the r top performing samples from Ω S are close to each other in the hyperparameter space, it might be a good idea to also include ran- dom samples from Ω S . The random samples could improve the generalization of the surrogate model and thus produce better training set for subsequent iterations of g.Another possibility is to use HTS until we have spent only a percentage of the avail- able budget and continue using the surrogate with other hyperparameter optimization algorithm, such as HORD [7]. In this case, HTS can be seen as a hyperparameter opti- mization initialization method.We optimize 8 hyperparameters of a CNN network, with the following architecture. The network start with two blocks of: convolutional layer, batch normalization layer, ReLU activation, dropout layer, and a max-pooling layer. The first block has convolutional layer with 32 filters and a kernel size of 5 × 5, while the second block a convolutional layer with 64 filters and a kernel size of 3. The max-pooling layers in the first and second block have a kernel size of 3 × 3 and 2 × 2 respectively. Finally, the CNN network has two blocks of: fully-connected (FC) layer, ReLU activation, and a dropout layer. For training we use the cross-entropy criterion and the stochastic gradient descent (SGD) algorithm. Due to time constraints and limited computational resources, we train the network only for 10 epochs. We tune the learning rate and momentum of the SGD algorithm, the dropout rate of the four dropout layers, and the number of nodes in the two FC layers. We search for optimal hyperparameter values in the ranges listed in Table 1.We employ the radial basis function (RBF) with polynomial tail as a generic surrogate model to approximate f S and f T . Specifically, we define a surrogate model of a func- tion with n observations as S(x) = n i=1 λ i φ( − x i + p(x). Here, φ(r) is the cubic spline RBF defined as φ(r) = r 3 and p(x) is the polynomial tail. The surro- gate model parameters are determined by solving the corresponding linear system of equations [12].Each hyperparameter x i is constrained to a range [x i,min , x i,max ]. We normalize each hyperparameter range to a range of [0,1] before using the hyperparameter sets as neural network training samples. Accordingly, we constrain the output of the network to be in the same range [0,1] and the translate hyperparameter values back to the original range before using as input to f T . In this way, we avoid the problem of mapping x to an undefined or outside range values. The neural network used to learn the function g consists of one hidden layer with 20k neurons and a sigmoid activation function (where k is the number of hyperparame- ters). To implement the surrogate model we use the open-source surrogate optimization toolbox pySOT [13]. The neural network for learning g and the CNN for performing the experiments are implemented in the Torch framework [14].The MNIST dataset is a popular benchmark dataset for classifying grayscale images of handwritten digits [15]. Following conventional experimental protocol on this dataset, we split the training images into the training set of 50,000 images and the validation set of 10,000 images. As a standard preprocessing, we normalized intensity values of all the images by subtracting their mean and dividing by the standard deviation. Throughout the experiments, we always use the error on the validation set as loss of the objective function we are optimizing.SVHN is a real-world image dataset obtained from house numbers in Google Street View images [16]. It is similat to MNIST (e.g., the images are of small cropped digits), but contains more labeled data (73,257 train images and 26,032 test images). Further, it poses the significantly harder task of recognizing digits and numbers in natural scene images. We randomly sample 10,000 training images and use them as a validation set.The CIFAR-10 dataset consists of 60,000 color images equally divided in 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck [17]. The dataset is split into five training batches and one test batch, each with 10,000 images. We choose the last training batch as a validation set and use the error on this set to compare the per- formance of the algorithms. We also normalized the intensity values of all the images in this dataset by subtracting their mean and dividing by the standard deviation.
