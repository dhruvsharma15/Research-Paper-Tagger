The ability to use language to reason about every-day visual input is a fundamental building block of human intelligence. Achieving this capacity to visually reason is thus a meaningful step towards artificial agents that truly understand the world. Advances in both image-based learning and language-based learning using deep neural networks have made huge strides in difficult tasks such as object recognition [1,2] and machine translation [3,4]. These advances have in turn fueled research on the intersection of visual and linguistic learning [5,6,7,8,9].To this end, [9] recently proposed the CLEVR dataset to test multi-step reasoning from language about images, as tradi- tional visual question-answering datasets such as [5,7] ask sim- pler questions on images that can often be answered in a single glance. Examples from CLEVR are shown in Figure 1. Struc- tured, multi-step reasoning is quite difficult for standard deep learning approaches [10,11], including those successful on traditional visual question answering datasets. Previous work highlights that standard deep learning approaches tend to ex- ploit biases in the data rather than reason [9,12]. To overcomeOur model processes the multi-modal question-image input us- ing a RNN and CNN combined via Conditional Batch Normal- ization (CBN). CBN has proven highly effective for image styl- ization [14,16], speech recognition [17], and traditional visual question answering tasks [15]. We start by explaining CBN in Section 2.1 and then describe our model in Section 2.2.Batch normalization (BN) is a widely used technique to improve neural network training by normalizing activations throughout the network with respect to each mini-batch. BN has been shown to accelerate training and improve generalization by re- ducing covariate shift throughout the network [18]. To explain BN, we define B = {Fi,.,.,.} N i=1 as a mini-batch of N sam- ples, where F corresponds to input feature maps whose sub- scripts c, h, w refers to the c th feature map at the spatial loca- tion (h, w). We also define γc and βc as per-channel, trainableThe visual pipeline extracts 14 × 14 image features using the conv4 layer of a ResNet-101 [2] pre-trained on ImageNet [20], as done in [10] for CLEVR. Image features are processed by a 3 × 3 convolution followed by several -3 for our model -CBN residual blocks with 128 feature maps, and a final clas- sifier. The classifier consists of a 1 × 1 convolution to 512 fea- ture maps, global max-pooling, and a two-layer MLP with 1024 hidden units that outputs a distribution over final answers.Each CBN residual block starts with a 1 × 1 convolution followed by two 3 × 3 convolutions with CBN as depicted in Figure 2. Drawing from [11,21], we concatenate coordinate feature maps indicating relative spatial position (scaled from −1 to 1) to the image features, each residual block's input, and the classifier's input. We train our model end-to-end from scratch with Adam (learning rate 3e −4 ) [22], early stopping on the validation set, weight decay (1e −5 ), batch size 64, and BN and ReLU throughout the visual pipeline, using only image- question-answer triplets from the training set. 3.1. CLEVR dataset scalars and as a constant damping factor for numerical stabil- ity. BN is defined at training time as follows:Conditional Batch Normalization (CBN) [14,15,16] in- stead learns to output new BN parametersˆγiparametersˆ parametersˆγi,c andˆβiandˆ andˆβi,c as a function of some input x i :where f and h are arbitrary functions such as neural networks. Thus, f and h can learn to control the distribution of CNN acti- vations based on x i . Combined with ReLU non-linearities, CBN empowers a conditioning model to manipulate feature maps of a target CNN by scaling them up or down, negating them, shutting them off, selectively thresholding them, and more. Each feature map is modulated independently, giving the conditioning model an ex- ponential (in the number of feature maps) number of ways to affect the feature representation.Rather than outputˆγioutputˆ outputˆγi,c directly, we output ∆ˆγi∆ˆγi,c, where:CLEVR is a generated dataset of 700K (image, question, an- swer, program) tuples. Images contain 3D-rendered objects of various shapes, materials, colors, and sizes. Questions are multi-step and compositional in nature, as shown in Figure 1. They range from counting questions ("How many green objects have the same size as the green metallic block?") to comparison questions ("Are there fewer tiny yellow cylinders than yellow metal cubes?") and can be 40+ words long. Answers are each one word from a set of 28 possible answers. Programs are an additional supervisory signal consisting of step-by-step instruc- tions, such as filter shape [cube], relate [right], and count, on how to answer the question. Program labels are difficult to generate or come by for real world datasets. Our model avoids using this extra supervision, learning to reason effectively directly from linguistic and visual input.Resultsˆsince initially zero-centeredˆγicenteredˆ centeredˆγi,c can zero out CNN feature map activations and thus gradients. In our implementation, we opt to output ∆ˆγi∆ˆγi,c rather thanˆγithanˆ thanˆγi,c, but for simplicity, in the rest of this paper, we will explain our method usingˆγiusingˆ usingˆγi,c.Our model consists of a linguistic pipeline and a visual pipeline as depicted in Figure 2. The linguistic pipeline processes a question q using a Gated Recurrent Unit (GRU) [19] with 4096 hidden units that takes in learned, 200-dimensional word em- beddings. The final GRU hidden state is a question embedding eq. Our results on CLEVR are shown in Table 1. Our model achieves a new overall state-of-the-art, outperforming humans and previous, leading models, which often use additional pro- gram supervision. Notably, CBN outperforms Stacked Atten- tion networks (CNN+LSTM+SA in 1) by 21.0%. Stacked At- tention networks are highly effective for visual question answer- ing with simpler questions [23] and are the previously leading model for visual reasoning that does not build in reasoning, making them a relevant baseline for CBN. We note also that our model's pattern of performance more closely resembles that of humans than other models do. Strong performance (&lt; 1% er- ror) in exist and query attribute categories is perhaps explained by our model's close resemblance to standard CNNs, which traditionally excel at these classification-type tasks. Our model also demonstrates strong performance on more complex categories such as count and compare attribute.Comparing numbers of objects gives our model more diffi- culty, understandably so; this question type requires more high- level reasoning steps -querying attributes, counting, and com- paring -than other question type. The best model from [10] beats our model here but is trained with extra supervision via 700K program labels. As shown in Table 1, the equivalent, more comparable model from [10] which uses 9K program labels sig- nificantly underperforms our method in this category.  To understand what our model learns, we use t-SNE [24] to visualize the CBN parameter vectors (γ, β), of 2,000 ran- dom validation points, modulating first and last CBN lay- ers in our model, as shown in Figure 4. The (γ, β) parameters of the first and last CBN layers are grouped by the low-level and high-level reasoning functions nec- essary to answer CLEVR questions, respectively. For example, the CBN parameters for equal color and query color are close for the first layer but apart for the last layer, and the same is true for equal shape and query shape, equal size and query size, and equal material and query material. Conversely, equal shape, equal size, and equal material CBN parameters are grouped in the last layer but split in the first layer. Similar patterns emerge when visualizing residual block activa- tions. Thus, we see that CBN learns a sort of function-based modularity, directly from language and image inputs and with- out an architectural prior on modularity. Simply with end-to- end training, our model learns to handle not only different types of questions differently, but also different types of question sub- parts differently, working from low-level to high-level processes as is the proper approach to answer CLEVR questions.Additionally, we observe that many points that break the previously mentioned clustering patterns do so in meaningful ways. For example, Figure 4 shows that some count questions have last layer CBN parameters far from those of other count questions but close to those of exist questions. Closer ex- amination reveals that these count questions have answers of either 0 or 1, making them similar to exist questions. ple where our model correctly counts two cyan objects and two yellow objects but simultaneously does not answer that there are the same number of cyan and yellow objects. In fact, it does not answer that the number of cyan blocks is more, less, or equal to the number of yellow blocks. These errors could be prevented by directly minimizing logical inconsistency, which is an inter- esting avenue for future work orthogonal to our approach.These types of mistakes in a state-of-the-art visual rea- soning model suggest that more work is needed to truly achieve human-like reasoning and logical consistency. We view CLEVR as a curriculum of tasks and believe that the key to the most meaningful and advanced reasoning lies in tackling these last few percentage points of error.An analysis of our model's errors reveals that 94% of its count- ing mistakes are off-by-one errors, indicating our model has learned underlying concepts behind counting, such as close re- lationships between close numbers. As shown in Figure 3, our CBN model struggles more on questions that require more steps, as indicated by the length of the corresponding CLEVR programs; error rates for questions requiring 10 or fewer steps are around 1.5%, while error rates for questions requiring 17 or more steps are around 5.5%, more than three times higher. Furthermore, the model sometimes makes curious reason- ing mistakes a human would not. In Figure 5, we show an exam- One leading approach for visual reasoning is the Program Gen- erator + Execution Engine model from [10]. This approach con- sists of a sequence-to-sequence Program Generator (PG), which takes in a question and outputs a sequence corresponding to a tree of composable Neural Modules, each of which is a two- layer residual block similar to ours. This tree of Neural Mod- ules is assembled to form the Execution Engine (EE) that then predicts an answer from the image. The PG+EE model uses a strong prior by training with program labels and explicitly modeling the compositional nature of reasoning. Our approach learns to reason directly from textual input without using addi- tional cues or a specialized architecture.This modular approach is part of a recent line of work in Neural Module Networks [13,25,26]. Of these, End-to-End Module Networks (N2NMN) [13] also tackle visual reasoning but do not perform as well as other approaches. These methods also use strong priors by modeling the compositionality of reasoning, using program-level supervision, and building per- module, hand-crafted neural architectures for specific functions. architecture conditions 50 BN layers of a pre-trained ResNet. We show that a few layers of CBN after a ResNet can also be highly effective, even for complex problems. We also show how CBN models can learn to carry out multi-step processes and rea- son in a structured way -from low-level to high-level. Additionally, CBN is essentially a post-BN, feature-wise affine conditioning, with BN's trainable scalars turned off. Thus, there are many interesting connections with other con- ditioning methods. A common approach, used for example in Conditional DCGANs [27], is to concatenate constant feature maps of conditioning information to the input of convolutional layers, which amounts to adding a post-convolutional, feature- wise conditional bias. Other approaches, such as LSTMs [28] and Hierarchical Mixtures of Experts [29], gate an input's fea- tures as a function of that same input (rather than a separate, conditioning input), which amounts to a feature-wise, condi- tional scaling, restricted to between 0 and 1. CBN consists of both scaling and shifting, each unrestricted, giving it more ca- pacity than many of these related approaches. We leave explor- ing these connections more in-depth for future work.Relation Networks (RNs) from [11] are another leading ap- proach for visual reasoning. RNs use an MLP to carry out pairwise comparisons over each location of extracted convolu- tional features over an image, including LSTM-extracted ques- tion features as input to this MLP. RNs then element-wise sum over the resulting comparison vectors to form another vector from which a final classifier predicts the answer. This approach is end-to-end differentiable and trainable from scratch to high performance, as we show in Table 1. Our approach lifts the explicitly relational aspect of this model, freeing our approach from the use of a comparison-based prior, as well as the scaling difficulties of pairwise comparisons over spatial locations.CBN itself has its own line of work. The results of [14,16] show that the closely related Conditional Instance Normaliza- tion is able to successfully modulate a convolutional style- transfer network to quickly and scalably render an image in a huge variety of different styles, simply by learning to output a different set of BN parameters based on target style. For visual question answering, answering general questions often of natu- ral images, de Vries et al. [15] show that CBN performs highly on real-world VQA and GuessWhat?! datasets, demonstrating CBN's effectiveness beyond the simpler CLEVR images. TheirWith a simple and general model based on CBN, we show it is possible to achieve state-of-the-art visual reasoning on CLEVR without explicitly incorporating reasoning priors. We show that our model learns an underlying structure required to answer CLEVR questions by finding clusters in the CBN parameters of our model; earlier parameters are grouped by low-level reason- ing functions while later parameters are grouped by high-level reasoning functions. Simply by manipulating feature maps with CBN, a RNN can effectively use language to influence a CNN to carry out diverse and multi-step reasoning tasks over an image. It is unclear whether CBN is the most effective general way to use conditioning information for visual reasoning or other tasks, as well as what precisely about CBN is so effective. Other ap- proaches [27,28,29,30,31,32,33] employ a similar, repetitive conditioning, so perhaps there is an underlying principle that ex- plains the success of these approaches. Regardless, we believe that CBN is a general and powerful technique for multi-modal and conditional tasks, especially where more complex structure is involved.
