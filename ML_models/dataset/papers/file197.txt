Deep learning for robotics vision demands highly effi- cient real-time solutions. A promising approach to deliver one such extremely efficient solution is through the use of low numeric precision deep learning algorithms. Operat- ing in lower precision mode reduces computation as well as data movement and storage requirements. Due to such efficiency benefits, there are many existing works that have proposed low-precision deep neural networks (DNNs), even down to 2-bit ternary mode [5] and 1-bit mode [4,1]. How- ever, the majority of existing works in low-precision DNNs sacrifice accuracy over the baseline full-precision networks. Further, most prior works target reducing the precision of the model parameters (network weights). This primarily benefits the inference step only when batch sizes are small.We observe that activation maps (neuron outputs) occupy more memory compared to the model parameters for batch sizes typical during training. This observation holds even during inference when batch size is 8 or more for modern networks. Based on this observation, we study schemes for training and inference using low-precision DNNs where we reduce the precision of activation maps as well as the model parameters without sacrificing the network accuracy. We re- duce the precision of activation maps (along with model pa- While most prior works proposing reduced-precision networks work with low precision weights (e.g. [1, 5, 4, 3]), we find that activation maps occupy a larger memory foot- print when using mini-batches of inputs. Using mini- batches of inputs is typical in training of DNNs and for multi-modal inference [2]. Figure 1 shows the memory footprint of activation maps and filter maps as batch size changes for 4 different networks during the training and in- ference steps. As batch-size increases, because of the filter reuse aspect across batches of inputs, the activation maps occupy significantly larger fraction of memory compared to the filter weights.Based on this observation, we reduce the precision of ac- tivation maps for DNNs to speed up training and inference steps as well as cut down on memory requirements. How- ever, a straightforward reduction in precision of activation maps leads to significant reduction in model accuracy. This has been reported in prior work as well [4]. We conduct a sensitivity study where we reduce the precision of acti- vation maps and model weights for AlexNet network using ILSVRC-12 dataset. Table 1 reports our findings. We find that reducing the precision of activation maps hurts model accuracy much more than reducing the precision of the filter parameters.To re-gain the model accuracy while working with reduced-precision operands, we increase the number of fil- ter maps in a layer. Increasing the number of filter maps increases the layer compute complexity linearly. Another knob we tried was to increase the size of the filter maps. However increasing the filter map spatial dimension in- 2x the number of filters, reduced-precision AlexNet is just 14% of the total compute cost 1 of the full-precision base- line.We also study how our scheme applies to bigger net- works. For this we study widening the number of filters in ResNet-34. When doubling the number of filters in ResNet- 34, we find that 4b weights and 8b activations surpass the baseline accuracy (baseline full-precision ResNet-34 is 73.2% top-1 accuracy and our reduced-precision ResNet-34 is 73.8% top-1 accuracy). The reduced-precision ResNet is 15.1% the compute cost of full-precision ResNet.  . One bit is reserved for sign-bit in case of weight values hence the use of 2 kâˆ’1 for these quantized values. With appropri- ate affine transformations, the convolution operations (the bulk of the compute operations in the network) can be done using quantized values followed by scaling with floating- point constants. This makes our scheme hardware friendly and can be used in embedded systems. This is the quanti- zation scheme for all the results reported above for AlexNet and ResNet. Table 1. AlexNet on ILSVRC-12 Top-1 accuracy % as precision of activations (A) and weights (W) changes. -is a data-point we did not experiment for. Bold numbers are top-1 accuracy with 2x filters. All results are with end-to-end training of the network from scratch. Noteworthy data-point is the comparison of 32b W and 32b A with 1x filter (baseline; 57.2%) with 4b W and 4b A with 2x filter (58.6%).creases the compute complexity quadratically. Although the number of raw compute operations increase linearly as we increase the number of filter maps in a layer, the compute bits required per operation is now a fraction of what is re- quired when using full-precision operations. As a result, with appropriate support, one can significantly reduce the dynamic memory requirements, memory bandwidth, com- putational energy and speed up the training and inference process. Table 1 also reports the accuracy of AlexNet when we double the number of filter maps in a layer. We find that with doubling of filter maps, AlexNet with 4b weights and 2b activations exhibits accuracy at-par with full-precision networks. Operating with 4b weights and 4b activations, surpasses the baseline accuracy by 1.44%. When doubling the number of filter maps, AlexNet's raw compute opera- tions grow by 3.9x compared to the baseline full-precision network, however by using reduced-precision operands the overall compute complexity is a fraction of the baseline. For example, with 4b operands for weights and activations and Vision, speech and NLP based applications have seen tremendous success with DNNs. However, DNNs are com- pute intensive workloads. In this paper, we present WRPN scheme to reduce the compute requirements of DNNs. While most prior works look at reducing the precision of weights, we find that activations contribute significantly more to the memory footprint than weight parameters when using mini-batches and thus aggressively reduce the preci- sion of activation values. Further, we increase the number of filter maps in a layer and show that widening of filters and reducing precision of network parameters does not sacrifice model accuracy. Our scheme is hardware friendly making it viable for deeply embedded system deployments which is useful for robotics applications.
