works well with short sentences (e.g., 20 words), but has difficulty with long sentences (e.g., 20 words), and particularly with sentences that are longer than those used for training. Training on long sentences is difficult because few available training corpora include sufficiently many long sentences, and because the computational over- head of each update iteration in training is linearly correlated with the length of training sentences. Additionally, by the nature of encoding a variable- length sentence into a fixed-size vector representa- tion, the neural network may fail to encode all the important details.In this paper, hence, we propose to translate sen- tences piece-wise. We segment an input sentence into a number of short clauses that can be confi- dently translated by the model. We show empiri- cally that this approach improves translation qual- ity of long sentences, compared to using a neural network to translate a whole sentence without seg- mentation.Up to now, most research efforts in statistical ma- chine translation (SMT) research have relied on the use of a phrase-based system as suggested in ( Koehn et al., 2003). Recently, however, an entirely new, neural network based approach has been proposed by several research groups (Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014;Cho et al., 2014b), showing promising re- sults, both as a standalone system or as an addi- tional component in the existing phrase-based sys- tem. In this neural network based approach, an en- coder 'encodes' a variable-length input sentence into a fixed-length vector and a decoder 'decodes' a variable-length target sentence from the fixed- length encoded vector.It has been observed in (Sutskever et al., 2014), (Kalchbrenner and Blunsom, 2013) and (Cho et al., 2014a) that this neural network approachThe RNN Encoder-Decoder (RNNenc) model is a recent implementation of the encoder-decoder approach, proposed independently in ( Cho et al., 2014b) and in (Sutskever et al., 2014). It con- sists of two RNNs, acting respectively as encoder and decoder. Each RNN maintains a set of hid- den units that makes an 'update' decision for each symbol in an input sequence. This decision de- pends on the input symbol and the previous hid- den state. The RNNenc in ( Cho et al., 2014b) uses a special hidden unit that adaptively forgets or re- members the previous hidden state such that the activation of a hidden unit h j at time t is com- puted by * Research done while these authors were visiting Uni- versité de Montréalz j and r j are respectively the update and reset gates. Additionally, the RNN in the decoder computes at each step the conditional probability of a target word:the phrases in the segmentation. Once the confi- dence score is defined, the problem of finding the best segmentation can be formulated as an integer programming problem. Let e = (e 1 , · · · , e n ) be a source sentence com- posed of words e k . We denote a phrase, which is a subsequence of e, with e ij = (e i , · · · , e j ).We use the RNN Encoder-Decoder to measure how confidently we can translate a subsequence e ij by considering the log-probability log p(f k | e ij ) of a candidate translation f k generated by the model. In addition to the log-probability, we also use the log-probability log p(e ij | f k ) from a re- verse RNN Encoder-Decoder (translating from a target language to source language). With these two probabilities, we define the confidence score of a phrase pair (e ij , f k ) as: where f t,j is the indicator variable for the j-th word in the target vocabulary at time t and only a single indicator variable is on (= 1) each time. c is the context vector, the representation of the input sentence as encoded by the encoder.Although the model in ( Cho et al., 2014b) was originally trained on phrase pairs, it is straight- forward to train the same model with a bilin- gual, parallel corpus consisting of sentence pairs as has been done in (Sutskever et al., 2014). In the remainder of this paper, we use the RNNenc trained on English-French sentence pairs ( Cho et al., 2014a).where the denominator penalizes a short segment whose probability is known to be overestimated by an RNN . The confidence score of a source phrase only is then defined asOne hypothesis explaining the difficulty encoun- tered by the RNNenc model when translating long sentences is that a plain, fixed-length vector lacks the capacity to encode a long sentence. When en- coding a long input sentence, the encoder may lose the track of all the subtleties in the sentence. Con- sequently, the decoder has difficult time recover- ing the correct translation from the encoded repre- sentation. One solution would be to build a larger model with a larger representation vector to in- crease the capacity of the model at the price of higher computational cost. In this section, however, we propose to segment an input sentence such that each segmented clause can be easily translated by the RNN Encoder- Decoder. In other words, we wish to find a segmentation that maximizes the total confidence score which is a sum of the confidence scores of We use an approximate beam search to search for the candidate translations f k of e ij , that maximize log-likelihood log p(f k |e ij ) ( Boulanger-Lewandowski et al., 2013). Let x ij be an indicator variable equal to 1 if we include a phrase e ij in the segmentation, and oth- erwise, 0. We can rewrite the segmentation prob- lem as the optimization of the following objective function:phrases chosen in the segmentation containing word e k . The constraint in Eq. (4) states that for each word e k in the sentence one and only one of the source phrases contains this word, (e ij ) i≤k≤j , is included in the segmentation. The constraint ma- trix is totally unimodular making this integer pro- gramming problem solvable in polynomial time.Let S k j be the first index of the k-th segment counting from the last phrase of the optimal seg- mentation of subsequence e 1j (S j := S 1 j ), and s j be the corresponding score of this segmentation (s 0 := 0). Then, the following relations hold: in parallel, since each phrase can be scored inde- pendently.With Eq. (5) we can evaluate s j incrementally. With the evaluated s j 's, we can compute S j as well (Eq. (6)). By the definition of S k j we find the optimal segmentation by decomposing e 1n inton ,n , where k is the index of the first one in the sequence S k n . This approach described above requires quadratic time with respect to sentence length.The proposed segmentation approach does not avoid the problem of reordering clauses. Unless the source and target languages follow roughly the same order, such as in English to French transla- tions, a simple concatenation of translated clauses will not necessarily be grammatically correct.Despite the lack of long-distance reordering 1 in the current approach, we find nonetheless signifi- cant gains in the translation performance of neural machine translation. A mechanism to reorder the obtained clause translations is, however, an impor- tant future research question.Another issue at the heart of any purely neu- ral machine translation is the limited model vo- cabulary size for both source and target languages. As shown in ( Cho et al., 2014a), translation qual- ity drops considerably with just a few unknown words present in the input sentence. Interestingly enough, the proposed segmentation approach ap- pears to be more robust to the presence of un- known words (see Sec. 5). One intuition is that the segmentation leads to multiple short clauses with less unknown words, which leads to more stable translation of each clause by the neural translation model. Finally, the proposed approach is computation- ally expensive as it requires scoring all the sub- phrases of an input sentence. However, the scoring process can be easily sped up by scoring phrases We evaluate the proposed approach on the task of English-to-French translation. We use a bilin- gual, parallel corpus of 348M words selected by the method of (Axelrod et al., 2011) from a combination of Europarl (61M), news com- mentary (5.5M), UN (421M) and two crawled corpora of 90M and 780M words respectively. 2 The performance of our models was tested on news-test2012, news-test2013, and news-test2014. When comparing with the phrase-based SMT system Moses ( Koehn et al., 2007), the first two were used as a development set for tuning Moses while news-test2014 was used as our test set.To train the neural network models, we use only the sentence pairs in the parallel corpus, where both English and French sentences are at most 30 words long. Furthermore, we limit our vocabu- lary size to the 30,000 most frequent words for both English and French. All other words are con- sidered unknown and mapped to a special token ( [UNK]).In both neural network training and automatic segmentation, we do not incorporate any domain- specific knowledge, except when tokenizing the original text data.We compare the proposed segmentation-based translation scheme against the same neural net- work model translations without segmentation. The neural machine translation is done by an RNN Encoder-Decoder (RNNenc) ( Cho et al., 2014b) trained to maximize the conditional probability of a French translation given an English sen- tence. Once the RNNenc is trained, an approxi- mate beam-search is used to find possible transla- tions with high likelihood. 3 This RNNenc is used for the proposed segmentation-based approach together with an- other RNNenc trained to translate from French to English. The two RNNenc's are used in the pro- posed segmentation algorithm to compute the con- fidence score of each phrase (See Eqs. (2)- (3)).We also compare with the translations of a con- ventional phrase-based machine translation sys- tem, which we expect to be more robust when translating long sentences.  We validate the proposed segmentation algorithm described in Sec. 3 by comparing against two baseline segmentation approaches. The first one randomly segments an input sentence such that the distribution of the lengths of random segments has its mean and variance identical to those of the seg- ments produced by our algorithm. The second ap- proach follows the proposed algorithm, however, using a uniform random confidence score.Max. number of unknown words  Table 1: BLEU score computed on news-test2014 for two control experi- ments. Random segmentation refers to randomly segmenting a sentence so that the mean and variance of the segment lengths corresponded to the ones our best segmentation method. Random confidence score refers to segmenting a sentence with randomly generated confidence score for each segment. p(f | e) + p(e | f ) Using both direct and reverse translation models without the short phrase penalty p(f | e) + p(e | f ) (p) Using both direct and re- verse translation models together with the short phrase penaltyFrom Table 1 we can clearly see that the pro- posed segmentation algorithm results in signifi- cantly better performance. One interesting phe- nomenon is that any random segmentation was better than the direct translation without any seg- mentation. This indirectly agrees well with the previous finding in ( Cho et al., 2014a) that the neural machine translation suffers from long sen- tences.The results in Table 2 clearly show the impor- tance of using both translation and inverse trans- lation models. Furthermore, we were able to get the best performance by incorporating the short phrase penalty (the denominator in Eq. (2)). From here on, thus, we only use the original formula- tion of the confidence score which uses the both models and the penalty.The proposed confidence score averages the scores of a translation model p(f | e) and an inverse translation model p(e | f ) and penalizes for short phrases. However, it is possible to use alternate As expected, translation with the proposed ap- proach helps significantly with translating long sentences (see Fig. 1). We observe that trans- lation performance does not drop for sentences of lengths greater than those used to train the RNNenc (≤ 30 words).Similarly, in Fig. 2 we observe that translation quality of the proposed approach is more robust   Table 2: BLEU scores computed on the develop- ment and test sets. See the text for the description of each approach. Moses refers to the scores by the conventional phrase-based translation system. The top five rows consider all sentences of each data set, whilst the bottom five rows includes only sentences with no unknown words to the presence of unknown words. We suspect that the existence of many unknown words make it harder for the RNNenc to extract the meaning of the sentence clearly, while this is avoided with the proposed segmentation approach as it effectively allows the RNNenc to deal with a less number of unknown words.In this paper we propose an automatic segmen- tation solution to the 'curse of sentence length' in neural machine translation. By choosing an appropriate confidence score based on bidirec- tional translation models, we observed significant improvement in translation quality for long sen- tences.Our investigation shows that the proposed segmentation-based translation is more robust to the presence of unknown words. However, since each segment is translated in isolation, a segmen- tation of an input sentence may negatively impact translation quality, especially the fluency of the translated sentence, the placement of punctuation marks and the capitalization of words.An important research direction in the future is to investigate how to improve the quality of the translation obtained by concatenating translated segments.Between the early 1970s , when the Boeing 747 jumbo defined modern long-haul travel , and the turn of the century , the weight of the average American 40-to 49-year-old male increased by 10 per cent , according to U.S. Health Department Data . Segmentation [[ Between the early 1970s , when the Boeing 747 jumbo defined modern long-haul travel ,][ and the turn of the century , the weight of the average American 40-to 49-year-old male] [ increased by 10 per cent , according to U.S. Health Department Data .]] ReferenceEntre le début des années 1970 , lorsque le jumbo 747 de Boeing a défini le voyage long-courrier moderne , et le tournant dusì ecle , le poids de l' Américain moyen de 40à40`40à 49 ans a augmenté de 10 % , selon les données du département américain de la Santé . With segmentation Entre les années 70 , lorsque le Boeing Boeing a défini le transport de voyageurs modernes ; et la fin dusì ecle , le poids de la moyenne américaine moyennè a l' ´ egard des hommes a augmenté de 10 % , conformément aux données fournies par le U.S. Department of Health Affairs . Without segmenta- tion Entre les années 1970 , lorsque les avions de service Boeing ont dépassé le prix du travail , le taux moyenétaitmoyen´moyenétait de 40 % .During his arrest Ditta picked up his wallet and tried to remove several credit cards but they were all seized and a hair sample was taken fom him. Segmentation [[During his arrest Ditta] [picked up his wallet and tried to remove several credit cards but they were all seized and] [a hair sample was taken from him.]] ReferenceAu cours de son arrestation , Ditta a ramassé son portefeuille et a tenté de retirer plusieurs cartes de crédit , mais elles ont toutesététoutes´toutesété saisies et on lui a prélevé unéchantillonun´unéchantillon de cheveux . With segmentation Pendant son arrestation J' ai utilisé son portefeuille et a essayé de retirer plusieurs cartes de crédit mais toutes lespì eces ontétéont´ontété saisies et unéchantillonun´unéchantillon de cheveux a ´ eté enlevé. Without segmentation Lors de son arrestation il a tenté de récupérer plusieurs cartes de crédit mais il a ´ eté saisi de tous les coups et des blessures."We can now move forwards and focus on the future and on the 90 % of assets that make up a really good bank, and on building a great bank for our clients and the United Kingdom," new director general, Ross McEwan, said to the press . Segmentation Without specifying the illness she was suffering from, the star performer of 'Respect' confirmed to the media on 16 October that the side effects of a treatment she was receiving were 'difficult' to deal with. Segmentation [[Without specifying the illness she was suffering from, the star performer of 'Respect'] [con- firmed to the media on 16 October that the side effects of a treatment she was receiving were] ['difficult' to deal with.]] ReferenceSans préciser la maladie dont elle souffrait , lacéì ebre interprète de Respect avait affirmé aux médias le 16 octobre que les effets secondaires d'un traitement qu'elle recevaitétaientrecevait´recevaitétaient "diffi- ciles". With segmentation Sans préciser la maladie qu'elle souffrait la star de l' 'oeuvre' de 'respect'. Il a ´ eté confirmé aux médias le 16 octobre que les effets secondaires d'un traitement ontétéont´ontété reçus. "difficile" de traiter . Without segmentation Sans la précision de la maladie elle a eu l'impression de "marquer le 16 avril' les effets d'un tel 'traitement'. Table 3: Sample translations with the RNNenc model taken from the test set along with the source sentences and the reference translations.
