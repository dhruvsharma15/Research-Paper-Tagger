An NLP machine learning task often involves clas- sifying a sequence of tokens such as a sentence or a document, i.e. approximating a function f 1 (s) ∈ [0, 1] (where f 1 may determine a domain, senti- ment, etc.). But there is a large class of prob- lems that involve classifying a pair of sentences, f 2 (s 0 , s 1 ) ∈ R (where s 0 , s 1 are sequences of to- kens, typically sentences).Typically, the function f 2 represents some sort of semantic similarity, that is whether (or how much) the two sequences are semantically related. This formulation allows f 2 to be a measure for tasks as different as topic relatedness, paraphras- ing, degree of entailment, a pointwise ranking task for answer-bearing sentences or next utter- ance classification.In this work, we adopt the working assumption that there exist certain universal f 2 type measures that may be successfuly applied to a wide variety of semantic similarity tasks -in the case of neu- ral network models trained to represent universal semantic comprehension of sentences and adapted to the given task by just fine-tuning or adapting the output neural layer (in terms of architecture or just weights). Our argument for preferring f 2 to f 1 in this pursuit is the fact that the other sentence in the pair is essentially a very complex label when training the sequence model, which can therefore discern semantically rich structures and dependen- cies. Determining and demonstrating such univer- sal semantic comprehension models across mul- tiple tasks remains a few steps ahead, since the research landscape is fragmented in this regard. Model research is typically reported within the context of just a single f 2 -type task, each dataset requires sometimes substantial engineering work before measurements are possible, and results are reported in ways that make meaningful model comparisons problematic.Our main aims are as follows. (A) Unify re- search within a single framework that employs task-independent models and task-specific adapta- tion modules. (B) Improve the methodology of model evaluation in terms of statistics, compar- ing with strong non-neural IR baselines, and in- troducing new datasets with better characteristics. (C) Demonstrate the feasibility of pursuing uni- versal, task-independent f 2 models, showing that even simple neural models learn universal seman- tic comprehension by employing cross-task trans- fer learning.The paper is structured as follows. In Sec. 2, we outline possible specific f 2 tasks and available datasets; in Sec. 3, we survey the popular non- neural and neural baselines in the context of these tasks; finally, in Sec. 4, we present model-task evaluations within a unified framework to estab- lish the watermark for future research as well as gain insight into the suitability of models across a variety of tasks. In Sec. 5, we demonstrate that transfer learning across tasks is helpful to power- fully seed models. We conclude with Sec. 6, sum- marizing our findings and outlining several future research directions.The tasks we are aware of that can be phrased as f 2 -type problems are listed below. In gen- eral, we primarily focus on tasks that have rea- sonably large and realistically complex datasets freely available. On the contrary, we have explic- itly avoided datasets that have licence restrictions on availability or commercial usage.Given a factoid question and a set of candidate answer-bearing sentences in encyclopedic style, the first task is to rank higher sentences that are more likely to contain the answer to the question. As it is fundamentally an Information Retrival task in nature, the model performance is com- monly evaluated in terms of Mean Average Preci- sion (MAP) and Mean Reciprocial Rank (MRR).This task is popular in the NLP research com- munity thanks to the dataset introduced in ( Wang et al., 2007) (which we refer to as wang), with six papers published between February 2015 and 2016 alone and neural models substantially im- proving over classical approaches based primarily on parse tree edits. 1 It is possibly the main re- search testbed for f 2 -style task models. This task has also immediate applications e.g. in Question Answering systems.In the context of practical applications, the so- far standard wang dataset has several downsides we observed when tuning and evaluating our mod- els, illustrated numerically in Fig. 1 -the set of candidate sentences is often very small and quite uneven (which also makes rank-based measures unstable) and the total number of individual sen- tence pairs as well as questions is relatively small. Furthermore, the validation and test set are very small, which makes for noisy performance mea- surements; the splits also seem quite different in the nature of questions since we see minimum correlation between performance on the validation and test sets, which calls the parameter tuning pro- cedures and epoch selection for early stopping into question. Alternative datasets WikiQA ( Yang et al., 2015) and InsuranceQA ( Tan et al., 2015) were proposed, but are encumbered by licence restric- tions. Furthermore, we speculate that they may suffer from many of the problems above 2 (even if they are somewhat larger).To alleviate the problems listed above, we are introducing a new dataset yodaqa/large2470 based on an extension of the curatedv2 ques- tion dataset (introduced in (Baudiš andŠediv´yandˇandŠediv´andŠediv´y, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the Yo- daQA question answering system (Baudiš, 2015) from English Wikipedia and labelled by matching the gold standard answers in the passages. 3 Motivated by another problem related to the Yo- daQA system, we also introduce another dataset wqmprop, where s 0 are again question sentences, but s 1 are English labels of properties that make a path within the Freebase knowledge base that connects an entity linked in the question to the cor- rect answer. This task (Property Selection) can be evaluated identically to the previous task, and so- lutions often involving Convolutional Neural Net- works have been studied in the Question Answer- ing literature ) ( Xu et al., 2016). Our sentences have been derived from the We- bQuestions dataset (Berant et al., 2013) extended with the moviesE dataset questions (originally in- troduced in (Baudiš andŠediv´yandˇandŠediv´andŠediv´y, 2015)); the prop- erty paths are based on the Freebase knowledge graph dump, generated based on entity linking and exploration procedure of YodaQA v1.5. 4 Fig. 1 compares the critical characteristics of the datasets. Furthermore, as apparent below, the baseline performances on the newly proposed datasets are much lower, which suggests that fu- ture model improvements will be more apparent in evaluation.( Lowe et al., 2015) proposed the new large-scale real-world Ubuntu Dialogue dataset for an f 2 -style task of ranking candidates for the next utterance in a chat dialog, given the dialog context. The tech- nical formulation of the task is the same as for An- swer Sentence Selection, but semantically, choos- ing the best followup has different concerns than choosing an answer-bearing sentence. Recall at top-ranked 1, 2 or 5 utterances out of either 2 or 10 candidates is reported; we also propose reporting the utterance MRR as a more aggregate measure. The newly proposed Ubuntu Dialogue dataset is based on IRC chat logs of the Ubuntu commu- nity technical support channels and contains casu- ally typed interactions regarding computer-related problems. 5 While the training set consists of in- dividual labelled pairs, during evaluation 10 fol- lowups to given message(s) are ranked. The se- quences might be over 200 tokens long.Our primary motivation for using this dataset is its size. The numerical characteristics of this dataset are shown in Table 1. 6 We use the v2 version of the dataset. 7 Research published on this dataset so far relies on simple neural models. ( Lowe et al., 2015) (Kadlec et al., 2015 We include two current popular machine learn- ing datasets for this task. The Stanford Natural Language Inference SNLI dataset ( Bowman et al., 2015) consists of 570k English sentence pairs with the facts based on image captions, and 10k + 10k of the pairs held out as validation and test sets. The SICK-2014 dataset ( Marelli et al., 2014) was in- troduced as Task 1 of the SemEval 2014 confer- ence and in contrast to SNLI, it is geared at specifi- cally benchmarking semantic compositional meth- ods, aiming to capture only similarities on purely language and common knowledge level, without relying on domain knowledge, and there are no named entities or multi-word idioms; it consists of 4500 training pairs, 500 validation pairs and 4927 testing pairs.For the SICK-2014 dataset, we also report re- sults on the Semantic Textual Similarity. This task originates in the STS track of the SemEval con- ferences ( Agirre et al., 2015) and involves scoring pairs of sentences from 0 to 5 with the objective of maximizing correlation (Pearson's r) with manu- ally annotated gold standard.One of the classic tasks at the boundary of Natural Language Processing and Artificial Intelligence is the inference problem of Recognizing Textual En- tailment ( Dagan et al., 2006) -given a pair of a factual sentence and a hypothesis sentence, we are to determine whether the hypothesis represents a contradiction, entailment or is neutral (cannot be proven or disproven).As our goal is a universal text comprehen- sion model, we focus on neural network models architecture-wise. We assume that the sequence is transformed using N -dimensional word embed- dings on input, and employ models that produce a pair of sentence embeddings E 0 , E 1 from the sequences of word embeddings e 0 , e 1 . Unless noted otherwise, a Siamese architecture is used that shares weights among both sentenes. A scorer module that compares the E 0 , E 1 sen- tence embeddings to produce a scalar result is con- nected to the model; for specific task-model con- figurations, we use either the dot-product module E 0 · E T 1 (representing non-normalized vector an- gle, as in e.g. ( Yu et al., 2014) or (Weston et al., 2014)) 8 or the MLP module that takes element- wise product and sum of the embeddings and feeds them to a two-layer perceptron with hidden layer of width 2N (as in e.g. ( Tai et al., 2015)). 9 For the STS task, we follow this by score regression using dataset-factoid-webquestions branch movies 5 In a manner, they resemble tweet data, but without the length restriction and with heavily technical jargon, inter- spersed command sequences etc. 6   class interpolation as in (Tai et al., 2015). When training for a ranking task (Answer Sen- tence Selection), we use the bipartite ranking ver- sion of Ranknet ( Burges et al., 2005) as the objec- tive; when training for STS task, we use Pearson's r formula as the objective; for binary classification tasks, we use the binary crossentropy objective.In order to anchor the reported performance, we report several basic methods. Weighed word overlaps metrics TF-IDF and BM25 ( Robertson et al., 1995) are inspired by IR research and pro- vide strong baselines for many tasks. We treat s 0 as the query and s 1 as the document, counting the number of common words and weighing them ap- propriately. IDF is determined on the training set.The avg metric represents the baseline method when using word embeddings that proved success- ful e.g. in ( Yu et al., 2014) or (Weston et al., 2014), simply taking the mean vector of the word em- bedding sequence and training an U weight matrix N ×2N that projects both embeddings to the same vector space, E i = tanh(U · ¯ e i ), where the MLP scorer compares them. During training, p = 1/3 standard (elementwise) dropout is applied on the input embeddings.A simple extension of the above are the DAN Deep Averaging Networks ( Iyyer et al., 2015), which were shown to adequately replace much more complex models in some tasks. Two dense perceptron layers are stacked between the mean and projection, relu is used instead of tanh as the non-linearity, and word-level dropout is used in- stead of elementwise dropout.rectional network with 2N GRU memory units 10 ( Cho et al., 2014) in each direction; the final unit states are summed across the per-direction GRUs to yield a 2N vector representation of the sen- tence. Like in the avg baseline, a projection matrix is applied on this representation and final vectors compared by an MLP scorer. We have found that applying massive dropout p = 4/5 both on the in- put and output of the network helps to avoid over- fitting even early in the training.  Kadlec et al., 2015). We apply a multi-channel convolution (Kim, 2014) with single-token channel of N convolutions and 2, 3, 4 and 5-token channels of N/2 convolu- tions each, relu transfer function, max-pooling over the whole sentence, and as above a projec- tion to shared space and an MLP scorer. Dropout is not applied.The RNN-CNN model aims to combine both re- current and convolutional networks by using the memory unit states in each token as the new repre- sentation of the token which is then fed to the con- volutional network. Inspired by (Tan et al., 2015), the aim of this model is to allow the RNN to model long-term dependencies and model contextual rep- resentations of words, while taking advantage of the CNN and pooling operation for crisp selection of the gist of the sentence. We use the same pa- rameters as for the individual models, but with no dropout and reducing the number of parameters by using only N memory units per direction. The idea of attention models is to attend preferren- tially to some parts of the sentence when building its representation (  Convolutional network weights are not shared between the two sentences and the convolutional network output is not projected before applying the MLP scorer. The CNN used here is single- channel with 2N convolution filters 3 tokens wide.chitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available. As a workaround, ensemble of world overlap count and neural model score is typically used to produce the final score. In line with this idea, in the An- swer Sentence Selection wang and large2470 datasets, we use the BM25 overlap baseline as an additional input to the MLP scoring module, and prune the scored samples to top 20 based on BM25. 12 Furthermore, we extend the embedding of each input token by several extra dimensions carrying boolean flags -bigram overlap, unigram overlap (except stopwords and interpunction), and whether the token starts with a capital letter or is a number.Particular hyperparameters are tuned primar- ily on the yodaqa/large2470 dataset unless noted otherwise in the respective results table caption. We apply 10 −4 L 2 regularization and use Adam optimization with standard parameters (Kingma and Ba, 2014). In the answer selection tasks, we train on 1/4 of the dataset in each epoch. After training, we use the epoch with best vali- dation performance; sadly, we typically observe heavy overfitting as training progresses and rarely use a model from later than a couple of epochs.To easily implement models, dataset loaders and task adapters in a modular fashion so that any model can be easily run on any f 2 -type task, we have created a new software pack- age dataset-sts that integrates a variety of datasets, a Python dataset adapter PySTS and a Python library for easy construction of deep neu- ral NLP models for semantic sentence pair scoring KeraSTS that uses the Keras machine learning li- brary (Chollet, 2015). The framework is available for other researchers as open source on GitHub. 11We use N = 300 dimensional GloVe embed- dings matrix pretrained on Wikipedia 2014 + Gi- gaword 5 ( Pennington et al., 2014) that we keep adaptable during training; words in the training set not included in the pretrained model are initial- ized by random vectors uniformly sampled from [−0.25, +0.25] to match the embedding standard deviation.Word overlap is an important feature in many f 2 -type tasks ( Yu et al., 2014) (Severyn and Mos- We report model performance averaged across 16 training runs (with different seeds). A consid- eration we must emphasize is that randomness plays a large role in neural models both in terms of randomized weight initialization and stochas- tic dropout. For example, the typical methodol- ogy for reporting results on the wang dataset is to evaluate and report a single test run after tun- ing on the dev set, 13 but wang test MRR has em- pirical standard deviation of 0.025 across repeated runs of our attn1511 model, which is more than twice the gap between every two successive papers pushing the state-of-art on this dataset! See the * - marked sample in Fig. 2 for a practical example of this phenomenon. Furthermore, on more com- plex tasks (Answer Sentence Selection in particu- lar, see Fig. 1) the validation set performance is not a great approximator for test set performance and a strategy like picking the training run with best val- idation performance would lead just to overfitting on the validation set.To allow comparison between models (and with future models), we therefore report also 95% con- fidence intervals for each model performance es- timate, as determined from the empirical standard deviation using Student's t-distribution. 14 In Fig. 2 to 4, we show the cross-task performance of our models. We can observe an effect analo- gous to what has been described in ( Kadlec et al., 2015) -when the dataset is smaller, CNN models are preferrable, while larger dataset allows RNN models to capture the text comprehension task bet- ter. IR baselines provide strong competition and finding new ways to ensemble them with models should prove beneficial in the future. 15 This is especially apparent in the new Answer Sentence Selection datasets that have very large number of sentence candidates per question. The attention mechanism also has the highest impact in this kind of Information Retrieval task.On the Ubuntu Dialog dataset, even the sim- ple LSTM model in our proposed setting beats the baseline performance reported by Lowe, while our RNN-CNN model establishes the new state-of-art, beating the three-hop Memory Network of (Dodge et al., 2015). It is not possible to statistically de- termine the relation of our models to state-of-art on the wang Answer Sentence Selection dataset. Our models clearly yet lag behind the state-of-art on the RTE and STS tasks, where we did not care- fully tune their parameters, but also did not em- ploy data augmentation strategies like synonyme substitution in (Mueller and Thyagarajan, 2016), which might be necessary for good performance on small datasets even when using transfer learn- ing.sion, we trained a model on the large Ubuntu Di- alogue dataset (Next Utterance Ranking task) and transferred the weights and retrained the model in- stance on other tasks. We used the RNN model for the experiment in a configuration with dot-product scorer and smaller dimensionality (which works much better on the Ubuntu dataset). This config- uration is shown in the respective result tables as Ubu. RNN and it consistently ranks as the best or among the best classifiers, dramatically outper- foring the baseline RNN model. 16 During our experiments, we have noticed that it is important not to apply dropout during re- training if it wasn't applied during the source model training, to balance the dataset labels, and we used the RMSprop training procedure since Adam's learning rate annealing schedule might not be appropriate for weight re-training. We have also tried freezing the weights of some layers, but this never yielded a significant improvement.( Bowman et al., 2015) have shown that such a model transfer is beneficial by reusing an RTE model trained on the SNLI dataset to the SICK- 2014 dataset. We have tried the same, shown as SNLI RNN, and while we see an improvement when reusing it on an RTE task, on other tasks it is the same or worse than the Ubuntu Dialogue based transfer, possibly because the Ubu. task sees more versatile and less clean data.To confirm the hypothesis that our models learn a generic task akin to some form of text comprehen- 14 Over larger number of samples, this estimate converges to the normal distribution confidence levels. Note that the confidence interval determines the range of the true expected evaluation, not evaluation of any measured sample. 15 We have tried simple averaging of predictions (as per (Kadlec et al., 2015)), but the benefit was small and incon- sistent.We have unified a variety of tasks in a single sci- entific framework of sentence pair scoring, and demonstrated a platform for general modelling of this problem and aggregate benchmarking of these models across many datasets. Promising initial transfer learning results suggest that a quest for generic neural model capable of task-independent text comprehension is becoming a meaningful pur- suit. The open source nature of our framework and the implementation choice of a popular and extensible deep learning library allows for high reusability of our research and easy extensions with further more advanced models.Based on our benchmarks, as a primary model for applications on new f 2 -type tasks, we can rec- ommend either the RNN-CNN model or transfer learning based on the Ubu. RNN model.  Due to the very wide scope of the f 2 -problem scope, we leave some popular tasks and datasets as future work. A popular instance of sen- tence pair scoring is the question answering task of the Memory Networks (supported by the baBi dataset) ( Weston et al., 2015). A realis- tic large question Paraphrasing dataset based on the AskUbuntu Stack Overflow forum had been recently proposed ( Lei et al., 2015). 17 In a multi-lingual context, sentence-level MT Quality Estimation is a meta-task with several available datasets. 18 While the tasks of Semantic Textual Similarity (supported by a dataset from the STS track of the SemEval conferences (Agirre et al., 17 The task resembles paraphrasing, but is evaluated as an Information Retrieval task much closer to Answer Sentence Selection.18 http://www.statmt.org/wmt15/ quality-estimation-task.html 2015)) and Paraphrasing (based on the Microsoft Research Paraphrase Corpus ( Dolan and Brockett, 2005) right now) are available within our frame- work, we do not report the results here as the models lag behind the state-of-art significantly and show little difference in results. Advancing the models to be competitive remains future work. A generalization of our proposed architecture could be applied to the Hypothesis Evidencing task of binary classification of a hypothesis sentence s 0 based on a number of memory sentences s 1 , for example within the MCText ( Richardson et al., 2013) dataset. We also did not include several major classes of models in our initial evaluation. Most notably, this includes serial RNNs with at- tention as used e.g. for the RTE task (Rocktäschel et al., 2015), and the skip-thoughts method of sen- tence embedding. ( Kiros et al., 2015  the research models further towards the real-world by allowing for wider sentence variability and less explicit supervision. But in particular, we believe that new models should be developed and tested on tasks with long sentences and wide vocabulary. 
