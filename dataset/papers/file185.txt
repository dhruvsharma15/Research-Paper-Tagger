Because of the "vanishing gradients" phenomenon, shallow network layers tend to have much smaller gradients than deep layers -sometimes differing by orders of magnitude from one layer to the next [10]. In most previous work in optimization for deep networks, methods either keep a global learning rate that is shared over all parameters, or use an adaptive learning rate specific to each parameter. Our method exploits the following observation: parameters in the same layer have gradients of similar magnitudes, and can thus efficiently share a common learning rate. Layer-specific learning rates can be used to accelerate layers with smaller gradients. Another ad- vantage of this approach is that by avoiding the computation of large numbers of parameter-specific learning rates, our method remains computationally efficient. Finally, as mentioned in Section I, to avoid slowing down learning at high-error low curvature saddle points, we also want our method to take large steps at low curvature points. layer's learning rate based only on gradients observed for all weights in a specific layer in the current iteration. Thus, our scheme avoids both storing gradient information from previous iterations and computing learning rates for each parameter; it is therefore less computationally and memory intensive when compared to AdaGrad. The proposed layer specific learning rates also works well on large scale datasets like ImageNet (when applied over SGD), where AdaGrad fails to converge to a good solution.The proposed method can be used with any existing optimization technique which uses a global learning rate, provides a layer-specific learning rate, and escapes saddle points quickly, all without sacrificing computation or memory usage. As we show in Section IV, using our adaptive learning rates on top of existing optimization techniques almost always improves performance on standard datasets.Let t (k) be the learning rate at the k-th iteration for any standard optimization method. In case of SGD, this would be given by equation 4, while for AdaGrad it would just be the global learning rate t as in equation 8. We propose to modify t (k) as follows:The proposed method can be used with any existing optimization technique which uses a global learning rate. This helps in getting a layer-specific learning rate, as well as, helps in escaping saddle points quicker, with very little computational overhead. As we show in Section IV, using our adaptive learning rates on top of existing optimization techniques almost always improves performance on standard datasets.Here t (k) l denotes the new learning rate for the parameters in the l-th layer at the k-th iteration and g(k) l denotes a vector of the gradients of the parameters in the l-th layer at the k-th iteration. Thus, we see that we use only the gradients in the same layer to determine the learning rate for that layer. It is also important to note that we do not use any gradients from previous iterations, and thus save on storage.From equation 9, we see that when the gradients in a layer are very large, the equation just reduces to using the normal learning rate t (k) . However, when the gradients are very small, we are more likely to be near a low curvature point. Thus, the equation scales up the learning rate to ensure that the initial layers of the network learn faster, and that we escape high- error low curvature saddle points quickly.We present image classification results on three standard datasets: MNIST, CIFAR10 and ImageNet (ILSVRC 2012 dataset, part of the ImageNet challenge). MNIST contains 60,000 handwritten digit images for training and 10,000 hand- written digit images for testing. CIFAR10 contains has 10 classes with 6,000 images in each class. ImageNet contains 1.2 million color images from 1000 different classes.We can use this layer-specific learning rate on top of SGD. Using equation 3, the update in that case, would be:where ∆x (k) l denotes the update in the parameters of the l-th layer at the k-th iteration.Similarly, we can modify AdaGrad's update equation (8) to use our modified learning rates.We use Caffe [16] to implement our method. Caffe provides optimization methods for Stochastic Gradient Descent (SGD), Nesterov's Accelerated Gradient (NAG) and AdaGrad. For a fair comparison between state-of-the-art methods, we add our adaptive layer-specific learning rate method on top of each of these optimization methods. In our experiments, we demonstrate the effectiveness of our algorithm on convolu- tional neural networks on 3 datasets. On CIFAR10, we use the same global learning rate as provided in Caffe. Since our method always increases the layer-specific learning rate (with respect to other optimization methods) based on the global learning rate, we start with a slightly smaller learning rate of 0.006 to make the learning less aggressive for the ImageNet experiment. SGD was initialized with the learning rate used in [2] for experiments done on ImageNet.Note that, unlike AdaGrad which uses a distinct learning rate for each parameter, we use a different learning rate for each layer, which is shared by all weights in that layer. Additionally, AdaGrad modifies the learning rate based on the entire history of gradients observed for that weight while we update a 1) MNIST: We use the same architecture as LeNet for our experiments on MNIST. We present the results of using our proposed layer-specific learning rates on top of stochastic gradient descent, Nesterov's accelerated gradient method and AdaGrad on the MNIST dataset. Since all methods converge very quickly on this dataset, we present the accuracy and loss only for the first 2,000 iterations. Table I shows the3.29 ± 0.22 3.05 ± 0.21 3.01 ± 0.19 2.84 ± 0.17 2.21 ± 0.14 1.95 ± 0.16 10001.89 ± 0.08 1.80 ± 0.13 1.92 ± 0.07 1.83 ± 0.18 1.68 ± 0.11 1.57 ± 0.14 14001.60 ± 0.11 1.49 ± 0.09 1.74 ± 0.12 1.52 ± 0.11 1.61 ± 0.08 1.61 ± 0.09 18001.52 ± 0.09 1.41 ± 0.12 1.56 ± 0.09 1.37 ± 0.09 1.41 ± 0.09 1.34 ± 0.08  Our Method 50kStep Down Our Method 60kStep Down SGD 50kStep Down SGD 60kStep Down  -1c) comparing SGD, NAG and AdaGrad, each with our adaptive layer-wise learning rates. For the SGD plot, we show results both when we step down the learning rate at 50,000 iterations as well as 60,000 iterations.mean accuracy and standard deviation when each method was run 10 times. We observe that our proposed layer-specific learning rate is consistently better than Nesterov's accelerated gradient, stochastic gradient descent and AdaGrad. In all the experiments, the proposed method also attains the maximum accuracy of 99.2% just like stochastic gradient descent, Nes- terov's accelerated gradient and AdaGrad.We use an implementation of AlexNet [2] in Caffe, a deep convolutional neural network architecture, for comparing our method with other optimization algorithms. AlexNet consists of 5 convolution layers followed by 3 fully connected layers. More details regarding the architecture can be found in the paper [2].2) CIFAR10: On CIFAR10 we use a convolutional neural network with 2 layers of 32 feature maps from 5 × 5 con- volution kernels, each followed by 3 × 3 max pooling layers. After this we have another convolution layer with 64 feature maps from a 5 × 5 convolution kernel followed by a 3 × 3 max pooling layer. Finally, we have a fully connected layer with 10 hidden nodes and a soft-max logistic regression layer. After each convolution layer a ReLu non-linearity is applied. This is the same architecture as specified in Caffe. For the first 60,000 iterations the learning rate was 0.001 and it was dropped by a factor of 10 at 60,000 and 65,000 iterations.On this dataset, we again observe that final error and loss of our method is consistently lower than SGD, NAG and AdaGrad (Table II). After step down, our adaptive method reaches a lower accuracy than both SGD and NAG. Note that just using our optimization method (without changing the network architecture) we can get an improvement of 0.32% over the mean accuracy for SGD. Even if we step down the learning rate at 50,000 iterations (taking 60000 iterations in total), we obtain an accuracy of 82.08%, which is better than SGD after 70,000 iterations, significantly cutting down on required training time Fig. 1. Since our method converges much faster when used with SGD, it is possible to perform the step down on the learning rate even earlier, potentially reducing training time even further. Although Adagrad does not perform very well on CIFAR10 with default parameters, we observe an improvement of 1.3% over the mean final accuracy, with again a significant speed-up in training time.Since AlexNet is a deep neural network with significant complexity, it is suitable to apply our method on this network architecture. Fig 2 shows the results of using our method over SGD. We observe that our method obtains significantly greater accuracy and lower loss after 100,000 and 200,000 iterations. Further, we are also able to reach the maximum accuracy of 57.5% on the validation set after 295,000 iterations which is achieved by SGD only after 345,000 iterations, resulting in a reduction of 15% in training time. Given that such a large model takes more than a week to train properly, this is a significant reduction. Our loss is also consistently lower than SGD across all iterations. In the existing model, we perform a step down by a factor of 10 after every 100,000 iterations. In order to analyze how our method performs when we reduce the number of training iterations, we vary the number of training iterations at a specific learning rate before performing a step down. Table III shows the final accuracy after 350,000 iterations of SGD and our method. Although the final accuracy drops slightly as we decrease the number of iterations after which we perform the step down in the learning rate, it is clearly evident that our method achieves better accuracy than SGD. Note that we report top-1 class accuracy. Since we use the Caffe implementation of the AlexNet architecture and do not use any data augmentation techniques, our results are slightly lower than those reported in [2].In this paper we propose a general method for training deep neural networks using layer-specific adaptive learning rates,     which can be used on top of any optimization method with a global learning rate. The method uses gradients from each layer to compute an adaptive learning rate for each layer. It aims to speed up convergence when the parameters are in a low curvature saddle point region. Layer-specific learning rates also enable the method to prevent slow learning in initial layers of the deep network, usually caused by very small gradient values.
