Deep learning has seen significant practical success and has had a profound impact on the conceptual bases of machine learning and artificial intelligence. Along with its practical suc- cess, the theoretical properties of deep learning have been a subject of active investigation. For expressivity of neural networks, there are classical results regarding their universality ( Leshno et al., 1993) and exponential advantages over hand-crafted features (Barron, 1993). Another series of theoretical studies have considered how trainable (or optimizable) deep hypothesis spaces are, revealing structural properties that may enable non-convex optimiza- tion ( Choromanska et al., 2015;Kawaguchi, 2016a). However, merely having an expressive and trainable hypothesis space does not guarantee good performance in predicting the val- ues of future inputs, because of possible over-fitting to training data. This leads to the study of generalization, which is the focus of this paper.Some classical theory work attributes generalization ability to the use of a low-capacity class of hypotheses (Vapnik, 1998;Mohri et al., 2012). From the viewpoint of compact representation, which is related to small capacity, it has been shown that deep hypothesis spaces have an exponential advantage over shallow hypothesis spaces for representing some classes of natural target functions ( Pascanu et al., 2014;Montufar et al., 2014;Livni et al., 2014;Telgarsky, 2016;Poggio et al., 2017). In other words, when some assumptions im- plicit in the hypothesis space (e.g., deep composition of piecewise linear transformations) are approximately satisfied by the target function, one can achieve very good generalization, compared to methods that do not rely on that assumption. However, a recent paper ( Zhang et al., 2017) has empirically shown that successful deep hypothesis spaces have sufficient capacity to memorize random labels. This observation has been called an "apparent para- dox" and has led to active discussion by many researchers (Arpit et al., 2017;Krueger et al., 2017;Hoffer et al., 2017;Wu et al., 2017;Dziugaite and Roy, 2017;Dinh et al., 2017). Zhang et al. (2017) concluded with an open problem stating that understanding such observationsLet x ∈ X be an input and y ∈ Y be a target. Let be a loss function. Let R[f ] be the expected risk of a function f , R[f ] = E x,y∼P [ (x), y)], where P is the true distribution. Let f A(S m ) : X → Y be a model learned by a learning algorithm A (including random seeds for simplicity) using a training dataset S m = {(x 1 , y 1 ), . . . , (x m , y m )} of size m. LetˆRLetˆ LetˆR S m [f ] be the empirical risk of f , ˆ R Sm [f ] = 1 m m i=1 (x i ), y i ) with {(x i , y i )} m i=1 = S m . Let F be a set of functions or a hypothesis space. Let L F be a family of loss functions associated with F , defined by L F = {g : f ∈ F, g(x, y) (x), y)}. All vectors are column vectors in this paper. For any given variable v, let d v be the dimensionality of the variable v.A goal in machine learning is typically framed as the minimization of the expected risk R[f A (Sm) ]. We typically aim to minimize the non-computable expected risk R[f A(Sm) ] by minimizing the computable empirical riskˆRriskˆ riskˆR S m [f A(Sm) ] (i.e., empirical risk minimization). One goal of the generalization theory is to explain and justify when and how minimizingˆR minimizingˆ minimizingˆR Sm [f A (Sm) ] is a sensible approach to minimizing R[f A(Sm) ] by analyzing the generalization gap R[f A (Sm)  Sm) ].In this section only, we use the typical assumption that S m is generated by i.i.d. draws according to the true distribution P ; in general, this paper does not utilize this assumption. Under this assumption, a primary challenge of analyzing the generalization gap stems from the dependence of f A(S m ) on the same dataset S m used in the definition ofˆRofˆ ofˆR Sm . Several approaches in statistical learning theory have been developed to handle this dependence. The hypothesis-space complexity approach handles this dependence by decoupling f A(Sm) from the particular S m by considering the worst-case gap for functions in the hypothesis space asand by carefully analyzing the right-hand side. Because the cardinality of F is typically (uncountably) infinite, a direct use of the union bound over all elements in F yields a vacuous bound, leading to the need to consider different quantities to characterize F ; e.g., Rademacher complexity and the Vapnik-Chervonenkis (VC) dimension. For example, if the codomain of is in [0,1], we have (Mohri et al., 2012, Theorem 3.1) that for any δ &gt; 0, with probability at least 1 − δ, sup lnwhere R m (L F ) is the Rademacher complexity of L F , which then can be bounded by the Rademacher complexity of F , R m (F ). For the deep-learning hypothesis spaces F , there are several well-known bounds on R m (F ) including those with explicit exponential dependence on depth ( Sun et al., 2016;Neyshabur et al., 2015b;Xie et al., 2015) and explicit linear dependence on the number of trainable parameters (Shalev-Shwartz and Ben-David, 2014).There has been significant work on improving the bounds in this approach, but all existing solutions with this approach still depend on the complexity of a hypothesis space or a sequence of hypothesis spaces.The stability approach deals with the dependence of f A(Sm) on the dataset S m by con- sidering the stability of algorithm A with respect to different datasets. The considered stability is a measure of how much changing a data point in S m can change f A(Sm) . For example, if the algorithm A has uniform stability β (w.r.t. and if the codomain of is in [0, M ], we have ( Bousquet and Elisseeff, 2002) that for any δ &gt; 0, with probability at least 1 − δ, R[f A(S m ) ] − ˆ R Sm [f A(S m ) ] ≤ 2β + (4mβ + M ) ln 1 δ2m .Based on previous work on stability (e.g., Hardt et al. 2015;Kuzborskij and Lampert 2017;Gonen and Shalev-Shwartz 2017), one may conjecture some reason for generalization in deep learning, the proving of which requires future work. The robustness approach avoids dealing with certain details of the dependence of f A(Sm) on S m by considering the robustness of algorithm A for all possible datasets. In contrast to stability, robustness is the measure of how much the loss value can vary w.r.t. the input space of (x, y). For example, if algorithm A is (Ω, ζ(•))-robust and the codomain of is upper-bounded by M , given a dataset S m , we have ( Xu and Mannor, 2012) that for any δ &gt; 0, with probability at least 1 − δ,The robustness approach requires an a priori known and fixed partition of the input space such that the number of sets in the partition is Ω and the change of loss values in each set of the partition is bounded by ζ(S m ) for all S m (Definition 2 and the proof of Theorem 1 in Xu and Mannor 2012). In classification, if the margin is ensured to be large, we can fix the partition with balls of radius corresponding to the large margin, filling the input space. Recently, this idea was applied to deep learning ( Sokolic et al., 2017a,b), producing insightful and effective generalization bounds, while still suffering from the curse of the dimensionality of a priori-known fixed input manifold. With regard to the above approaches, flat minima can be viewed as the concept of low variation in the parameter space; i.e., a small perturbation in the parameter space around a solution results in a small change in the loss surface. Several studies have provided ar- guments for generalization in deep learning based on flat minima ( Keskar et al., 2017). However, Dinh et al. (2017) showed that flat minima in practical deep learning hypothe- sis spaces can be turned into sharp minima via re-parameterization without affecting the generalization gap, indicating that it requires further investigation. Zhang et al. (2017) empirically demonstrated that several deep hypothesis spaces can mem- orize random labels, while having the ability to produce zero training error and small test errors for particular natural datasets (e.g., CIFAR-10). They also empirically observed that regularization on the norm of weights seemed to be unnecessary to obtain small test errors, in contradiction to conventional wisdom. These observations suggest the following problem to be solved:with a sufficiently complex deep-learning hypothesis space F f , producing the- oretical insights and distinguishing the case of "natural" problem instances (P, S m ) (e.g., images with natural labels) from the case of other problem instances (P , S m ) (e.g., images with random-labels).Supporting and extending the empirical observations by Zhang et al. (2017), we provide a theorem (Theorem 1) stating that the hypothesis space of over-parameterized linear models can memorize any training data and decrease the training and test errors arbitrarily close to zero (including zero) with the norm of parameters being arbitrarily large, even when the parameters are arbitrarily far from the ground-truth parameters. Furthermore, Corollary 2 shows that conventional wisdom regarding the norm of the parameters w can fail to explain generalization, even in linear models that might be seemingly not over-parameterized. All proofs for this paper are presented in the appendix. Corollary 2. If n ≤ m and if rank(M ) &lt; n, then statement (ii) in Theorem 1 holds.Whereas Theorem 1 and Corollary 2 concern test errors instead of expected risk (in order to be consistent with empirical studies), Proposition 8 in Appendix A.1 proves the same phenomena for expected risk for general machine learning models not limited to deep learning and linear hypothesis spaces; i.e., Proposition 8 in Appendix A.1 states that none of small capacity, low complexity, stability, robustness, and flat minima is necessary for generalization in general machine learning for each given problem instance (P, S m ). We capture the essence of all of these observations in the following remark.of a hypothesis f with a true distribution P and a dataset S m are completely determined by the tuple (P, S m , f ), independently of other factors, such as a hypothesis space F (and hence its properties such as capacity, Rademacher complexity, pre-defined bound on norms, and flat- minima) and properties of random datasets different from the given S m (e.g., stability and robustness of the learning algorithm A). In contrast, the conventional wisdom states that these other factors are what matter. This has created the "apparent paradox" in the literature.From these observations, we propose the following open problem:of a hypothesis f with a pair (P, S m ) of a true distribution and a dataset, producing theoretical insights, based only on properties of the hypothesis f and the pair (P, S m ).Solving Problem 2 for deep learning implies solving Problem 1, but not vice versa. Problem 2 encapsulates the essence of Problem 1 and all the issues from our Theorem 1, Corollary 2 and Proposition 8.The empirical observations in ( Zhang et al., 2017) and our results above may seem to contradict the results of the generalization theory. However, there is no contradiction, and the apparent inconsistency arises from differences in assumptions. Indeed, under certain assumptions, many results in statistical learning theory have been shown to be tight and insightful (e.g., Mukherjee et al. 2006;Mohri et al. 2012). Figure 1 illustrates the differences in assumptions in statistical learning theory and some empirical studies. On one hand, in statistical learning theory, a distribution P and a dataset S m are usually unspecified except that P is in some set P and a dataset S m ∈ D is drawn randomly according to P (typically with the i.i.d. assumption). On the other hand, in most empirical studies and in our theoretical results (Theorem 1 and Proposition 8), a distribution P is still unknown yet specified (e.g., via a real world process) and a dataset S m is specified and usually known (e.g., CIFAR-10 or ImageNet). Intuitively, whereas statistical learning theory considers a set P × D because of weak assumptions, some empirical studies can focus on a specified point (P, S m ) in a set P × D because of stronger assumptions. Therefore, by using the same terminology such as "expected risk" and "generalization" in both cases, we are susceptible to confusion and apparent contradiction. Figure 1: An illustration of differences in assumptions. Statistical learning theory analyzes the generalization behaviors of f A(S m ) over randomly-drawn unspecified datasets S m ∈ D according to some unspecified distribution P ∈ P. Intuitively, statistical learning theory concerns more about questions regarding a set P × D because of the unspecified nature of (P, S m ), whereas certain empirical studies (e.g., Zhang et al. 2017) can focus on questions regarding each specified point (P, S m ) ∈ P × D.Lower bounds, necessary conditions and tightness in statistical learning theory are typi- cally defined via a worst-case distribution P worst ∈ P. For instance, classical "no free lunch" theorems and certain lower bounds on the generalization gap (e.g., Mohri et al. 2012, Sec- tion 3.4) have been proven for the worst-case distribution P worst ∈ P. Therefore, "tight" and "necessary" typically mean "tight" and "necessary" for the set P × D (e.g., through the worst or average case), but not for each particular point (P, S m ) ∈ P × D. From this viewpoint, we can understand that even if the quality of the set P × D is "bad" overall, there may exist a "good" point (P, S m ) ∈ P × D.Several approaches in statistical learning theory, such as the data-dependent and Bayesian approaches (Herbrich and Williamson, 2002;Dziugaite and Roy, 2017), use more assump- tions on the set P ×D to take advantage of more prior and posterior information; these have an ability to tackle Problem 1. However, these approaches do not apply to Problem 2 as they still depend on other factors than the given (P, S m , f ). For example, data-dependent bounds with the luckiness framework (Shawe- Taylor et al., 1998;Herbrich and Williamson, 2002) and empirical Rademacher complexity (Koltchinskii and Panchenko, 2000;Bartlett et al., 2002) still depend on a concept of hypothesis spaces (or the sequence of hypothesis spaces), and the robustness approach ( Xu and Mannor, 2012) depend on different datasets than a given S m via the definition of robustness (i.e., in Section 2, ζ(S m ) is a data-dependent term, but the definition of ζ itself and Ω depend on other datasets than S m ).We note that analyzing a set P × D is of significant interest for its own merits and is a natural task along the field of computational complexity (e.g., categorizing a set of problem instances into subsets with or without polynomial solvability). Indeed, the situation where theory focuses more on a set and many practical studies focus on each element in the set is prevalent in computer science (see the discussion in Appendix B.1 for more detail). We further validate the logical consistency in our observations in Appendix B.2 and propose several practical roles of generalization theory in Appendix B.3.In the previous section, we extended Problem 1 to Problem 2, and identified the different assumptions in theoretical and empirical studies. Accordingly, this section aims to solve these problems, both in the case of each specified dataset and the case of random unspecified datasets. To achieve this goal, this section presents a direct analysis of neural networks, rather than deriving results about neural networks from more generic theories based on capacity, Rademacher complexity, stability, or robustness. This section focuses on the] with a training dataset S m and with squared loss. For 0-1 loss with multi-labels, our probabilistic bound is presented in Appendix A.2.We consider general neural networks of any depth that have the structure of a directed acyclic graph (DAG) with ReLU nonlinearity and/or max pooling. This includes any struc- ture of a feedforward network with convolutional and/or fully connected layers, potentially with skip connections. For pedagogical purposes, we first discuss our model description for layered networks without skip connections, and then describe it for DAGs.Layered nets without skip connections Let h (l) (x, w) ∈ R n l be the pre-activation vector of the l-th hidden layer, where n l is the width of the l-th hidden layer, and w represents the trainable parameters. Let H be the number of hidden layers. For layered networks without skip connections, the pre-activation (or pre-nonlinearity) vector of the l-th layer can be written aswith a boundary definitionwhere σ (l−1) represents nonlinearity via ReLU and/or max pooling at the (l − 1)-th hidden layer, and W (l) ∈ R n l ×n l−1 is a matrix of weight parameters connecting the (l − 1)-th layer to the l-th layer. Here, W (l) can have any structure (e.g., shared and sparse weights to represent a convolutional layer). Let ˙ σ (l) (x, w) be a vector with each element being 0 or 1 such that σ (l) h, which is an element-wise product of the vectors ˙ σ (l) (x, w) and h (l) (x, w). Then, we can write the pre-activation of the k-th output unit at the last layer l = H + 1 asBy expanding h (l) (x, w) repeatedly and exchanging the sum and product via the distributive law of multiplication,where) and x j 0 with the change of indices (i.e., σ j (x, w) and ˉ x j respectively contain the n 0 numbers and n 1 • • • n H numbers of the same copy of each ˙ σ j H j H−1 ...j 1 (x, w) and x j 0 ). Note that j represents summation over all the paths from the input x to the k-th output unit.DAGs Remember that every DAG has at least one topological ordering, which can be used to to create a layered structure with possible skip connections (e.g., see Healy and Nikolov 2001;Neyshabur et al. 2015b). In other words, we consider DAGs such that the pre-activation vector of the l-th layer can be written aswith a boundary definitionwhere W (l,l ) ∈ R n l ×n l is a matrix of weight parameters connecting the l -th layer to the l-th layer. Again, W (l,l ) can have any structure. Thus, in the same way as with layered networks without skip connections, for all k ∈ {1, . . . , d y },where j represents the summation over all paths from the input x to the k-th output unit; i.e., ˉ w k,j ˉ σ j (x, w)ˉ x j is the contribution from the j-th path to the k-th output unit. Each of ˉ w k,j , ˉ σ j (x, w) and ˉ x j is defined in the same manner as in the case of layered networks without skip connections. In other words, the j-th path weight ˉ w k,j is the product of the weight parameters in the j-th path, and ˉ σ j (x, w) is the product of the 0-1 activations in the j-th path, corresponding to ReLU nonlinearity and max pooling; ˉ σ j (x, w) = 1 if all units in the j-th path are active, and ˉ σ j (x, w) = 0 otherwise. Also, ˉ x j is the input used in the j-th path. Therefore, for DAGs, including layered networks without skip connections,whereare the vectors of the size of the number of the paths.Theorem 4 solves Problem 2 (and hence Problem 1) for neural networks with squared loss by stating that the generalization gap of a w with respect to a problem (P, S m ) is tightly analyzable with theoretical insights, based only on the quality of the w and the pair (P, S m ). We do not assume that S m is generated randomly based on some relationship with P ; the theorem holds for any dataset, regardless of how it was generated. Let w Theorem 4. Let {λ j } j and {u j } j be a set of eigenvalues and a corresponding orthonormal set of eigenvectors of G. Let θ. Proof idea. From Equation (1) with squared loss, we can decompose the generalization gap into three terms:By manipulating each term, we obtain the desired statement. See Appendix D.1 for a complete proof.In Theorem 4, there is no concept of a hypothesis space or pre-specified bound on a norm of weights. Instead, it indicates that if the norm of the weights ˉ w Sm k 2 at the end of learning process with the actual given S m is small, then the generalization gap is small, even if the norm ˉ w Sm k 2 is unboundedly large during the learning process with S m or at anytime with any dataset other than S m .Importantly, in Theorem 4, there are two other significant factors in addition to the norm of the weights ˉ w Sm k 2 . First, the eigenvalues of G and v measure the concentration of the given dataset S m with respect to the (unknown) P in the space of the learned representationHere, we can see the benefit of deep learning from the viewpoint of "deep-path" feature learning: even if a given S m is not concentrated in the original space, optimizing w can result in concentrating it in the space of z. Similarly, c y measures the concentration of 2 2 , but c y is independent of w and unchanged after a pair (P, S m ) is given. Second, the cos θ terms measure the similarity between ˉ w Sm k and these concentration terms. Because the norm of the weights ˉ w Sm k 2 is multiplied by those other factors, the generalization gap can remain small, even if ˉ w Sm k 2 is large, as long as some of those other factors are small.Based on a generic bound-based theory, Neyshabur et al. (2015a,b) proposed to control the norm of the path weights ˉ w k 2 , which is consistent with our direct bound-less result (and which is as computationally tractable as a standard forward-backward pass 1 ). Unlike 1. From the derivation of Equation (1), one can compute ˉ w Sm 2 k 2 with a single forward pass using element- wise squared weights, an identity input, and no nonlinearity. One can also follow the previous paper (Neyshabur et al., 2015a) for its computation. the previous results, we do not require a pre-defined bound on ˉ w k 2 over different datasets, but depend only on its final value with each S m as desired, in addition to more tight insights (besides the norm) via equality as discussed above. In addition to the pre-defined norm bound, these previous results have an explicit exponential dependence on the depth of the network, which does not appear in our Theorem 4. Similarly, some previous results specific to layered networks without skip connections ( Sun et al., 2016;Xie et al., 2015) contain the 2 H factor and a bound on the product of the norm of weight matrices,( 2 F because the latter contains all of the same terms as the former as well as additional non-negative additive terms after expanding the sums in the definition of the norms.Therefore, unlike previous bounds, Theorem 4 generates these new theoretical insights based on the tight equality (in the first line of the equation in Theorem 4).While the previous subsection tightly analyzed each given point (P, S m ), this subsection considers the set P × D (P, S m ), where D is the set of possible datasets S m endowed with an i.i.d. product measure P m where P ∈ P (see Section 3.1).In Equation (2), the generalization gap is decomposed into three terms, each of which contains the difference between a sum of dependent random variables and its expectation. The dependence comes from the fact thatSm )] are dependent over the sample index i, because of the dependence of w Sm on the entire dataset S m . We then observe the following: in hwith respect to w is zero everywhere (except for the measure zero set, where the derivative does not exist). Therefore, each step of the (stochastic) gradient decent greedily chooses the best direction in terms of ˉ w (with the current2 for more detail). This observation leads to a conjecture that the dependence ofSm )] via the training process with the whole dataset S m is not entirely "bad"in terms of the concentration of the sum of the terms with z i .As a first step to investigate the dependence of z i , we evaluated the following novel two- phase training procedure that explicitly breaks the dependence of z i over the sample index i.We first train a network in a standard way, but only using a partial training dataset S αm = {(x 1 , y 1 ), . . . , (x αm , y αm )} of size αm, where α ∈ (0, 1) (standard phase). We then assign the value of w Sαm to a new placeholder w σ := w Sαm and freeze w σ , meaning that as w changes, w σ does not change. At this point, we have that hWe then keep training only the ˉ w Sαm k part with the entire training dataset of size m (freeze phase), yielding the final model via this two-phase training procedure asNote that the vectors w σ = w  . See Appendix D.3 for a simple implementation of this two-phase training procedure that requires at most (approximately) twice as much computational cost as the normal training procedure.We implemented the two-phase training procedure with the MNIST and CIFAR-10 datasets. The test accuracies of the standard training procedure (base case) were 99.47% for MNIST (ND), 99.72% for MNIST, and 92.89% for CIFAR-10. MNIST (ND) indicates MNIST with no data augmentation. The experimental details are in Appendix D.4. Our source code is available at: http://lis.csail.mit.edu/code/gdl.html Figure 2 presents the test accuracy ratios for varying α: the test accuracy of the two- phase training procedure divided by the test accuracy of the standard training procedure. The plot in Figure 2 begins with α = 0.05, for which αm = 3000 in MNIST and αm = 2500 in CIFAR-10. Somewhat surprisingly, using a much smaller dataset for learning w σ still resulted in competitive performance. A dataset from which we could more easily obtain a better generalization (i.e., MNIST) allowed us to use smaller αm to achieve competitive performance, which is consistent with our discussion above.We now prove a probabilistic bound for the hypotheses resulting from the two-phase training algorithm. Let˜zwhere w σ := w Sαm , as defined in the two-phase training procedure above. Our two-phase training procedure forces˜zforces˜forces˜z αm+1 , . . . , ˜ z m over samples to be independent random variables (each˜zeach˜ each˜z i is dependent over coordinates, which is taken care of in our proof), while maintaining the competitive practical performance of the output model˜h model˜ model˜h. As a result, we obtain the following bound on the generalization gap for the practical deep models˜hmodels˜ models˜h, and cAssume that for all i ∈ {αm + 1, . . . , m},. Theorem 5. Suppose that Assumption 1 holds. Assume that S m \ S αm is generated by i.i.d. draws according to true distribution P . Assume that S m \ S αm is independent of S αm . Let f A(Sm) be the model learned by the two-phase training procedure with S m . Then, for each w σ := w Sαm , for any δ &gt; 0, with probability at least 1 − δ,where β  Our proof does not require independence over the coordinates of˜zof˜ of˜z i and the entries of the random matrices˜zmatrices˜ matrices˜z i ˜ z i (see the proof of Theorem 5). The bound in Theorem 5 is data-dependent because the norms of the weights ˉ w S m k de- pend on each particular S m . Similarly to Theorem 4, the bound in Theorem 5 does not contain a pre-determined bound on the norms of weights and can be independent of the concept of hypothesis space, as desired; i.e., Assumption 1 can be also satisfied without referencing a hypothesis space of w,, 1}. However, unlike Theorem 4, Theorem 5 implicitly contains the properties of datasets dif- ferent from a given S m , via the pre-defined bounds in Assumption 1. This is expected since Theorem 5 makes claims about the set of random datasets S m instead of each instantiated S m . Therefore, while Theorem 5 presents a strongly-data-dependent bound (over random datasets), Theorem 4 is tighter for each given S m ; indeed, the main equality of Theorem 4 is as tight as possible.Theorems 4 and 5 provide generalization bounds for practical deep learning models that do not necessarily have explicit dependence on the number of weights, or exponential dependence on depth or effective input dimensionality. Although the size of the vector ˉ w S m k can be exponentially large in the depth of the network, the norms of the vector need not be. Because˜hBecause˜ Because˜h The previous section presented a direct analysis of neural networks with squared loss to address Problems 1 and 2. While this illustrates the advantage of a direct analysis of each hypothesis space and each loss function, it requires future work to cover different cases of practical interest. Accordingly, for a general case, this section notes a simple way to avoid Problem 1, at the cost of less theoretical insight for training and additional computation for validation.In practical deep learning, we typically adopt the training-validation paradigm, usu- ally with a held-out validation set. We then search over hypothesis spaces by changing architectures (and other hyper-parameters) to obtain low validation error. In this view, we can conjecture the reason why deep learning can sometimes generalize well as follows: it is partially because we can obtain a good model via search using a validation dataset.Indeed, as an example, Remark 6 states that if validation error is small, it is guaranteed to generalize well, regardless of its capacity, Rademacher complexity, stability, robustness, and flat minima. Let S Here, F val is defined as a set of models f that is independent of a held-out validation dataset S (val) m val , but can depend on the training dataset S m . For example, F val can contain a set of models f such that each element f is a result at the end of each epoch during training with at least 99.5% training accuracy. In this example, |F val | is at most (the number of epochs) × (the cardinality of the set of possible hyper-parameter settings), and is likely much smaller than that because of the 99.5% training accuracy criteria and the fact that a space of many hyper-parameters is narrowed down by using the training dataset as well as other datasets from different tasks. If a hyper-parameter search depends on the validation dataset, F val must be the possible space of the search instead of the space actually visited by the search. We can also use a sequence {F (j) val } j (see Appendix B). The bound in Proposition 6 is non-vacuous and tight enough to be practically mean- ingful. For example, consider a classification task with 0-1 loss. Set m val = 10, 000 (e.g., MNIST and CIFAR-10) and δ = 0.1. Then, even in the worst case with C = 1 and γ 2 = 1 and even with |F val | = 1, 000, 000, 000, we have with probability at least 0.9 that[f ] + 6.94% for all f ∈ F val . In a non-worst-case scenario, for example, with C = 1 and γ 2 = (0.05) 2 , we can replace 6.94% by 0.49%. With a larger validation set (e.g., ImageNet) and/or more optimistic C and γ 2 , we can obtain much better bounds. Although Proposition 6 poses the concern of increasing the generalization bound when using a single validation dataset with too large |F val |, the rate of increase is only ln |F val | and ln |F val |. We can also avoid dependence on the cardinality of F val using Remark 7. Unlike the standard use of Rademacher complexity with a training dataset, the set F val can depend on the training dataset S m in any manner, and hence F val differs significantly from the typical hypothesis space defined by the parameterization of models. We can thus end up with a very different effective capacity and hypothesis complexity (as selected by model search using the validation set) depending on whether the training data are random or have interesting structure which the neural network can capture.It is very difficult to make a detailed characterization of how well a hypotheses generated by a learning algorithm will generalize, in the absence of detailed information about the given problem instance. Traditional learning theory addresses this very difficult question and has developed bounds that are as tight as possible given the generic information available. In this paper, we have worked toward drawing stronger conclusions by developing theoretical analyses tailored for the situations with more detailed information, including actual neural network structures, and actual performance on a validation set.The bounds in Section 5 have the potential to address Problem 1, but do not solve Problem 2, because of the dependence on a set F val . Theorem 5 partially addresses Problems 1 and 2 but leaves an issue for Problem 2 because of the subtle dependence on datasets different from a given S m . This illustrates the important subtlety of Problem 2. Theorem 4 solves Problem 2 (and hence Problem 1) but with the limited applicability to squared loss. Future work is required to extend the direct analysis to different loss functions. Theorem 4 also suggests the possibility of solving the following open problem:of a hypothesis f with a pair (P, S m ), producing theoretical insights while partially yet provably preserving the partial order of (P, S m , f ) that is defined by the standard less- than-or-equal relation of the valueAny theoretical insights without the partial order preservation can be misleading as it can change the ranking of the preference of (P, S m , f ). Theorem 4 solves Problem 3 by preserving the exact ordering via equality without bounds. However, for different loss functions and different hypothesis spaces, it may also be beneficial to consider a weaker notion of order preservation to gain analyzability and useful insights as in Problem 3.Theorem 4 suggests several directions on deriving new algorithms if we notice the fol- lowing fact. Instead of regularizing a possibly loose upper bound on the generalization gap, regularizing an approximated generalization gap would work well too in practice. Appendix A.3 proposes a family of new regularization methods, which is not based on our new theory, but illustrates this idea of approximation (as opposed to upper bound). In Appendix A.3, the proposed method based on the idea of approximation was empirically shown to improve base models and achieve competitive performance on MNIST and CIFAR-10 benchmarks.Our discussion with Proposition 6 and Remark 7 suggests another open problem: ana- lyzing the role and influence of human intelligence on generalization (see Appendix B for some examples). While this is a hard question, understanding it would be beneficial to further automate the role of human intelligence towards the goal of artificial intelligence. The empirical margin lossˆRlossˆ lossˆR[f ] is defined asˆRasˆ asˆR, where margin,ρ is defined as follows:andTheorem 9. Assume that S m \ S αm is generated by i.i.d. draws according to true distribu- tion P . Assume that S m \ S αm is independent of S αm . Fix ρ &gt; 0 and w σ . Let F be the set of the models with the two-phase training procedure. Suppose thatand max k ˉ w k 2 ≤ C w for all f ∈ F . Then, for any δ &gt; 0, with probability at least 1 − δ, the following holds for all f ∈ F :Proof. Define S m σ asRecall the following fact: using the result by Koltchinskii and Panchenko (2002), we have that for any δ &gt; 0, with probability at least 1 − δ, the following holds for all f ∈ F :where R mσ (F ) is Rademacher complexity defined asHere, ξ i is the Rademacher variable, and the supremum is taken over all k ∈ {1, . . . , d y } and all w allowed in F . Then, for our parameterized hypothesis spaces with any frozen w σ ,Because square root is concave in its domain, by using Jensen's inequality and linearity of expectation,Putting together, we have thatIn general, theoretical bounds can be too loose to be directly used in practice. Accordingly, this section illustrates the use of theoretical insight to guide search in practice based on approximation instead of upper bound.In this section, we focus on multi-class classification problems with d y classes, such as object classification with images. Accordingly, we analyze the expected risk with 0-1 losswhere f (x) = argmax k∈{1,...,dy} (h (H+1) k (x)) is the model prediction, and y(x) ∈ {1, . . . , d y } is the true label of x (see Section 2.4.1 in Mohri et al. 2012 for an extension to stochastic labels).An application of the result by Koltchinskii and Panchenko (2002) yields the following statement: given a fixed ρ &gt; 0, for any δ &gt; 0, with probability at least 1 − δ, for all f ∈ F ,where R m (F ) is a model complexity defined asHere, ξ i are the Rademacher variables (i.e., independent uniform random variables in {−1, +1}), and the supremum is taken over all k ∈ {1, . . . , d y } and all hPrevious theories ( Sun et al., 2016;Neyshabur et al., 2015b;Xie et al., 2015) characterize the generalization gap by the upper bounds on the model complexities via the norms of the weight matrices and exponential dependence on the depth 2 H . A close look at the proofs reveals that the norms of weight matrices come from the Cauchy-Schwarz inequality, which can induce a very loose bound. Moreover, the exponential factor 2 H comes from bounding the effect of nonlinearity at each layer, which can also induce a loose bound, resulting in a gap between theory and practice.This section proposes to solve this issue by directly approximating the model complexity R m (F ), instead of deriving a possibly too-loose bound on it (for worst possible measures). Here, the need for the approximation comes from the fact that R m (F ) contains the ex- pectation with unknown measure on dataset S m . The approximation of R m (F ) essentially reduces to the approximation of the expectation over the dataset S m . In contrast to the worst-case bounds that motivated previous methods, the approximated R m (F ) and whole generalization gap do not necessarily grow along with the norms of the weights, as desired.The theoretical insight in the previous section suggests the following family of methods: given any architecture and method, add a new regularization term for each mini-batch aswhere x i is drawn from some distribution approximating the true distribution of x, ξ 1 , . . . , ξ ˉ m are independently and uniformly drawn from {−1, 1}, ˉ m is a mini-batch size and λ is a hyper-parameter. Importantly, the approximation of the true distribution of x is only used for regularization purposes and hence needs not be precisely accurate (as long as it plays its role for regularization). For example, it can be approximated by populations generated by a generative neural network and/or an extra data augmentation process. For simplicity, we call this family of methods as Directly Approximately Regularizing Complexity (DARC).In this paper, we evaluated only a very simple version of the proposed family of meth- ods as a first step. That is, our experiments employed the following simple and easy-to- implement method, called DARC1:|h where x i is the i-th sample in the training mini-batch. The additional computational cost and programming effort due to this new regularization is almost negligible becauseis already used in computing the original loss. This simplest version was derived by approximating the true distribution of x with the empirical distribution of the training data and the effect of the Rademacher variables via absolute values.We evaluated the proposed method (DARC1) by simply adding the new regularization term in equation (4)      Table 1 shows the error rates comparable with previous results. To the best of our knowledge, the previous state-of-the-art classification error is 0.23% for MNIST with a single model ( Sato et al., 2015) (and 0.21% with an ensemble by Wan et al. 2013). To further investigate the improvement, we ran 10 random trials with computationally less expensive settings, to gather mean and standard deviation (stdv). For MNIST, we used fewer epochs with the same model. For CIFAR-10, we used a smaller model class (pre- activation ResNet with only 18 layers). Table 2 summarizes the improvement ratio: the new model's error divided by the base model's error. We observed the improvements for all cases. The test errors (standard deviations) of the base models were 0.53 (0.029) for MNIST (ND), 0.28 (0.024) for MNIST, and 7.11 (0.17) for CIFAR-10 (all in %). Table 3 summarizes the values of the regularization termfor each obtained model. The models learned with the proposed method were significantly different from the base models in terms of this value. Interestingly, a comparison of the base cases for MNIST (ND) and MNIST shows that data augmentation by itself implicitly regularized what we explicitly regularized in the proposed method.For MNIST:We used the following fixed architecture:Layer 1 Convolutional layer with 32 filters with filter size of 5 by 5, followed by max pooling of size of 2 by 2 and ReLU.Layer 2 Convolution layer with 32 filters with filter size of 5 by 5, followed by max pooling of size of 2 by 2 and ReLU.Layer 3 Fully connected layer with output 1024 units, followed by ReLU and Dropout with its probability being 0.5.Layer 4 Fully connected layer with output 10 units.Layer 4 outputs h (H+1) in our notation. For training purpose, we use softmax of h (H+1) . Also, f (x) = argmax(h (H+1) (x)) is the label prediction.We fixed learning rate to be 0.01, momentum coefficient to be 0.5, and optimization algorithm to be (standard) stochastic gradient decent (SGD). We fixed data augmentation process as: random crop with size 24, random rotation up to ±15 degree, and scaling of 15%. We used 3000 epochs for Table 1, and 1000 epochs for Tables 2 and 3. For CIFAR-10:For data augmentation, we used random horizontal flip with probability 0.5 and random crop of size 32 with padding of size 4.For Table 1, we used ResNeXt-29(16 × 64d) ( Xie et al., 2016). We set initial learning rate to be 0.05 and decreased to 0.005 at 150 epochs, and to 0.0005 at 250 epochs. We fixed momentum coefficient to be 0.9, weight decay coefficient to be 5 × 10 −4 , and optimization algorithm to be stochastic gradient decent (SGD) with Nesterov momentum. We stopped training at 300 epochs.For Tables 2 and 3, we used pre-activation ResNet with only 18 layers (pre-activation ResNet-18) ( He et al., 2016). We fixed learning rate to be 0.001 and momentum coefficient to be 0.9, and optimization algorithm to be (standard) stochastic gradient decent (SGD). We used 1000 epochs.Theorem 4 solves Problem 2 with the limited applicability to certain neural networks with squared loss. In contrast, a parallel study (Kawaguchi and Bengio, 2018) presents a novel generic learning theory to solve Problem 2 for general cases in machine learning. It would be beneficial to explore both a generic analysis (Kawaguchi and Bengio, 2018) and a direct analysis in deep learning (this paper) to get tighter results and insights that are tailored for each particular case in deep learning.Our discussion with Proposition 6 and Remark 7 suggests another open problem: ana- lyzing the role and influence of human intelligence in generalization. For example, human intelligence seems to be able to often find good architectures (and other hyper-parameters) that get low validation errors (without non-exponentially large |F val | in Proposition 6, or a low complexity of L F val in Remark 7). A close look at the deep learning literature seems to suggest that this question is fundamentally related to the process of science and engi- neering, because many successful architectures have been designed based on the physical properties and engineering priors of the problems at hand (e.g., hierarchical nature, convo- lution, architecture for motion such as that by Finn et al. 2016, memory networks, and so on). While this is a hard question, it might be beneficial to advance the understanding of human intelligence from this aspect too in order to consider partially automating it.In previous bounds with a hypothesis space F , if we try different hypothesis spaces F depending on S m , the basic proof breaks down. An easy recovery at the cost of an extra quantity in a bound is to take a union bound over all possible F j for j = 1, 2, . . . where we pre-decide {F j } j without dependence on S m (or simply consider the "largest" F ⊇ F j , which can result in a very loose bound). Similarly, if we need to try many w σ := w Sαm depending on the whole S m in Theorem 5, we can take a union bound over w σ } j without dependence on S m \ S αm but with dependence on S αm . We can do the same with Proposition 6 and Remark 7 to use manym val with a predefined sequence.The situation where theoretical studies focus on a set of problems and practical applications care about each element in the set is prevalent in machine learning and computer science literature, not limited to the field of learning theory. For example, for each practical prob- lem instance q ∈ Q, the size of the set Q that had been analyzed in theory for optimal exploration in Markov decision processes (MDPs) were demonstrated to be frequently too pessimistic, and a methodology to partially mitigate the issue was proposed (Kawaguchi, 2016b). Bayesian optimization would suffer from a pessimistic set Q regarding each problem instance q ∈ Q, the issue of which was partially mitigated ( Kawaguchi et al., 2015).Moreover, characterizing a set of problems Q only via a worst-case instance q ∈ Q (i.e., worst-case analysis) is known to have several issues in theoretical computer science, and so-called beyond worst-case analysis (e.g., smoothed analysis) is an active area of research to mitigate the issues.Statistical learning theory can be considered to provide two types of statements relevant to the scope of this paper. The first type (which comes from upper bounds) is logically in the form of "p implies q," where p := "the hypothesis-space complexity is small" (or another statement about stability, robustness, or flat minima), and q := "the generalization gap is small." Notice that "p implies q" does not imply "q implies p." Thus, based on statements of this type, it is entirely possible that the generalization gap is small even when the hypothesis-space complexity is large or the learning mechanism is unstable, non-robust, or subject to sharp minima.The second type (which comes from lower bounds) is logically in the following form: in a set U all of all possible problem configurations, there exists a subset U ⊆ U all such that "q implies p" in U (with the same definitions of p and q as in the previous paragraph). For example, Mohri et al. (2012, Section 3.4) derived lower bounds on the generalization gap by showing the existence of a "bad" distribution that characterizes U . Similarly, the classical no free lunch theorems are the results with the existence of a worst-case distribution for each algorithm. However, if the problem instance at hand (e.g., object classification with MNIST or CIFAR-10) is not in such a U in the proofs (e.g., if the data distribution is not among the "bad" ones considered in the proofs), q does not necessarily imply p. Thus, it is still naturally possible that the generalization gap is small with large hypothesis- space complexity, instability, non-robustness, and sharp minima. Therefore, there is no contradiction or paradox.From the discussions above, we can see that there is a logically expected difference between the scope in theory and the focus in practice; it is logically expected that there are problem instances where theoretical bounds are pessimistic. In order for generalization theory to have maximal impact in practice, we must be clear on a set of different roles it can play regarding practice, and then work to extend and strengthen it in each of these roles. We have identified the following practical roles for theory:Role 1 Provide guarantees on expected risk.Role 2.1 to be small for a given fixed S m , and/or Role 2.2 to approach zero with a fixed model class as m increases.Role 3 Provide theoretical insights to guide the search over model classes.Proof. For any matrix M , let Col(M ) and Null(M ) be the column space and null space of M . Since rank(Φ) ≥ m and Φ ∈ R m×n , the set of its columns span R m , which proves statement (i). Let w * = w * 1 + w * 2 where Col(w * 1 ) ⊆ Col(M T ) and Col(w * 2 ) ⊆ Null(M ). For statement (ii), set the parameter as w :By setting A = ΦC 1 and B = Φ test C 1 with a proper normalization of C 1 yields (a) and (b) in statement (ii) (note that C 1 has an arbitrary freedom in the bound on its scale because its only condition is Col(C 1 ) ⊆ Col(M T )). At the same time with the same parameter, since Col(F , which grows unboundedly as α → ∞ without changing A and B, proving (c) in statement (ii).Thus, from Equation (1) with the squared loss, we can decompose the generalization gap into three terms asAs G is a real symmetric matrix, we denote an eigendecomposition of G as G = U ΛU where the diagonal matrix Λ contains eigenvalues as Λ jj = λ j with the corresponding orthogonal eigenvector matrix U ; u j is the j-th column of U . Then,By using these,Note that σ(x, w) is 0 or 1 for max-pooling and/or ReLU nonlinearity. Thus, the derivative of z = [ˉ x • ˉ σ(x, w)] with respect to w is zero everywhere (except at the measure zero set where the derivative does not exists). Thus, by the chain rule (and power rule), the gradient of the loss with respect to w only contain the contribution from the derivative of h (H+1) k with respect to ˉ w, but not with respect to w in z.Directly implementing Equation (3) requires the summation over all paths, which can be computationally expensive. To avoid it, we implemented it by creating two deep neural networks, one of which defines ˉ w paths hierarchically, and another of which defines w σ paths hierarchically, resulting in the computational cost at most (approximately) twice as much as the original cost of training standard deep learning models. We tied w σ and ˉ w in the two networks during standard phase, and untied them during freeze phase.Our source code is available at:http://lis.csail.mit.edu/code/gdl.html The computation of the standard network without skip connection can be re-written as:where W σ and only trained W (l) during freeze phase. By following the same derivation of Equa- tion (1), we can see that this defines the desired computation without explicitly computing the summation over all paths. By the same token, this applies to DAGs.For all MNIST(ND), MNIST and CIFAR-10, we used the same settings as those for Tables  2 and 3 in Section A.3.3. That is, for all experiments, we still used the same fixed value of (λ/ ˉ m) = 0.001 with ˉ m = 64. Other experimental detail is also identical to what is described in Appendix A.3.4, except that we used 1000 epochs for each standard and freeze phase.Proof. We do not require the independence over the coordinates of˜zof˜of˜z i and the entries of random matrices˜zmatrices˜ matrices˜z i ˜ z i because of the definition of independence required for matrix Bern- stein inequality (for i y i ≤ β 3 . Since Equation (5) always hold deterministically (with or without such a dataset), the desired statement of this theorem follows.Proof. Consider a single fixed f ∈ F val . Since F val is independent from the validation dataset, κ f,1 , . . . , κ f,m val are independent zero-mean random variables, given a fixed f ∈ F val . Thus, we can apply Bernstein inequality, yielding By noticing that the solution of with the minus sign results in &lt; 0, which is invalid for Bernstein inequality, we obtain the valid solution with the plus sign. Then, we havewhere we used that √ a + b ≤ √ a + √ b. By tanking the negation of the statement, we obtain that for any δ &gt; 0, with probability at least 1 − δ, for all f ∈ F val ,
