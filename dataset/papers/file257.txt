Our model processes the question-image input using FiLM, illustrated in Figure 2. We start by explaining FiLM and then describe our particular model for visual reasoning. where γ i,c and β i,c modulate a neural network's activations F i,c , whose subscripts refer to the i th input's c th feature or feature map, via a feature-wise affine transformation:Our FiLM model consists of a FiLM-generating linguis- tic pipeline and a FiLM-ed visual pipeline as depicted in Figure 3. The FiLM generator processes a question x i us- ing a Gated Recurrent Unit (GRU) network ( Chung et al. 2014) with 4096 hidden units that takes in learned, 200- dimensional word embeddings. The final GRU hidden state is a question embedding, from which the model predictsf and h can be arbitrary functions such as neural networks. Modulation of a target neural network's processing can be based on the same input to that neural network or some other input, as in the case of multi-modal or conditional tasks. For CNNs, f and h thus modulate the per-feature-map distribu- tion of activations based on x i , agnostic to spatial location. In practice, it is easier to refer to f and h as a single func- tion that outputs one (γ, β) vector, since, for example, it is often beneficial to share parameters across f and h for more efficient learning. We refer to this single function as the FiLM generator. We also refer to the network to which FiLM layers are applied as the Feature-wise Linearly Mod- ulated network, the FiLM-ed network.FiLM layers empower the FiLM generator to manipulate feature maps of a target, FiLM-ed network by scaling them up or down, negating them, shutting them off, selectively thresholding them (when followed by a ReLU), and more. Each feature map is conditioned independently, giving the FiLM generator moderately fine-grained control over acti- vations at each FiLM layer.As FiLM only requires two parameters per modulated fea- ture map, it is a scalable and computationally efficient con- ditioning method. In particular, FiLM has a computational cost that does not scale with the image resolution.i,· ) for each n th residual block via affine projection. The visual pipeline extracts 128 14 × 14 image feature maps from a resized, 224 × 224 image input using either a CNN trained from scratch or a fixed, pre-trained feature extractor with a learned layer of 3 × 3 convolutions. The CNN trained from scratch consists of 4 layers with 128 4 × 4 kernels each, ReLU activations, and batch normalization, similar to prior work on CLEVR ( Santoro et al. 2017). The fixed feature extractor outputs the conv4 layer of a ResNet- 101 ( He et al. 2016) pre-trained on ImageNet ( Russakovsky et al. 2015) to match prior work on CLEVR ( Johnson et al. 2017a;2017b). Image features are processed by several - 4 for our model -FiLM-ed residual blocks (ResBlocks) with 128 feature maps and a final classifier. The classifier consists of a 1 × 1 convolution to 512 feature maps, global max-pooling, and a two-layer MLP with 1024 hidden units that outputs a softmax distribution over final answers.Each FiLM-ed ResBlock starts with a 1 × 1 convolu- tion followed by one 3 × 3 convolution with an architec- ture as depicted in Figure 3. We turn the parameters of batch normalization layers that immediately precede FiLM layers off. Drawing from prior work on CLEVR ( Santoro et al. 2017) and visual reasoning ( Watters et al. 2017), we concatenate two coordinate feature maps indi- cating relative x and y spatial position (scaled from −1 to 1) with the image features, each ResBlock's input, and the classifier's input to facilitate spatial reasoning.We train our model end-to-end from scratch with , batch size 64, and batch normalization and ReLU throughout FiLM-ed network. Our model uses only image-question-answer triplets from the training set with- out data augmentation. We employ early stopping based on validation accuracy, training for 80 epochs maximum. Fur- ther model details are in the appendix. Empirically, we found FiLM had a large capacity, so many architectural and hyper- parameter choices were for added regularization.We stress that our model relies solely on feature-wise affine conditioning to use question information influence the visual pipeline behavior to answer questions. This approach differs from classical visual question answering pipelines which fuse image and language information into a single embedding via element-wise product, concatenation, atten- tion, and/or more advanced methods ( Yang et al. 2016;Lu et al. 2016;Anderson et al. 2017).  (Kim, Song, and Bengio 2017), and Conditional Batch Norm for general visual question answering on complex scenes such as VQA and GuessWhat?! (de ). This work complements our own, as we seek to show that feature-wise affine conditioning is effective for multi-step reasoning and understand the underlying mechanism behind its success.Notably, prior work in CN has not examined whether the affine transformation must be placed directly after nor- malization. Rather, prior work includes normalization in the method name for instructive purposes or due to implemen- tation details. We investigate the connection between FiLM and normalization, finding it not strictly necessary for the affine transformation to occur directly after normalization. Thus, we provide a unified framework for all of these meth- ods through FiLM, as well as a normalization-free relaxation of this approach which can be more broadly applied.Beyond CN, there are many connections between FiLM and other conditioning methods. A common approach, used for example in Conditional DCGANs (Radford, Metz, and Chintala 2016), is to concatenate constant feature maps of conditioning information with convolutional layer input.Though not as parameter efficient, this method simply re- sults in a feature-wise conditional bias. Likewise, concate- nating conditioning information with fully-connected layer input amounts to a feature-wise conditional bias. Other ap- proaches such as WaveNet (van den Oord et al. 2016a) and Conditional PixelCNN (van den Oord et al. 2016b) directly add a conditional feature-wise bias. These approaches are equivalent to FiLM with γ = 1, which we compare FiLM to in the Experiments section. In reinforcement learning, an alternate formulation of FiLM has been used to train one game-conditioned deep Q-network to play ten Atari games ( Kirkpatrick et al. 2017), though FiLM was neither the focus of this work nor analyzed as a major component.Other methods gate an input's features as a function of that same input, rather than a separate conditioning in- put. These methods include LSTMs for sequence model- ing (Hochreiter and Schmidhuber 1997), Convolutional Se- quence to Sequence for machine translation (Gehring et al. 2017), and even the ImageNet 2017 winning model, Squeeze and Excitation Networks (Hu, Shen, and Sun 2017). This approach amounts to a feature-wise, conditional scaling, re- stricted to between 0 and 1, while FiLM consists of both scaling and shifting, each unrestricted. In the Experiments section, we show the effect of restricting FiLM's scaling to between 0 and 1 for visual reasoning. We find it noteworthy that this general approach of feature modulation is effective across a variety of settings and architectures.There are even broader links between FiLM and other methods. For example, FiLM can be viewed as using one network to generate parameters of another network, mak- ing it a form of hypernetwork (Ha, Dai, and Le 2016). Also, FiLM has potential ties with conditional computation and mixture of experts methods, where specialized network sub- parts are active on a per-example basis (Jordan and Jacobs 1994;Eigen, Ranzato, and Sutskever 2014;Shazeer et al. 2017); we later provide evidence that FiLM learns to selec- tively highlight or suppress feature maps based on condi- tioning information. Those methods select at a sub-network level while FiLM selects at a feature map level.In the domain of visual reasoning, one leading method is the Program Generator + Execution Engine model (Johnson et al. 2017b). This approach consists of a sequence- to-sequence Program Generator, which takes in a question and outputs a sequence corresponding to a tree of compos-   RNs then element-wise sum over the resulting comparison vectors to form another vector from which a final classi- fier predicts the answer. We note that RNs have a compu- tational cost that scales quadratically in spatial resolution, while FiLM's cost is independent of spatial resolution. No- tably, since RNs concatenate question features with MLP in- put, a form of feature-wise conditional biasing as explained earlier, their conditioning approach is related to FiLM.CLEVR is a synthetic dataset of 700K (image, question, an- swer, program) tuples (Johnson et al. 2017a). Images con- tain 3D-rendered objects of various shapes, materials, col- ors, and sizes. Questions are multi-step and compositional in nature, as shown in Figure 1. They range from counting questions ("How many green objects have the same size as the green metallic block?") to comparison questions ("Are there fewer tiny yellow cylinders than yellow metal cubes?") and can be 40+ words long. Answers are each one word from a set of 28 possible answers. Programs are an additional supervisory signal consisting of step-by-step instructions, such as filter shape [cube], relate [right], and count, on how to answer the question.Baselines We compare against the following methods, dis- cussed in detail in the Related Work section:• Q-type baseline: Predicts based on a question's category.• LSTM: Predicts using only the question.First, we test our model on visual reasoning with the CLEVR task and use trained FiLM models to analyze what FiLM learns. Second, we explore how well our model generalizes to more challenging questions with the CLEVR-Humans task. Finally, we examine how FiLM performs in few- shot and zero-shot generalization settings using the CLEVR Compositional Generalization Test. In the appendix, we pro- vide an error analysis of our model. Our code is available at https://github.com/ethanjperez/film. Results FiLM achieves a new overall state-of-the-art on CLEVR, as shown in Table 1, outperforming humans and previous methods, including those using explicit models of reasoning, program supervision, and/or data augmentation.  For methods not using extra supervision, FiLM roughly halves state-of-the-art error (from 4.5% to 2.3%). Note that using pre-trained image features as input can be viewed as a form of data augmentation in itself but that FiLM performs equally well using raw pixel inputs. Interestingly, the raw pixel model seems to perform better on lower-level ques- tions (i.e. querying and comparing attributes) while the im- age features model seems to perform better on higher-level questions (i.e. compare numbers of objects). tures on objects that are not referred to by the answer but are referred to by the question. The latter example provides ev- idence that the final MLP itself carries out some reasoning, using FiLM to extract relevant features for its reasoning.Activation Visualizations Figure 4 visualizes the distri- bution of locations responsible for the globally-pooled fea- tures which the MLP in the model's final classifier uses to predict answers. These images reveal that the FiLM model predicts using features of areas near answer-related or question-related objects, as the high CLEVR accuracy also suggests. This finding highlights that appropriate fea- ture modulation indirectly results in spatial modulation, as regions with question-relevant features will have large acti- vations while other regions will not. This observation might explain why FiLM outperforms Stacked Attention, the next best method not explicitly built for reasoning, so signifi- cantly (21%); FiLM appears to carry many of spatial atten- tion's benefits, while also influencing feature representation. Figure 4 also suggests that the FiLM-ed network carries out reasoning throughout its pipeline. In the top example, the FiLM-ed network has localized the answer-referenced ob- ject alone before the MLP classifier. In the bottom example, the FiLM-ed network retains, for the MLP classifier, fea- FiLM Parameter Histograms To analyze at a lower level how FiLM uses the question to condition the visual pipeline, we plot γ and β values predicted over the validation set, as shown in Figure 5 and in more detail in the appendix (Fig- ures 16 to 18). γ and β values take advantage of a sizable range, varying from -15 to 19 and from -9 to 16, respec- tively. γ values show a sharp peak at 0, showing that FiLM learns to use the question to shut off or significantly sup- press whole feature maps. Simultaneously, FiLM learns to upregulate a much more selective set of other feature maps with high magnitude γ values. Furthermore, a large frac- tion (36%) of γ values are negative; since our model uses a ReLU after FiLM, γ &lt; 0 can cause a significantly differ- ent set of activations to pass the ReLU to downstream layers than γ &gt; 0. Also, 76% of β values are negative, suggest- ing that FiLM also uses β to be selective about which acti- vations pass the ReLU. We show later that FiLM's success is largely architecture-agnostic, but examining a particular model gives insight into the influence FiLM learns to exert in a specific case. Together, these findings suggest that FiLM learns to selectively upregulate, downregulate, and shut off feature maps based on conditioning information.FiLM Parameters t-SNE Plot In Figure 6, we visualize FiLM parameter vectors (γ, β) for 3,000 random valida- tion points with t-SNE. We analyze the deeper, 6-ResBlock version of our model, which has a similar validation accu- racy as our 4-ResBlock model, to better examine how FiLM layers in different layers of a hierarchy behave. First and last layer FiLM (γ, β) are grouped by the low-level and high-level reasoning functions necessary to answer CLEVR questions, respectively. For example, FiLM parameters for equal color and query color are close for the first layer but apart for the last layer. The same is true for shape, size and material questions. Conversely, equal shape, equal size, and equal material FiLM parameters are grouped in the last layer but split in the first layer -like- wise for other high level groupings such as integer compar- ison and querying. These findings suggest that FiLM layers learn a sort of function-based modularity without an archi- tectural prior. Simply with end-to-end training, FiLM learns to handle not only different types of questions differently, but also different types of question sub-parts differently; the FiLM model works from low-level to high-level processes as is the proper approach. For models with fewer FiLM lay- ers, such patterns also appear, but less clearly; these models must begin higher level reasoning sooner. Using the validation set, we conduct an ablation study on our best model to understand how FiLM learns visual reasoning. We show results for test time ablations in Figure 7, for archi- tectural ablations in Table 2, and for varied model depths in Table 3. Without hyperparameter tuning, most architectural ablations and model depths outperform prior state-of-the-art on training from only image-question-answer triplets, sup- porting FiLM's overall robustness. Table 3 also shows using the validation set that our results are statistically significant.Effect of γ and β To test the effect of γ and β separately, we trained one model with a constant γ = 1 and another with β = 0. With these models, we find a 1.5% and .5% accuracy drop, respectively; FiLM can learn to condition the CNN for visual reasoning through either biasing or scaling alone, albeit not as well as conditioning both together. This result also suggests that γ is more important than β.To further compare the importance of γ and β, we run a series of test time ablations (Figure 7) on our best, fully- trained model. First, we replace β with the mean β across the training set. This ablation in effect removes all condition- ing information from β parameters during test time, from a model trained to use both γ and β. Here, we find that ac- curacy only drops by 1.0%, while the same procedure on γ results in a 65.4% drop. This large difference suggests that, in practice, FiLM largely conditions through γ rather than β. Next, we analyze performance as we add increasingly more Gaussian noise to the best model's FiLM parameters at test time. Noise in gamma hurts performance significantly more, showing FiLM's higher sensitivity to changes in γ than in β and corroborating the relatively greater importance of γ.Restricting γ To understand what aspect of γ is most ef- fective, we train a model that limits γ to (0, 1) using sig-Overall Model Overall    Table 2: CLEVR val accuracy for ablations, trained with the best architecture with only specified changes. We report the standard deviation of the best model accuracy over 5 runs.moid, as many models which use feature-wise, multiplica- tive gating do. Likewise, we also limit γ to (−1, 1) using tanh. Both restrictions hurt performance, roughly as much as removing conditioning from γ entirely by training with γ = 1. Thus, FiLM's ability to scale features by large mag- nitudes appears to contribute to its success. Limiting γ to (0, ∞) with exp also hurts performance, validating the value of FiLM's capacity to negate and zero out feature maps. Conditional Normalization We perform an ablation study on the placement of FiLM to evaluate the relation- ship between normalization and FiLM that Conditional Nor- malization approaches assume. Unfortunately, it is difficult to accurately decouple the effect of FiLM from normaliza- tion by simply training our corresponding model without normalization, as normalization significantly accelerates, regularizes, and improves neural network learning (Ioffe and Szegedy 2015), but we include these results for com- pleteness. However, we find no substantial performance drop when moving FiLM layers to different parts of our model's ResBlocks; we even reach the upper end of the best model's performance range when placing FiLM after the post-normalization ReLU in the ResBlocks. Thus, we decouple the name from normalization for clarity regarding where the fundamental effectiveness of the method comes from. By demonstrating this conditioning mechanism is not closely connected to normalization, we open the doors to ap- plications other settings in which normalization is less com- mon, such as RNNs and reinforcement learning, which areResidual Connection Removing the residual connection causes one of the larger accuracy drops. Since there is a global max-pooling operation near the end of the network, this finding suggests that the best model learns to primar- ily use features of locations that are repeatedly important throughout lower and higher levels of reasoning to make its final decision. The higher accuracies for models with FiLM modulating features inside residual connections rather than outside residual connections supports this hypothesis. Table 3 shows model performance by the number of ResBlocks. FiLM is robust to varying depth but less so with only 1 ResBlock, backing the earlier theory that the FiLM-ed network reasons throughout its pipeline.To assess how well visual reasoning models generalize to more realistic, complex, and free-form questions, the CLEVR-Humans dataset was introduced ( Johnson et al. 2017b). This dataset contains human-posed questions on CLEVR images along with their corresponding answers.The number of samples is limited -18K for training, 7K Figure 8: Examples from CLEVR-Humans, which introduces new words (underlined) and concepts. After fine-tuning on CLEVR-Humans, a CLEVR-trained model can now reason about obstruction, superlatives, and reflections but still struggles with hypothetical scenarios (rightmost). It also has learned human preference to primarily identify objects by shape (leftmost).for validation, and 7K for testing. The questions were col- lected from Amazon Mechanical Turk workers prompted to ask questions that were likely hard for a smart robot to an- swer. As a result, CLEVR-Humans questions use more di- verse vocabulary and complex concepts.  Results Our model achieves state-of-the-art generalization to CLEVR-Humans, both before and after fine-tuning, as shown in Table 4, indicating that FiLM is well-suited to han- dle more complex and diverse questions. Figure 8 shows ex- amples from CLEVR-Humans with FiLM model answers. Before fine-tuning, FiLM outperforms prior methods by a smaller margin. After fine-tuning, FiLM reaches a consider- ably improved final accuracy. In particular, the gain in ac- curacy made by FiLM upon fine-tuning is more than 50% greater than those made by other models; FiLM adapts data- efficiently using the small CLEVR-Humans dataset. Notably, FiLM surpasses the prior state-of-the-art method, Program Generator + Execution Engine (PG+EE), after fine-tuning by 9.3%. Prior work on PG+EEs explains that this neural module network method struggles on ques- tions which cannot be well approximated with the model's module inventory ( Johnson et al. 2017b). In contrast, FiLM has the freedom to modulate existing feature maps, a fairly flexible and fine-grained operation, in novel ways to reason about new concepts. These results thus provide some evi- dence for the benefits of FiLM's general nature. red, green, purple, or cyan; in Condition B, cubes and cylin- ders swap color palettes. Both conditions contain spheres of all colors. CLEVR-CoGenT thus indicates how a model an- swers CLEVR questions: by memorizing combinations of traits or by learning disentangled or general representations.Results We train our best model architecture on Condition A and report accuracies on Conditions A and B, before and after fine-tuning on B, in Figure 9. Our results indicate FiLM surpasses other visual reasoning models at learning general concepts. FiLM learns better compositional generalization even than PG+EE, which explicitly models compositional- ity and is trained with program-level supervision that specif- ically includes filtering colors and filtering shapes.Sample Efficiency and Catastrophic Forgetting We show sample efficiency and forgetting curves in Figure 9. FiLM achieves prior state-of-the-art accuracy with 1/3 as much fine-tuning data. However, our FiLM model still suf- fers from catastrophic forgetting after fine-tuning.To test how well models learn compositional concepts that generalize, CLEVR-CoGenT was introduced (Johnson et al. 2017a). This dataset is synthesized in the same way as CLEVR but contains two conditions: in Condition A, all cubes are gray, blue, brown, or yellow and all cylinders are Zero-Shot Generalization FiLM's accuracy on Condi- tion A is much higher than on B, suggesting FiLM has mem- orized attribute combinations to an extent. For example, the model learns a bias that cubes are not cyan, as learning this training set bias helps minimize training loss.To overcome this bias, we develop a novel FiLM-based zero-shot generalization method. Inspired by word embed- ding manipulations, e.g. "King" -"Man" + "Woman" = "Queen" ( Mikolov et al. 2013), we test if linear manipula- Figure 9: CoGenT results. FiLM ValB accuracy reported on ValB without the 30K fine-tuning samples (Figure). Accu- racy before and after fine-tuning on 30K of ValB (Table).  (2) - (3). This method corrects our model's answer from "rubber" to "metal".tion extends to reasoning with FiLM. We compute (γ, β) for "How many cyan cubes are there?" via the linear com- bination of questions in the FiLM parameter space: "How many cyan spheres are there?" + "How many brown cubes are there?" − "How many brown spheres are there?". With this (γ, β), our model can correctly count cyan cubes. We show another example of this method in Figure 10. We evaluate this method on validation B, using a parser to automatically generate the right combination of questions. We test previously reported CLEVR-CoGenT FiLM mod- els with this method and show results in Figure 9. With this method, there is a 3.2% overall accuracy gain when train- ing on A and testing for zero-shot generalization on B. Yet this method could only be applied to 1/3 of questions in B. For these questions, model accuracy starts at 71.5% and jumps to 80.7%. Before fine-tuning on B, the accuracy be- tween zero-shot and original approaches on A is identical, likewise for B after fine-tuning. We note that difference in the predicted FiLM parameters between these two methods is negligible, likely causing the similar performance.We achieve these improvements without specifically training our model for zero-shot generalization. Our method simply allows FiLM to take advantage of any concept dis- entanglement in the CNN after training. We also observe that convex combinations of the FiLM parameters -i.e. be- tween "How many cyan things are there?" and "How many brown things are there?" -often monotonically interpolates the predicted answer between the answers to endpoint ques- tions. These results highlight, to a limited extent, the flexi- bility of FiLM parameters for meaningful manipulations.As implemented, this method has many limitations. How- ever, approaches from word embeddings, representation learning, and zero-shot learning can be applied to directly optimize (γ, β) for analogy-making ( Bordes et al. 2013;Guu, Miller, and Liang 2015;Oh et al. 2017). The FiLM-ed network could directly train with this procedure via back- propagation. A learned model could also replace the parser. We find such avenues promising for future work.We show that a model can achieve strong visual reasoning using general-purpose Feature-wise Linear Modulation lay- ers. By efficiently manipulating a neural network's interme- diate features in a selective and meaningful manner using FiLM layers, a RNN can effectively use language to mod- ulate a CNN to carry out diverse and multi-step reasoning tasks over an image. Our ablation study suggests that FiLM is resilient to architectural modifications, test time ablations, and even restrictions on FiLM layers themselves. Notably, we provide evidence that FiLM's success is not closely con- nected with normalization as previously assumed. Thus, we open the door for applications of this approach to settings where normalization is less common, such as RNNs and re- inforcement learning. Our findings also suggest that FiLM models can generalize better, more sample efficiently, and even zero-shot to foreign or more challenging data. Overall, the results of our investigation of FiLM in the case of visual reasoning complement broader literature that demonstrates the success of FiLM-like techniques across many domains, supporting the case for FiLM's strength not simply within a single domain but as a general, versatile approach.Q: Is there a big brown ob- ject of the same shape as the green thing? A: Yes (P: No)We examine the errors our model makes to understand where our model fails and how it acts when it does. Examples of these errors are shown in Figures 12 and 13.Q: What number of other things are the same material as the big gray cylinder? A: 6 (P: 5) Occlusion Many model errors are due to partial occlusion.These errors may likely be fixed using a CNN that operates at a higher resolution, which is feasible since FiLM has a computa- tional cost that is independent of resolution. Logical Consistency The model sometimes makes curious reasoning mistakes a human would not. For example, we find a case where our model correctly counts one gray object and two cyan objects but simultaneously answers that there are the same number of gray and cyan objects. In fact, it answers that the num- ber of gray objects is both less than and equal to the number of yellow blocks. These errors could be prevented by directly mini- mizing logical inconsistency, an interesting avenue for future work orthogonal to FiLM.Q: What shape is the big metal thing that is the same color as the small cylinder? A: Cylinder (P: Sphere) Q: How many other things are the same material as the tiny sphere? A: 3 (P: 2)Figure 12: Some image-question pairs where our model pre- dicts incorrectly. Most errors we observe are due to partially occluded objects, as highlighted in the three first examples.Rather than output γi,c directly, we output ∆γi,c, where:since initially zero-centered γi,c can zero out CNN feature map activations and thus gradients. In our implementation, we opt to output ∆γi,c rather than γi,c, but for simplicity, throughout our pa- per, we explain FiLM using γi,c. However, this modification does not seem to affect our model's performance on CLEVR statistically significantly.We present training and validation curves for best model trained from image features in Figure 11. We observe fast accuracy gains initially, followed by slow, steady increases to a best validation ac- curacy of 97.84%, at which point training accuracy is 99.53%. We train on CLEVR for 80 epochs, which takes 4 days using 1 NVIDIA TITAN Xp GPU when learning from image features. For practical reasons, we stop training on CLEVR after 80 epochs, but we observe that accuracy continues to increase slowly even after- wards.Answer How many gray things are there? 1 How many cyan things are there? 2 Are there as many gray things as cyan things?Yes Are there more gray things than cyan things? No Are there fewer gray things than cyan things? Yes Figure 13: An interesting failure example where our model counts correctly but compares counts erroneously. Its third answer is incorrect and inconsistent with its other answers.We     
