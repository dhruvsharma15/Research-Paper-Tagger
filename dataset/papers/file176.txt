Statistical learning theory provides tight and insightful re- sults under its assumptions and for its objectives (e.g., Vapnik 1998;Mukherjee et al. 2006;Mohri et al. 2012). As the training datasets are considered as random variables, statis- tical learning theory was initially more concerned with the study of data-independent bounds based on the capacity of the hypothesis space (Vapnik, 1998), or the classical sta- bility of learning algorithm (Bousquet &amp; Elisseeff, 2002). Given the observations that these data-independent bounds could be overly pessimistic for a "good" (training) dataset, data-dependent bounds have also been developed in sta- tistical learning theory, such as the luckiness framework (Shawe-Taylor et al., 1998;Herbrich &amp; Williamson, 2002), empirical Rademacher complexity of a hypothesis space (Koltchinskii &amp; Panchenko, 2000;Bartlett et al., 2002), and the robustness of learning algorithm (Xu &amp; Mannor, 2012).The non-statistical nature of our proposed theory can be of practical interest on its own merits. For example, the non- statistical nature captures well a situation wherein a dataset to learn a model is specified and fixed first (e.g., a UCL dataset, ImageNet, a medical image dataset, etc.), rather than remaining random with a certain distribution. Once a dataset is actually specified, there is no randomness re- maining over the dataset (although one can artificially cre- ate randomness via an empirical distribution). For exam- ple, Zhang et al. (2017) empirically observed that given a fixed (deterministic) dataset (i.e., each of CIFAR10, Ima- geNet, and MNIST), test errors can be small despite the large capacity of hypothesis space and a possible instability of the learning algorithm. Understanding and explaining this empirical observation has become an active research area (Arpit et al., 2017;Krueger et al., 2017;Hoffer et al., 2017;Wu et al., 2017;Dziugaite &amp; Roy, 2017;Dinh et al., 2017;Bartlett et al., 2017;Brutzkus et al., 2017).For convenience within this paper, the proposed theory is called analytical learning theory, due to its non-statistical nature. A firm understanding of analytical learning theory might be conceptually challenging, as it requires a different style of thinking and a shift of technical basis from statis- tics (e.g., concentration inequalities) to measure theory. We present the foundation of analytical learning theory in Sec- tion 3, several applications in Sections 4-5, and additional discussions in Section 6.In machine learning, a typical goal is to return a modeîmodeî y A(Sm) via a learning algorithm A given a datasetS m = {s (1) , . . . , s (m) } such that the expected error E μ [LˆyLˆy A(Sm) ] E z [LˆyLˆy A(Sm) (z)] with respect to a true (unknown) normalized measure μ is minimized. Here, LˆyLˆy is a function that combines a loss function and a modeîmodeî y; e.g., in supervised learning, LˆyLˆy(z) = y(x), y), where z = (x, y) is a pair of an input x and a target y. Because the expected error E μ [LˆyLˆy A(S m ) ] is often not computable, we usually approximate the expected error by an empiri- cal errorˆEerrorˆ errorˆE Z m [LˆyLˆy A(S m ) ] ], where the case of Z m = S m is of particular interest (e.g., in the empirical risk minimization). One of the goals of learning theory is to explain and validate when and how minimizingˆEminimizingˆ minimizingˆE S m [LˆyLˆy A(S m ) ] is a sensible approach to mini- mizing E μ [LˆyLˆy A(S m ) ] by analyzing the generalization gap. In the following, we define a quality of a dataset, called discrepancy, and a quality of a function, called variation in the sense of Hardy and Krause. These definitions have been used in harmonic analysis, number theory, and numer- ical analysis (Krause, 1903;Hardy, 1906;Hlawka, 1961;Niederreiter, 1978;Aistleitner et al., 2017). This study adopts these definitions in the context of machine learn- ing. Intuitively, the star-discrepancy. . , t j k ) with other original variables be- ing fixed to be one. The variation of f j 1 ...j k on [0,1] k in the sense of Vitali is defined as, where P k is the set of all partitions of [0,1] k . The variation of f on [0,1] d in the sense of Hardy and Krause is defined asDiscrepancy of dataset with respect to a measure. For} with respect to a normalized Borel measure ν on a set B t is defined asThe following proposition might be helpful in intuitively understanding the concept of the variation as well as in computing it when applicable. All the proofs in this paper are presented in Appendix B. Proposition 1. Suppose that f j1...j k is a function for which } with respect to a normalized Borel measure ν is defined asVariations of a function. Let ∂ l be the partial derivative operator; that is, ∂ l g(t 1 , . . . , t k ) is the partial derivative of a function g with respect to the l-th coordinate at a pointThis study considers the problem of analyzing the gen- Sm) ], includ- ing the case of Z m = S m . Whenever we write Z m , it is always including the case of Z m = S m . With our notation, one can observe that the generalization gap is fully and deterministically specified by a tuple or a prob- lem instance (μ, S m , Z m , LˆyLˆy A(Sm) ), where we identify P 1 , . . . , m P k is a set of finite sequences dμ, which is a deterministic mathematical ob- ject. Accordingly, we introduce the following notion of strong instance-dependence:contain the complexity of a non-singleton hypothesis space (and its ordering). Moreover, the definition of robustness itself depends on ˉ S m , which is not equal to S m or Z m (Xu &amp; Mannor, 2012). Therefore, all of these data-dependent bounds are not strongly instance-dependent.-A mathematical object ϕ in the theory of the generaliza- tion gap of the tuple (μ, S m , Z m , LˆyLˆy A(S m ) ) is said to be strongly instance-dependent if the object ϕ is invariant un- der any change of any mathematical object that contains or depends on anyAnalytical learning theory is designed to be strongly instance-dependent. To achieve this goal, unlike statistical learning theory, analytical learning theory directly analyzes the (deterministic) mathematical object. Because of the difference in the settings Any generalization bound that depends on a non-singleton hypothesis space H = {ˆy{ˆy A(S m ) }, such as ones with Rademacher complexity and VC dimension, is not strongly instance-dependent because the non-singleton hypothesis space containsˆycontainsˆ containsˆy = ˆ y A(Sm) , and the bound is not invari- ant under an arbitrary change of H. The definition of sta- bility itself depends on ˉ S m that is not equal to S m and Z m (Bousquet &amp; Elisseeff, 2002), making the correspond- ing bounds be not strongly instance-dependent. Moreover, a generalization bound that depends on a concept of ran- dom datasets ˉ S m different from S m and Z m (e.g., an ad- ditive term O( 1/m) that measures a deviation from an expectation over ˉ S m = S m , Z m ) is not strongly instance- dependent, because the bound is not invariant under an ar- bitrary change of ˉ S m . and the aims, analytical learning theory is not designed to compete with previous learning theory results, which are based on a statistical viewpoint. Table 1 summarizes the major simplified differences between statistical learn- ing theory and analytical learning theory, as the rest of this paper clarifies these differences. See Appendix A.2 for a graphical illustration of the difference.Let (Z, Σ, μ) be any (unknown) normalized measure space that defines the expected error, E μ [LˆyLˆy] = Z LˆyLˆy dμ. Here, the measure space may correspond to an input-target pair as Z = X × Y for supervised learning, the generative hidden space Z of X × Y for unsupervised / generative models, or anything else of interest (e.g., Z = X ). Let T * μ be the pushforward measure of μ under a mapData dependence does not imply strong instance- dependence. For example, in the data-dependent bounds of the luckiness framework (Shawe-Taylor et al., 1998;Herbrich &amp; Williamson, 2002), the definition of ω-smallness of the luckiness function contains a non-singleton hypothe- sis space H, a sequence of non-singleton hypothesis spaces (ordered in a data-dependent way by a luckiness function), and a supremum over H with the probability over datasets ˉ S m = S m (with Z m = S m ) (e.g., see Definition 4 in Her- brich &amp; Williamson 2002 with contraposition). The data- dependent bounds with empirical Rademacher complexity (Koltchinskii &amp; Panchenko, 2000;Bartlett et al., 2002) also depend on a non-singleton hypothesis space and its empir- ical Rademacher complexity. These bounds can be made more data-dependent by considering a sequence of hypoth- esis spaces instead, but such data-dependent bounds still )} be the image of the dataset Z m under T . Let |ν|(E) be the total variation of a measure ν on E. For vectors a, b: a ≤ t ≤ b}, where ≤ denotes the product order; that is, a ≤ t if and only if a j ≤ t j for j = 1, . . . , d. This study adopts the convention that the infimum of the empty set is positive infinity. ] can be the training error with Z m = S m or the test/validation error with Z m = S m . We can also set S m to be the whole training-validation-test dataset with Z m = S m or Z m being any other dataset. While Z m ⊆ Z (to define thê E Z m [LˆyLˆy A(Sm) ]), a dataset S m is arbitrary and S m Z is allowed when Z m = S m . Theorem 1. For any LˆyLˆy, let F [LˆyLˆy] be a set of all pairsy A(S m ) in the end of the learning process (e.g., over- parameterization in deep learning potentially makes the non-convex optimization easier; see Dauphin et al. 2014;Choromanska et al. 2015;. This is consistent with both Theorem 1 and practical observa- tions in deep learning, although it can be puzzling from the viewpoint of previous results that explicitly or implic- itly penalizes the use of more complex "larger" hypothesis spaces (e.g., see Zhang et al. 2017).where B(A) indicates the Borel σ-algebra on A. Then, for any dataset pair (S m , Z m ) (including Z m = S m ) and any LˆyLˆy A(Sm) ,, andRemark 3. The reason why both analytical learning the- ory and certain practical observations can possibly obtain results that might appear to contradict statistical learning theory is the difference in the settings or the sets of assump- tions. The larger complexities of a hypothesis space and in- stability/nonrobustness (or other properties of the learning algorithm) indeed degrade the statistical guarantee over dif- ferent random problem instances particularly with a worst- case distribution ˉ μ = μ; however, it can improve the an- alytical guarantee and practical performances for "good" problem instances (μ, S m , Z m , LˆyLˆy A(S m ) ).For example, no free lunch theorems in machine learning are well-known results, typically proven with the worst-where z i ∈ Z m , and ν f is a signed measure cor- responding to f asTheorem 1 holds for each individual instance (μ, S m , Z m , LˆyLˆy A(S m ) ), for example, without taking a supremum over a set of other instances. In contrast, typically in previous bounds, when asserting that an upper bound holds on E μ [LˆyLˆy] − ˆ E Sm [LˆyLˆy] for anyˆyanyˆ anyˆy ∈ H (with high probability), what it means is that the upper bound holds on supˆy∈Hsupˆ supˆy∈H(with high proba- bility). Thus, in classical bounds including data-dependent ones, as a H gets larger and more complex, the bounds tend to become more pessimistic for the actual instancêinstancê y A(Sm) (learned with the actual instance S m ). By additionally using the standard i.i.d. assumption, Proposition 2 provides a general bound on the star- discrepancy D * [T * μ, T (Z m )] that appears in Theorem 1. Proposition 2 is a direct consequence of ( Heinrich et al., 2001, Theorem 2). Remark 1. The bound and the equation in Theorem 1 are strongly instance-dependent, and in particular, invariant to hypothesis spaces H and the properties of learning algo- rithm A over datasets different from a given dataset S m (and Z m ) (e.g., stability and robustness). They are fully determined by each given instance (μ, S m , Z m , LˆyLˆy A(S m ) ) without dependence on other instances.Remark 2. Theorem 1 together with Remark 1 has a sig- nificant consequence in practice. For example, even if the true model is contained in some "small" hypothesis space H 1 , we might want to use a much more complex "larger" hypothesis space H 2 in practice such that the optimization becomes easier and the training trajectory reaches a better Remark 4. Proposition 2 is not probabilistically vacuous in the sense that we can increase c 2 to obtain 1 − δ &gt; 0, at the cost of increasing the constant c 2 in the bound. Forcing 1 − δ &gt; 0 still keeps c 2 constant without dependence on relevant variables such as d and m . This is because 1 − δ &gt; 0 if c 2 is large enough such that c 1 c 2 2 &lt; e 2c 2 , which depends only on the constants.Using Proposition 2, one can immediately provide a sta- tistical bound via Theorem 1 over random Z m . To see how such a result differs from that of statistical learning theory, consider the case of Z m = S m . Whereas statistical Generalization in Machine Learning via Analytical Learning Theory learning theory applies a statistical assumption to the whole object  2017, Section 5) point out the need for further theoretical studies to better un- derstand precisely what makes a learned model generalize well. Moreover, numerous related papers, even with their focus on deep learning, essentially ask the open question of understanding generalization with rich hypothesis spaces, algorithmic instability and nonrobustness (e.g., see Bartlett et al. 2017), which is applicable to linear models.where the term V [LˆyLˆy A(S m ) ] is strongly instance- dependent. Indeed, in this case, the whole bound in Equation (1) is strongly instance-dependent. See Appendix A.3 for a conceptual discussion of using a statistical assumption when Z m = S m .Theorem 1 with Remarks 1-3 answers the above open ques- tion abstractly for machine learning and deep learning in general. This section provides a more concrete answer for the case of linear models, which is a simple case that still captures the essence of the question.be the training dataset of the input- target pairs as sIn Equation (1), it is unnecessary for m to approach infin- ity in order for the generalization gap to go to zero. This is because it is multiplied by V [LˆyLˆy A(Sm) ], which is a quality of a learned modeî y A(S m ) . This strongly supports the con- cept of one-shot learning, in contrast to traditional results.). LetˆyLetˆ Letˆy A(Sm) = ˆ W φ(•) be the learned model at the end of any training process. For example, in empirical risk minimization, ˆ W is an output of the training process,For the purpose of the non-statistical decomposition of], instead of Theorem 1, we might be tempted to conduct a simpler decomposition with the Hölder inequality or its variants. However, such a sim- pler decomposition is dominated by a difference between the true measure and the empirical measure on an arbi- trary set in high-dimensional space, which suffers from the curse of dimensionality. Indeed, the proof of Theorem 1 is devoted to reformulating E μ [LˆyLˆy A(Sm) ] − ˆ E Sm [LˆyLˆy A(Sm) ] via the equivalence in the measure and the variation before taking any inequality, so that we can avoid such an issue. That is, the star-discrepancy evaluates the difference in the measures on high-dimensional boxes with one vertex at the origin, instead of on an arbitrary set.is any normalized measurable function, corresponding to fixed features. For any given variable v, let d v be the dimension- ality of the variable v. The goal is to minimize the expected errorIn this subsection only, we assume that the target output y is structured such that y = W * φ(x) + ξ, where ξ is a zero- mean random variable independent of x. Many columns of W * can be zeros (i.e., sparse) such that W * φ(x) uses a small portion of the feature vector φ(x). Thus, this struc- tured label assumption can be satisfied by including a suf- ficient number of elements from a basis with uniform ap- proximation power (e.g., polynomial basis, Fourier basis, a set of step functions, etc.) to the feature vector φ(x) up to a desired approximation error. Note that we do not assume any knowledge of W * .The following proposition proves the existence of a dataset Z m with a convergence rate that is asymptotically faster in terms of the dataset size m . This is a direct consequence of (Aistleitner &amp; Dick, 2014, Theorem 2).Let μ x be the (unknown) normalized measure for the input x (corresponding to the marginal distribution of (x, y)). Let)}  i=1 be the input part and the (unknown) input-noise part of the same train- ing dataset as S m , respectively. We do not assume access tõ S m . Let W l be the l-th column of the matrix W .Theorem 2. Assume that the labels are structured as de- scribed above andˆWandˆ andˆW − W * &lt; ∞. Then, Theorem 1 implies that m .4. Linear models possibly with rich hypothesis spaces, algorithmic instability and nonrobustness , and Generalization in Machine Learning via Analytical Learning TheoryTheorem 2 (structured labels), even with an identical hy- pothesis space (and the same Rademacher complexity and capacity) and learning algorithm. Here, we consider the normalization of y such that y ∈ [0, 1] dy . Let μ s be the (unknown) normalized measure for the pair s = (x, y).Remark 5. The bound in Theorem 2 (i.e., the right-hand- side of Equation (2)) is minimized (to be the noise term A 2 only) if and only ifˆWifˆ ifˆW = W * (see Appendix A.4 for pathological cases). Therefore, minimizing the bound in Theorem 2 is equivalent to minimizing the expected error Theorem 3. Assume the unstructured labels as described above. Let M = sup t∈ [0,1] ˆ W t − y ∞ . Assume thatˆW thatˆ thatˆW &lt; ∞ and M &lt; ∞. Then, Theorem 1 implies that2 ] or generalization error (see Appendix A.4 for further details). Furthermore, the bound in Theorem 2 holds with equality ifˆWifˆ ifˆW = W * . Therefore, the bound in Theorem 2 is tight in terms of both the minimizer and its value. In contrast to the conventional wisdom, 1 Theorem 2 tightly concludes that all that matters is how closêwhere T (s) = (φ(x), y), f (t, y) = in the end of the learning process, producing a strongly instance-dependent bound. As in other important theories, although it becomes apparent in hindsight, Theorem 2 pro- vides a crucial practical insight: we should makê W closer to W * at the end of the learning process, even if it increases the bound on the norm as well as the capacity and complexity of a hypothesis space, and even if it decreases the stability and robustness of the learning algorithm.and A 2 , we can straightforwardly apply the probabilistic bounds under the standard i.i.d. statistical assumption. From Proposition 2, with high probability, D Unlike in the structured case (Theorem 2), minimizing the bound on the generalization gap in this case requires us to bound the norm ofˆWofˆ ofˆW . This corresponds to the tradi- tional wisdom from statistical learning theory, except that we do not require a pre-defined bound on over ran- dom problem instances; Theorem 3 is only sensitive to the actual value ofˆWofˆ ofˆW at the end of learning process with the given problem instance, producing a strongly instance- dependent bound. The generalization gap goes to zero as [T * μ s , T (S m )] approaches zero via certain statistical assumption as in statistical learning theory. We can guar- antee it via Proposition 2 as follows: with high probability,[φ * μ x , φ(X m )] to approach zero to minimize the expected error; irrespective of whether the training dataset satisfies a certain statistical assumption to bound D * [φ * μ x , φ(X m )], we can minimize the expected error via makingˆWmakingˆ makingˆW closer to W * as shown in Theorem 2.In this subsection, we discard the structured label assump- tion in the previous subsection and consider the worst case scenario where y is a variable independent of x. This corresponds to the random label experiment by Zhang et al. (2017), which posed another open question: how to theoretically distinguish the generalization behaviors with structured labels from those with random labels. General- ization behaviors in practice are expected to be significantly different in problems with structured labels or random la- bels, even when the hypothesis space (and Rademacher complexity) and learning algorithm remain unchanged.For linear models with over-parameterization, a recent im- pactful paper ( Zhang et al. 2017, Section 5) poses open questions: "is it then possible to generalize with such a rich model class and no explicit regularization?" and "do all global minima generalize equally well?". Theorems 2 and 3 shed light on those questions with a mathematically tight reasoning for each problem instance (see Remark 5).In this section, we consider the applications of Theorem 1 to general models in machine learning including deep neural networks, in order to understand the qualitative na- ture of learning under the setting wherein the generalization gap is decomposed without statistical assumptions. Deriv- ing more concrete statements for each specific model type (e.g., convolutional neural networks with ReLU units) is left to future work. We first state that the results from the previous section can be applied to deep learning.As desired, Theorem 3 (unstructured labels) predicts a completely different generalization behavior from that in 1 A good description of the conventional wisdom is given in ( Zhang et al., 2017;Bartlett et al., 2017).- resentations φ, instead of fixed features. Let φ(x) represent the last hidden layer in a neural network or the learned rep- resentation in representation learning in general. Consider the squared loss (square of output minus target). Then, the identical proofs of Theorems 2 and 3 work with the learned representation φ.An important question in representation learning such as deep learning is how to consider the quality of learned rep- resentations. The following example provides a general re- sult to answer such a question with practical insights. trated with generative models or auto-encoders by showing how interpolating between the representations of two im- ages (in representation space) corresponds (when projected in image space) to other images that are plausible (are on or near the manifold of natural images), rather than to the sim- ple addition of two natural images ( Bengio et al., 2009b). [T * μ, T (Z m )] is small, it means that the learned rep- resentation φ is effective at minimizing the generalization gap. This insight can be practically exploited by aiming to make T * μ flatter and spread out the data points T (Z m ) in a limited volume. It would also be beneficial to directly regularize an approximated D * [T * μ, T (Z m )] with the un- known μ replaced by some known measures (e.g., a finite- support measure corresponding to a validation dataset).General Example 1 partially supports the concept of the [f j1...j k ] can be viewed as mea- suring how entangled the j 1 , . . . , j k -th variables are in a space of a learned (hidden) representa- tion.We can observe this from the definition of V (k) [f j1...j k ] or from Proposition 1 as:is the k-th order cross partial deriva- tives across the j 1 , . . . , j k -th variables. If all the variables in a space of a learned (hidden) representation are com- pletely disentangled in this sense, V It has been empirically observed that deep networks (par- ticularly in the unsupervised setting) tend to transform the data distribution into a flatter one closer to a uniform distri- bution in a space of a learned representation (e.g., see Bengio et al. 2013). If the distribution T * μ with the learned rep- resentation T is uniform, then there exist better bounds on Aistleitner, 2011). Intuitively, if the measure T * μ is non- flat and concentrated near a highly curved manifold, then there are more opportunities for a greater mismatch be- tween T * μ and T (Z m ) to increase D * [T * μ, T (Z m )] (see Appendix A.5 for pathological cases). This intuitively sug- gests the benefit of the flattening property that is sometimes observed with deep representation learning: it is often illus- This indicates that we can regularize V [LˆyLˆy A(Sm) ] in some space Z to control the generalization gap. For exam- ple, letting the model y A(S m ) be invariant to a subspace that is not essential for prediction decreases the bound on V [LˆyLˆy A(Sm) ]. As an extreme example, if x = g(y, ξ) with some generative function g and noise ξ (i.e., a setting con- sidered in an information theoretic approach), ˆ y A(Sm) be- ing invariant to ξ results in a smaller bound on V [LˆyLˆy A(Sm) ]. This is qualitatively related to an information theoretic ob- servation such as in (Achille &amp; Soatto, 2017).  Appendix B.7 for this derivation), which establishes a tightness of Theorem 1 with the 0-1 loss as follows: for any dataset pairstrongly instance-dependent, particularly on the learned model LˆyLˆy A(Sm) in the end, Theorem 1 supports the con- cept of curriculum learning ( Bengio et al., 2009a), which directly guides the learning to obtain a good model y A(S m ) in the end. Moreover, consider certain types of curricu- lum learning that can violate statistical assumptions and degrade statistical guarantees. Theorem 1 supports even such types of curriculum learning, because learning a bet- ter model y A(S m ) in the end decreases V [f ].By generating strongly instance-dependent bounds, throughout this paper, Theorem 1 has been shown to answer the following open questions: 1) how to math- ematically analyze generalization behaviors of machine learning models possibly with arbitrarily rich hypothesis spaces (large capacity, Rademacher complexity, etc.) and non-stable/non-robust learning algorithms, and 2) how to theoretically distinguish generalization behaviors with structured labels from unstructured random labels. Perhaps more importantly, Theorem 1 presents a way to measure the quality of the learned model y A(S m ) along with each instance (μ, S m , Z m , LˆyLˆy A(S m ) ), and has produced several new theoretical insights on the generalization gap.As we discussed in Section 3.2, V [f ] is always strongly instance-dependent, but a probabilistic bound on D *[T * μ, T (S m )] may not always remain strongly instance- dependent, depending on the choice of T . For example, if T is learned with S m , we cannot directly adopt the prob- abilistic bound on D * Theorem 1 also provides a different theoretical insight that has another immediate practical consequence on how to handle a dataset. In Theorem 1, the quality of a dataset S m is determined by the product of its similarity to μ (the star-discrepancy) and the quality of the learned function LˆyLˆy A(Sm) through the dataset (the variation). That is, while conventional wisdom based on statistical learning theory tells us to collect a statistically "good" (e.g., i.i.d.) dataset from a distribution closer to the true μ, Theorem 1 tells us that there may be better choices. According to Theo- rem 1, collecting a dataset that can induce a better modeîmodeî y A(S m ) with a lower empirical error and lower variation of LˆyLˆy A(Sm) also results in better generalization gap and expected error, even if the similarity to μ is reduced and the statistical property (e.g., independence) becomes in- valid (e.g., in Example 2, even if D [T * μ, T (S m )] from Section 3.2, be- cause T (S m ) does not satisfy the i.i.d. assumption. In such a case, one can derive new probabilistic bounds on D * with learned representation T via the following standard statis- tical approach. Consider a set Φ such that T ∈ Φ and Φ is independent of S m . Then, by applying Proposition 2 with a union bound over a cover of Φ, we can obtain probabilis- tic bounds on D * with the log of the covering number of Φ for all representations T ∈ Φ, including the learned T . As in data-dependent approaches (e.g., Shawe-Taylor et al. 1998), one can also consider a sequence of sets {Φ j } j such that T ∈ ∪ j Φ j . However, in both cases, the bounds on] now depend on the set Φ (via its covering number) or the sequence {Φ j } j (via the ordering in j and a complexity of Φ j ) that depends onˆyonˆ onˆy = ˆ y A(Sm) , and hence are not strongly instance-dependent. This is a limitation of our result toward a complete learning theory with strong instance-dependence, and solving it is left to future work. * [μ, S m ] approaches infinity, if V [LˆyLˆy A(Sm) ] goes to zero faster, the generaliza- tion gap approaches zero). This is consistent with emerging practical heuristics in deep learning; it is becoming a com- mon practice to add new data points to explicitly improve a currently learned model (e.g., by probing the model), rather than considering the statistical property in the dataset.From a practical viewpoint, this might not be a major lim- itation, since a probabilistic bound on D * is unnecessary for the convergence of the generalization gap. The con- vergence of V [f ] to zero (faster than the increase rate of D * ) is sufficient for the convergence of generalization gap, and D * would also decrease deterministically. Moreover, one can simply measure the empirical error with another dataset Z m = S m (e.g., held-out validation dataset) to getwith high probability), where the representation T is learned with S m (since the i.i.d. condition can now be straightforwardly satisfied).As we discussed in Section 3.2, Theorem 1 produces gen- eralization bounds that can be zero even with m = 1 (and m = 1), supporting the concept of one-shot learn- ing. This is true in general, even if the dataset com- pletely differs from the measure μ (e.g., learning with different distributions). This is because although it in- creasesThe fact that Theorem 1 is invariant to a hypothesis space H and certain details of a learning algorithm A can make it difficult to understand their effects. However, as we move towards the goal of artificial intelligence, H and A would become extremely complex, which can pose a challenge in theory. From this viewpoint, our theory can also be consid- ered as a methodology to avoid such a challenge, producing theoretical insights for intelligent systems with arbitrarily complex H and A, so long as other conditions are imposed on the actual functions being computed by them.To recognize certain differences in analytical learning the- ory and statistical learning theory, it is good to remember the basics of the mathematical logics such as the difference in "∀x, ∃y" v.s. "∃y, ∀x". Typically in statistical learning theory, some upper bound holds over a fixed H or a fixed A with a certain property with high probability over different datasets. Usually in analytical learning theory, some upper bound holds individually for each problem instance.   Sm) , which is a deterministic quantity of the tuple (μ, Sm, LˆyLˆy A(Sm) ). Intuitively, whereas analytical learning the- ory analyzes q directly, statistical learning theory focuses more on analyzing the set Q that contains q. The set Q is defined by the sets of possible measures μ and randomly-drawn differ- ent datasets Sm and the hypothesis space H or learning algorithm A. case distribution characterizing q in Q. Such a necessary condition is only proven to be necessary for the worst-case q ∈ Q, but is not proven to be necessary for others q = q . Intuitively, we are typically analyzing the quality of the set Q, instead of each individual q ∈ Q. Figure 2 shows a graphical illustration of a difference in the scopes of statistical learning theory and analytical learning theory. Here, μ m is the product measure.In this view, it becomes clear what is going on in some potentially surprising empirical observations such as in ( Zhang et al., 2017). Intuitively, whereas statistical learn- ing theory focuses more on analyzing the set Q, each ele- ment such as q (e.g., a "good" case or structured label case) and q (e.g., the worst-case or random label case) can sig- nificantly differ from each other. Data-dependent analyses in statistical learning theory can be viewed as the ways to decrease the size of Q around each q.In the setting of statistical learning theory (Figure 2 (a)), our typical goal is to analyze the random expected error E μ [LˆyLˆy A(Sm) ] over the random datasets S m by fixing a hypothesis space and/or learning algorithm over random datasets. Due to the randomness over S m , we do not know where q exactly lands in Q. The lower bound and neces- sary condition in the setting of statistical learning theory is typically obtained via a worst-case instance q in Q. For ex- ample, classical no free lunch theorems and lower bounds on the generalization gap via VC dimension (e.g., Mohri et al. 2012, Section 3.4) have been derived with the worst- In contrast, analytical learning theory (Figure 2 (b)) ignores the set Q, and focuses on each q only, allowing tighter re- sults for each "good" q ∈ Q beyond the possibly "bad" quality of the set Q overall.It is important to note that analyzing the set Q is of great interest on its own merits, and statistical learning theory has advantages over our proposed learning theory in this sense. Indeed, analyzing a set Q is a natural task along the way of thinking in theoretical computer science (e.g., categorizing a set Q of problem instances into polynomial solvable set or not). This situation where theory focuses more on Q and practical studies care about each q ∈ Q is prevalent in computer science even outside the learning theory. For example, the size of Q analyzed in theory for optimal exploration in Markov decision processes (MDPs) has been shown to be often too loose for each practical problem instance q ∈ Q, and a way to partially mitigate this issue was recently proposed (Kawaguchi, 2016). Sim- ilarly, global optimization methods including Bayesian op- timization approaches may suffer from a large complex Q for each practical problem instance q ∈ Q, which was par- tially mitigated in recent studies ( Kawaguchi et al., 2015;2016). imize the expected error in Theorem 1 by minimizing] becomes marginal without the randomness over S m .Several recent studies also consider a stochastic property of learning algorithms (e.g., Zahavy et al. 2016).The bound is always minimized ifˆWifˆ ifˆW = W * , but it is not a necessary condition in a pathological case where the star- discrepancy D * is zero and A 1 can be zero withˆWwithˆ withˆW = W * .Furthermore, the issues of characterizing a set Q only via a worst-case instance q (i.e., worst-case analysis) are well- recognized in theoretical computer science, and so-called beyond worst-case analysis (e.g., smoothed analysis) is an active research area to mitigate the issues.In Section 4.1, the optimal solution to minimize the ex-To see this, we can expand the expected error asMoreover, a certain qualitative property of the set Q might tightly capture that of each instance q ∈ Q. While prov- ing such an assertion seems challenging in general (prov- ing that a upper bound on ∀q ∈ Q matches a lower bound ∃q ∈ Q is not sufficient), one can study it with empirical experiments (e.g., see Hestness et al. 2017).Using a statistical assumption on a dataset Z m with Z m = S m is consistent with a practical situation where a dataset S m is given first instead of remaining random. For Z m = S m , we can view this formulation as a mathematical mod- eling of the following situation. Consider S m as a random variable when collecting a dataset S m , and then condition on the event of getting the collected dataset S m once S m is specified, focusing on minimization of the (future) ex- pected error E μ [LˆyLˆy A(Sm) ] of the modeî y A(Sm) learned with this particular specified dataset S m .where the last line follows that ξ is a zero-mean random variable independent of x. From the last line of the above equation, we can conclude the above statement about the minimizer.If T * μ is concentrated in a single point, then D * In this view, we can observe that if we draw an i.i.d. dataset S m , a dataset S m is guaranteed to be statistically "good" with high probability in terms of D D * d m via Proposition 2). Thus, col- lecting a training dataset in a manner that satisfies the i.i.d. condition is an effective method. However, once a dataset S m is actually specified, there is no longer randomness over S m , and the specified dataset S m is "good" (high probabil- ity event) or "bad" (low probability event). We get a "good" dataset with high probability, and we obtain probabilistic guarantees such as Equation (1).We use the following fact in our proof.In many practical studies, a dataset to learn a model is specified first as, for example, in studies with CIFAR-10, ImageNet, or UCI datasets. Thus, we might have a sta- tistically "bad" dataset S m with no randomness over S m when these practical studies begin. Even then, we can min- Proof. By the definition, we have thatBy the mean value theorem on the single variable t j k ,follows the proof of theorem 1.6.12 in (Ash &amp; DoleansDade, 2000); we proceed from simpler cases to more gen- eral cases as follows. In the case of f being an indicator function of some set A as f = 1 A , we have that where c. Thus, by repeatedly applying the mean value theorem,where c) for all l ∈ {1, . . . , k}. Thus, In the case of f being a non-negative simple function as ) and taking it out from the sum, we obtain the first statement.The second statement follows the fact that if ∂ 1,...,k f j 1 ...j k (t where the second line follows what we have proved for the case of f being an indicator function. )| is continuous and Riemann integrable. Thus, the right hand side on the above equa- tion coincides with the definition of the Riemann integral of |∂ 1,...,k f j1...j k (t In the case of f being a non-negative Borel measurable function, let (f k ) k∈N be an increasing sequence of sim- ple functions such that f (ω) = lim k→∞ f k (ω), ω ∈ Ω. Then, by what we have proved for simple func- tions, we have The proof of Theorem 1 relies on several existing proofs from different fields. Accordingly, along the proof, we also track the extra dependencies and structures that ap- pear only in machine learning, to confirm the applicability of the previous proofs in the problem of machine learning. Thus, while presenting the proof is necessary for the pur- pose of this paper, each component of the proof by itself is not intended to be an original contribution. Let 1 A be an indicator function of a set A. Let Ω = [0, 1] d . Let 1 = (1, 1, . . . , 1) ∈ Ω and 0 = (0, 0, . . . , 0) ∈ Ω as in a standard convention. The following lemma follows theo- rem 1.6.12 in (Ash &amp; Doleans-Dade, 2000).In the case of f = f + − f − being an arbitrary Borel measurable function, we have already proved the desired statement for each f + and f − , and by the definition of Lebesgue integration, the statement for f holds.Proof of Theorem 1. With Lemmas 1 and 2, the proof fol- lows that of theorem 1 in (Aistleitner &amp; Dick, 2015). Proof of Lemma 2. By Lemma 1, f is a Borel measur- able function. The rest of the proof of this lemma directly where the second line follows the condition of T and f and the third line follows Lemma 2. In the following, weand by the Fubini-Tonelli theorem and linearity, Let ˉ A n = {ˉ ω i } i=1 be such a set. For each fixed f , let f n be a left-continuous function such that f n (ω) = f (ω) for all ω ∈ ˉ A n ∪ T (Z m ) and V [f n ] ≤ V [f ]. This definition of f n is non-vacuous and we can construct such a f n as follows. Let G be the d-dimensional grid generated by the set {0}∪{1}∪ ˉ A n ∪T (Z m ); G is the set of all points ω ∈ Ω such that for k ∈ {1, . . . , d}, the k-th coordinate value of ω is the k-th coordinate value of some element in the set {0} ∪ {1} ∪ ˉ A n ∪ T (Z m ). We can construct a desired f n by setting f n (ω) = f (succ n (ω)), where succ n (ω) outputs an unique element t ∈ G satisfying the condition that t ≥ ω and t ≤ t for all t ∈ {t ∈ G : t ≥ ω}.Then, by triangle inequality, we writewhich proves the second statement of this theorem by noticing that f (t) = ν f ( [t, 1]) + f (1). Moreover, this im- Because f n is left-continuous, we can apply our previous result to the first and the second terms; the first term is at most
