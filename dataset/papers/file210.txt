Deep learning with artificial neural networks has enabled rapid progress on applications in engi- neering (e.g., Krizhevsky et al., 2012;Hannun et al., 2014) and basic science (e.g., Di Lena et al., 2012;Lusci et al., 2013;Baldi et al., 2014). Usually, the parameters in the linear components are learned to fit the data, while the nonlinearities are pre-specified to be a logistic, tanh, rectified linear, or max-pooling function. A sufficiently large neural network using any of these common nonlinear functions can approximate arbitrarily complex functions (Hornik et al., 1989;Cho &amp; Saul, 2010), but in finite networks the choice of nonlinearity affects both the learning dynamics (especially in deep networks) and the network's expressive power.Designing activation functions that enable fast training of accurate deep neural networks is an active area of research. The rectified linear activation function (Jarrett et al., 2009;Glorot et al., 2011), which does not saturate like sigmoidal functions, has made it easier to quickly train deep neural net- works by alleviating the difficulties of weight-initialization and vanishing gradients. Another recent innovation is the "maxout" activation function, which has achieved state-of-the-art performance on multiple machine learning benchmarks ( Goodfellow et al., 2013). The maxout activation function computes the maximum of a set of linear functions, and has the property that it can approximate any convex function of the input. Springenberg &amp; Riedmiller (2013) replaced the max function with a probabilistic max function and Gulcehre et al. (2014) explored an activation function that replaces the max function with an L P norm. However, while the type of activation function can have a significant impact on learning, the space of possible functions has hardly been explored.One way to explore this space is to learn the activation function during training. Previous efforts to do this have largely focused on genetic and evolutionary algorithms (Yao, 1999), which attempt to select an activation function for each neuron from a pre-defined set. Recently, Turner &amp; Miller (2014) combined this strategy with a single scaling parameter that is learned during training.In this paper, we propose a more powerful adaptive activation function. This parametrized, piecewise linear activation function is learned independently for each neuron using gradient descent, and can represent both convex and non-convex functions of the input. Experiments demonstrate that like other piecewise linear activation functions, this works well for training deep neural networks, and we obtain state-of-the-art performance on multiple benchmark deep learning tasks.Here we define the adaptive piecewise linear (APL) activation unit. Our method formulates the activation function h i (x) of an APL unit i as a sum of hinge-shaped functions,The result is a piecewise linear activation function. The number of hinges, S, is a hyperparameter set in advance, while the variables a The number of additional parameters that must be learned when using these APL units is 2SM , where M is the total number of hidden units in the network. This number is small compared to the total number of weights in typical networks. Figure 1 shows example APL functions for S = 1. Note that unlike maxout, the class of func- tions that can be learned by a single unit in- cludes non-convex functions. In fact, for large enough S, h i (x) can approximate arbitrarily complex continuous functions, subject to two conditions:Theorem 1 Any continuous piecewise-linear function g(x) can be expressed by Equation 1 for some S, and a i , b i , i ∈ 1, ..., S, assuming that:1. There is a scalar u such that g(x) = x for all x ≥ u.2. There are two scalars v and α such that ∇ x g(x) = α for all x &lt; v. This theorem implies that we can reconstruct any piecewise-linear function g(x) over any subset of the real line, and the two conditions on g(x) constrain the behavior of g(x) to be linear as x gets very large or small. The first condi- tion is less restrictive than it may seem. In neu- ral networks, g(x) is generally only of interest as an input to a linear function wg(x) + z; this linear function effectively restores the two degrees of freedom that are eliminated by constraining the rightmost segment of g(x) to have unit slope and bias 0.Proof Let g(x) be piecewise linear with K + 2 linear regions separated by ordered boundary points b 0 , b 1 , ...,b K , and let a k be the slope of the k-th region. Assume also that g(x) = x for all x ≥ b K .We show that g(x) can be expressed by the following special case of Equation 1:The first term has slope a 0 in the range (−∞, b 0 ) and 0 elsewhere. Each element in the summation term of Equation 2 has slope a k over the range (b k−1 , b k ) and 0 elsewhere. The last three terms together have slope 1 when x ∈ (b K , ∞) and 0 elsewhere. Now, g(x) and h(x) are continuous, their slopes match almost everywhere, and it is easily verified that h(x) = g(x) = x for x ≥ b K . Thus, we conclude that h(x) = g(x) for all x.In this section we compare the proposed approach to learning activation functions with two other nonlinear activation functions: maxout ( Goodfellow et al., 2013), and network-in-network ( Lin et al., 2013).We observe that both maxout units and network-in-network can learn any nonlinear activation func- tion that APL units can, but require many more parameters to do so. This difference allows APL units to be applied in very different ways from maxout and network-in-network nonlinearities: the small number of parameters needed to tune an APL unit makes it practical to train convolutional networks that apply different nonlinearities at each point in each feature map, which would be com- pletely impractical in either maxout networks or network-in-network approaches.Maxout. Maxout units differ from traditional neural network nonlinearities in that they take as input the output of multiple linear functions, and return the largest:Incorporating multiple linear functions increases the expressive power of maxout units, allowing them to approximate arbitrary convex functions, and allowing the difference of a pair of maxout units to approximate arbitrary functions.Networks of maxout units with a particular weight-tying scheme can reproduce the output of an APL unit. The sum of terms in Equation 1 with positive coefficients (including the initial max(0, x) term) is a convex function, and the sum of terms with negative coefficients is a concave function. One could approximate the convex part with one maxout unit, and the concave part with another maxout unit:where c and d are chosen so thatIn a standard maxout network, however, the w vectors are not tied. So implementing APL units (Equation 1) using a maxout network would require learning O(SK) times as many parameters, where K is the size of the maxout layer's input vector. Whenever the expressive power of an APL unit is sufficient, using the more complex maxout units is therefore a waste of computational and modeling power.Network-in-Network. Lin et al. (2013) proposed replacing the simple rectified linear activation in convolutional networks with a fully connected network whose parameters are learned from data. This "MLPConv" layer couples the outputs of all filters applied to a patch, and permits arbitrarily complex transformations of the inputs. A depth-M MLPConv layer produces an output vector ffrom an input patch x ij via the series of transformationsAs with maxout networks, there is a weight-tying scheme that allows an MLPConv layer to repro- duce the behavior of an APL unit:where the function κ(k) maps from hinge output indices k to filter indices κ, and the coefficientThis is a very aggressive weight-tying scheme that dramatically reduces the number of parameters used by the MLPConv layer. Again we see that it is a waste of computational and modeling power to use network-in-network wherever an APL unit would suffice.However, network-in-network can do things that APL units cannot-in particular, it efficiently cou- ples and summarizes the outputs of multiple filters. One can get the benefits of both architectures by replacing the rectified linear units in the MLPconv layer with APL units.Experiments were performed using the software package CAFFE ( Jia et al., 2014). The hyperpa- rameter, S, that controls the complexity of the activation function was determined using a validation set for each dataset. The a s s i and b i parameters were regularized with an L2 penalty, scaled by 0.001. Without this penalty, the optimizer is free to choose very large values of a s i balanced by very small weights, which would lead to numerical instability. We found that adding this penalty improved re- sults. The model files and solver files are available at https://github.com/ForestAgostinelli/Learned- Activation-Functions-Source/tree/master.The CIFAR-10 and CIFAR-100 datasets (Krizhevsky &amp; Hinton, 2009) are 32x32 color images that have 10 and 100 classes, respectively. They both have 50,000 training images and 10,000 test im- ages. The images were preprocessed by subtracting the mean values of each pixel of the training set from each image. Our network for CIFAR-10 was loosely based on the network used in ( Srivastava et al., 2014). It had 3 convolutional layers with 96, 128, and 256 filters, respectively. Each kernel size was 5x5 and was padded by 2 pixels on each side. The convolutional layers were followed by a max-pooling, average-pooling, and average-pooling layer, respectively; all with a kernel size of 3 and a stride of 2. The two fully connected layers had 2048 units each. We applied dropout ( Hinton et al., 2012) to the network as well. We found that applying dropout both before and after a pooling layer increased classification accuracy. The probability of a unit being dropped before a pooling layer was 0.25 for all pooling layers. The probability for them being dropped after each pooling layers was 0.25, 0.25, and 0.5, respectively. The probability of a unit being dropped for the fully connected layers was 0.5 for both layers. The final layer was a softmax classification layer. For CIFAR-100, the only difference was the second pooling layer was max-pooling instead of average-pooling. The baseline used rectified linear activation functions.When using the APL units, for CIFAR-10, we set S = 5. For CIFAR-100 we set S = 2. Table 1 shows that adding the APL units improved the baseline by over 1% in the case of CIFAR-10 and by almost 3% in the case of CIFAR-100. In terms of relative difference, this is a 9.4% and a 7.5% decrease in error rate, respectively. We also try the network-in-network architecture for CIFAR-10 ( Lin et al., 2013). We have S = 2 for CIFAR-10 and S = 1 for CIFAR-100. We see that it improves performance for both datasets.We also try our method with the augmented version of CIFAR-10 and CIFAR-100. We pad the image all around with a four pixel border of zeros. For training, we take random 32 x 32 crops of the image and randomly do horizontal flips. For testing we just take the center 32 x 32 image. To the best of our knowledge, the results we report for data augmentation using the network-in-network architecture are the best results reported for CIFAR-10 and CIFAR-100 for any method.In section 3.4, one can observe that the learned activations can look similar to leaky rectified linear units (Leaky ReLU) ( Maas et al., 2013). This activation function is slightly different than the ReLU because it has a small slope k when the input x &lt; 0.In ( Maas et al., 2013), k is equal to 0.01. To compare Leaky ReLUs to our method, we try different values for k and pick the best value one. The possible values are positive and negative 0.01, 0.05, 0.1, and 0.2. For the standard convolutional neural network architecture k = 0.05 for CIFAR-10 and k = −0.05 for CIFAR-100. For the network-in-network architecture k = 0.05 for CIFAR-10 and k = 0.2 for CIFAR-100. APL units consistently outperform leaky ReLU units, showing the value of tuning the nonlinearity (see also section 3.3). Table 1: Error rates on CIFAR-10 and CIFAR-100 with and without data augmentation. This in- cludes standard convolutional neural networks (CNNs) and the network-in-network (NIN) architec- ture ( Lin et al., 2013). The networks were trained 5 times using different random initializations - we report the mean followed by the standard deviation in parenthesis. The best results are in bold.CIFAR-10 CIFAR-100 Without Data Augmentation CNN + ReLU ( Srivastava et al., 2014) 12.61% 37.20% CNN + Channel-Out ( Wang &amp; JaJa, 2013) 13.2% 36.59% CNN + Maxout ( Goodfellow et al., 2013) 11.68% 38.57% CNN + Probout (Springenberg &amp; Riedmiller, 2013) 11 The Higgs-to-τ + τ − decay dataset comes from the field of high-energy physics and the analysis of data generated by the Large Hadron Collider ( Baldi et al., 2015). The dataset contains 80 million collision events, characterized by 25 real-valued features describing the 3D momenta and energies of the collision products. The supervised learning task is to distinguish between two types of physical processes: one in which a Higgs boson decays into τ + τ − leptons and a background process that produces a similar measurement distribution. Performance is measured in terms of the area under the receiver operating characteristic curve (AUC) on a test set of 10 million examples, and in terms of discovery significance (Cowan et al., 2011) in units of Gaussian σ, using 100 signal events and 5000 background events with a 5% relative uncertainty.Our baseline for this experiment is the 8 layer neural network architecture from ( Baldi et al., 2015) whose architecture and training hyperparameters were optimized using the Spearmint al- gorithm (Snoek et al., 2012). We used the same architecture and training parameters except that dropout was used in the top two hidden layers to reduce overfitting. For the APL units we used S = 2. Table 2 shows that a single network with APL units achieves state-of-the-art performance, increasing performance over the dropout-trained baseline and the ensemble of 5 neural networks from ( Baldi et al., 2015). Table 2: Performance on the Higgs boson decay dataset in terms of both AUC and expected dis- covery significance. The networks were trained 4 times using different random initializations -we report the mean followed by the standard deviation in parenthesis. The best results are in bold.  Table 3 shows the effect of varying S on the CIFAR-10 benchmark. We also tested whether learning the activation function was important (as opposed to having complicated, fixed activation functions). For S = 1, we tried freezing the activation functions at their random initialized positions, and not allowing them to learn. The results show that learning activations, as opposed to keeping them fixed, results in better performance. Table 3: Classification accuracy on CIFAR-10 for varying values of S. Shown are the mean and standard deviation over 5 trials. In figure 4, for each layer, 1000 activation functions (or the maximum number of activation functions for that layer, whichever is smaller) are plotted. One can see that there is greater variance in the learned activations for CIFAR-100 than there is for CIFAR-10. There is greater variance in the learned activations for Higgs→ τ + τ − than there is for CIFAR-100. For the case of Higgs→ τ + τ − , a trend that can be seen is that the variance decreases in the higher layers.We have introduced a novel neural network activation function in which each neuron computes an independent, piecewise linear function. The parameters of each neuron-specific activation function are learned via gradient descent along with the network's weight parameters. Our experiments demonstrate that learning the activation functions in this way can lead to significant performance improvements in deep neural networks without significantly increasing the number of parameters. Furthermore, the networks learn a diverse set of activation functions, suggesting that the standard one-activation-function-fits-all approach may be suboptimal.   
