Nowadays, people tend to use images, videos, and GIFs to express their emotions and opinions alongside text. For example, images shared on websites such as Flickr and Instagram are often accompanied with a short description to augment or further explain the post. This hybrid, or multi- modal, data is particularly prevalent on social me- dia, where users frequently express their opinions on a wide range of topics.Accurate sentiment classification has many ap- plications. It can help companies understand how customers feel about their products, politicians un- derstand how people are responding to a policy proposal, or be used to investigate cultural differ- ences ( Pappas et al., 2016). Effectively making use of multi-modal data in sentiment classifica- tion, images and text for example, is valuable since it expands the sources of information available to organizations to understand the people who are en- gaging with them or their products.Sentiment classification is both an interesting and a hard problem due to the inherent subjectiv- ity in identifying sentiment. Perception of senti- ment is often shaped by one's culture. Different cultures exhibit varying degrees of positivity (Pappas et al., 2016), and the same description or im- age may evoke a different sentiment depending on where the person you ask is from. Furthermore, sentiment classification is hard because it can be ambiguous. The same image can evoke mixed feelings in a viewer. It is not a contradiction to look at an image and feel happy and sad at the same time. This notion is also reflected in our lan- guage, with phrases such as "bittersweet", which nicely capture the ambiguity of sentiment. Conse- quently, most datasets that have been gathered for sentiment analysis are either small or only weakly labeled. As a result, they tend to be noisy, with examples exhibiting large variation within senti- ment classes, as well as some examples being mis- labeled.Up until now, most sentiment analysis research has been focused on text. Whilst this is still a hard problem, significant progress has been made in this direction (for example Socher et al. (2013b)). However, the increasing use of multiple modes to express opinions means that it is important to try and learn to recognise sentiment using sources of information other than text. Ultimately, the goal is to combine each of these sources of information to produce a more nuanced and accurate picture of sentiment.The objective of this project was to determine whether the use of images was helpful in identify- ing the sentiment of multi-modal data consisting of images and text. Since sentiment classification is often not clear cut, we also looked for a way to better understand the outputs of our models. To do so, we took inspiration from the increased use of word embeddings, a dense representation of a word in vector space which encourages informa- tion sharing between similar words, across a wide range of natural language processing tasks, and the application of this idea to output / label space as in Frome et al. (2013) and Socher et al. (2013a). By forcing our models to output their results in an em-bedded space, we were better able to assess how the predicted sentiment of different examples from our data related to each other, and whether they aligned with our intuition about which datapoints should have been difficult or easy to classify.A final goal of this project was to see if this ap- proach could help sentiment classifiers generalize to unknown sentiments. Our final models were not able to generalize to unknown classes. However, we believe this is an interesting area for further work and that models which output results into an embedded space are a step in the right direction.• the associated image is still accessible• text fields are in English and spelled correctly• text fields combined contain at least 10 words As a result of the data preprocessing, the final dataset contained about 186k data points.For each experimental setup that follows, the data (after further pruning, if any), was shuffled and split into training (70%), validation (20%), and test sets (10%).Our work was based on the large-scale multilin- gual visual sentiment concept ontology (MVSO) ( Jou et al., 2015). This dataset consists of more that 7.36 million xml files with Flickr images metadata. The original dataset contains multilin- gual data (12 languages), for our project we re- stricted ourselves to the English subset of the data.Each xml file contains the following informa- tion:The primary metric we used to compare models was accuracy. It worth noting that for the Embed- ding Classifier (thorough description of Embed- ding Classifier is given in Section 3) cosine dis- tance was used to measure similarity between the generated word embeddings and true labels em- beddings.• image URL• text fields: title, description, tags Each xml file is associated with a related sentiment-based adjective-noun with a sentiment score. For example, ANP "infinite sadness" has sentiment score -2.128, while ANP "nice smile" has an associated score 2.019. The English subset of the data contains about 4200 distinct English ANPs. The example of a data entry is shown in Figure 1.As an input for some of our models, we used fea- tures extracted from images by running our im- age data though a state-of-art pre-trained models: AlexNet ( Krizhevsky et al., 2012) and ResNet ( He et al., 2015). These models were trained for im- age recogintion/classification task, and since many first layers of the neural networks learn image spe- cific information (edges, shape, colors, etc.) be- fore learning to classify them, we utilized these models' architecture to make our models less com- plex. We applied a number of filters to the original dataset, and kept only files that satisfy the follow- ing criteria:Only a limited amount of work has been done on sentiment analysis of images. The works that in- spired our project were Frome et al. (2013) and You et al. (2015). Frome et al. (2013) proposed a model that matches a-state-of-art performance on 1000-class ImageNet object recognition chal- lenge by utilizing semantic information learned from text labels. Frome et al. (2013) used a pre- trained neural language model and a pre-trained state-of-the-art deep neural network for visual ob- ject recognition complete with a traditional soft- max output layer. Then they took the lower lay- ers of the pre-trained visual object recognition net- work and re-trained them to predict the vector rep- resentation of the image label text as learned by the language model. In this work, the final model was capable of classifying unseen classes by gen- erating word embedding of the 'predicted' class text label. You et al. (2015) performed a sentiment classi- fication of Flickr and Twitter images with a Con- volutional Neural Networks (CNN) model. To the best of our knowledge You et al. (2015) were the first to apply deep learning methods to image sen- timent classification, and achieved state of the art results by doing so. We used a similar approach to You et al. (2016b) to combine our models, but differ by using an LSTM model to generate the text features. Addi- tionally, we explore the use of gates to control how much information from the text and image features are used for the final classification task. dataset to a much smaller set to try and improve the reliability of our training examples, and the re- sults we obtained did not approach state of the art. Therefore, we decided to try another labeling ap- proach Section 4.1.2 and use ANP pairs and as- sociated sentiment scores to label image-caption pairs. This is the same approach taken by You et al. (2015) and You et al. (2016b).The most comparable set of experiments to ours were from You et al. (2016a). Their methodol- ogy for gathering images was very similar, apply- ing filters to exclude examples with no image or text for example. Their dataset was also the most comparable size compared to ours, 250k data- points, split almost equally between positive and negative classes. Other research that utilized the same dataset, You et al. (2015) for example, used a much larger set of 0.5 million images. We there- fore use results of You et al. (2016a) as bench- marks for our image only, text only, and combined models. We also include You et al. (2015)'s results for image sentiment classification as a state of the art comparison for this task. Finally, whilst we ex- perimented with both two (positive, negative), and three (positive, negative, neutral) sentiment classi- fiers, You et al. (2015) and You et al. (2016a) only provide results for two classes. Consequently, we have no benchmarks for our three class sentiment classifiers.To summarize, our project compares with re- lated work in the following ways:Finding a well labeled sentiment dataset is chal- lenging. There were no readily available large datasets that contain images with associated cap- tions and the sentiment labels. We therefore used the same dataset as You et al. (2015) to conduct our experiments, the MVSO dataset. You et al. (2015) used the ANP pairs and associated scores to assign labels to particular datapoints, although they are not specific about how they translated from ANP score to sentiment labels. Furthermore, this approach could be improved upon since data- points are only labeled indirectly, due to the ANP pair that is associated with it, rather than due to the specific content of the image and text. That is why, our initial idea was to label the dataset on our own (Section 4.1.1) by labeling the images with the sentiment label the associated caption carries. As we discuss in Section 4.1.1, the generated la- bels were not reliable, requiring us to filter our 1. As in You et al. (2015), we experiment with sentiment image classification (Classi- cal classifier)per these two parts.1. In the first set of experiments, for classifi- cation, we use only image features as input. Within this set, there are two different label- ing approaches 2. In the second set of experiments, we also in- clude textual information as input.The Classical Classifier and Embedding Classi- fier networks that we created were relatively sim- ple feed-forward networks. The assumption for using these was that, since the image features are obtained from a very sophisticated network like AlexNet, these features would be sufficiently elab- orate to provide enough distinguishing power to our classifier networks.For this section, only features obtained from the image are used as input. The features used here are 4096-dimensional, obtained using AlexNet. tool, as compared to the previous one. This also proved to be much faster at generating labels.However, as is the case with any machine label- ing scheme, the labeled data contained quite a lot of noisy data. This also produced an overwhelm- ingly very large number of neutral labels (about 70%). In order to overcome this and have approx- imately equally balanced classes, the dataset was reduced to about 15.2K samples.Softmax classifier network (Classical Classi- fier)The first type of network we use is a feed-forward neural network, which takes as input a 4096- dimensional vector of image features obtained from the prerained AlexNet (section 2.4), and out- put a softmax layer, with every node representing a probability of a particular sentiment class 3 .We tried a variety of hidden layer architectures, and our experimental results are summarized in the sub-sections below.Fixed parameters (unless otherwise specified):• Data-points: approximately 5K per class• number of epochs = 20In this labeling approach, we use existing senti- ment prediction tools to examine the image's title and description fields, and generate labels based on these inputs. The first tool we tried for labeling the dataset was Stanford NLP sentiment predictor from the Stanford Core NLP Package 1 . This tool works by building a parse tree for each given sentence, and using the parse tree to generate sentiment scores/labels for the sentences (Richard Socher and Potts (2013) for details).The Stanford package assumes that the incom- ing sentences have proper punctuation and sen- tence structuring, while in our dataset even after the pre-processing most of the text fields repre- sent short phrases that lack proper punctuation and structure. Analysis of the produced labels made us disregard labels generated by this package and try Google Cloud Natural Language API 2 instead.This tool provides sentiment scores for given text. We manually set thresholds in the range of possible scores to distinguish and assign sentiment labels. It was easier to generate fairly accurate la- bels for each multi-sentence text input using this• batch size = 64• loss function = categorical cross-entropy• optimizer = stochastic gradient descent• learning rate = 0.001Experiments with 3 classes (positive, neutral, negative) are shown in Table 1.  Experiments with 2 classes (positive, negative) are shown in Table 2. 5 corpus 4 . In the embedding classifier we also used a feed-forward neural network, which takes a 4096-dimensional vector of image features gen- erated with the AlexNet as input and outputs a 50- dimensional projection vector associated with that particular label (by this, we mean the textual/word representation of the label). At validation/testing time, the label assigned is the one whose embed- ding is closest to this 50-dimensional output.We tried a variety of hidden layer architectures, and our experimental results are summarized in the sub-sections below. .633Fixed parameters:• Data-points: approximately 5K per class Table 2: Classical Classifier results with 2 classes In = Dimension of Input, Dns = Dense layer, D-out = dropout, S-max = Softmax, ReLU -Rectifier• number of epochs = 20• batch size = 64In both, the two class and three class classifica- tion schemes, it was observed that fitting the train- ing data was not difficult, with training accuracy almost always being above 95%, and often close to 100%. However, as the validation results show, the model overfits and fails to generalize. Attempts to fight overfitting with regularization, dropout in particular, did not prove to be useful. The reason for this could be the relatively small training data set we got after balancing the classes. The softmax classifier on 2 sentiment classes had an accuracy of .633. While not as accurate as we had hoped, this classifier was able to achieve accuracy better than the visual classifier implemented by You et al. (2016a), which also used the MVSO dataset, and achieved an accuracy of .616.This confirms our initial hypothesis that rela- tively simple feed-forward networks have enough capacity, given sufficiently good features, to fit a fairly complex training set, but generalizing is still a challenge.Embedding Classifier network Besides the Classical Classifier, which is trained to predict a set of classes, we trained a model to pre- dict a dense (vector) representation of sentiment label. The word embeddings for each label (pos- itive, neutral, negative) were obtained from the set of pre-trained word vector representations pro- vided by GloVe: Global Vectors for Word Rep- resentation ( Pennington et al., 2014). We used word embeddings of dimension 50 obtained from the model trained on Wikipedia 2014 + Gigaword• loss function = cosine proximity• optimizer = root mean square propagation• embedding similarity measure = cosine simi- larityExperimental results with 3 classes (positive, neutral, negative) are shown in Table 3.  Experimental results with 2 classes (positive, negative) are shown in Table 4.The results with the embedding-based classi- fier were similar to the softmax classifier (classical classifier): models overfitted the training set and  Table 4: Embedding Classifier results with 2 classes In = Dimension of Input, Dns = Dense layer, D- out = dropout, S-max = Softmax, ReLU -Rectifier could not generalize the validation data. This clas- sifier performed slightly better (accuracy: .617) than the visual classifier implemented by You et al. (2016a), also using the MVSO dataset (accuracy: .616).We hypothesized that the reasons for the mod- els not being able to generalize could be: a result of noise in the machine-labeled data, and small size of the training dataset. To remedy this, our next approach was to try a new labeling technique, which also yields a larger training dataset.These thresholds resulted in the equally- balanced classes.Note: in this approach, in addition to having as input 4096-dimensional feature vectors obtained from AlexNet, we have also, in some places, used raw pixel data of dimensions 32*32*3 from the images. The input type is specified when describ- ing the results.Softmax classifier network As was the case in 4.1.1, the first type of network we use is a feed-forward neural network, which takes as input a 4096-dimensional vector of image features, and has as output a softmax layer, with every node representing a sentiment class.We tried a variety of hidden layer architectures, and our experimental results are summarized in the sub-sections below.Fixed parameters (unless otherwise specified):• Data-points: approximately 38K per class• epochs = 40• batch size = 64• loss function = categorical cross-entropy• optimizer = stochastic gradient descentTo combat noise in the machine-labeled data, we tried labeling using adjective-noun pairs. This ap- pears to be a popular labeling technique, used in several related works. As was mentioned in Section 2.1 that every image in the MSVO dataset is associated with a adjective-noun pair (ANP) and a corresponding sentiment score.The sentiment scores fall in a range from -2.022 to 2.16. To obtain well-balanced classes and get well defined labels (high sentiment score is asso- ciated with highly positive label, and vice versa highly negative sentiment score is associated with the negative label; the neutral labels are associated with scores in the zero region) the following way:• learning rate = 0.001• momentum = 0.9• positive class has sentiment score from 2.16 to 0.035Experimental results with 3 classes (positive, neutral, negative) are shown in Table 5.Experimental results with 2 classes (positive, negative) are shown in Table 6.Embedding Classifier Similar to the 4.1.1, the second type of network we try is also a feed-forward neural network, which takes as input a 4096-dimensional vector of image features. The output is a projection layer of 50 nodes.We tried a variety of hidden layer architectures, and our experimental results are summarized in the sub-sections below.Fixed parameters (unless otherwise specified):• approximately 38K per class• neutral class has sentiment score from 0.034 to -0.034• batch size = 64• negative class has sentiment score -2.022 to -0.035• number of epochs = 10• optimizer = root mean square propagation  Table 5: Classical Classifier results with 3 classes In = Dimension of Input, Dns = Dense layer, D-out = dropout, S-max = Softmax, ReLU -Rectifier Table 6: Classical Classifier results with 2 classes In = Dimension of Input, Conv = Convolution layer, Dns = Dense layer, D-out = dropout, S-max = Softmax, ReLU -Rectifier• embedding similarity measure = cosine simi- larityExperimental results with 2 classes (positive, negative) are shown in Table 7.Image features extracted from AlexNet yielded better results compared to using the raw images. This is consistent with our expectations given how well AlexNet image features have performed on a range of image classification tasks. The co- sine proximity loss function was also significantly more effective than mean squared error whilst changes in other features had more minor ef- fects. Furthermore, the results obtained via this approach, which examined adjective-noun pairs associated with each image in order to assign la- bels, were no better than the previous machine- labeling approach. Once again, fitting to the train- ing data was not difficult, but achieving high accu- racy on the validation data was.At this point, we moved to combining visual in- formation with textual information.We used the 50 layer model for the ResNet and extracted features from the penultimate layer of this network. The features were of dimen- sion 2048. We preprocessed the images since the model takes an input image size of (224x224). So all the images were converted from (32x32) to the required dimension.To explore the contribution of images in a multi- modal sentiment classifier, we combined image features extracted from a pretrained ResNet neu- ral network with an LSTM model which used the title and description associated with an image to predict the sentiment.To generate our textual model, we experimented with a number of different LSTM architectures and hyperparamters. All text data was prepro- cessed in the same way. First the title and de- scription were concatenated with a period separat- ing the two components. Then we tokenized the resulting text, retaining punctuation but removing whitespace. Datapoints with more than 101 to- kens were truncated to reduce computation time and encourage learning by the model, since it re- duces the maximum history that could be stored in the LSTM hidden state. Practically, only a few datapoints have more than 100 tokens, (ap- proximately 700), however some examples were very long. Datapoints with fewer than 101 tokens were postpadded with zeros. Finally we sorted all words based on their frequency and converted each token to an integer, using an adaptation of Yasumasa Miyamoto's (a fellow student in the StatNLP class) code for creating a word dictio- nary from text files. Final vocabulary size is 103k words. External and internal links occurred quite .808often and were given special 'href' and 'rel' to- kens. Each of the LSTM models consisted of an embedding layer, which learnt a dense representa- tion of dimension 200 per token. All models were trained on a single AWS EC2 c4.4xlarge instance, and each model took 4 -6 hours to train, with some experiments running in parallel.We varied the number of layers, directional- ity of layers (single and bi-directional), size of the hidden state, number of training epochs, and optimizer. We found that increasing the size of the hidden layer and adding bi-directionality im- proved results, whereas adding more layers and adding dropout did not help. This suggests that the model had sufficient capacity and that any fur- ther improvements were likely to come from other sources such as more data or a better treatment of the input data. Our best classical classifier model had a single bidirectional layer with a hidden state size of 300, then a dense layer with a softmax acti- Since the LSTM models took a long time to train, instead of training the embedding model from scratch, we used our best classical model as the starting point, removed the softmax layer and added a linear projection layer to the 50 dimen- sional embedding space. All models were trained using stochastic gradient descent with a learning rate of 0.01. We found that a hinge loss trained for 10 epochs yielded the best results, and that adding complexity to the model in the form of ad- ditional dense layers before the projection layer significantly degraded performance. Our best re- sult achieved 80.8% on the validation set. Inter- estingly, projecting the results to embedded space added 1ppt to the model's performance. Further- more 80.8% significantly outperforms the textual baseline of 62.4% and even outperforms the sim- plest tree-LSTM combined visual-textual model from You et al. (2016a), which achieved 80.4%. a softmax activation function to give us our pre- dicted sentiment class. It achieved 80.8% (80.95% was our best observed result with this model but we were not able to replicate it) on the validation set, a slight improvement over the linear model. Given time constraints we restricted our experi- ments to two sentiment classes, positive and nega- tive. A full summary of our results is displayed in the table number 8.Finally, to project the model to embedding space, we used the output from the layer below the softmax of our best model. Unfortunately, our results were slightly below that of the classical classifier and our best text-only embedding model, with an accuracy of 79.8%. However this result still compares favourably with other reported com- bined models, outperforming the early, late fusion and CCR combined models, which had an accu- racy of 62.1%, 65.0% and 67.2% respectively. Ta- ble 9 contains a summary of our results.To combine both types of data, we extracted text features from our text model by using the output from the layer below the softmax layer. First we concatenated the text and image features and im- plemented a simple linear regression on top of the concatenated feature vector. This achieved an ac- curacy of 80.7% on the validation set compared to our best previous classical model result of 79.6%. Since the textual model was the best model, we wanted to find an architecture that would make use of this model mostly, but would use information from the image model when it was relevant. We tried to implement this by writing a custom gat- ing layer in Keras that had trainable weights. This layer simply multiplied the inputs with the weights elementwise, producing an output with the same dimensions as the input. We hoped that this would enable the network to learn what information to let through from the image and text features.Validation Accuracy loss=mse, 5 epochs .784 loss=cosine, 5 epochs .793 loss=hinge, 5 epochs .661 loss=mse, 10 epochs .786 loss=cosine, 10 epochs .798 loss=hinge, 10 epochs .781 Table 9: combined textual-visual model, embed- ding spacePractically we experimented with two types of gating layer, the first (GL1), had no constraints on the weight values. The second (GL2) constrained the weights to take on values between 0 and 1. Our best model applied GL1 to the textual features, compressed the visual features to a vector of di- mension 600 using a dense layer with a relu activa- tion, then applied GL2 to the visual features. Af- ter that, the results were concatenated, and dropout of 0.3 was applied before a final dense layer with Our best classical model achieved 80.8% accu- racy on the validation set and 80.9% on the test set. It is encouraging that our test set performance is slightly higher than our validation set perfor- mance, indicating that we are not overfitting the validation set. Our per class accuracy was ≈ 84% for the positive class, and ≈ 77% for the negative class. Whilst the accuracy per class is fairly bal- anced, it shows that our model is slightly better at identifying positive sentiment than negative senti- ment.Our best embedded model achieved 79.8% ac- curacy on the validation set and 78.6% on the test set. It also is not overfitting the validation data, but does have more imbalanced per class accuracy on the test set of ≈ 89% for the positive class and ≈ 68% for the negative class indicating that in the embedded space it is harder to classify negative sentiment.The best results along with the associated base- lines are shown in Appendix 1 Figure 6.To further investigate our results, we applied t-distributed stochastic neighbor embedding (t- SNE) to reduce dimensionality of our data. Ap- pendix 2 Figure 6 shows the result of our best Embedding classifier trained on 2 classes. The plot depicts, for each data point, the title of the folder in which the data point is contained (images with the same adjective-noun pair are grouped into the same folder), colored based on the true labels (green for positive, black for negative). The model was trained only on the text description of images and, interestingly, was able to cluster data points coming from the same folder. This aligns with our intuition that different folders should share similar, more nuanced, sentiments. It is particularly ex- citing that the model was able to recognize some of these clusters without having any explicit in- formation about them, and only being trained to output either the positive or negative word embed- ding. Some folder names are shown in the bold font meaning that data points from the same folder were plotted were close to each other in the vec- tor space. For example, we can see cluster of the following folders: "ecological disaster", "global protest", "wonderful nature", "beautiful day".ther explore under what circumstances the images contain more useful information than text data for the purposes of sentiment classification and how best to take advantage of this. Given more time we would have liked to use the output from our best image model instead of the ResNet features to generate our image features. Following this, a natural next step would be to a use a larger dataset and to use an image model which achieves closer to state of the art results as part of the combined model architecture.Plotting our datapoints in sentiment embedding space also yielded some interesting and intuitive results. Based on this it does seem that distances in sentiment embedding space have meaning, and that given more time and data it may be possible to classify an unknown sentiment class using this approach.
