In recent years, the development of deep neural models based on Re- stricted Boltzman machines (RBMs) pretraining has revitalised the use of artificial neural networks (ANNs) in automatic speech recog- nition (ASR) as well as in many other fields (see [1,2] for extensive reviews). A key factor that determines the usability of applications based on speech recognition is the latency or lag of the system. In dialogue systems, e.g., long latencies may disrupt the natural turn- taking in the human-machine conversation. In other specific applica- tions the lag may even be more critical. A typical example involves systems that use ASR to drive the lip movements of an avatar in real time to support telepresence [3,4,5]. The latency in a typical speech recogniser based on a hybrid be- tween Neural Networks (NNs) and Hidden Markov Models (HMMs) is determined by a number of factors:• the hardware (sound card) introduces some lag in digitising the speech samples and making them available to the drivers. Typical values are in the order of milliseconds; • the speech samples are returned by the driver in buffers of a certain size (this could be as long as half a second, but can be reduced to a few ms); • in spectral based feature extraction, speech samples are grouped into windows (frames) often around 25-40 ms in length;In [6] we used a hybrid of Recurrent Neural Networks (RNNs) and HMMs that was specifically designed for low latency process- ing. The feature extraction was based on Mel Frequency Cepstral Coefficients (MFCCs) without time derivatives and the RNN did not receive any future feature frame in input, thus limiting the latency of the RNN to the size of the feature extraction window. The purpose of that study, however, was limited to evaluating the effect of varying the look-ahead length of the Viterbi decoder in the system.Nearly all recent methods that use DNN based acoustic models for ASR employ symmetric context windows as input [7,8,9,10,11] and are therefore affected by a certain latency. In [12], time de- layed neural networks were used with asymmetric context windows in some cases. Similarly in [13] context windows with more frames on the left context were used. However, we are not aware of a sys- tematic and detailed investigation on the effect of the context win- dow asymmetry.In this study, we want to determine the relationship between la- tency of the DNN model and its performance. In order to do this, we analyse how the performance of the recogniser proposed in [8] varies as the alignment of the input context window is shifted back or forward in time. It is not our intent to report on state-of-the-art results, but to give and indication on the relative effects of shifting the input window. Also, we report results on Phoneme Error Rate (PER) on the TIMIT data, because we want to have precise control over shifts in time and therefore require carefully annotated data.For our experiments, we use a Context Dependent Deep Neural Net- work (CD-DNN) trained to estimate the posterior probabilities of a Output at t0Input Window ......Future Frames t0 Fig. 1. Illustration of the method: A sequence of 11 speech feature frames constitutes the input to the neural network. The context win- dow is not necessarily symmetric with respect to the current frame (t0). In the illustration a shift of -3 was applied.set of senones given a sequence of input feature vectors. The proba- bility estimations are then used in a Hidden Markov Model (HMM) in combination with a bigram phoneme-level language model for phonetic recognition.The set of senones and their alignment with the speech utter- ances in the dataset is determined by training a context dependent HMM recogniser based on Gaussian Mixture Models. The number of senones is reduced with decision tree based clustering.The DNN training procedure has been well described in [9]. The weights in the CD-DNN are initialised using a Deep Belief Network (DBN), that is, a stack of Restricted Boltzmann Machines (RBMs). The DBN is trained generatively by fitting the layers one at a time (greedily) by means of the contrastive divergence procedure. The final output layer in the CD-DNN is a generalised softmax (GSM) layer representing a distribution over senones. Given the genera- tive initialisation, the full model is fine-tuned with back-propagation training. The input to the model is a context window of n succes- sive frames of raw filterbank feature vectors. The feature vectors are normalised to zero mean and unit variance. We use filterbank fea- tures instead of MFCCs because they have been reported to achieve good results in combination with DNNs without the need for time derivatives that would increase the latency [14].The experiments are based on the KALDI and PDNN+KALDI recipes [16,8] and are performed on the TIMIT corpus. The standard 462 speaker training set was used for training. All SA utterances were removed to prevent bias due to the similarity of the utterances. The training set was further divided into 95% training and 5% val- idation set for regularisation during the back propagation training procedure. Results are reported on the 24-speaker core test set.The 40 channels filterbank features were computed using a 40 ms Hamming window with 10 ms increments. The inputs to the DNNs in our experiments are context windows of 11 consec- utive frames. This length of context window seems to be optimal for this application according to [7]. Also following the results in [7], we use a DNN with 4 hidden layers of size 1024. The softmax output layer represents the distribution of posteriors over 1984 dif- ferent senones (based on the 61 phonemes in the TIMIT standard phoneme set). The resulting topology of the network is, therefore, 440 × 1024 × 1024 × 1024 × 1024 × 1984. The learning rate is initialised to 0.08, and then dropped by half whenever the difference between the previous epoch and current epoch drops below a certain threshold.We optimised the acoustic scale in the decoder by running the decoder on the development set with 8 different scales. The scales were in the formThe optimal values for the acoustic scale were always contained between the extreme values we tested, suggesting that they correspond to real optima, see also Table 1 in Section 4. These optimal values were then used to decode the test data. The insertion penalty for our experiments was not op- timised. After decoding, the 61 phone classes are mapped to a set of 39 classes as in [17] for evaluation.Our baseline results correspond to the input window being cen- tered with respect to the current frame, with 5 context frames on either side. This corresponds, in our notation, to zero shift (see Fig- ure 2). A positive shift corresponds to a shift of context window in the future. We tested shifts from −10 to 10 with increments of one frame. Additionally, we tested shifts of −15 and −20 frames. For every shift value, the whole training and evaluation procedure is re- peated. It is interesting to notice that for shifts above +5 and below −5 the context window does not contain the current frame, and the recogniser will try to predict the current phoneme exclusively based on context.We use a Viterbi decoder to generate the phoneme sequences and a phoneme-level bigram model estimated on the training set. The acoustic scale used in the decoder to tune acoustic and language models was optimised for each test independently on the develop- ment test and the optimal value was then used on the test set. The de- coder follows the lattice generation and pruning approach described in [15].We used KALDI for feature extraction, selection of the senones and alignment of the senone transcriptions to the speech data. To speed up the deep neural networks training, we used two NVIDIA TI- TAN GTX GPUs. We also used the symbolic computations software Theano [18], which is well optimised to do symbolic algebra for GPUs. The generative training of the RBMs took about 3 minutes for one epoch over the entire training set, and the fine-tuning with back propogation took about 2 minutes for one full pass.Differently from previous studies [7,8,9,10,11], we vary the alignment of the context window with respect to the current frame (see Figure 1). We train a different recogniser for each alignment and we analyse its performance in terms of Phoneme Error Rate (PER) as a function of the window shift.The results are shown in Figure 2 for shifts between −10 and +10 and Table 2  input window shift, whereas the right plot details the different kind of errors (substitutions, deletions and insertions).The best PER of 22.0% (SD 3.8) occurs for a window shift of −2, i.e., for a window that is not symmetric around the current frame but has more past than future frames.The performance does not degrade varying the shift up to −5 frames (the current feature vector is still included in the input win- dow). The PER, however, starts increasing when the shift is beyond −5 frames and the network does not receive the current feature vec- tor in input. If we consider positive shifts for completeness, we can observe a similar behaviour, although the graph is not perfectly sym- metric and the PER for positive shifts is generally slightly higher than for negative shifts of the same amplitude.The acoustic scale used for decoding was optimised on the de- velopment set and then used on the test set. As a comparison, Ta- ble 1 shows the optimal values of the acoustic scale if optimised on the development and test set. In most cases, the same optimal value was obtained. In the cases when different values are obtained, the corresponding difference in % PER was no grater than 0.5%.Looking at the right plot in Figure 2, we can observe that the number of insertions is relatively constant with respect to the win- dow shift. The substitutions increase when the window is shifted with respect to the current frame, but the errors that vary the most with window shifts are deletions. Table 2 shows results for selected window shifts, including the extreme cases −20 and −15 that are not reported in Figure 2 for clarity of illustration. The performance for extreme shifts drops considerably and the degradation is mostly accounted for by deletions and substitutions.  Table 1. Acoustic scales optimised on the test and development set for each window shift.This study presents a systematic analysis of the effect of shifting the context input window in a CD-DNN+HMM phonetic recogniser with respect to the current frame. The goal is investigating the pos- sibility to reduce the latency of such a speech recogniser for applica- tions with specific requirements, but the results reported here are of general interest.Our results on the TIMIT database suggest that a context win- dow slightly shifted back in time is superior compared to the sym- metric context window used in most speech recognisers. However, the improvement in performance is small compared to the variabil- ity (standard deviation), and this observation should be confirmed by testing on other data sets.More interestingly, our results suggest that shifting the context window back in time up to 5 frames (50 ms) does not introduce no- ticeable degradation in the system performance. Larger shifts intro- duce a gradual but progressively steeper degradation. As a conse- quence, without modifying the ASR method in [8], we can reduce the latency of the system of at least 50 ms, without any degradation in performance. We can reduce the latency even more if some degra- dation can be tolerated by the application. This reduction in latency, although small in size, can potentially improve the usability of ASR in many applications, especially if latency is critical as in real-time lip synchronisation for telepresence.It is important to note that the insertion penalty was not opti- Shift % PER (SD) % SUB (SD) % DEL (SD) % INS (SD) -20 77.  Table 2. Recognition performance for selected window shifts mised in our experiments, and for all the different window shifts, the deletion error was always greater than insertion error. In future work, we will investigate if we can reduce the effect of window shift by optimising the insertion penalty for each shift. As in any study on speech recognition, the possibility to gener- alise our results outside the scope of phonetic recognition needs to be verified with specific tests. For example, it would be interesting to test if systems with longer time dependencies (lexical models and more complex language models), would be affected by the window shifts in a similar way. The GeForce GTX TITAN and TITAN X used for this research were donated by the NVIDIA Corporation. Giampiero Salvi is par- tially supported by the IGLU project (CHIST-ERA, Vetenskapsrådet 2015-06814).
