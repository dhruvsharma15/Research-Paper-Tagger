For large-scale tasks like image classification, the general practice in recent times [1] has been to train large Convolutional Neural Network (CNN) models. Even with large datasets, the risk of over- fitting runs high because of the large model size. As a result, strong regularizers are required to restrict the complexity of these models. Dropout [2] is a stochastic regularizer that has been widely used in recent times. However, the rule itself was proposed as a heuristic -with the objective of re- ducing co-adaption among neurons. As a result, it's behaviour was (and still is) not well understood. Gal and Gharamani [3] showed that dropout implicitly performs approximate Bayesian inference - making it a Bayesian Neural Net.Bayesian Neural Nets (BNNs) view parameters of a Neural Network as random variables rather than fixed unknown quantities. As a result, there exists a distribution of possible values that each parameter can take. By placing an appropriate prior over these random variables, it is possible to restrict the model's capacity and implicitly perform regularization. The theoretical attractiveness of these methods is that one can now use tools from probability theory to work with these models. What advantages do BNNs offer over plain Neural Nets? First, they inherently capture uncertainty - both in the model parameters as well as predictions. Second, they are ideal for learning from small amounts of data. Third, a Bayesian approach has the advantage of distilling complex assumptions about the model in the form of prior distributions.Inference over BNNs is typically intractable. As a result, one often uses approximations to the pos- terior distribution. MCMC and Variational Inference (VI) [4] are two popular methods for perform- ing these approximations. In recent times, VI has emerged as the preferred method of performing this approximation as it is scalable to large models. When using VI, it is common to assume in- dependence of model parameters. For Neural Networks, this assumption may seem unnecessarily stringent. After all, weights in a particular filter are highly correlated to produce specific patterns (an oriented edge, for instance). However, different filters in a CNN are more-or-less independent as they compute different features. In fact, it might even be advantageous to enforce independence of different filters through VI, as they reduce co-adaptation among features. In this work, we strive to enforce independence among features rather than weights.The overall contributions of the paper are as follows:• We derive a Bayesian approach to performing inference with neural networks. In doing so, we introduce a rich family of regularizers -Generalized Dropout (GD).• We perform experimental analysis with Dropout++, a set of methods under GD, to under- stand it's behaviour.• We perform experiments with Stochastic Architecture Learning, another set of methods under GD, and show that they can be used to select the width of neural networks.• We test Dropout++ on standard networks and show that it can be used to boost performance.In this section, we shall formally introduce the notion of BNNs and also discuss our proposed method. Let f (·; w) denote a neural network function with parameters w. For a given input x, the neural network produces y = f (x; w) a probability distribution over possible labels (through softmax) for a classification problem. Given training data D, the parameter vector w is updated using Bayes' Rule.After computing the posterior distribution, we perform inference on a new data point x as follows.Since the neural network produces a probability distribution over labels, P (y|x, w) = f (x; w)Computing the posterior from equation 1 is intractable due to the complicated neural network struc- ture. In Variational Inference, we define another distribution q(w), called the variational distribu- tion, which approximates the posterior P (w|D). This distribution is used instead of the posterior in equation 2 for inference.One common assumption when working with the variational distribution is the mean-field assump- tion, which requires thati This states that all parameters in w are independent. Such an assumption is certainly not true, especially at the feature-level, where parameters are highly correlated. Works like those of Denil et al. [5] explicitly show that parameters of a NN can be predicted given other parameters -hinting at the large amount of correlation present. Trying to enforce independence among such weights may end up having adverse effects.It is very difficult to overcome the independence assumption within the framework of VI. In the next section, we introduce an approach to overcome this difficulty.We add multiplicative gates after each feature / neuron in the neural network, as in Figure 1a. These gates modulate the output of each neuron. Let these new parameters be denoted by θ. Let us also assume that they lie in [0,1]. Intuitively, these gates now control the relative importance of each particular feature as viewed by the next layer. Such relative importance may be fundamentally uncertain given a particular feature. Hence, it may be useful to think of gate parameters θ as random variables. We shall now crystallize all these assumptions in the form of choices for the variational distribution.We first place the following prior-hyperprior pair over the gate parameters θ , and a prior over the regular parameters w.Note that the products are over all possible variables defined in the network. Here, k denotes the bernoulli parameters and also needs to be estimated along with θ, w. Now, given that we use vari- ational inference, let us now define the forms of the variational distributions q(w, θ, k; µ) we use.Note that even though we make an independence assumption on the weights w (equation 5), we overcome the disadvantages described in the previous section by effectively not being Bayesian with respect to w, using a delta distribution. Also note that we use the same parameter µ 2 for both distributions q 2 (.) and q 3 (.). While it is true that using different parameters for both distributions could make the formulation more powerful, we use the same parameter for simplicity. Now we write equations describing the variational approximation by using the definitions above.Our objective is now to solve equation 6. We observe that the exhaustive summation in equation 6 in intractable for large models. A popular method to deal with this is to use a Monte-Carlo approximation of the summation. However, even this may be infeasible for large models. As a result, we further approximate this with a single Monte-Carlo sample. In other words, we perform the following approximation:While this approximation seems to be drastic, we soon shall see that Classic Dropout also implicitly performs the same approximation.Given all the assumptions and approximations discussed above, we now write the complete objective function we aim to solve. Since the variational distributions for w and k are delta distributions, we shall now use w, k instead of µ 1 , µ 2 in our notations, for simplicity.In the expression above, we have used the fact that P (k; α, β) is a beta distribution. This form of the objective function 7, with gates θ, k constitutes the Generalized Dropout regularizer.Let us now briefly look at the behaviour of the beta distribution at various values of α, β, as shown in Figure 1b. We shall refer to each of these specific cases as different versions of Dropout++. For reasons to be discussed later, we shall refer to the last case as Stochastic Architecture Learning (SAL). • Dropout++ (0.5), where α = β &gt; 1: k = 0.5 is the most probable value of k.• Dropout++ (flat), where α = β = 1: All values of k are equally probable.• Dropout++ (1), where α = 1, β &gt; 1: k = 1 is the most probable value of k.• Dropout++ (0), where α &gt; 1, β = 1: k = 0 is the most probable value of k.• SAL, where α &lt; 1, β &lt; 1: k = 0 and k = 1 are the most probable values of k.Note that Dropout++ (0.5) becomes indistinguishable from Classic Dropout (with 0.5 Dropout rate) at α = β → ∞. To obtain other Dropout rates, we simply ensure α = β. In the next section, we shall discuss another algorithm called Architecture Learning, and how it relates to the SAL method above.Srinivas and Babu [6] recently introduced a method to learn the width and depth of neural network architectures. They also add additional learnable parameters similar to gates. Also, their objective function has the following form in our notation.Note that our objective function 7 looks very similar to this when α, β &lt; 1 and α &lt; β, except that we use log k instead of k. Another difference is that they use a heaviside threshold to select θ rather than sampling from a bernoulli. We observe that this is equivalent to taking a maximum likelihood sample from the bernoulli distribution. Given these similarities, we found it apt to name the corresponding method with α, β &lt; 1 as Stochastic Architecture Learning, as it is a stochastic version of the algorithm described above.Most surprisingly, we find that the motivation to arrive at this algorithm was completely different -they intended to minimize the number of neurons in the network. We arrive at a very similar formulation from a purely Bayesian perspective.In this section, we shall attempt to provide an intuitive explanation for Generalized Dropout. Going back to Fig. 1a, each neuron is augmented with a gate which learns values between 0 and 1. This is enforced by our regularizers and well as by parameter clipping. During the forward pass, we treat each of these gate values as probabilities and toss a coin with that probability. The output of the coin toss is used to block / allow neuron outputs. As a result of the learning, important features tend to have higher probability values than unimportant features.At test time, we do not perform any sampling. Rather, we simply use the real-valued probability values in the gate variables. This approximation -called re-scaling -is used in classical Dropout as well.What do the different Generalized Dropout methods do? Intuitively, they place restriction on the gate values (probabilities) that can be learnt. As an example, Dropout++ (0) encourages most gate values to be close to 0, with only a few important ones being high. On the other hand, Dropout++ (1) encourages gates values to be close to 1. Intuitively, this means that Dropout++ (0) restricts the capacity of a layer by a large amount, whereas Dropout++ (1) hardly changes anything. SAL, on the other hand, encourages neurons to be close to either 0 or 1. In contrast to other methods, SAL produces neural network layers that are very close to being deterministic -neurons close to 0 are almost never 'on' and those close to 1 are almost always 'on'. Dropout++ (flat) is also unique in the sense that it doesn't place any restriction on the gate values. As a result, we do not require to set any hyper-parameters for this method. From a Bayesian Perspective, when we have no prior beliefs on what the gate values should be, we use the most non-informative prior -which is Dropout++ (flat) in this case.Dropout++ (0.5) encourages values to be close to 0.5. If the regularization constants are increased, then gate values other than 0.5 are penalized more and more heavily. In the limiting case we get Dropout, where any deviation from probability value of 0.5 is "infinitely" penalized.Given our formalism of stochastic gate variables, it is unclear how one might compute error gra- dients through them. Bengio et al. [7] investigated this problem for binary stochastic neurons and empirically verified the efficacy of different solutions. They conclude that the simplest way of computing gradients -the straight-through estimator works best overall. This involves simply back-propagating through a stochastic neuron as if it were an identity function. If the sampling step is given by θ ∼ bernoulli(k), then the gradient dθ dk = 1 is used. Another issue of consideration is that of ensuring that k always lies in [0,1] so that it is a valid bernoulli parameter. Bengio et al. [7] use a sigmoid activation over k. Our experiments showed clipping functions worked better. This can be thought of as a 'linearized' sigmoid. The clipping function is given by the following expression.The overall sampling function is hence given by θ ∼ bernoulli(clip(k)), and the straight-through estimator is used to estimate gradients overall.Here we shall discuss how to apply this to convolutional layers. Let us assume that the output feature map from a convolutional layer is k × k × n, i.e; n feature maps of size k × k. Classical dropout samples k × k × n bernoulli random variables and performs pointwise multiplication with the output feature map. We follow the same for Generalized Dropout as well.However, if we wish to perform architecture selection like Architecture Learning [6], we need to select a subset of the n feature maps. In this case, we only have n gate variables, multiplying to the output of each feature map. When a gate is close to zero, and entire feature map's output becomes close to zero at test time. By selecting few feature maps out of n, we determine which of the n filters in the previous layer are essential.There are plenty of works which aim to extend Dropout. DropConnect [8] stochastically drops weights instead of neurons to obtain better accuracy on ensembles of networks. As stated earlier, using the independence assumption for weights may not be correct. Indeed, DropConnect is shown to work on only fully connected layers. Standout [9] is a version of Dropout where the dropout rate depends on the output activations of a layer. Variational Dropout [10] proposes a Bayesian inter- pretation for Gaussian Dropout rather than the canonical multiplicative Dropout. By considering multiplicative Dropout, we make important connections to Architecture Learning / neuron pruning. Gal and Gharamani [3] showed a Bayesian interpretation for binary dropout and show that test per- formance improves by performing Monte-Carlo averaging rather than re-scaling. For simplicity, we use the re-scaling method at test time for Generalized Dropout. Our work can be seen as an extension of this work by considering a hyper-prior along with a bernoulli prior.Hinton and Van Camp [11] first introduced variational inference for making Neural Networks Bayesian. Recent work by Graves [12] and Blundell et al. [13] further investigated this notion by using different priors and relevant approximations for large networks. Probabalistic Backpropaga- tion [14] is an algorithm for inferring marginal posterior probabilities for special classes of Bayesian Neural Networks. Our method is different from any of these methods as they are all Bayesian over the weights, whereas we are only Bayesian with respect to the gates.In this section, we perform experiments with the Generalized Dropout family to test their usefulness. First, we perform a wide variety of analysis with the Generalized Dropout family. Later, we study some specific applications of this method. We perform experiments primarily using Theano [15] and Lasagne.We shall now analyze the behaviours of different members of Generalized Dropout family to find out which ones are useful. For the experiments on the MNIST dataset, we use the standard LeNet-like architecture [16], which consists of two 5 × 5 convolutional layers with 20 and 50 filters, and two fully connected layers with 500 and 10 (output layer) neurons. While there is nothing particularly special about this architecture, we simply use this as a standard net to analyze our method.We investigate whether Generalized Dropout indeed has any advantage over Dropout in terms of accuracy. Here, we apply Dropout and Generalized Dropout only to the last fully connected layer. Our experiments reveal that for the network considered, the accuracies achieved by any Generalized Dropout method are not always strictly better than Dropout, as shown in Figure 2a. This indicates that most of the regularization power of Dropout comes from the independence assumption of Vari- ational Inference, rather than particular values of the dropout parameter. This is a surprising result which we shall use to our advantage in the paper.However, we note that for small data-sizes, Dropout++ (0) seems to be advantageous over Dropout (Figure 2b). This is possibly because Dropout++ (0) forces most neurons (but not all) to have very low capacity due to low value of the parameters. Inspired from the above results about Dropout++ (0), we look at the relationship between using different layer-widths for the fully connected layer and the learnt gate parameters. Intuitively, it is natural to assume that larger layers should learn lower gate values, whereas smaller layers should learn much higher values, if we wish for the overall capacity of the layer to remain roughly the same. Our experiments confirm this intuition as shown in Figure 2c.  Table 1: Architecture selection capability of our method on a LeNet-like baseline. The first row is the architecture and performance of the baseline LeNet network. We see that larger β/α ratios result in smaller networks at the cost of network performance.We also test if this flexibility translates to higher accuracy numbers over a fixed dropout value, and we find this to be indeed the case. We find that for small layer-widths, Dropout (at p = 0.5 for example), tends to remove too many neurons, while Dropout++ adjusts it's parameter values to account for small layer-widths, as shown in Figure 2d. ResNet-32 [17] 7.46 6.73 6.8 - - ResNet-56 [17] 6.75 6.1 6.18 - - GenericNet [18] 6.86 6.6 6.75 6.32 6.46 Table 2: Applying Dropout++ after each layer in standard networks decreases error rate (%).Dropout++ learnt values (with init = 1.0) are close to 0.98. As a result, Dropout at p = 0.98 per- forms similar to this learnt value. With Dropout++ init = 0.5, the learnt values are close to p = 0.8. For ResNets, small values of Dropout makes training difficult.Initialization of good parameters is known to play a key-role in generalization of deep learning systems. To test whether this holds for the newly introduced Generalized Dropout parameters as well, we try different initializations of the Generalized Dropout parameters. In this example, we simply initialize all gates to a single constant value. As expected, we find that the choice of this initialization is much less crucial when compared to setting the Dropout value, as shown in Figure  2e.The choice of initialization, however, affects training time. As an example, it is empirically observed that Dropout with p = 0.1 is much slower than p = 0.9. Therefore, it is helpful to have higher Dropout rates to facilitate faster training. To help faster training in Dropout++, we simply initialize with p = 1.0, i.e; start with a network with no Dropout and gradually learn how much Dropout to add. We observe that this indeed helps training time and at the same time provides the flexibility of Dropout, as shown in Figure 2f.Until this point, we have focussed on using Generalized Dropout on the fully connected layers. Sim- ilar effects hold when we apply these to convolutional layers as well. Here, we visualize the learnt parameters in convolutional layers. First, we add Dropout++ only to the input layer. The resulting gate parameters are shown in Figure 2g. We observe a similar effect when we add Dropout++ only to the first convolutional layer, as shown in Figure 2h, which shows the average gate map of all the convolutional filters in that layer. In both cases, we observe that Dropout++ learns to selectively attend to the centre of the image rather than towards the corners.This has multiple advantages. First, by not looking at the corners of each feature, we can potentially decrease model evaluation time. Second, this breaks translation equivariance implicit in convolu- tions, as in our case certain spatial locations are more important for a filter than others. This could be helpful when using CNNs for face images (for example), where a filter need not look for an "eye" everywhere in the image. Such locally connected layers have been previously used in works such as DeepFace [19]. Dropout++ could offer a more natural way to incorporate such an assumption.We shall now attempt to use Stochastic Architecture Learning (SAL) to automatically learn the required layer width of the network. The inherent assumption here is that the initial architecture is over-complete, and that a sub-set of neurons is sufficient to get similar performance. We first learn the parameters of the network using SAL regularizer, later we prune neurons with low gates parameters. Figure 2i shows that SAL learns gate parameters that are often close to either 0 or 1, resulting in a much sharper rise compared to the other methods. We use this sharp rise as a criterion to select the width of a layer. We observe that varying the β/α parameter encourages the method to get smaller architectures, sometimes at the cost of accuracy, as shown in Table 1.So far we have studied the various properties of Generalized Dropout by performing various ex- periments on LeNet. We shall now shift to larger networks to test the effectiveness of Dropout++. Modern networks mainly use dropout only in the fully connected layers, or simply not at all, ow- ing to much powerful regularizers such as Batch Normalization. Here we shall take such networks, simply add Dropout++ (flat) after each layer, and see if we get an increase in accuracy. We per-form experiments with ResNet32, ResNet56 and a Generic VGG-like network, all trained on the CIFAR-10 dataset. As in Table 2, we see that for all three models, adding Dropout++ is largely helpful.We have proposed Generalized Dropout, a family of methods that generalize Dropout-like be- haviour. One set of methods in this family, Dropout++, is an adaptive version of Dropout. Stochastic Architecture Learning is another set of methods that performs architecture selection. An uninformed choice of the Dropout parameter usually hurts performance. Dropout++ helps in setting a useful pa- rameter value regardless of factors such as layer width and initialization. Experiments show that it is generally beneficial to simply add Dropout++ (flat) after every layer of a Deep Network.
