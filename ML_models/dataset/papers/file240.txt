Most of the recent work on deep learning has focused on ways to regularize network behavior to avoid over-fitting. Dropout [8] has been widely accepted as an effective way for deep network regularization. Dropout was initially pro- posed to avoid co-adaptation of feature detectors, but it turns out it can also be regarded as an efficient ensemble model. The maxout network [7] is a newly proposed micro architecture of deep networks, which works well with the dropout technique. It sets the state-of-the-art performance on many popular image classification datasets. In retro- spect, both methods follow the same approach: they restrict updates triggered by a training sample to affect only a sparse sub-graph of the network. We note that neural science re- searchers have come up with what can be perceived as a similar principle from years of study of the human brain: It is not the shape of the signal, but the pathway along which the signal flows, that determines the functionality [10]. We use this principle as an important clue to explore the encod- ing capabilities of deep networks.In this paper we provide a new insight into a possible reason for the success of maxout, namely that it partially takes advantage of what we call "sparse pathway coding", a much more robust way of encoding categorical information than encoding by magnitudes. In sparse pathway encoding, the pathway selection itself carries significant amount of the categorical information. With a carefully designed scheme, the network can extract pattern-specific pathways during training time and recognize the correct pathway at inference time. Guided by this principle, we propose a new class of network architectures called "channel-out networks". Un- like the maxout network, this type of networks does not only form sparse input pathways a posteriori, but it also actively selects outgoing pathways. We run experiments on channel-out networks using several image classification benchmarks, all of which show similar or better perfor- mance results compared with maxout. In fact, the channel- out network sets new state-of-the-art performance on some image classification datasets that are on the "harder" end of the spectrum (i.e., those with lower current state-of-the- art performance), namely CIFAR-100 and STL-10, demon- strating its potential to encode large amounts of informa- tion with higher level of complexity. Channel-out is just one example of using the sparse pathway encoding princi- ple, which we believe is a promising research direction for designing other types of deep network architectures.In this section we briefly review the concepts of dropout [8] and maxout [7], and introduce the concept of sparse pathway encoding.Dropout regularizes the network during the training phase by randomly crossing out some of the nodes upon the processing of each training sample. Therefore it restricts the updates to happen only along a relatively sparse sub- network. As has been pointed out in a number of papers [8,18,20], dropout networks can be regarded as perform- ing efficient model averaging of a large ensemble of models randomly sampled from the original network, so that each model learns a different representation of the data.The maxout network is a recently proposed micro- architecture that is significantly different from traditional networks: the activation function does not take a normal single-input-single-output form, but instead the maximum of the activations from several candidate nodes. In [7], the advantage of maxout over normal differentiable acti- vation functions (such as tanh) was attributed to its better approximation to the exact model averaging, and the ad- vantage of maxout over the rectified linear activation func- tion was attributed to easier optimization on maxout net- works. Here we propose another insight of the power of the maxout network. The maxout node activates only one of the candidate input pathways depending on the input, and when the gradient (the novel information discovered from the current training sample) is back-propagated, only that selected pathway is updated with the information, which it encodes (Figure 1). Therefore the information conveyed by each training sample is compactly encoded onto a local por- tion of the network, instead of being disseminated across the entire network. We call this behavior as "sparse path- way coding". Moreover, due to the local invariance of the max-linear function, the network can "remember" the cor- rect pathway selection for the same or similar test sample. At inference time, if a test sample is similar to a certain pattern encoded, then it is more likely to activate a path- way that is similar to the one activated by this pattern. Cor- rect inference can therefore be made with awareness of that pattern-specific pathway.To sum up, The maxout network encodes information sparsely on distinct pathways, and has the capability of re- trieving the correct pathway at inference time. This obser- vation is well in line with the famous principle in neural science, "The information conveyed by an action potential is determined not by the form of the signal but by the path- way the signal travels in the brain" [10]. max-out node winner max(·) Figure 1: Sparse pathway coding by maxout networks. Ef- fective information flows only along the red links.link selection is ever made. Higher layer pathway selections are not aware of the selections made in lower layers. A nat- ural extension is to endow the network with the capability of active output pathway selection, which may result in a more efficient pathway coding. Along this line, we propose a class of new deep network architectures called "channel- out networks". A channel-out network is characterized by channel-out groups (Figure 2). At the end of the linear portion in a typical layer (can be fully connected, convolutional, or lo- cally connected), output nodes are arranged into groups, and for each group, a special channel selection function is performed to decide which channel opens for further in- formation flow. Only the activation of the selected chan- nel is passed through, all other channels are blocked off (this is realized by masking the corresponding channel out- put by 0). When the gradient is back-propagated through this channel-out layer, it only passes through the open channels selected during forward propagation. Formally, we first define a vector-valued channel selection function f (a 1 , a 2 , ..., a k ) which takes as input a vector of length k and outputs an index set of length l (l &lt; k). Elements of the index set are selected from the domain {1, 2, ..., k}. Then with an input vector (typically the previous layer output) a = (a 1 , a 2 , ..., a k ) ∈ R k , a channel-out group implements the following activation functions:where I(·) is the indicator function, i indexes the candidates in the channel-out group, a i is the i th 3. Channel-Out Networks 3.1. Pushing the sparse pathway concept further candidate input, and h i is the output (Figure 2). There are many possible choices of the channel selection function f (·). To ensure good per- formance, we require that the channel selection function possesses the following properties:Although maxout networks implement the concept of sparse pathway coding, the pathways are formed "a pos- teriori", meaning that a pathway selection is only implicitly implemented with already-formed input paths, but no active• The function must be piece-wise constant, and the piece-wise constant regions should not be too small. Intuitively, the function has to be "regular enough" to ensure robustness against the noise in the data.   • The pre-image size of each possible index output must be of almost the same size. In other words, each chan- nel in the channel-out group should be equally likely to be selected as we process the training examples (so that the information capacity of the network is fully utilized).(a) Automobile (b) Frog• The computation cost for evaluating the function must be as low as possible. The characteristics of a channel-out network is better il- lustrated through a comparison with the maxout network ( Figure 3). In a maxout network, a maxout node only takes the maximum value of the candidates, without the outgoing links being aware of the index. In contrast, in the channel- out network, the channel-out group knows about the index of the open channel, and output link selection is made possi- ble by this extra piece of information. In summary, the main characteristic of a channel-out network is that a channel-out group uses channel selection information to determine fu- ture inference pathways.In a deep convolutional network for 2D image classifica- tion, channel-out groups can be implemented by perform- ing the channel selection function across groups of feature maps after convolution/multiplication. Similar to maxout networks, channel-out can be regarded as a special kind of cross-feature pooling. Moreover, in addition to the chan- nel output value, h f (a) , the index vector itself f (a) should also be recorded into an index matrix, so that the open path- way can be retrieved during back-propagation. We can further take advantage of the index vector to implement sparse convolution and sparse matrix multiplication, avoid- ing wasteful computations of multiplications by zeros. This can potentially make the computation very fast. Training a channel-out network that has a similar number of parame- ters as a maxout network, assuming a scalar channel selec- tion function, can in theory be done k times faster than the maxout network, where k is the size of channel-out/maxout groups. We are currently working on this fast implementa- tion.To confirm that pathway selection is indeed indicative of the patterns in the data, we record the pathway selec- tions of a well-trained maxout model and a channel-out model (with max(·) channel selection function) using the CIFAR-10 dataset. For ease of visualization and analysis, we set the size of maxout/channel-out groups to 2, so that we can use 0/1 to represent the pathway selection at each maxout/channel-out group. Figure 4 shows the channel-out pathway patterns of two classes, where each row records the pathway selections of all channel-out groups (only part of them are shown in the window) when performing infer- ence on a specific test sample. We can clearly observe many class-specific string patterns, indicating that pathway selec- tion patterns indeed subsume important clue of categoriza- tion. Some of the salient patterns have been marked with red rectangles.To better visualize the space of pathway patterns, we perform PCA analysis on the pathway pattern vectors and project them into the three dimensional space. Figures 5  and 6 show the results for channel-out and maxout, respec- tively. We can see that clusters have been well formed.  5   25  airplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck   20   airplane  automobile  bird  cat  deer  dog  frog  horse  ship  Theorem: Any piece-wise continuous function defined on a compact domain in Euclidean space can be approx- imated arbitrarily well by a max(·) two-layer channel- out network with one hidden channel-out group with fi- nite number of candidate nodes. That is, denote the tar- get function to be approximated as T (x), then ∀� &gt; 0, we can find a two-layer channel-out network with one hid- den channel-out group that implements a functionHere by a "piece-wise con- tinuous function" we refer to a function that is constituted of finite number of continuous segments.The proof is straightforward and is not of much technical significance. It is listed in the Appendix section. 3.3. Channel-out network does more efficient path- way switching Although these clusters are not perfect, they still demon- strate that considerable amount of categorical information has been encoded by pathway patterns. We can also see that the channel-out model results in better clusters than those generated by maxout. For example, the frog class (yellow stars) is better separated from the other classes in the channel-out case. Another interesting observation is that, while the channel-out and maxout models have been trained independently with completely random parameter initialization, the relative 3D positions of the classes in the visualization show very similar spatial structures. This im- plies that the pathway pattern provides a very robust repre- sentation of the underlying patterns in the data.In the next few sub-sections we argue from different per- spectives why the channel-out network manifests the con- cept of sparse pathway selection better than maxout, and why it can potentially exhibit better performances on clas- sification tasks.Universal representation/approximation property is a good indication of the expressive power of a model. The fact that a two-layer circuit of logic gates can represent any boolean function [14] implies the nonlinear expressive power of early shallow neural networks. Existence of func- tions that are computable with polynomial-size networks with k layers while requiring exponential-size networks with k − 1 layers implies the benefit of using deep archi- tectures [1]. More recently, the fact that maxout networks are universal approximator of any continuous function is an indication of its good performance [7]. Channel-out net- works also have strong universal approximation properties. Take the max(·) channel selection function as an example, we can prove the following theorem:Local learning behaviors on maxout and channel-out net- works can be categorized into two classes: activation tuning (mild change) and pathway switching (critical change). Ac- tivation tuning happens when the desired parameter update is not significant enough to change the pathway selection, otherwise pathway switching occurs. Channel-out and max- out networks show very different behaviors upon pathway switching. We discuss it under a simple scenario in which we repeatedly present the same training sample to the net- work. For a maxout node, when pathway switching hap- pens, the gradient propagated to the maxout node must be to decrease the activation. However, the effective activa- tion decrease will be thresholded by the activation level of the second largest candidate activation, which slows down, but does not steer away the decreasing trend (we are as- suming that the network structure above this maxout node is relatively invariant across different presentations of the training sample). Under this scenario, updates will typically alternate between candidate pathways, reducing the overall efficiency of updates. In contrast, for a channel-out net- work, whenever a pathway switches in a channel-out group, the effective structure above the channel-out group is dras- tically changed due to the distinct output links selected by the channel-out group, resulting in significant change in the desired direction of the activation update. In other words, when current pathway is too far from fitting the data well, the channel-out group tends to switch and start training a new pathway from scratch. Clearly this is a more desired behavior for pathway switching.In this section we consider different principles under- lying sparse pathway methods (channel-out/maxout) and dropout in terms of regularizing network training. We argue that the strengths of sparse pathway selection and dropout complement each other. This explains the main reason that sparse pathway methods, when combined with dropout, outperforms traditional neural network models. We believe that sparse pathway encoding is a general direction that is well worth pursuing in future research for designing deep network architectures. Maxout and channel-out networks are just two examples along this direction. Recall that dropout, with each presentation of a training sample, samples a sub-network and encodes the informa- tion revealed by the training sample onto this sub-network. Since the sampling of data and sub-networks are indepen- dent processes, in a statistical sense the information pro- vided by each training sample will eventually be "squeezed" into all these sub-networks (third row of Figure 7).The advantage of such scheme, as has been pointed out in various papers [8,18,20], is that the same piece of in- formation is encoded into many different representations, adding to the robustness at inference time. The side-effect, which has not been highlighted before, is that encoding conflicting pieces of information densely into sub-networks with small capacities causes interference problem. Data samples of different patterns (classes) attempt to build dif- ferent, maybe highly conflicting network representations. When the sub-network is not large enough to hold all the information, conflicting parts tend to cancel each other, re- sulting in significant information loss. An extreme case is the practice of training several networks independently (on the same dataset) and combine them by naive averaging of the link weights, which clearly does not make much sense in general.Note that although dropout is disseminating the training set information over the entire network, it is significantly different from a normal deep network without any regu- larization. For the latter, retrieval of information relies on overall cooperation of the entire network, where each sub- network only encodes PART of the information (second row of Figure 7). In contrast the dropout network tries to en- code the entire information in EACH sub-network. Sub- networks interact with each other more by averaging (or voting), rather than by cooperation.Dropout + Channel-out/Maxout encoding pattern Figure 7: Information encoding patterns. Each bin of the network capacity represents a certain size sub-network. Dropout tends to encode all patterns to each capacity bin, re- sulting in efficient use of network capacity but high level of interference; Sparse pathway methods tend to encode each pattern to a specific sparse sub-network, resulting in least interference but waste of network capacity; The best ap- proach is the combination of the two schemes.In contrast to dropout, sparse pathway regularization methods tend to encode information in a more specialized way. Each pattern of the training data is likely to be en- coded onto one or a few specialized sub-networks (for the simplest case, readers can consider each class as of one pat- tern). This is illustrated in the fourth row of Figure 7.Clearly, sparse pathway methods mitigate the interfer- ence problem caused by dropout. The problem with pure sparse path way regularization, however, is that network capacity could be under-utilized. Since sub-networks are highly pattern-specific, and there are only a few fundamen- tal patterns in the data, only a small subset of the entire net- work is selected and trained, leaving the rest of the network capacity in an "idle" state. This is a waste of the network capacity. Now it is not hard to understand why combining dropout and sparse pathway encoding could result in bet- ter performance: the combination takes advantage of the strengths of both methods to do more efficient information coding, as illustrated in the last row of Figure 7.Notice that encoding data pattern(s) into sparse path- ways is also different from encoding partial patterns into sub-networks, as is done in a normal network without reg- ularization. In a non-regularized network, the information encoded on a sub-network is not likely to form complete patterns, while in sparse pathway methods a certain sub- network always tries to encode complete patterns. Here we note also the significant difference between sparse pathway methods and standard sparse feature learning techniques [3,13,16], which is that a sparse pathway model must also possess the ability of RECOGNIZING the correct pathway during inference time.An empirical observation in support of our arguments about dropout/sparse pathway encoding pattern is seen when we take extremes of either of the regularization di- rections in our experiments. We observe that when dropout rate is set too high (number of alive nodes too few for each training sample), the dominating problem is under-fitting, where we see both training and test set precision cease to improve at very early epochs. When no dropout is applied, the dominating problem is over-fitting, where we see train- ing set precision growing very fast while test set precision is not catching up with the improvement of training set fitness. This is in line with our major assumption about deficiencies of each method: dropout causes interference, while pure sparse pathway encoding forgoes using redundant network capacity to enhance robustness, thus is more vulnerable to over-fitting.Precision Maxout+Dropout [7] 88.32% Channel-out+Dropout 86.80% CNN+Spearmint [17] 85.02% Stochastic Pooling [21] 84.87% Table 1: Best methods on CIFAR-10In this section we show the performance of the channel- out network on several image classification benchmarks. For all the channel-out networks used in our experiments, the channel selection function is the max(·) function. We run tests on CIFAR-10, CIFAR-100 [11] and STL-10 [4], significantly outperforming the state-of-the-art results on the last two datasets.Our implementation is built on top of the efficient convo- lution CUDA kernels developed by Alex Krizhevsky [11]. Most experiments are done using a Tesla C2050 GPU. Due to the time limit, we had to control the size of our networks, and we could only put a limited effort on optimizing the hyper-parameters (totally relying on manual trials). Despite of these constraints, we still managed to get state-of-the-art performance on CIFAR-100 (63.4%) and STL-10 (69.5%), indicating the great potential of channel-out networks on difficult classification tasks. We believe that we can fur- ther improve our current result of channel-out networks on CIFAR-10 (86.80%) if parameters are better tuned.We did not put our effort on developing optimized channel-out networks for easier image classification tasks, such as MNIST [12] and SVHN [15] (digit images) since we believe these tasks will not demonstrate the strengths of channel-out networks, which is better at encoding a large amount of highly variant patterns.The CIFAR-10 dataset [11] consists of 32 × 32 × 3 small color images of 10 object classes. There are 5000 train- ing images and 1000 test images for each class. There are significant variations in the images regarding shape and pose, and the images are not well centered. Previous ex- periments on CIFAR-10 typically take two methodologies: with or without data augmentation. To better compare the generalization power of different models, we focus on the non-augmented version, namely using the whole images without any translation, flipping or rotation. However, we did perform ZCA whitening using the make cifar10 gcn whitened.py code in the pylearn2 repository [6].The network used for CIFAR-10 experiment consists of 3 convolutional channel-out layers, followed by a fully con- nected channel-out layer, and then the softmax layer. The best model has 64-192-192 filters for the corresponding convolutional layers, and 1210 nodes in the fully connected layer. Each convolutional layer consists of a linear convolu- tion portion, a max pooling portion and the channel-out por- tion. The fully-connected layer has a fully-connected linear portion and a channel-out portion. The channel-out group sizes are set as 2-2-2-5 for corresponding layers. Dropout with probability 0.5 is applied to the inputs of layers 2,3,4, and dropout with probability 0.2 is applied to the input of the first layer -the image itself. Here we would like to thank Goodfellow, etc. for providing an opensource for the max- out code in pylearn2, since many of our hyper-parameters for the CIFAR-10 dataset were tuned with the help of the .yaml configuration files in the pylearn2 repository.The best test precision of the channel-out network is 86.80%, which is not as good as the state-of-the-art reported in [7] using the maxout network (88.32%), but is better than any of the other previous methods (as far as we know). The best results on CIFAR-10 without data augmentation are summarized in Table 1. We believe that the channel-out performance could be further improved if we use a larger network and the hyper-parameters are better tuned.Aware of the fact that the performance can be highly de- pendent on hyper-parameter settings, we implemented our own code for maxout networks and compared them with our model using two different control settings: similar num- ber of parameters or similar number of feature maps. For the former case, the maxout network is of size 96-256-256- 1210 (number of filters/feature maps in each layer). A max- out network of this size has a similar total number of pa- rameters (weights) as our channel-out network. For the lat- ter case, it is of size 64-192-192-1210, i.e. having exactly same number of feature maps as the channel-out network. Note that this results in significantly less parameters in the maxout network. The results are shown in Table 2.We can see that the channel-out network has nearly same performance as a maxout network with similar number of parameters, but is significantly better than a maxout net- work with same number of feature maps. We argue that the second comparison is reasonable in the sense that although having different numbers of parameters, their implemen- tations require the same number of multiplication/addition  Precision Channel-out+Dropout 63.41% Maxout+Dropout [7] 61.43% Stochastic Pooling [21] 57.49% Receptive Field Learning [9] 54.23%  operations, resulting in a similar training time. The faster version of the channel-out network is still under construc- tion, which will take advantage of the structured sparseness of outputs of channel-out groups. We predict that it will be at least significantly faster than the maxout network with a similar number of parameters.Figure 9: Example images in STL-10 and CIFAR-10 datasets: STL-10 dataset has more variations.The CIFAR-100 dataset [11] is similar to CIFAR-10, but with 100 classes. There are 500 training images and 100 test images for each class. Both the larger number of labels and the smaller training sets make the task much more difficult than the CIFAR-10 dataset.The channel-out network developed for the CIFAR-100 has 3 convolutional layers followed by a fully connected layer and the softmax layer, with feature maps 80-176-176- 1210. The channel-out group sizes are 2-2-2-5. Similar to the CIFAR-10 experiment, we applied 0.2 dropout proba- bility to the input layer and 0.5 dropout to all other lay- ers. Images are pre-whitened and when presented to the network each time, they are horizontally flipped with prob- ability 0.5. The test set precision was 63.41%, improving the current state-of-the-art by nearly 2 percentage points. Table 3 shows the best results on CIFAR-100.Motivated by the better performance on the CIFAR-100, we perform another experiment to illustrate the fact that channel-out networks are better at harder tasks where the data patterns show more variances. We extract 10, 20, 50, 100 classes from the original dataset (both training and test- ing) to generate 4 training-testing pairs. We train a channel- out and a maxout network with a similar number of param- eters for each of the four classification tasks. The perfor- mance results are shown in Figure 8. Note that to facili- tate faster experimentation, we have deliberately used much smaller networks for this series of experiments.The results justify our claim that channel-out is not best suited for the case when there is relatively a small amount of information that needs to be stored (the 10-class task). In such a case, interference between patterns is less of a problem. As the patterns become more complex, channel- out surpasses maxout due to its better implementation of sparse pathway encoding. Notice the unusual fact that both methods perform better for the 100-class task than for the 50-class task. This may be due to uneven distribution of difficulty among the data. The first 50 classes seem to be more difficult to discriminate.STL-10 [4] contains images from 10 classes, typically with more variant shapes and more complex backgrounds than CIFAR-10 images. Figure 9 shows the comparison be- tween STL-10 and CIFAR-10 images. The original images are of size 96 × 96 × 3. There are 500 training and 800 test images for each class. There are additional 100,000 unla- beled images for unsupervised learning.Our best channel-out network for STL-10 consists of three convolutional layers followed a fully-connected layer and the softmax layer, with feature maps 64-176-256-1212. The channel-out group sizes are set as 4-4-4-4. Dropout is applied same way as before. To reduce the complex- ity, we down-sampled all images to 32 × 32. Images are pre-whitened, and horizontally flipped with probability 0.5 when presented to the network. We got the state-of-the-art test precision of 69.5%, which improves the current state-Channel-out+Dropout 69.5% Hierarchical Matching Pursuit [2] 64.5% Discriminative Learning of SPN [5] 62.3% Table 4: Best methods on STL-10 of-the-art by 5%. Best results on STL-10 are shown in Ta- ble 4. Note that in our experiments the network is trained with the training set only. The unsupervised corpus is only involved in the whitening process. The good performance on STL-10 shows that channel- out network does a better job at discriminating more vari- ant patterns, and ruling out the interference of complex backgrounds. This further demonstrates that information is stored more efficiently in a channel-out network.We have introduced the concept of sparse pathway cod- ing and argued that this can be a robust and efficient way for encoding categorical information in a deep network. Using sparse pathway encoding, the interference between conflict- ing patterns is mitigated, and therefore when combined with dropout, the network can utilize the network capacity in a more effective way. Along this direction we have proposed a novel class of deep networks, the channel-out networks. Our experiments show that channel-out networks perform very well on image classification tasks, especially for the harder tasks with more complex patterns. We believe that the concept of sparse pathway coding is well worth pursu- ing further for designing robust deep networks.
