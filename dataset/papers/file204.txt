• We constrain our filters to be the product of a learnable radial function R(r) and a spherical harmonic Y (l)m (ˆ r).• Our filter choice requires the structure of our network to be compatible with the algebra of geometric tensors.Convolutional neural networks are translation-equivariant, which means that features can be identified anywhere in a given input. This capability has contributed significantly to their widespread success.In this paper, we present a family of networks that enjoy much richer equivariance: the symmetries of 3D Euclidean space. This includes 3D rotation equivariance (the ability to identify a feature in any 3D rotation and its orientation) and 3D translation equivariance.We call these tensor field networks because every layer of our network inputs and outputs tensor objects: scalars, vectors, and higher-order tensors at every geometric point in the network. Tensor fields are ubiquitous in geometry, physics, and chemistry.There are three main benefits. First, this is more efficient than data augmentation to obtain 3D rotation-invariant out- put, making computation and training less expensive. This isOur motivation was to design a universal architecture for deep learning on atomic systems (such as molecules or ma- terials), but our network design is quite general. We expect tensor field networks to be useful in many tasks involving 3D geometry. For example, tensor field networks could be used to process 3D images in a rotation-and translation- equivariant way. We mention other potential applications in Section 6.In this paper, we explain the mathematical conditions such a 3D rotation-and translation-equivariant network must satisfy, provide several examples of equivariant compatible network components, and give examples of tasks that this family of networks can uniquely perform.the point convolutions, radial functions, and self-interaction layers. All models that rely only on pairwise distances or angles between points have the weakness (unlike this work) that they can not distinguish between chiral inputs (e.g. left hand and right hand), which have identical pairwise distances but are different shapes in 3D.Our work builds upon Harmonic Networks ( Worrall et al., 2017), which uses discrete convolutions and filters com- posed of circular harmonics to achieve 2D rotation equiv- ariance, and SchNet (Schütt et al., 2017), which presents a rotation-invariant network using continuous convolutions. The mathematics of rotation equivariance in 3D is much more complicated than in 2D because rotations in 3D do not commute; that is, for 3D rotation matrices A and B, AB = BA in general (see Reisert &amp; Burkhardt (2009) for more about the mathematics of tensors under 3D rotations). Cohen et al. (2018) introduce Spherical CNNs, which are equivariant models on spherical spaces. Our network differs from this work because it is equivariant to all the symmetries of 3D Euclidean space, both 3D rotation and translation, and because it directly operates on more general data types, such as points. The networks presented in each of these three papers can each be emulated by a tensor field network.The other major approach to modeling 3D atomic systems is to voxelize the space ( Wallach et al., 2015;Boomsma &amp; Frellsen, 2017;Torng &amp; Altman, 2017). In general, these are subject to significant expense, no guarantees of smooth transformation under rotation, and edge effects from the voxelization step.An introduction to the concepts of steerability and equiv- ariance in the context of neural networks can be found in Cohen &amp; Welling (2017), which focuses on discrete sym- metries.A representation D of a group G is a function from G to square matrices such that for all g, h ∈ G,Many other authors have investigated the problems of rota- tion equivariance in 2D, such as Zhou et al. (2017); Gonzalez et al. (2016); Li et al. (2017). Typically these work by looking at rotations of a filter and differ in exactly which rotations and how that orientation information is preserved (or not) higher in the network.A function L : X → Y (for vector spaces X and Y) is equiv- ariant with respect to a group G and group representationsOther authors have dealt with similar issues of invariance or equivariance under particular input transformations. G- CNNs (Cohen &amp; Welling, 2016) create invariance with fi- nite symmetry groups (unlike the continuous groups in this work). Cohen et al. (2018) use spherical harmonics and Wigner D-matrices but only address spherical signals (two dimensional data on the surface of a sphere). Kondor et al. (2018) use tensor algebra to create neural network layers that extend Message Passing Neural Networks ( Gilmer et al., 2017), but they are permutation group tensors (operating under permutation of the indices of the nodes) not geometric tensors.Invariance is a type of equivariance where D Y (g) is the identity for all g. We are concerned with the group of symmetry operations that includes isometries of 3D space and permutations of the points.Composing equivariant networks L 1 and L 2 yields an equiv- ariant network L 2 • L 1 (proof in supplementary material). Therefore, proving equivariance for each layer of a network is sufficient to prove (up to numerical accuracy) that a whole network is equivariant.The networks presented in Qi et al. (2016;2017) operate on point clouds and use symmetric functions to encode per- mutation invariance. Qi et al. (2017) employ a randomized sampling algorithm to include pooling in the network. These networks do not include rotation equivariance.Other neural networks have been designed and evaluated on atomic systems using nuclei centered calculations. Many of these capture 3D geometry just by including the pairwise distance between atoms (e.g. SchNet (Schütt et al., 2017) and the graph convolutional model from Faber et al.). Our work notably draws from SchNet (Schütt et al., 2017) for Furthermore, if a network is equivariant with respect to two transformations g and h, then it is equivariant to the com- position of those transformations gh (by the definition of a representation). This implies that demonstrating permuta- tion, translation, and rotation equivariance individually is sufficient to prove equivariance of a network to the group (and corresponding representations) containing all combi- nations of those transformations. As we describe shortly, translation and permutation equivariance are manifest in our core layers, so we will focus on demonstrating rotation equivariance.Tensor field networks act on points with associated features. A layer L takes a finite set S of vectors in R 3 and a vector in X at each point in S and outputs a vector in Y at each point in S, where X and Y are vector spaces. We write this aswhere r a ∈ R 3 are the point coordinates and x a ∈ X , y a ∈ Y are the feature vectors (a being an indexing scheme over the points in S). (To simplify the formulas here, we are assuming that inputs and outputs are at the same points in R 3 , but this is not necessary.) This combination of R where. (For layers in this paper, only the action of D Y (g) on Y that will be nontrivial, so we will use a convention of omitting the R 3 layer output in our equations.)3The key to attaining local rotation equivariance is to restrict our convolution filters to have a particular form. The fea- tures will have different types corresponding to whether they transform as scalars, vectors, or higher tensors.with another vector space can be written as R 3 ⊕ X , where ⊕ refers to the direct sum operation, where vectors in each space are concatenated and matrices acting on each space are combined into a larger block diagonal matrix.We now describe the conditions on L for equivariance with respect to different input transformations.Condition:To make our analysis and implementation easier, we decom- pose representations into irreducible representations. The irreducible representations of SO(3) have dimensions 2l +1 for l ∈ N (including l = 0) and can be chosen to be unitary. We will use the term "rotation order" to refer to l in this expression. The rotation orders l = 0, 1, 2 correspond to scalars, vectors, and symmetric traceless matrices, respec- tively. The group elements are represented by D (l) , which are called Wigner D-matrices (see Gilmore (2008)); they map elements of SO (3) where P σ ( r a , x a ) := ( r σ(a) , x σ(a) ) and σ permutes the points to which the indices refer.All of the layers that we will introduce in Section 4 are manifestly permutation-equivariant because we only treat point clouds as a set of points, never requiring an imposed order like in a list. In our implementation, points have an array index associated with them, but this index only ever used in a symmetric way.We could additionally have equivariance with respect to O(3), the group of 3D rotations and mirror operations (a discrete symmetry). We would augment our convolution using the method for obtaining equivariance with respect to discrete symmetries described in Cohen &amp; Welling (2017). We discuss this a bit more in Section 5.2.1.3.2. Translation equivariancewhere T t ( r a , x a ) := ( r a + t, x a ). This condition is analo- gous to the translation equivariance condition for CNNs. It is possible to use a more general translation equivariance condition, where the operator on the right-hand side of the equation also acts upon the Y representation subspace, but we will not consider that in this paper.At each level of a tensor field network, we have a finite set S of points in R 3 with a vector in a representation of SO(3) (that has been decomposed into a direct sum of irreducible representations) associated with each point. This is essentially a finite version of a tensor field, which was our inspiration for this network design.All of the layers in Section 4 are manifestly translation- equivariant because we only ever use differences between two points r i − r j (for indices i and j).At each point, we can have multiple instances of l- rotation-order representations corresponding to different features of that point. We will refer to these different instances as channels. We implement this object V (l) acmThe group of proper 3D rotations is called SO(3), which is a manifold that can be parametrized by 3 numbers (see Goodman &amp; Wallach (1998)). Let D X be a representation of SO(3) on a vector space X (and D Y on Y). Acting with g ∈ SO(3) on r ∈ R 3 we write as R(g) r, and acting onas a dictionary with key l of multidimensional arrays each with shapes [|S|, n l , 2l + 1] (where n l is the number of channels with representation l) corresponding to [point, channel, representation index]. See Figure 1 for an example of how to encode a simple system in this notation.We will describe tensor field network layers and prove that they are equivariant. To prove that a layer is equivariant, we only have to prove rotation equivariance for a given rotation order. This requires showing that when the point cloud rotates and the input features are transformed, the output features transform accordingly. (All of these layers will be manifestly permutation-invariant and translation-Tensor Field NetworksOur filters and layer input each inhabit representations of SO (3) (that is, they both carry l and m indices). In order to produce output that we can feed into downstream layers, we need to combine the layer input and filters in such a way that the output also transforms appropriately (by inhabiting a representation of SO (3)). A tensor product of representations is a prescription for combining two representations l 1 and l 2 to get another representationThis layer is the core of our network and builds upon the pointwise convolutions in ( . We start by considering the action of a general pointwise filter transfor- mation where C are called Clebsch-Gordan coefficients (see Grif- fiths (2017)). Note that l is any integer between |l 1 − l 2 | and (l 1 + l 2 ) inclusive, and m is any integer between −l and l inclusive.The crucial property of the Clebsch-Gordan coefficients that we need to prove equivariance of this layer isNote that our design is strictly more general than standard convolutional neural networks, which can be treated as a point cloud S of a grid of points with regular spacing. m : S 2 → C (where m can be any integer between −l and l), are an orthonormal basis for functions on the sphere. These functions are equivariant to SO(3); that is, they have the property that for g ∈ SO(3) andˆrandˆ andˆr ∈ S 2 ,Filters inhabit one representation, inputs another, and out- puts yet another. We need the Clebsch-Gordan coefficients to combine inputs and filters in such a way that they yield the desired output representation: m For our filters to also be rotation-equivariant, we restrict them to the following form:where l i and l f are non-negative integers corresponding to the rotation order of the input and the filter, respectively;: R ≥0 → R are learned functions; andˆrandˆ andˆr and r are the direction unit vector and magnitude of r, respectively. Filters of this form inherit the transformation property of spherical harmonics under rotations because R(r) is a scalar in m. This choice of filter restriction is analogous to the use of circular harmonics in Worrall et al. (2017) (though we do not have an analog to the phase offset because of the non-commutativity of SO (3)).where r ab := r a − r b and the subscripts I, F , and O denote the representations of the input, filter, and output, respec- tively. A point convolution of an l F filter on an l I input yields outputs at 2 max(l I , l F ) + 1 different rotation orders l O (one for each integer between |l I − l F | and (l I + l F ), inclusive), though in designing a particular network, we may choose not to calculate or use some of those outputs.Using the equivariance of the filter F and the property of the Clebsch-Gordan coefficients Equation 2, we can show the rotation equivariance of point convolutions, Equation 1 (detailed proof in supplementary material, Section C):Putting a self-interaction layer after a one-hot encoding in- put is the same as the embedding layer described in .This aspect of the design, and this proof of its correctness, is the core of our result.The weight matrix W is a scalar transform with respect to the representation index m, so the D-matrices commute with W , straightforwardly implying that this layer is equiv- ariant. Equivariance for l = 0 is straightforward because The most general way to combine incoming channels of the same rotation order before the next point convolution layer is to concatenate along the channel axis (increasing the number of input channels), then apply a self-interaction layer. This concatenation operation is equivariant because concatenation does not affect the representation index.The Wigner D-matrices areOur nonlinearity layer acts as a scalar transform in the l spaces (that is, along the m dimension). For l = 0 channels, we can usewhere R are the usual rotation matrices on R 3 . The spheri- cal harmonics are for some function η (0) : R → R and biases bc . For l &gt; 0 channels, our nonlinearity has the formThe Clebsch-Gordan coefficients are just the familiar ways to combine scalars and vectors: For 1 ⊗ 1 → 0,where η (l) : R → R can be different for each l andwhich is the dot product for 3D vectors. For 1 ⊗ 1 → 1,(It may be necessary to regularize this norm near the origin for this layer to have desirable differentiability properties.) which is the cross product for 3D vectors. The 0 ⊗ 0 → 0 case is just regular multiplication of two scalars, and 0⊗1 → 1 and 1 ⊗ 0 → 1 corresponds to scalar multiplication of a vector.Note that =because D is a unitary representation. Therefore, this layer is a scalar transform in the representation index m, so it is equivariant.We follow  in using point convolutions to scale feature vectors elementwise and using self-interaction layers to mix together the components of the feature vectors at each point. Self-interaction layers are analogous to 1x1 convolutions, and they implicitly act like l = 0 (scalar) types of filters:A global pooling operation that we use in our demonstra- tions is summing the feature vectors over each point. This operation is equivariant because D-matrices act linearly.In general, each rotation order has different weights because there may be different numbers of channels n l correspond- ing to that rotation order. For l = 0, we may also use biases.We chose experiments that are simple enough to be easily understandable yet suggestive of richer problems. They each highlight a different aspect of our framework. EachTensor Field Networks of these tasks is either unnatural or impossible in existing frameworks.We have focused on understanding how to construct and train these models in controlled tasks because of the com- plexity of the architecture. In future work, we will apply our network to more complicated and useful tasks.of about how these points are generated and about the hy- perparameters for our radial functions are given in the sup- plementary material.) For the moment of inertia task, we also designate a different special point at which we want to calculate the moment of inertia tensor.In these tasks, we used radial functions identical to those used in : We used radial basis func- tions composed of Gaussians, and two dense layers are applied to this basis vector. The number, spacing, and stan- dard deviation of the Gaussians are hyperparameters. We used a shifted softplus activation function log(0.5e x + 0.5) for our radial functions and nonlinearity layers. We implemented our models in TensorFlow ( Abadi et al., 2015). Our code is available at https://github.com/ tensorfieldnetworks/tensorfieldnetworks Our networks are simple: for learning Newtonian gravity, we use a single l = 1 convolution with 1 channel; for learning the moment of inertia tensor, we use a single layer comprised of l = 0 and l = 2 convolutions with 1 channel each. See Figure 2 for a diagram of these networks. We use elementwise summed L2 losses for the difference of the acceleration vectors and for the difference of the moment of inertia tensor. We get excellent agreement with the Newto- nian gravity inverse square law after a 1,000 training steps and with the moment of inertia tensor radial functions after 10,000 steps.Network types:To demonstrate the simplest non-trivial tensor field net- works, we train networks to calculate acceleration vectors of point masses under Newtonian gravity and the moment of inertia tensor at a specific point of a collection of point masses. These tasks only require a single layer of point convolutions to demonstrate. Furthermore, we can check the learned radial functions against analytical solutions.The acceleration of a point mass in the presence of other point masses according to Newtonian gravity is given bywhere we define r np := r n − r p . (We choose units where G N = 1, for simplicity.) The moment of inertia tensor is used in classical mechanics to calculate angular momentum. Objects generally have different moments of inertia depending on which axis they rotate about, and this is captured by the moment of inertia tensor (see Landau &amp; Lifshitz (1976)):In Figure 3, we show that the single radial filter is able to learn the 1/r 2 law for gravitational accelerations. (Related figures for the moment of inertia task can be found in the supplementary material.)The moment of inertia tensor is a symmetric tensor, so it can be encoded using a 0 ⊕ 2 representation. (The supple- mentary material contains a more detailed explanation of how to go from irreducible representations to matrix tensor representation.)We could have obtained equivariant accelerations by using a pure l = 0 network to predict the gravitational potential φ : R 3 → R, a scalar field, and then taking derivatives of φ with respect to the point coordinates to obtain the forces and accelerations (as in, e.g., ). However, many important vector quantities, such as magnetic fields in electromagnetism, cannot be derived from scalar fields.For both of these tasks, we input to the network a set of random points with associated random masses. (Details  Network type: 0 → 0 In three dimensions, there are 8 unique shapes made of 4 adjacent cubes (up to translations and rotations); see Fig- ure 4. We call these shapes 3D Tetris, and represent them using points at the center of each cube.There are two shapes in 3D Tetris that are mirrors of each other. Any network that relies solely upon distances (such as SchNet) or angles between points (such as ANI-1 ( Smith et al., 2017)) cannot distinguish these shapes. Our network can.A tensor field network that contains a path with an odd number of odd-rotation-order filters has the possibility of detecting local changes in handedness. This is because A tensor field network that does not include such paths will be invariant to mirror symmetry. As an example, a 0 → 0 network can correctly classify all 3D Tetris shapes except for the chiral shapes. The smallest network that can classify chiral shapes must include the path 0We demonstrate the rotation equivariance of our network by classifying 3D Tetris pieces in the following way: During training, we input to the network a dataset of shapes in a single orientation, and it outputs a classification of which shape it has seen. To demonstrate the equivariance of the network, we test the network with shapes from the same dataset that have been rotated and translated randomly. Our network performs perfectly on this task.− → 0 (superscripts correspond to filter rotation order), which yields the vector triple productof the filter vectors F i for each layer. This combination picks up a minus sign under mirror operations.Network type:We use a 3-layer network that includes the following for every layer: all possible paths with l = 0 and l = 1 con- volutions, a concatenation (if necessary), a self-interaction layer, and a rotation-equivariant nonlinearity. We only use the l = 0 output of the network since the shape classes are invariant under rotation and hence scalars. To get a classifi- cation from the l = 0 output of the network, we sum over the output of all points (global pooling).In this task, we randomly remove a point from a shape and ask the network to replace that point. This is a first step toward general isometry-equivariant generative models for 3D point clouds.We output an array of scalars, a special scalar, and one vector at each point. The vector, δ a , indicates where the missing point should be relative to the starting point r a (both direction and distance) and the array of scalars indicates the point type. The special scalar is put through softmax and used as a probability p a measuring the confidence in that point's vote. We aggregate votes for location using a weighted sum:  We train on molecular structures for this task because molecules can be comprised of a small number of atoms (making the task computationally easy to demonstrate) and because precise positions are important for chemical behav- ior.We define an accurate placement of the missing atom po- sition to be within 0.5 angstroms of the removed atom po- sition with the correct atom type according to argmax of the returned atom type array. We choose the 0.5 angstrom cutoff because it is smaller than the smallest covalent bond, a hydrogen-hydrogen bond which has a length of 0.7 angstroms. We present the accuracy of our predictions in Table 1. We train for 225 epochs and show comparable pre- diction accuracy on the test sets of 19, 23, and 25-29 atoms as the training set of 5-18 atoms. We include a breakdown of the accuracy and distance mean absolute error (MAE) in the supplementary material.We trained on structures in the QM9 dataset, a collection of 134,000 molecules with up to nine heavy atoms (C, N, O, F); including hydrogens, there are up to 29 atoms per molecule ( Ramakrishnan et al., 2014).Our network is most accurate for carbon and hydrogen miss- ing atoms (over 90%), moderately accurate for oxygen and nitrogen (about 70-80%), and not accurate for fluorine atoms (since there are only 50 examples in our training set, they might not be seen during training due to random selection of the missing atom).Our network is 3 layers deep, with 15 channels per rotation order at each layer. The input features to the network are a one-hot encoding of atom types at each point. We use 2- layer radial functions with 4 Gaussians with centers evenly spaced from 0 to 2.5 angstroms and variance that is half of the spacing. Most covalent bonds are 1-2 angstroms in length, so our radial functions are smaller than the diameter of the molecules of the QM9 dataset.We have explained the theory of tensor field networks and demonstrated how they work on simple examples.We train on a 1,000 molecule subset of molecules that have 5 to 18 atoms. In a single epoch, we train on each molecule in the training set (1,000 molecules with 5-18 atoms) with one randomly selected atom deleted (1,000 examples total per epoch). We then test the network on a random selection of 1,000 molecules with 19 atoms, 1,000 molecules with 23 atoms, and 1,000 molecules with 25-29 atoms.We hope that tensor field networks can be applied to a wide range of phenomena: In the context of atomic systems, we hope to train networks to predict properties of large and heterogeneous systems, learn molecular dynamics, calculate electron densities (as inputs to density functional theory algorithms), and hypothesize new stable structures. Ulti- mately, we hope to design new useful materials, drugs, and chemicals.For each epoch during training, we use each molecule once and randomly remove one atom. During evaluation, for each molecule, we make a prediction for every possible atom that could be removed. For example, for a molecule with 10 atoms, we will generate 10 predictions, each with a different atom missing. This is why the number of predictions is larger than the number of molecules.For more general physics, we see potential applications in modeling complex fluid flows, analyzing detector events in particle physics experiments, and studying configurations of stars and galaxies. We see other applications in 3D per- ception, robotics, computational geometry, and bioimaging.for some function F . This transform yieldsFor equivariant L, the following diagram is commutative for all g ∈ G:If a function is equivariant with respect to two transforma- tions g and h, then it is equivariant to the composition of those transformations:where we defineas in the main text.for all g, h ∈ G and x ∈ X ; that is, the following diagram is commutative:C. Proof of equivariance of point convolution layer Figure S1. Condition for layer rotation equivarianceThat is, the following is commutative:Under a rotation r a → R(g) r a , we know that r ab → R(g) r ab andbecause of the transformation properties of the spherical harmonics Y m .We can represent input as a continuous function that is non- zero at a finite set of points (using Dirac δ functions):  Figure S3. Equivariance of Clebsch-Gordan coefficients. Note that each D may refer to a different irreducible representation. Figure S4. Diagrammatic proof of point convolution rotation equiv- ariance.We can write the moment of inertia tensor as Figure S5 shows the excellent agreement of our learned radial functions for filters l = 0 and l = 2 to the analytical solution. The expression that R (2) is multiplying is the 3D symmetric traceless tensor, which can be constructed from the l = 2 spherical harmonic. To get agreement with the moment of inertia tensor as defined in the main text, we must haveThe number of points is uniformly randomly selected from 2 through 10. The masses are scalar values that are randomly chosen from a uniform distribution from 0.5 to 2.0. The coordinates of the points are randomly generated from a uniform distribution to be inside a cube with sides of length 4 for gravity and 1 for moment of inertia.We use 30 Gaussian basis functions whose centers are evenly spaced between 0 and 2. We use a Gaussian variance that is one half the distance between the centers. We use a batch size of 1. For the test set, we simply use more randomly generated points from the same distribution. 
