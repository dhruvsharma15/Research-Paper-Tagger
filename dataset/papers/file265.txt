T RADITIONALLY, computers are operated by either a keyboard and a mouse or touch. They provide feedback to the user primarily via visual clues on a display. This human-computer interaction model can be unintuitive to a human user at first, but it allows the user to express its intent clearly, as long as their goal is supported and they are equipped with sufficient knowledge to operate the machine. A spoken dialogue system (SDS) aims to make the human- computer interaction more intuitive by equipping computers with the ability to translate between human and computer language, thereby relieving humans of this burden and creating an intuitive interaction model. More specifically, the objective of an SDS is to help a human user achieve their goal in a specific domain (eg. hotel booking), using speech as the This  form of communication. Recent advances in artificial intelli- gence (AI) and reinforcement learning (RL) have established the necessary technology to build the first generation of commercial spoken dialogue systems deployable as regular household items. Examples of such systems are Amazon's Alexa, Google's Home or Apple's Siri. While initially built as voice-command systems, over the years these systems have become capable of sustaining dialogues that can span a few turns.Spoken dialogue systems are complex as they have to solve many challenging problems at once, under significant uncer- tainty. They have to recognise spoken language, decode the meaning of natural language, understand the user's goal while keeping track of the history of a conversation, determine what information to convey to the user, convert that information into natural language, and synthesise the sentences into speech that sounds natural. This work focuses on one particular step in this pipeline: devising a policy that determines the information to convey to the user, given our belief of their goal.This policy has been traditionally planned out by hand using flow-charts. This was a manual and inflexible process with many drawbacks that ultimately led to systems that were unable to converse intelligently. To overcome this, the policy optimisation problem has been formulated as a reinforcement learning problem [1], [2], [3]. In this formulation, the computer takes actions and gets rewards. An algorithm aims to learn a policy that maximises the rewards through learning to take the best actions based on the state of the dialogue. Since the number of possible states can be very large (potentially infinite), complex and universal function approximators such as neural networks have been deployed as the policy [4], [5], [6]. There is a recent trend in the last years to model text-to- text dialogues with a neural network and tackle it as a sequence to sequence model. Initial attempts to do this underestimate the fact that planning is needed and treat the problem in a purely supervised fashion [7]. More recently RL learning has also been applied yielding improvements [8], [9], [10], [11]. While we focus here on traditional modular approaches, everything that we describe is also applicable to end-to-end modelling.Using neural networks for policy optimisation is challenging for two reasons. First, there is often little training data available for an SDS as the data often comes from real humans. The system should be able to train quickly in an on-line setting while the training data is being gathered from users, to make the data to be gathered useful. Neural networks often exhibit too much bias or high variance when the volume of training data is small, making it difficult to quickly train them in a stable way. Second, the success or failure of a dialogue may be the only information available to the system to train the policy on. Dialogue success depends crucially on most actions in the dialogue, making it difficult to determine which individual actions contributed to the success, or led to the failure of a dialogue. This problem is exacerbated by the large size of the state space: the system will potentially never be in the same state twice.We address the above problems in the following ways: 1) We deploy the ACER algorithm [12], [13] which has previously showed promising results on simpler gaming tasks. 2) We analyse the algorithm detail highlighting its theoreti- cal advantages: low variance, safe and efficient learning. 3) We test the algorithm on a dialogue task with delayed rewards and test it alongside state-of-the-art methods in this task 4) After confirming its supremacy on a small action space, we deploy the algorithm on a two orders of magnitude larger action space. 5) We confirm our findings in a human evaluation. The rest of the paper is organised as follows. First, we give a brief introduction to dialogue management and define the main concepts. Then, in section III we review reinforcement learning. This is followed with an in-depth description of the ACER algorithm in section IV. Then, in section V we describe the architecture deployed to allow the application of ACER to a dialogue problem. The results of the extensive evaluation is given in section VII. In section VIII we give conclusions and future work directions. derived from the (noisy) dialogue act. To deal with the inherent uncertainty of the input, the dialogue is modelled as a partially observable Markov decision process (POMDP) [3]. The belief state is a vector representing a probability distribution over the different goals, intents and histories that occur in the dialogue. The role of the belief tracker is to accurately estimate this probability distribution and this is normally done using a version of a recurrent neural neural network [16].The job of the dialogue manager is to take the user's dialogue acts, a semantic representation of the input, and determine the appropriate response also in the format of a dia- logue act [14], [15]. The function that chooses the appropriate response is called the policy. The role of dialogue management is two-fold: tracking the dialogue state and optimising the policy.A policy is a probability distribution over possible user actions given the current belief state, and is commonly written as P π (a|b) = π(a|b). Here, a is the action and b is the output of the belief tracker, which is interpreted as a vector of probabilities 1 . In order to define the optimal policy, we need to introduce a utility function (reward) that describes how good taking action a is in state b. The reward for a complete dialogue depends on whether the user was successful in reaching their goal and the length of the conversation, such that short successful dialogues are preferred. Thus, the last dialogue interaction gains a reward based on whether the dialogue was successful, and every other interaction loses a small constant reward, penalising for the length of the dialogue. The task of policy optimisation is to maximise the expected cumulative reward in any state b when following policy π, by choosing the optimal action a from the set of possible actions A. Finding the optimal policy is computationally prohibitive even for very simple POMDPs. We can view the partially observable Markov decision process (POMDP) as a continuous-space Markov decision process (MDP) in terms of policy optimisation, where the states are the belief states [17]. This allows us to apply function approximation to solve the problem of policy optimisation. This is the approach we adopt in this work.We call the user's overall goal for a dialogue the user goal, i.e. booking a particular flight or finding information about a restaurant. The user works towards this goal in every dialogue turn. In each dialogue turn, the short-term goal of the user is called the user intent. Examples of user intent are: confirm what the system said, inform the system on some criteria, and request more information on something.The belief tracker is the memory unit of the SDS, with the aim to track the user goal, the user intent and the dialogue history. For the state to satisfy the Markov property it can only depend on the previous state and the action taken. Therefore, the state needs to encode enough information about what happened in the dialogue previously to maintain the conversation. By tracking the dialogue history we ensure that the state satisfies the Markov property. The user intent is System actions are the dialogue acts that the system can give as a response. This is called the action space or the master action space. Due to its large size, training a dialogue policy in this action space is difficult. Some algorithms do not converge to the optimal policy, converge very slowly, or, in rare cases, have prohibitive computational demands 2 . To alleviate this problem, we use the summary action space which contains a much smaller number of actions. If a policy is trained on the summary action space, the action selected by the policy needs to be converted to a master action. The conversion is a set of heuristics that attempts to find the optimal slots to inform on given the belief state.Using the summary action space provides the clear benefit of a simpler dialogue policy optimisation task. On the other hand, the necessary heuristics to map to the master action space need to be manually constructed for each domain. This means that the belief state needs to be human interpretable. This limits the applicability of neural networks for belief tracking where the belief state is compactly represented as a hidden layer in the neural network.The description of the summary and master actions that we consider is given in Appendix A.Not every system action is appropriate in every situation (belief state). For example, inform is not a valid action at the very beginning of the dialogue, when the system has not yet received any information on what kind of entity the user is looking for. An execution mask is constructed by the designer that ensures that only valid actions are selected by the policy: the probability of invalid actions is set to zero. The execution mask depends on the current belief state. Note that removing this mask inherently complicates the task of policy learning, as the policy then has to learn not to select inappropriate actions based on the belief state.In reinforcement learning, an agent interacts with the envi- ronment in discrete time steps. In each time step, the agent observes the environment as a belief state vector b t and chooses an action a t from the action space A. After performing action a t , the agent observes a reward r t produced by the environment.The cumulative discounted return is the future value of an episode of interactions. For the t th timestep, we calculate this as R t = same as the learned policy π, in other words, we evaluate and improve the same policy that is used to make decisions. In contrast, off-policy methods evaluate and improve a policy different from the one used to generate the data, i.e. the behaviour policy and the learned policy can be different. The advantage of off-policy methods is that the optimal policy can be even while we are choosing sub-optimal actions according to the behaviour policy. Standard reinforcement learning algorithms require that the state space is discrete. Therefore, the belief state of a dialogue manager is often discretised to allow standard algorithms to be applied [19]. Alternatively, function approximation can be applied for Q, V or π. In [20], linear function approxima- tion was applied to value functions. As parametric function approximation can limit the optimality of the solution, GP- SARSA [21] instead models the Q-function as a Gaussian process. The key here is the kernel function which models the correlation between different states and allows uncertainty to be estimated. This is crucial for learning from a small number of samples. More recently, neural network function approximation was used to approximate the Q-value function, known as deep Q-learning (DQN), obtaining human-level performance across challenging video games [22]. The policy π can be also modelled directly by deep networks leading to the resurgence of actor-critic methods [23]. The actor is improving its policy π through interactions being directed by the critic (value function V ).The discount factor γ trades-off the importance of immediate and future rewards. The goal of the agent is to find a policy that maximises the expected discounted cumulative return for every state. We define the value of a state-action pair (b t , a t ) under policy π to be the Q-value function, the expectation of the return for belief state b t and action a t :and the value of a state b t is the value function, which is the expected return only conditioned on the belief state b t :In both definitions, the expectation is taken over the states the environment could be in after performing the current action, and the future actions selected by policy π [18]. As the reinforcement learning scenarios become more chal- lenging, the agent estimates value functions from trial and error by interacting with a simulated or real environment. These estimates are accurate only in the limit of infinite observations for each state-action pair, thus a requirement for the behaviour policy µ is to maintain exploration, i.e. to keep visiting all state-action pairs with non-zero probability. The behaviour policy is the policy used to generate the data during learning. For on-policy methods, the behaviour policy is the A number of deep learning algorithms were previously applied to dialogue management. It has been shown in [24] that DQN enables learning strategic agents with negotiation abilities operating on a high-dimensional state space. The performance of actor-critic models on task-oriented dialogue systems was analysed in [6]. These models can also be naturally bootstrapped with a small number of dialogues via supervised pre-training. They reported superior performance compared to GP-SARSA in a noise-free environment. How- ever, the compared GP-SARSA did not utilise uncertainty estimates which were previously found to be crucial for effective learning [21].Uncertainty estimates can be incorporated into DQN using Bayes-by-Backprop [25]. Initial results show an improvement in learning efficiency compared to vanilla DQN.A number of recent works investigated end-to-end using gradient descent techniques [26], [27], [7], where belief track- ing and policy optimisation are optimised jointly. While, end- to-end modelling goes beyond the scope of this work, we note that the presented algorithm is applicable also in that setting.This paper builds on recent breakthroughs in deep rein- forcement learning (DRL) and applies them to the problem of dialogue management. In particular, we investigate recently proposed improvements to the actor-critic method [12]. The goal is a stable and sample efficient learning algorithm that performs well on challenging policy optimisation tasks in the SDS domain. Recent advances in DRL apply several meth- ods, including experience replay [28], truncated importance sampling with bias correction [12], the off-policy Retrace algorithm [13] and trust region policy optimisation [23] to various challenging problems. The core of this paper is to investigate to what extent these advances are applicable to the dialogue policy optimisation task with a large action space. These methods were recently combined in the actor critic with experience replay (ACER) algorithm and tested in gaming environments. To this end, we explain actor critic with experience replay (ACER) in detail and investigate the steps needed to apply it to SDS. Unlike in games, where these methods have been previously applied, we investigate dialogues with large and uncertain belief states and very large action spaces. This necessitates function approximation in reinforcement learning, but previously examined methods in SDS are data-inefficient, unstable or computationally too expensive. We investigate ACER as a means to overcome these limitations.Continuing from Equation (1), the approximation of the true gradient can be derived:where ρ(a|b) = π(a|b) µ(a|b) are the importance sampling (IS) weights. The advantage function A θ is used in place of the Q-function for an unbiased estimate with a lower variance:In off-policy setting, the advantage function is approximated as asIn order to use experience replay in an actor-critic method, an off-policy version of the actor-critic method is needed. The objective is to find a policy that maximises the expected discounted return. This is equivalent to maximising the value of the initial state with input parameter vector ω:The unbiased estimatorAnother way of expressing the same objective is to maximise the cumulative reward received from the average state [29]. For behaviour policy µ, let the occupancy frequency d µ be defined as:results in high variance, due to the IS weight that has to be calculated for the entire episode. The temporal difference (TD) estimationAccording to the new definition of J(ω), V is weighted by d µ because µ was used to collect the experience:only requires a single IS weight. However, this estimation is biased: the value function update of the current state is based on the current estimate of the value function for the next state. This leads to slow convergence or no convergence at all. It is possible to combine both methods and create an estimator that trades off bias and variance according to a parameter λ. [29] estimate Q π as:where π is the optimal policy. The off-policy version of the Policy Gradient Theorem [30] is used to derive the gradients ∇ ω J(ω) ≈ g(ω):The states are encountered in proportions according to d µ just by sampling from the experience memory, so there is no need to estimate d µ explicitly. Estimating Q π , however, is more difficult: the off-policy interactions are gathered ac- cording to µ, and we need the Q-function under a different - current policy π.To account for this, the importance sampling (IS) weights [31] could be used. To achieve stable learning, we use an estimation method that achieves low variance by considering state-action pairs in isolation, applying only one IS weight for each.The constant λ controls the bias-variance trade-off: setting λ to 0 results in an equivalent estimation as in Equation (2), with a low variance but high bias. Conversely, setting λ to 1 results in high variance as many IS weights will be producted. This has the advantage of propagating the final reward further to the starting state which reduces bias.The Retrace algorithm ( [13]) attempts to estimate the cur- rent Q-function from off-policy interactions in a safe and efficient way, with small variance. Throughout this discussion, we call a method safe if its estimate of Q π can be proven to converge to Q π . The updated estimate of the Q-function, Q ret is computed based on state-action trajectories sampled from the replay memory:introduce bias in the gradient estimation. We call the residual the bias correction term.The methods that stem from this framework differ only in their definition of eligibility traces c s . This framework introduces changes to the actor-critic model. Instead of approximating V and π with NNs and estimating Q in a closed-form equation to compute the update targets, both π and Q are estimated with NNs. V is then computed from π and Q:where [·] + = max(0, ·). The weight of the bias correction term, [ρ(a t |b t ) − c] + , can still be unboundedly large. This can be solved by sampling the action from the distribution π rather than µ [12]:We focus on Retrace proposed by [13] where c s = λ min (1, ρ(a s |b s )). Ideally, we need a method that is safe, has low variance and is as efficient as possible. Retrace solves this trade-off by setting the traces "dynamically", based on the IS weights. In the near on-policy case, it is efficient as IS weights will be about 1, preventing the traces from vanishing. It has low variance because the IS weights are clipped at 1. It is also safe for any π and µ. The goal of this discussion is limited to conveying the intuition behind Retrace, but a full proof of safety is available in [13].There are two key advantages of this formulation:Let us investigate the computational cost of deriving Q ret from Q in a na¨ıvena¨ıve way. For each episode sampled from the replay memory, and for each state-action pair, we need to visit the remaining part of the episode to calculate the expectation of errors under µ according to Equation (3). This quadratic element of the computational cost can be reduced to a linear one by deriving Q ret in a recursive way. For an episode trajectory b 1:T , a 1:T sampled from the replay memory, Equation (3) becomes:• The bias correction term ensures that the estimate of the gradient remains unbiased.• The bias correction term is only active when ρ(a|b) &gt; c, and otherwise the formulation is equivalent to Equa- tion (5). When active, the bias correction weight falls between 0 and 1. To apply this method, called the truncation with bias correction trick by [12], we have to overcome a problem with the advantage function estimation. Before, we estimatedfor belief-action pairs that we sampled from the replay mem- ory, Equation (4). For the bias correction term however, only the belief is sampled from the memory, and all the actions are considered and weighted by the current policy π. Due to the way Q ret is formulated, it learns from rewards, and only learns belief-action pairs that have been visited and sampled from the replay memory. Thus the estimation is not available for the bias correction term, so we use the output of the NN, Q, to estimate the advantage function for that term:We will use this more computationally efficient, recursive formulation of Q ret .Currently, we calculate the policy gradient as:where the expectation is taken over the replay memory, and ρ(a t |b t ) = π(at|st) µ(at|st) . An issue with this approximation is that the IS weights ρ(a t |b t ) are potentially unbounded, introducing significant variance. To solve this problem, we clip the IS weights from above by a constant c: ρ(a t |b t ) = min{c, ρ((a t |b t ))}. We can split the equation into two parts, one involving the truncated IS weight, and the other the residual. We also need to estimate the residual, otherwise we Typically, the step size parameter in the gradient descent is calculated assuming the that the policy parameter space is Euclidian. However, this has a major shortcoming: small changes in the parameter space can lead to erratic changes in the output policy [32], [33]. This could lead to unstable learning or a learning rate too small for quick convergence. This is solved in the Natural Actor Critic algorithm by considering the natural gradient [34].Instead of computing the exact natural gradient, we can approximate it. For the natural gradient, the distance metric tensor is the Fisher information matrix:It can be shown [35] thatWhere KL is the Kullback-Leibler divergence. Thus, instead of directly restricting the learning step-size with the natu- ral gradient method, we can approximate the same method by restricting the Kullback-Leibler divergence between the current policy π parametrised by ω, and the updated policy parametrised by ω + α · ∇ ω J, for learning rate α. This method is called trust region policy optimisation (TRPO), introduced by [23]. Their method, however, relies on repeated computations of Fisher matrices for each update, which can be prohibitively expensive. [12] introduces an efficient TRPO method that we will adopt instead. Our description of the method largely follows theirs with additional explanations and necessary adaptation to our discrete action-space SDS domain.To begin with, [12] proposes that the KL-divergence to the updated policy should be measured not from the current policy, but from a separate average policy instead. This stabilises the algorithm by preventing it from gaining momentum in a specific direction. Instead, it is restricted to stay around a more stable average policy π a . The average policy is parametrised with ω a , where ω a represents a running average of all previous policy parameters. It is updated softly after each learning step as:Since the constraint is linear, the overall optimisation problem reduces to a simple quadratic programming problem. Thus, a closed-form solution can be derived using the KKT conditions [36]:β is a hyperparameter that controls the amount of history to maintain in the average policy. A value close to zero makes the average policy forget the history very quickly, reducing the effect of calculating the distances from the average policy instead of the current one. A value close to one will prevent the average policy to adjust to the current policy, or slows this adjustment process down. TRPO can be formulated as an optimisation problem, where we aim to find z that minimises the L2-distance between z and the vanilla gradient g(ω) from 6. This is a quadratic minimisation. In addition, our aim is for the divergence constraint to be formulated in a linear way, which will allow to derive a closed-form solution. Since z will be used for the parameter update, we have ω = ω + αz, where ω denotes the updated parameter vector. We can approximate the KL divergence after the policy update using a first-order Taylor expansion:ACER is the result of all methods presented in this section. With on-policy exploration, it is a modified version of A2C. Both ACER and A2C use experience replay and sample from their memories to achieve high sample efficiency. The difference between them is that ACER additionally employs TRPO, and that it uses a Q-function estimator instead of a V -function estimator as the critic. When off-policy, it uses truncated importance sampling with bias correction [12] to reduce the variance of IS weights without adding bias. The Retrace algorithm is used to compute the targets based on the observed rewards in a safe, efficient way, with low bias and variance.This training algorithm is presented in pseudocode (Al- gorithm 2), and is called from the master ACER algorithm (Algorithm 1). It performs exploration, i.e. the op- timal action learned so far with probability 1 − and a random action with probability A hyperparameter batch size controls the number of dialogues considered for a training step, and n controls the number of training steps for each new dialogue gathered. We will investigate the effect of various hyperparameters and how to set them in Section VII-E. Call training algorithm (Algorithm 2) with {M, θ, ω, ω a , π, Q, α, β, γ, δ} So the increase in KL divergence in this step is 9: until convergenceWe can constrain this increase to be small by setting δ, such thatIn this section we detail the steps needed to apply ACER to a dialogue task.where the learning rate α is left out, since it is a constant and can be incorporated into δ. Letting k = ∇ ω KL [π(·|ω a )||π(·|ω)], the optimisation problem with lin- earised KL divergence constrain is [12]:A. Learning in summary action spaceLet us design the neural networks for actor-critic for a dialogue management task. On top of the input of the belief state, we build two hidden layers, h 1 and h 2 . The heads of the NN are the functions π and Q. Both hidden layers h 1 and h 2 are shared between the predictors of Q and π. Weight sharing subject to k T z ≤ δ Algorithm 2 ACER training algorithm 1: Input: {M, θ, ω, ω a , π, Q, α, β, γ, δ} 2: Initialise g = 0, dθ = 0 3: for each dialogue {b 1:N , a 1:N , r 1:N , µ} in M dofor 1 = N to 1 do 5: Fig. 2. Architecture of the actor-critic neural network for the master action space.ρ t ← min(r, ρ t )C ← ρ(a|bt,ω)−c ρ(a|bt,ω)B ← a π(a|b t , ω) C ∇ ω log π(a|b t , ω)A B. Learning in master action space 1) Master actions for ACER: In addition to applying ACER on the summary space, we also applied it on the master action space. However, to make this efficient, the NN architecture was redesigned.In the case of the CamInfo domain (see Appendix A), there are 8 informable slots of an entity, each with a binary choice on whether we inform on it. Thus, a single inform action makes up 2 8 = 256 separate master actions, only differing in what they inform on. We want to incorporate the fact that these actions are very similar into the design of the NN architecture. We achieve this by breaking the policy π into a summary policy π s , corresponding to the 15- dimensional summary action space, and a payload policy π p , corresponding to the 256 choices of the payload of an inform action. We break the Q function up similarly into Q s and Q p . We reconstruct the 1035-dimensional master policy π (see Appendix A). and master Q-function Q as follows: for each summary action A, can be beneficial as it reduces the number of parameters to train. Furthermore, in a dialogue system, we expect a strong positive correlation between π and Q. The architecture is illustrated in Figure 1.The activation function for layers h 1 and h 2 is rectified linear unit (ReLU), which was chosen empirically as it led to faster training. The activation function for π is softmax, which converts the inputs to a probability distribution with values between 0 and 1, summing up to 1. There is no activation func- tion for the output Q, as we want it to have an unlimited range, both from above and below (as rewards can be negative). All the connections in the NN are fully connected, which imposes the least structural constraints on the architecture. We perform our experiments on the Cambridge Restaurants domain, the details of which are given in Appendix A. In this domain the belief state is represented by a 268-dimensional vector. This is the input of the NN. Our layer h 1 consists of 130 neurons and h 2 has 50 neurons. These numbers were chosen empirically, with the goal in mind to force the NN to encode all information about the belief state relevant to π and Q in the bottleneck layer of 50 neurons, thereby learning a mapping that generalises better. The output vectors π and Q have the dimensionality of the action space. Initially, we experiment with the summary action space, which has 15 actions (see Appendix A for details).• If A does not have a payload (i.e. is not an inform action), append the corresponding summary values from π s and Q s onto π and Q. • Otherwise, for each payload P of the 256 possible choices, append π s (A) · π p (P ) to π. This is because the probability of choosing action A with payload P is modelled as the product of the probability of choosing A and that of choosing P . For each P , we also append Q s (A) + Q p (P ) to Q, allowing the payload network to learn an offset of Q achieved by choosing a particular payload. The complete NN architecture is illustrated in Figure 2. It is important to note that only the architecture of the NNs is changed and the training algorithm is unchanged. In fact, the NNs are treated as a black box by ACER. These output is a 1035-dimensional vector for master action space.2) Master actions for GP-SARSA: We compare ACER to GP-SARSA algorithm. This is an on policy algorithm that approximates the Q-function as a Gaussian process (GP) and therefore is very sample-efficient [21]. The key is the use of a kernel function which defines correlations between different parts of the input space. Similarly to ACER, the Gaussian process (GP) method needs to be adjusted before we deploy it on master action space. The core of a GP is the kernel function, which in the case of GP-SARSA is defined as:We recall that the kernel function defines our a priori belief of the covariance between any two belief-action pairs. This kernel is a multiplication of a scalar product of the beliefs and a Kronecker delta on the actions. The latter has the effect that any two different actions are considered completely independent. While this might be a good approximation for summary actions, a more elaborate action kernel is required for master actions. This could introduce the idea that two inform actions with slightly different payloads are expected to have similar results on the same belief state, thus showing higher covariance.Our new action kernel returns 0 for actions a and a that stem from different summary actions. Otherwise, a and a are the same inform action with differing payloads. In this case, we calculate the kernel based on the cosine similarity of the two payloads, treating the payloads as vectors describing the sets of slots to inform on. Let us call a s and a p the summary action and the payload corresponding to a. a p is represented as a vector where each entry is either 0 or 1, depending on whether the corresponding slot is informed on. Writing a p = A. Evaluation set-upfor the normalised version of the payload vector, the [36] for a proof of k being a valid kernel function.In the case of GP-SARSA on master action space, the training algorithm is unchanged. Only the kernel function is adjusted to incorporate the idea of similarity between master actions. The GP can thus be trained on the 1035-dimensional master action space.It is important to highlight some limitations of this work. This work is not addressing the problem of modelling policy with large action space where there are no similarities between the system actions. On the contrary, we focus on large action spaces where we can establish some relations between the actions, either by sharing the weights in the neural network architecture as in Section V-B1 or by defining special kernel functions as in Section V-B2. Although, this might seem limiting, in practice, in any task-oriented dialogue, actions will bear a lot of similarities. This used to be addressed by producing a smaller summary space of distinct actions, but we believe that the proposed approach scales better, removes hand-crafting and leads to the better performance. The latter hypothesis is investigated in the next section.We compare our implementation of ACER two NN-based algorithms, namely eNAC ( [38]) and A2C and to a non- parametric algorithm GP.Experiments are run as follows. First, the total number of dialogues or iterations (4000) is broken down into milestones (20 milestones of 200 iterations each). As the training over the total number of iterations progresses, a snapshot of the state of the training (all NN weights, hyperparameters, and replay memory) is saved at each milestone. A separate run of 4000 iterations is then performed without any training steps, where each of the saved snapshots are tested for 200 iterations. No training and no exploration is being performed during the testing phase; instead of the greedy policy with respect to π is used to derive the next action. This informs us on the performance of the system as if it stopped training at a specific milestone, allowing us to observe the speed of convergence and the performance of early milestones, discounting for the exploration.We run the evaluation 15 times and average the results, to reduce the variance arising from different random initialisa- tions. We compare the average per-episode reward obtained by the agent, the average number of turns in a dialogue and the percentage of successful dialogues. The reward is defined as 20 for a successful dialogue minus the number of turns in the dialogue. The number of maximum turns is limited to 25, after which, if the user did not achieve their goal, the dialogue is deemed unsuccessful. The discount factor γ is set to 0.99 for all algorithms where it is applicable. For NN-based algorithms, the size of a minibatch, on which the training step is performed, is 64. For algorithms employing experience replay, the replay memory has a capacity of 2000 interactions. For NN-based algorithms, exploration is used, with linearly reducing from 0.95 down to 0 over the training process.In this section, we evaluate the performance of ACER incorporated in an SDS. We find that ACER delivers the best performance and fastest convergence among the compared NN-based algorithms (eNAC and A2C) implemented in the PyDial dialogue toolkit [37]. We also deploy the algorithm in a more challenging setting without the execution mask aiding action selection. Next, we investigate the effect of different hyperparameter selections, and the algorithm's stability against it. Then, we deploy ACER and GP on master action space. Finally, we investigate how resilient different algorithms are to semantic errors and changing testing conditions. We use the agenda-based user simulator, with the focus belief tracker for all experiments. For details, see [37]. The agenda-based user simulator [39] consists of a goal which is a randomly generated slot-value pairs that the entity that the user seeks must be satisfied and an agenda which is a dynamic stack of dialogue acts that the user elicits in order to satisfy the goal. The simulated user consist of deerministic and stochastic decisions which govern its behaviour capable of generating complex behaviour. A typical dialogue starts by user expressing what it is looking for, or waiting for the system to prompt it. Then it checks whether the offered entity satisfies all the constraints. In that process it sometimes changes its goal and asks for something else, making it more difficult for the system to satisfies its goal. Once it settles on the offered entity, it asks for additional information, such as address or phone- number. For a dialogue to be deemed successful, the offered entity needs to match the last user goal. Also, the system must provide all further information that the user simulator asked for. The reward is delayed and only given at the end of the dialogue. No reward is given for partially completed tasks.    We run our experiments with and without the execution mask and compare success rates (Figure 3 and Figure 6). In general, as expected, algorithms converge slower without the execution mask, while the final performance of GP and ACER remain somewhat below their performances with the mask. This is also expected as a mapping learned by RL is rarely as precise as a hard-coded solution to a problem (execution mask). GP shows faster initial convergence than ACER, as the latter shows a more steady progress without unexpected dips in performance. They remain comparable in every other regard. In the initial environment, the simulated semantic error rate is 0% both for training and testing. The learning rate α = 0.001. Instead of a simple gradient descent on the loss function, we use the Adam Optimiser, which associates mo- mentum to the gradient [40]. To discourage the algorithm from learning a trivial policy, we subtract 1% of the policy entropy from the loss function. The ACER-specific hyperparameters are: c = 5, δ = 1, β = 0.99, n = 1. The results are given in Figure 3 and Figure 4 where the shaded are represents a 95% confidence interval.We observe that ACER is comparable to GP in terms of speed of convergence, sample efficiency, success rate, rewardsACER has several additional hyperparameters compared to more traditional algorithms. We investigate the effect of hyper- parameters c, δ, β, and n on the algorithm's performance. To better illustrate the differences, we run the tests in a more challenging setting, without the execution mask. For every analysed parameter, we kept the rest of the hyperparameters set to values providing the best results from section C. a) Importance Weight threshold c: This value is the upper bound of IS weight; weights higher than c are truncated. Setting this value too high diminishes the effect of weight truncation, while a value too low will rely more on the less accurate bias correction term. From Figure 7, we see that c = 5 delivers the highest convergence rate and a good final performance. We also see that for the wide range of values from c = 1 to c = 20, there is no big difference in final performance, suggesting that the algorithm is relatively stable in face of varying this hyperparameter.     b) KL divergence constraint δ: This value constrains the KL divergence between an updated policy and the running average policy. Setting it too high allows radical jumps, setting it too low slows the convergence down (Figure 8). We can see that a setting of δ = 10 or δ = 50 results in erratic changes in the performance of ACER, while δ = 0.5 and δ = 1 are sensible choices. c) Average policy update weight β: In Figure 9, we can see that for β ≤ 0.9, the average policy forgets the history too quickly, allowing the policy to gain momentum in any direction and thus preventing it from converging to a good performance. For β = 0.95, the policy converges quickly, while β = 0.99 results in a somewhat conservative algorithm, where the KL divergence constraint keeps the policy near a slowly changing average. β = 0.99 still converges to a good result, but does so somewhat slower than in case of β = 0.95. d) Training iterations n: Setting the number of training steps per episode n higher allows the algorithm to learn more from the gathered experience. However, if n is too high, the training might diverge due to the policy moving too much ( Figure 10). For n = 1, convergence is quick and performance is good, while for n = 10, performance stays poor throughout. For n = 30 and n = 50, the algorithm diverges completely.ACER compares favourably to other NN-based algorithms, but performs about equally if not slightly worse than GP in our experiments. The experiments were run on the summary action space, which only has 15 actions. In a more difficult scenario, we may have orders of magnitude more actions. In such scenarios, the computational cost of GPs can be prohibitive as it needs to invert the Gram matrix [21]. If ACER still performs well under the same scenario, it might may be the overall best method to apply to larger action spaces. This is because ACER does not have the prohibitive computational cost of GP, and is expected to train much more quickly.To test our hypotheses, we deploy ACER on the master action space according to Section V-B1, and on the summary space ( Figure 11). Both experiments were run with the execu- tion mask. Convergence is slower on the master action space. This is expected due to having to choose between vastly higher number of actions on the master action space (1035 as opposed to 15). However, ACER is still surprisingly effective on the master action space, converging to about the same performance as on the summary space. We note that this is without any modification to the training algorithm, only the underlying NN is changed. ACER achieves the best results in terms of speed of convergence and final performance on master action space out of NN-based SDS policy optimiser algorithms.To investigate further whether ACER is the best choice of algorithm on the master action space, we modify GP to run on master action space according to Section V-B2. We compare ACER and GP both on summary and master action spaces, without the execution mask in Figure 12. Both GP and ACER show slower speed of convergence on master action space. This is expected, as the random initialisation of a policy  on master action space will be much less sensible than an initialisation on the summary space, the latter taking advantage of the hard-coded summary to master action mapping method. However, it is surprising to see that all experiments converged to roughly the same performance of about 97% success rate, except for GP on summary, which has a final success rate of 98%-99%. This suggests that both ACER and GP can handle large action spaces quite efficiently. To our knowledge, this is the first time learning on the master action space from scratch was successfully attempted. GP is more sample efficient than ACER on the challenging master action space without execution mask. However, it requires vastly more computational resources to run: this experiment took 6.45 hours to run with ACER, and 8.63 days with GP 3 . Arguably, the extra computational cost overshadows the disadvantage of ACER, that it has to be run for more iterations to converge.So far, our experiment settings were quite idealised, train- ing and testing policies under a perfect simulator with no semantic errors. However, in real life the automatic speech recognition (ASR) component is very likely to make errors as well as the spoken language understanding (SLU) compo- nent. Therefore, in reality, the pipeline surrounding the policy 3 The running times were measured on an Azure cloud machine with a 16-core CPU and 64GB of RAM.optimiser deals with substantial uncertainty, which tends to introduce errors [41]. We ultimately want to measure how well a policy optimiser can learn the optimal strategy in face of noisy semantic-level input. In our experiments, we control this by the semantic error rate, the rate at which a random noisy input is introduced to the optimiser to simulate an error scenario. In other words, a 15% semantic error rate means that with 0.15 probability a semantic concept (slot, value or dialogue act type) presented to the dialogue manager is incorrect. We focus on two desirable properties of a policy. First, ideally, the policy would learn not to trust the input as much, and ask questions until it is sure about the user goal, just like a real human would if the telephone line is noisy. Second, an ideal policy would not only adjust to the error rate of the training conditions, but would dynamically adjust to the conditions of the dialogue it is in. If the policy adjusts too much to the training conditions, it is said to overfit. This could severely limit the policy's deployability.We test key algorithms for these two desirable properties. eNAC, the best known NN-based policy optimiser [5] to this date, is compared to ACER and GP. ACER and GP are also compared to their respective variants in master action space. We run the test as follows: first, we train the algorithms under 15% semantic error rate until convergence, with the execution mask. Then we take the fully trained policy and test it under a range of semantic error rates, ranging from 0% to 50% to measure the policies' generalisation properties. This is something that is never the case in games so this aspect of learning is rarely examined but it is of utmost importance for spoken dialogue systems. We present results in Figure 13 and Figure 14 with a 95% confidence interval.Success rate and reward follow the same trends. As ex- pected, we see a general downwards trend for each algorithm as the semantic error rate increases. There is however no apparent spike in performance at the 15% semantic error rate of the training process, indicating that none of the algorithms overfit to this setting. We can see that the performance of eNAC is far behind all the other algorithms. ACER and GP are closer in performance, but GP on summary space consistently beats ACER on summary space.It might be surprising that both ACER and GP perform better when trained on the master action space as opposed  to the summary space, given that they performed worse in previous experiments. However, those experiments had no se- mantic errors, and a hand-crafted rigid mapping from summary actions to master actions, that relied on the belief state to find the best payload for an inform action. Under a higher semantic error rate, the belief state will be noisy and this mapping may not perform optimally. This highlights the benefits of expanding the scope of artificial intelligence in SDS: AI can be more versatile than hand-coded mappings, especially when the mapping performs decision making under uncertainty.• A version of ACER [12] designed for SDSs shows better results than the current state-of-the-art for neural network-based policy optimisers [5].• This implementation of ACER is also able to train efficiently in the master action space, showing the best performance among neural network-based policy optimis- ers, as reported by [6] and [5].• Our implementation of GP with a redesigned kernel function achieves the best performance on master action space, which previously was not possible.In the previous sections, the training and testing is per- formed on the same simulated user. To test the generalisation capabilities of the proposed methods, we evaluate the trained dialogue policies in interaction with human users in a similar set-up as in [42]. To recruit the users, we use the Amazon Mechanical Turk (AMT) service where volunteers can call our dialogue system and rate it. Around 900 dialogues were gathered. Three policies (GP and ACER on summary action space and ACER on master action space) were trained with 15% semantic error rate to accommodate for ASR errors using set-up from previous sections. Then, learnt policies were incorporated into SDS pipeline with a commercial ASR system.The MTurk users were asked to find restaurants that have particular features as defined by the given task. Subjects were randomly allocated to one of the three analysed systems. After each dialogue the users were asked whether they judged the dialogue to be successful or not which is then translated to a reward measure. Table I presents averaged results with one standard deviation. All models differ indiscernibly with regards to success rate performing very well. However, ACER trained on master action space achieves considerably higher reward (and in turn smaller number of turns) than models working on summary action space.GP suffers from an inherently high computational cost, making the algorithm unsuitable in higher volume action spaces. In such cases, the fact that ACER can be trained well on the master action space indicates that it may be the best currently known method to train policies with large action spaces.As agents powered by machine learning gain more intel- ligence, they can be applied to more challenging domains. Using the master action space is a good example of this: a hard-coded mapping between summary and action spaces can be used to simplify the task of the AI agent. However, as we have shown, it is no longer required to train in this action space. There is an algorithm (ACER) that can finally bridge the semantic gap between summary and master action spaces without the help of domain-specific code written explicitly for this mapping 4 . This has three benefits: first, training on master action space outperforms the mapping based on fixed code, when uncertainty (semantic errors) is involved. Second, it allows us to build a more generally applicable system, with less work required to deploy it in differing domains. Third, it allows us to consider domains that have vastly higher action spaces, even if there is no clear way to convert those action spaces into small summary action spaces (such as a general purpose dialogue system).ACER fits well into other SDS research directions too. Successful policy optimisers need to be sample efficient and be able to be trained quickly, to avoid subjecting human users to poor dialogue performance for long. ACER uses experience replay for sample efficiency, together with many methods aimed at reducing bias and variance of the estimator, to achieve quick training.We introduce some of the many directions in which this work could be continued. Recently, [5] combined supervised learning (SL) with deep reinforcement learning (DRL) to investigate the performance of an agent bootstrapped with SL and trained further with DRL. The NNs of ACER are compatible with that approach. This may decrease the overall The policy optimisation algorithms presented in this paper improves the state-of-the-art in spoken dialogue systems (SDS) in three ways: interactions required for convergence, as well as increase sample efficiency.Both of our settings, training on summary and on master action space, considered static action spaces only. Under this framework, the entire policy would have to be retrained if a new action or payload were to be introduced. This could hurt the maintainability of a real-life dialogue system, as it would be expensive to extend the database schema or the list of actions. Ideally, the training algorithm could adapt to such changes made, being able to retain its pre-existing knowledge of the old actions and this is an important topic to investigate in the future.restaurant also has a name, which we will always inform on. Thus, the system has a choice between 2 8 = 256 different ways it can inform on a restaurant. A specific choice is referred to as the payload of an inform action.• reqmore is a simple action that prompts the user to provide more input.• bye is used to end the call, normally only as a response to the user's intention to end the call. For the CamInfo domain, there are 4 · 2 8 = 1024 inform actions and 3·3+2 = 11 other actions, making up 1035 actions in total. We call this action space the master action space. In the summary space, the inform actions do not specify which slots to inform on, leaving only 4 separate inform actions, and 15 actions in total.We define the action space in the CamInfo restaurants domain. Most information-seeking domains have a similar overall architecture.• request + slot where slot is an informable slot such as area, food, or pricerange. This action prompts the user to specify their criteria on a slot, eg. "Which area are you interested in?" • confirm + slot where slot is an informable slot. This action prompts the user to confirm their criteria on a slot that they may or may not have already mentioned. Due to errors accumulating during the decoding pipeline (speech recognition, semantic decoding, belief tracking), the system has to deal with considerable uncertainty, but it can attempt to increase its certainty in the user's criteria by using a confirm action, eg. "Did you say you want an expensive restaurant?" • select + slot where slot is an informable slot. This action prompts a user to select a value for the slot from a specified list of values. This is less open-ended than a request action and more open-ended than a confirm action, eg. "Would you like Indian or Korean food?".• inform + method + slots action provides information on a restaurant. The associated method specifies how the restaurant to give information on should be chosen. The standard method is to choose the first result in the ontology that matches the user criteria specified so far. The method can also be byname, in which case the system believes that the user asked about a specific restaurant by referring to its name, and information on that restaurant should be provided. If the method is requested, we inform on the same restaurant we informed on last, if it is alternatives then we pick another restaurant that matches the user's criteria (if possible). There are several properties of a restaurant, with a binary choice for each of them on whether the system wants to inform on it in a dialogue turn or not. The informable slots for restaurants are: area, food type, description, phone number, price-range, address, postcode and sig- nature.We note that some of these slots are also requestable, allowing a user to query a restaurant based on those slots. These slots are area, food type and price-range. A Below is an example dialogue between a user looking for a restaurant with a medium price range, and a system that internally translates between summary and master actions. System responses are written as Sys: summary action → master action: Pei-Hao (Eddy) Su is a PhD candidate under the supervision of Professor Steve Young in Dialogue Systems Group at Cambridge University. His re- search interests centre on applying deep learning, reinforcement learning and Bayesian approaches to dialogue management and reward estimation, with the aim of building systems that can learn directly from human interaction. He has published around 30 peer-reviewed papers across top speech and NLP conferences and he received the best student paper award at ACL 2016.
