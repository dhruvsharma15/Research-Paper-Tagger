Minimum Bayes risk (MBR) training [1][2][3] has been shown to be an effective way to train neural net-based acoustic models and is widely used for state-of-the-art speech recognition systems [4][5][6]. MBR training minimizes an expected loss, where the loss measures the distance between a reference and a hypothe- sis. State-level MBR (sMBR) training [7,8] is a popular form of MBR training which uses a loss defined at the frame level based on clustered context-dependent phonemes or subphonemes. It is computationally tractable, has an elegant lattice-based for- mulation using the expectation semiring [9,10], and has been shown to perform favorably in terms of word error rate (WER) compared to alternative losses that have been proposed [8,11].Given the prevalence of word error rate as an evaluation metric, it is natural to consider using it during MBR train- ing. We refer to this as word-level edit-based MBR (word-level EMBR) training. However expected word edit distance over a lattice is harder to compute than the expected frame-level loss used by sMBR training. As a result, many approximations to the true edit distance have been proposed for lattice-based MBR training [2,7,8,[12][13][14][15]. Gibson provides a systematic review of the strengths and weaknesses of various approximations [15, chapter 6]. Heigold et al. show that it is possible to exactly com- pute expected word edit distance over a lattice by first expand- ing the lattice using an approach sometimes termed error mark- ing [16,17]. While impressive, error marking is computation- ally and implementionally non-trivial, and increases the size of the lattices. The memory required for FST determinization dur- ing error marking increases rapidly with utterance length, and van Dalen and Gales found a practical limit of around 10 words with the conventional determinization algorithm or around 20 words with an improved, memory-efficient algorithm [17]. The time complexity also means error marking is unlikely to be compatible with on-the-fly lattice generation.In this paper we propose an alternative approach to lattice- based word-level EMBR training. The gradient of the expected loss optimized by MBR training may itself be written as an ex- pectation, allowing the gradient to be approximated by sam- pling. The samples are paths through the lattice used during conventional sMBR training. This approach is extremely flexi- ble as it places almost no restriction on the form of loss that may be used. We use this approach to perform word-level EMBR training for two speech recognition tasks and show that this im- proves WER compared to sMBR training. To our knowledge this is also the first work to investigate word-level EMBR train- ing for state-of-the-art acoustic models based on neural nets.Similar forms of sampled MBR training have been pro- posed previously. Graves performed sampled EMBR training for the special case of a CTC model, noting that the special structure of CTC allows trivial sampling and specialized ap- proaches to reduce the variance of the samples [18]. Speech recognition may be viewed as a simple reinforcement learning problem where an action consists of outputting a given word sequence. In this view MBR training is learning a stochastic policy, and the sampling-based approach described here is very similar to the REINFORCE algorithm [19,20], though here we sample from a probability distribution which is globally nor- malized instead of locally normalized, and there are differences in the form of variance reduction used.It should be noted that some previous investigations have reported better WER from optimizing a state-level or phoneme- level criterion than a word-level criterion [2]  [15, chapter 7]. However some have reported the opposite [14]. We find the word-level criterion more effective in our experiments but do not investigate this question systematically.In the remainder of this paper we review the sequence-level probabilistic model used by MBR training ( §2 and §3), describe MBR training ( §4), discuss EMBR training ( §5), describe our sampling-based approach to MBR training ( §6), and describe our experimental results ( §7).In this section we review how a weighted finite state transducer (FST) [21] may be globally normalized to obtain a probabilistic model. The model used for MBR training is defined in this way.A weighted FST defined over the real semiring (also called the probability semiring) may naturally be turned into a prob- abilistic model over a sequence of its output labels [22]. Each edge e in the weighted FST has a real-valued weight we ≥ 0 and an optional output label taken from a specified output al- phabet, and an optional input label taken from a specified input alphabet. The absence of an input or output label is usually de-noted with a special epsilon label. A path π through the FST from the initial state to the final state 1 consists of a sequence of edges, and the weight of the path w(π) is defined as the product e∈π we of the weights of its edges. The probability of a path is defined by globally normalizing this weight function: a T × Q matrix which has a one at (t, qt) for each frame t and zeros elsewhere. The gradient of the log probability is given byThis is a Markovian distribution. The output label sequence y(π) associated with a path π is the sequence of non-epsilon output labels along the path. The probability of a sequence y of output labels is defined as the sum of the probabilities of all paths consistent with y:Minimum Bayes risk (MBR) training [1][2][3] minimizes the ex- pected lossπ:y(π)=yIn this section we briefly recap a conditional probabilistic model often used for speech recognition. This probabilistic model is an important component in sMBR training.An acoustic model specifies a mapping z = z(x, λ) from an acoustic feature vector sequenceis typically as- sociated with a cluster of context-dependent phonemes or sub- phonemes [23]. In this paper the acoustic model is a stacked long short-term memory (LSTM) network [24]. Given a logit sequence z, we define a weighted FST U (z) as the compositionof a score FST S(z) and a decoder graph FST C • L • G. The decoder graph is itself a composition and incorporates weighted information about context-dependence (C), pronunciation (L) and word sequence plausibility (G) [25]. The score FST input and output alphabets and the decoder graph input alphabet are {1, . . . , Q}. The decoder graph output alphabet is the vocab- ulary. We refer to U (z) as the unrolled decoder graph since composition with S(z) effectively unrolls the decoder graph over time. The score FST S(z) has a simple "sausage" structure with T + 1 states and an edge from state t to state t + 1 with input and output label q and log weight ztq for each frame t and cluster index q. The conditional probabilistic model P(y|x, λ) over a word sequence y = [yj] as a function of λ, where the loss L(y, yref) specifies how bad it is to output y when the reference word sequence is yref. Broadly speaking MBR concentrates probability mass: a sufficiently flexible model trained to convergence with MBR will assign a probability of 1 to the word sequence(s) with smallest loss.State-level minimum Bayes risk (sMBR) training [7,8] de- fines the loss L(π, πref), now over π instead of y, to be the num- ber of frames at which the current cluster index differs from the reference cluster index. The sequence of time-aligned ref- erence cluster indices is typically obtained using forced align- ment. This loss has a number of disadvantages compared to minimizing the number of word errors [15, chapter 6]. How- ever it has the advantage that the expected loss and its gradient may be computed tractably using an elegant formulation based on the expectation semiring [9,10]. The crucial property of the loss L that makes it tractable is that the loss of a path can be decomposed additively over the edges in the path.Typically it is not feasible to perform sMBR training over the full unrolled decoder graph U (z), and a lattice containing a subset of paths is used instead [4,27]. There has been some in- terest recently in performing exact "lattice-free" sequence train- ing using a simplified decoder graph [28].  is obtained by applying the procedure described in §2 to the unrolled decoder graph U (z(x, λ)). This givesGiven the prevalence of word error rate (WER) as an evaluation metric, a natural loss to use for MBR training is the edit dis- tance between the reference and hypothesized word sequences. We refer to MBR training using a Levenshtein distance as the loss as edit-based MBR (EMBR) training, and to using the num- ber of word errors as word-level EMBR training. The number of word errors is the result of a dynamic programming computa- tion and does not decompose additively over the edges in a path, meaning the expectation semiring approach cannot be applied without modification. As discussed in §1, it is in fact possible to exactly compute the expected number of word errors over a lattice by expanding the lattice using error marking, but this ap- proach produces larger lattices, in practice limits the maximum utterance length, and is not compatible with on-the-fly lattice generation [16,17].where z = z(x, λ) and w(π, z) is the weight of a path. It will be helpful to know the gradient of the log weight and log probability with respect to the acoustic logits. Due to the way the score FST is constructed, the sequence of non-epsilon input labels encountered along a path through U (z) consists of a cluster index qt for each frame t ∈ {1, . . . , T }. The overall log weight log w(π, z) has an additive contribution t ztq t from the acoustic model, and the gradient 6. Sampled MBR training ∂ ∂z log w(π, z) consists of In this section we look at a simple sampling-based approach to computing the MBR loss value and its gradient. This approach is essentially agnostic to the form of loss used, allowing us to perform EMBR training simply and efficiently. We consider the expected loss as a function of z rather than λ since this provides the gradients required to implement sampled MBR in a modular graph-of-operations framework such as TensorFlow [29].Samples from P(π|z) can be drawn efficiently using back- ward filtering-forward sampling [30]. First the conventional backward algorithm is used to compute the sum βi of the weights of all partial paths from each FST state i to the fi- nal state. The FST is then reweighted using β as a potential function, i.e. replacing the weight we of each edge e, which goes from some state i to some state j, with β −1 i weβj [31]. This results in an equivalent FST which is locally normalized or stochastic, i.e. the sum of edge weights leaving each state is one [31]. Samples from the reweighted FST can be drawn us- ing simple ancestral sampling, i.e. sample an edge leaving the initial state, then an edge leaving the end state of the sampled edge, and repeat until we reach the final state. In our implemen- tation the β values are computed once to draw multiple samples and the reweighting is performed on-the-fly during sampling.We can approximate the expected loss using a straightfor- ward Monte Carlo approximation:  (2)) takes a path to a word se- quence, get gammas takes a path to a T × Q matrix with a one for each (frame, cluster index) which occurs in the path (see §3), and get loss computes Levenshtein distance in the case of EMBR training.where each πi is an independent sample from P(π|z) and Li = L(y(πi), yref). We write Li to denote averaging over samples. The true gradient, writing L(π) for L(y(π), yref), isWe compared sMBR and sampled word-level EMBR training for two model architectures on two speech recognition tasks.π This can be re-expressed using (5) asThus the MBR gradient is a scalar-vector covariance [10], i.e. it is of the form E[xy] − ExEy, and an unbiased estimate isWe refer to using (12) during gradient-based training as sampled MBR training. It has the intuitive interpretation that samples with worse-than-average loss have the log weight of the cor- responding path reduced, and samples with better-than-average loss have the log weight of the corresponding path increased. The subtraction of Li in (12) makes the estimated gradient in- variant to an overall additive shift in loss, and may be seen as performing variance reduction 3 , often regarded as extremely important in sampling-based policy optimization for reinforce- ment learning [32][33][34].Example code for sampled MBR training is shown in Fig- ure 1. Our implementation consisted of a TensorFlow op with input acoustic logits and output expected loss. 3 Sampled MBR without variance reduction may be defined byThe variance reduced version we have presented is equivalent to approximating EL(π) ∂ ∂z log P(π|z) − EL(π)E ∂ ∂z log P(π|z) as. Both are unbiased estimates of the desired gradient since E ∂ ∂z log P(π|z) = 0.We compared the performance of sMBR and EMBR training for a 2-channel query recognition task (Google Home) using a CTC-style model where the set of clustered context-dependent phonemes includes a blank symbol. Our acoustic feature vector sequence was 480-dimensional with a 30 ms frame step, and consisted of a stack of three 10 ms frames of 80 log mel filterbanks for each channel. A stacked unidirectional LSTM acoustic model with 5 layers of 700 cells each was used, followed by a linear layer with output dimen- sion 8192 (the number of clusters). These "raw" logits were post-processed by applying log normalization with a softmax- then-log then adding −1.95 to the logit for CTC blank and scaling the logits for the remaining clusters by 0.5 to mimic what is often done during decoding for CTC models; this post- processing does not seem to be critical for good performance. The initial CTC system was trained from a random initialization using 739 million steps of asynchronous stochastic gradient de- scent, using context-dependent phonemes as the reference la- bel sequence. The sMBR and EMBR systems were trained for a further 900 million steps, with the best systems selected by WER on a small held-out dev set being obtained after a total of roughly 1035 million and 1569 million steps respectively. For both CTC and MBR training, each step computed the gradient on one whole utterance. Lattices for MBR training were gener- ated on-the-fly. Alignments for sMBR training were computed on-the-fly. The decoder graph used during MBR training was constructed from a weak bigram language model estimated on the training data, and a CTC-style context-dependent C trans- ducer with blank symbol was used [35]. EMBR training used 100 samples per step. The WER used during EMBR training was computed with language-specific capitalization and punc- tuation normalization. The learning rate for EMBR training was  set 5 times larger than for sMBR training because the number of word errors is typically smaller than the number of frame errors, and the typical gradient norm values during sMBR and EMBR training reflected this. We verified that using a 5 times larger learning rate for sMBR was detrimental. The training data was a voice search-specific subset of a corpus of around 20 million anonymized utterances with artificial room reverba- tion and noise added. WER was evaluated on three corpora of anonymized 2-channel Google Home utterances.WER on the dev set during training is presented in Fig- ure 2. Somewhat contrary to common wisdom that sMBR train- ing converges quickly, we observe WER continuing to improve for around 300 million steps. After 1 billion total steps, sMBR training gets worse on the dev set and stays roughly constant on the training set (not shown), meaning it is suffering from overfitting. EMBR training takes a long time to achieve its full gains; indeed it is not clear it has converged even at 1.6 billion total steps. EMBR performance appears to be as good or better than sMBR after any number of steps. An EMBR step with 100 samples in our set-up took around the same time as an sMBR step. EMBR has little overhead because sampling paths given the beta probabilities is cheap and because overall computation time for both sMBR and EMBR is typically dominated by the stacked LSTM computations and lattice generation.The evaluation results are presented in Table 1. We can see the sampled word-level EMBR training gave a 4-5 % relative gain over sMBR training.consisted of a stack of four 10 ms frames of 128 log mel filter- banks. The stacked LSTM had 5 layers of 600 cells each. An initial cross-entropy system was trained for 305 million steps. The sMBR and EMBR systems were trained for a futher 420 million steps, and the models with best WER on a held-out dev set were at around 680 million and 720 million total steps re- spectively. The systems were trained on a corpus of around 15 million anonymized voice search and dictation utterances with artificial reverb and noise added and evaluated on four voice search test sets including one with noise added.WER on the dev set during training is presented in Figure 3, and results are presented in Table 2. In these experiments we used the same learning rate for sMBR and EMBR training since we were not yet fully aware of the mismatch in dynamic range; using a smaller learning rate may improve sMBR performance. We observed consistent gains from word-level EMBR training on this task.  Overall 65k 6.6 6.2 -5% Table 1: Word error rates for CTC-style 2-channel Google Home model trained with sMBR and word-level EMBR.We have seen that sampled word-level EMBR training provides a simple and effective way to optimize expected word error rate during training, and that this improves empirical performance on two speech recognition tasks with disparate architectures.We also compared the performance of sMBR and EMBR train- ing for a 1-channel voice search task. Our acoustic feature vec- tor sequence was 512-dimensional with a 30 ms frame step, and Many thanks to Erik McDermott for his patient and helpful feedback on successive drafts of this manuscript, and to Has¸imHas¸im Sak for many fruitful and enjoyable discussions while imple- menting sMBR and EMBR training in TensorFlow.
