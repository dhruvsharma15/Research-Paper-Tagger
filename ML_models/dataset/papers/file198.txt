The key takeaways from this paper may be summarized as follows:Deep neural networks are typically trained using supervised learning on large, carefully annotated datasets. However, the need for such datasets restricts the space of problems that can be addressed. This has led to a proliferation of deep learning results on the same tasks using the same well-known datasets. However, carefully annotated data is difficult to obtain, especially for classification tasks with large numbers of classes (requiring extensive annotation) or with fine-grained classes (requiring skilled annotation).• Deep neural networks are able to generalize after training on massively noisy data, instead of merely memorizing noise. We demonstrate that standard deep neural networks still perform well even on training sets in which label accuracy is as low as 1 percent above chance. On MNIST, for example, performance still exceeds 90 percent even with this level of label noise (see Figure 1). This behavior holds, to varying extents, across datasets as well as patterns of label noise, including when noisy labels are biased towards confused classes.• A sufficiently large training set can accommodate a wide range of noise levels. We find that the minimum dataset size required for effective training increases with the noise level (see Figure 9). A large enough training set can accommodate a wide range of noise levels. Increasing the dataset size further, however, Deep Learning is Robust to Massive Label Noise does not appreciably increase accuracy (see Figure 8).MNIST -Uniform label noise• High levels of label noise decrease the effective batch size, as noisy labels roughly cancel out and only a small learning signal remains. As such, dataset noise can be partly compensated for by larger batch sizes and by scaling the learning rate with the effective batch size.Model architectureLearning from noisy data. Several studies have investi- gated the impact of noisy datasets on machine classifiers. Approaches to learn from noisy data can generally be cate- gorized into two groups: In the first group, approaches aim to learn directly from noisy labels and focus on noise-robust algorithms, e.g., Beigman &amp; Klebanov (2009) . The second group comprises mostly label-cleansing methods that aim to remove or correct mis- labeled data, e.g., Brodley &amp; Friedl (1999). Methods in this group frequently face the challenge of disambiguat- ing between mislabeled and hard training examples. To address this challenge, they often use semi-supervised ap- proaches by combining noisy data with a small set of clean labels ( Zhu, 2005). Some approaches model the label noise as conditionally independent from the input image (Natarajan et al., 2013;Sukhbaatar et al., 2014) and some propose image-conditional noise models ( Veit et al., 2017;Xiao et al., 2015). Our work differs from these approaches in that we do not aim to clean the training dataset or propose new noise-robust training algorithms. Instead, we study the behavior of standard neural network training procedures in settings with massive label noise. We show that even without explicit cleaning or noise-robust algorithms, neural networks can learn from data that has been diluted by an arbitrary amount of label noise.   (2017)). In these studies an increase in noise is assumed to decrease not only the proportion of correct examples, but also their absolute number. In con- trast to these studies, we separate the effects and show in §4 that a decrease in the number of correct examples is more destructive to learning than an increase in the number of noisy labels.Analyzing the robustness of neural networks. Several investigative studies aim to improve our understanding of convolutional neural networks. One particular stream of re- search in this space seeks to investigate neural networks by analyzing their robustness. For example, Veit et al. (2016) show that network architectures with residual connections have a high redundancy in terms of parameters and are robust to the deletion of multiple complete layers during test time. Further, Szegedy et al. (2014) investigate the ro- bustness of neural networks to adversarial examples. They show that even for fully trained networks, small changes in the input can lead to large changes in the output and thus misclassification. In contrast, we are focusing on non- adversarial noise during training time. Within this stream of research, closest to our work are studies that focus on the impact of noisy training datasets on classification per- In this work, we are concerned with scenarios of abundant data of very poor label quality, i.e., the regime in which falsely labeled training examples vastly outnumber correctly labeled examples. In particular, our experiments involve observing the performance of deep neural networks on multi- class classification tasks as label noise is increased.To formalize the problem, we denote the number of original training examples by n. To model the amount of noise, we dilute the dataset by adding α noisy examples to the train- ing set for each original training example. Thus, the total number of noisy labels in the training set is αn. Note that by varying the noise level α, we do not change the available number of original examples. Thus, even in the presence of high noise, there is still appreciable data to learn from, if we are able to pick it out. This is in contrast to previous work (e.g., Sukhbaatar   plies a decrease in the absolute number of correct examples.In the following experiments we investigate three different types of label noise: uniform label-swapping, structured label-swapping, and out-of-vocabulary examples. Thus, the noisy label is allowed to be dependent on the correct class but not on the image itself.A key assumption in this paper is that unreliable labels are better modeled by an unknown stochastic process rather than by the output of an adversary. This is a natural assumption for data that is pulled from the environment, in which antag- onism is not to be expected in the noisy annotation process. Deep neural networks have been shown to be exceedingly brittle to adversarial noise patterns ( Szegedy et al., 2014). In this work, we demonstrate that even massive amounts of non-adversarial noise present far less of an impediment to learning.the ratio α of randomly labeled examples to cleanly labeled examples from 0 (no noise) to 100 (only 11 out of 101 labels are correct, as compared with 10.1 for pure chance). For the more challenging dataset CIFAR-10, we vary α from 0 to 10. For the most challenging dataset ImageNet, we let α range from 0 to 5. We compare various architectures of neural networks: multilayer perceptrons with different num- bers of hidden layers, convolutional networks (ConvNets) with different numbers of convolutional layers, and residual networks (ResNets) with different numbers of layers ( He et al., 2016). We evaluate performance after training on a test dataset that is free from noisy labels. Full details of our experimental setup are provided in §3.4.As a first experiment, we will show that common training procedures for neural networks are resilient even to settings where correct labels are outnumbered by labels sampled uniformly at random at a ratio of 100 to 1. For this ex- periment we focus on the task of image classification and work with three commonly used datasets, MNIST ( LeCun et al., 1998), CIFAR-10 ( Krizhevsky &amp; Hinton, 2009) and ImageNet ( Deng et al., 2009).Our results show that, remarkably, it is possible to attain over 90 percent accuracy on MNIST, even when there are 100 randomly labeled images for every cleanly labeled example, to attain over 85 percent accuracy on CIFAR-10 with 10 random labels for every clean label, and to attain over 70 percent top-5 accuracy on ImageNet with 5 random labels for every clean label. Thus, in this high-noise regime, deep networks are able not merely to perform above chance, but to attain accuracies that would be respectable even without noise.In Figures 1 and 2 we show the classification performance with varying levels of label noise. For MNIST, we vary Further, we observe from Figures 1 and 2 that larger neural network architectures tend also to be more robust to label noise. On MNIST, the performance of a perceptron de- cays rapidly with increasing noise (though it still attains 40 percent accuracy, well above chance, at α = 100). The per- formance of a multilayer perceptron drops off more slowly, and the ConvNet is even more robust. Likewise, for CIFAR- . Illustration of uniform and structured noise models. In the case of structured noise, the order of false labels is important; we tested decreasing order of confusion, increasing order of confu- sion, and random order. The parameter δ parameterizes the degree of structure in the noise. It defines how much more likely the second most likely class is over chance. δ -Degree of structuredness 10, the accuracy of the residual network drops more slowly than that of the smaller ConvNets. This observation pro- vides further support for the effectiveness of ConvNets and ResNets in particular for applications where noise tolerance may be important.We have seen that neural networks are extremely robust to uniform label noise. However, label noise in datasets gathered from a natural environment is unlikely to follow a perfectly uniform distribution. In this experiment, we investigate the effects of various forms of structured noise on the performance of neural networks. Figure 4 illustrates the procedure used to model noise structure. Figure 5. Performance on MNIST with fixed α = 20 noisy labels per clean label. Noise is drawn from three types of structured distribution: (1) "confusing order" (highest probability for the most confusing label), (2) "reverse confusing order", and (3) random order. We interpolate between uniform noise, δ = 0, and noise so highly skewed that the most common false label is as likely as the correct label, δ = 1. Except for δ ≈ 1, performance is similar to uniform noise.In the uniform noise setting, as illustrated on the left side of Figure 4, correct labels are more likely than any individual false label. However, overall false labels vastly outnumber correct labels. We denote the likelihood over chance for a label to be correct as Note that = 1/(1 + α), where α is the ratio of noisy labels to certainly correct labels. To induce structure in the noise, we bias noisy labels to certain classes. We introduce the parameter δ to parameterize the degree of structure in the noise. It defines how much more likely the second most likely class is over chance. With δ = 0 the noise is uniform, whereas for δ = 1 the second most likely class is equally likely as the correct class. The likelihood for the remaining classes is scaled linearly, as illustrated in Figure 4 on the right. We investigate three different setups for structured noise: labels biased towards easily confused classes, towards hardly confused classes and towards random classes.class, labels are assigned most frequently to (1) those most often confused with the correct class, (2) those least often confused with it, and (3) in a random order. We determine commonly confused labels by training the network repeat- edly on a small subset of MNIST and observing the errors it makes on a test set.The results show that deep neural nets are robust even to structured noise, as long as the correct label remains the most likely by at least a small margin. Generally, we do not observe large differences between the different models of noise structure, only that bias towards random classes seems to hurt the performance a little more than bias towards con- fused classes. This result might help explain why we often observe quite good results from real world noisy datasets, where label noise is more likely to be biased towards related and confusing classes. Figure 5 shows the results on MNIST for the three different types of structured noise, as δ varies from 0 to 1. In this experiment, we train 4-layer ConvNets on a dataset that is diluted with 20 noisy labels for each clean label. We vary the order of false labels so that, besides the correctIn the preceding experiments, we diluted the training sets with noisy examples drawn from the same dataset; i.e., falsely labeled examples were images from within other categories of the dataset. In natural scenarios, however, noisy examples likely also include categories not included in the dataset that have erroneously been assigned labels within the dataset.  . Performance on CIFAR-10 for varying amounts of noisy labels from different sources. Noisy training examples are drawn from (1) CIFAR-10 itself, but mislabeled uniformly at random, (2) CIFAR-100, with uniformly random labels, and (3) white noise with mean and variance chosen to match those of CIFAR-10. Noise drawn from CIFAR-100 resulted in only half the drop in perfor- mance observed with noise from CIFAR-10 itself, while white noise examples did not appreciable affect performance. Comparison of the effect of reusing images vs. using novel images as noisy examples. Essentially no difference is observed between the two types of noisy examples, supporting the use of repeated examples in our experiments.Thus, we now consider two alternative sources for noisy training examples. First, we dilute the training set with examples that are drawn from a similar but different dataset. In particular, we use CIFAR-10 as our training dataset and dilute it with examples from CIFAR-100, assigning each image a category from CIFAR-10 at random. Second, we also consider a dilution of the training set with "examples" that are simply white noise; in this case, we match the mean and variance of pixels within CIFAR-10 and again assign labels uniformly at random.be relevant but mislabeled. However, it is likely that many examples will not be from any classes under consideration and therefore will influence training less negatively. In fact, it is possible that such examples might increase accuracy, if the erroneous labels reflect underlying similarity between the examples in question. Figure 6 shows the results obtained by a six-layer ConvNet on the different noise sources for varying levels of noise. We observe that both alternative sources of noise lead to better performance than the noise originating from the same dataset. For noisy examples drawn from CIFAR-100, per- formance drops only about half as much as when noise originates from CIFAR-10 itself. This trend is consistent across noise levels. For white noise, performance does not drop regardless of noise level; this is in line with prior work that has shown that neural networks are able to fit random input ( Zhang et al., 2017). This indicates the scenarios con- sidered in Experiments 1 and 2 represent in some sense a worst case.All models are trained with AdaDelta (Zeiler, 2012) as opti- mizer and a batch size of 128. For each level of label noise we train separate models with different learning rates in {0.01, 0.05, 0.1, 0.5} and pick the learning rate that results in the best performance. Generally, we observe that the higher the label noise, the lower the optimal learning rate. We investigate this trend in detail in §5.In natural scenarios, we may expect massively noisy datasets to fall somewhere in between the cases exemplified by CIFAR-10 and CIFAR-100. That is, some examples will In Experiments 1 and 2, noisy labels are drawn from the same dataset as the labels guaranteed to be correct. This involves drawing the same example many times from the dataset, giving it the correct label once, and in every other instance picking a random label according to the noise dis- tribution in question. We show in Figure 7 that performance would have been comparable had we been able to draw noisy labels from an extended dataset, instead of repeating images. Specifically, we train a convolutional network on a subset of MNIST, with 2,500 certainly correct labels and with noisy labels drawn either with repetition from this set of 2,500 or without repetition from the remaining examples in the MNIST dataset. The results are essentially identi- cal between repeated and unique examples, supporting our For example, at α = 10, 2,000 clean labels are needed to attain 90% performance, while at α = 50, 10,000 clean labels are needed.Training set size to reach target accuracy Figure 9. Relationship between the amount of noise in the dataset and the critical number of clean training examples needed to achieve high test accuracy. Different curves reflect different target accuracies. We observe that the required amount of clean data increases slightly more than linearly in the ratio of noisy to clean data.setup in the preceding experiments.Underlying the ability of deep networks to learn from noisy data is the size of the data in question. It is well-established, see e.g., Deng et al. (2009); Sun et al. (2017), that traditional deep learning relies upon large datasets. We will now see how this is particularly true of noisy datasets.In Figure 8, we compare the performance of a ConvNet on MNIST as the size of the training set varies. In particu- lar, we show how the performance of the ConvNet varies with the number of cleanly labeled training examples. We compare the performance of the same ConvNet trained on MNIST diluted with different amounts of noisy labels sam- pled uniformly. For example, for the blue curve of α = 10 and 1,000 clean labels, the network is trained on 11,000 examples: 1,000 cleanly labeled examples and 10,000 with random labels. In Figure 10, we show that a similar pattern occurs when a ResNet with 18 layers is trained on ImageNet with noisy labels.networks. This critical amount of clean data depends on the noise level; in particular, it increases as the noise level rises. Note that as the amount of clean data increases, the size of the overall noisy training set increases still faster. In Figure 9, we re-plot the data from Figure 8 to show how the amount of clean data required to attain a threshold accuracy varies as a function of noise. It appears that the requisite amount of clean data rises slightly faster than linear in the ratio of noisy to clean examples. Since performance rapidly levels off past the critical threshold, the main requirement for the clean training set is to be of sufficient size.Generally, we observe that independent of the noise level the networks benefit from more data and that, given suffi- cient data, the networks reach similar results. Further, the results indicate that there seems to be a critical amount of clean training data that is required to successfully train the The success of training with substantial label noise comes as a surprise. Stochastic gradient descent and its variations consist of walks through the space of networks in locally optimal directions. Adding noise to this process means that in many cases the step taken is in an erroneous direction. It is well known (see e.g. ( Shalev-Shwartz et al., 2017;Jin et al., 2017;Kawaguchi, 2016;Ge et al., 2017)) that the space of networks has abundant hard-to-escape saddle points, flat regions, and local optima; therefore, it might be supposed that too many erroneous steps could lead to a part of the space from which further optimization becomes impossible. Our result that networks generalize even when trained with almost entirely incorrect labels suggests that trajectories in network space are more flexible than might be expected.  gives better performance, reflecting our analysis that increasing amounts of noise reduce the effective batch size. We approximate the limit of infinite batch size by training without noisy labels, using instead the noisy loss function Hα (see (3)).In this section, we analyze one aspect of the standard learn- ing procedure which reduces the stochasticity of the gradient updates: averaging gradients over a batch. In the preceding sections, our results were obtained by training neural net- works with fixed batch size and running a parameter search to pick the optimal learning rate from five possible values. We now look in more detail into how different batch sizes and learning rates affect learning on noisy datasets.function.In classification tasks, we are given an input x and aim to predict the class f (x) ∈ {1, 2, . . . , m}. The value f (x) is encoded within a neural network by the 1-hot vector y(x) such thatIn Figure 11, we compare the performance of a simple 2- layer ConvNet on MNIST with increasing noise, as batch size varies from 32 to 256. (Note that all other experiments in this paper are performed with a fixed batch size of 128.) We observe that increasing the batch size provides greater robustness to noisy labels. One possible explanation for this behavior is that within a batch, gradient updates from randomly sampled noisy labels roughly cancel out, while gradients from correct examples that are marginally more frequent sum together and contribute to learning. By this logic, large batch sizes are more robust to noise since the mean gradient over a larger batch is closer to the gradient for correct labels.for 1 ≤ k ≤ m. Then, the standard cross-entropy loss over a batch X is given by:wherê y is the predicted vector and X denotes the ex- pected value over the batch X. We assumê y is normalized (e.g. by the softmax function) so that the entries sum to 1.To investigate this explanation further, we also consider the theoretical case of infinite batch size, in which gradients are averaged over the entire space of possible inputs at each training step. While this is often impossible to perform in practice, we can simulate such behavior by an auxiliary loss For a training set with noisy labels, we may consider the la- bel f (x) given in the training set to be merely an approxima- tion to the true label f 0 (x). Consider the case of n training examples, and αn noisy labels that are sampled uniformly at random from the set {1, 2, . . . , m}. Then, f (x) = f 0 (x) with probability 1 1+α , and otherwise it is 1, 2, . . . , m, each with probability α m(1+α) . As batch size increases, the ex- pected value over the batch X is approximated more closely by these probabilities. In the limit of infinite batch size,  (Krizhevsky, 2014). As noisy labels reduce the effective batch size, we would expect that lower learning rates perform better than large learning rates as noise increases. Figure 12 shows the performance of a 4-layer ConvNet trained with different learning rates on CIFAR-10 for varying label noise. As expected, we observe that the optimal learning rate decreases as noise increases. For example, the optimal learning rate for the clean dataset is 1, while, with the introduction of noise, this learning rate becomes unstable. equation (2) takes the form of a noisy loss function H α :In this paper, we studied the behavior of deep neural net- works on training sets with very noisy labels. In a series of experiments, we have demonstrated that learning is robust to an essentially arbitrary amount of label noise, provided that the number of clean labels is sufficiently large. We have further shown that the required number of clean labels in- creases slightly above linear with the ratio of noisy to clean labels. Finally, we have observed that noisy labels reduce the effective batch size, an effect that can be mitigated by larger batch sizes and downscaling the learning rate. It is worthy of note that although deep networks appear robust to even high degrees of label noise, clean labels still always perform better than noisy labels, given the same quantity of training data. Further, one still requires expert- vetted test sets for evaluation. Lastly, it is important to reiterate that our studies focus on non-adversarial noise.We can therefore compare training using the cross-entropy loss with αn noisy labels to training using the noisy loss function H α without noisy labels. The term on the right- hand side of (3) represents the noise contribution, and is clearly minimized wherê y k are all equal. As α in- creases, this contribution is weighted more heavily against −−logˆylogˆ logˆy f0(x) X , which is minimized atˆyatˆ atˆy(x) = y(x).We show in Figure 11 the results of training our 2-layer Con- vNet on MNIST with the noisy loss function H α , simulating αn noisy labels with infinite batch size. We can observe that the network's accuracy does not decrease as α increases. This can be explained by the observation that for large batch- sizes increasing α is merely decreasing the magnitude of the true gradient, rather than altering its direction.Our work suggests numerous directions for future investiga- tion. For example, we are interested in how label-cleaning and semi-supervised methods affect the performance of net- works in a high-noise regime. Are such approaches able to lower the threshold for training set size? Finally, it remains to translate the results we present into an actionable trade-off between data annotation and acquisition costs, which can be utilized in real world training pipelines for deep networks on massive noisy data.
