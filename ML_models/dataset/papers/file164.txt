Reinforcement learning (RL) methods in realistic environments typically need to deal with incom- plete and noisy state information resulting from partial observability as formalized by Partially Ob- servable Markov Decision Processes (POMDPs) [Son71]. In addition, they often need to deal with non-Markovian problems where there are significant dependencies on earlier states. Both POMDPs and non-Markovian problems largely defy traditional fully parametric value function or policy based approaches and currently require handcrafted state estimators based on accurate knowledge of the system. In this context, the use of neural networks used a value function or policy over a rein- forcement learning paradigm for solving continuous control problems has a long history. Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedom [BG15, HWS + 15, LFDA15]. However, all of this work still assumes a fully observed state.A naive alternative to using memory is learning reactive stochastic policies [SJJ94] which simply map observations to probabilities of actions. The underlying assumption is that state-information does not play a crucial role during most parts of the problem and that using random actions can prevent the policy from getting stuck in an endless loop for ambiguous observations. Often, this strategy is far from optimal and algorithms that use some form of memory remain necessary. In summary, when a perfect model and a precisely estimated state can not be assumed, an optimal pol- icy is likely to be memory-based. However, works on policy gradient methods with memory have been rare so far, and largely limited to finite-state controllers [Abe03, MPKK99]. More recently, some work has been proposed for a partially observable instance of the ATARI 2600 framework [BNVB13, HS15] but that work makes no attempt to provide an attention mechanism over an accu- mulative memory.Rather, those authors suggest to use a fixed size memory model, Long Short Term Memory (LSTM), for control learning.In this paper, we extend the above LSTM approaches to more sophisticated policy representations capable of representing an observed state using a memory enhanced architecture called Gated End- to-End Memory Policy Network. With this model, policy gradient type of algorithm can effectively learn policies for POMDPs using an unbounded memory by leveraging an attention mechanism over the past observations. As a result, policy updates can depend on any event in the history. We show that our method outperforms other RL methods on a proposed benchmark task: continuous control in a non-Markovian trading environments.The paper is organized as follows: Section 2 formulates POMDPs and discusses the use of reinforce- ment learning for POMDPs. Section 3 proposes the usage of Gated End-to-End Memory Networks for memory-enhanced reinforcement learning. In this section, a derivation of the model as policy network is presented. Then, Section 4 describes the trading and optimized execution tasks chosen for evaluation purposes. The pertinence of such environment, developed using the OpenAi Gym framework is discussed. Finally, Section 5 presents results the two task using 8 real indices.In the standard paradigm of Reinforcement Learning, an agent interacts with an environment E dur- ing a potentially infinite number of discrete time steps. At each time step t, the agent observes a state s t ∈ S and chooses an action a t from some set of admissible actions A by using its pol- icy π, where π is a function from states s t to actions a t . As a result, the agent observes the next state s t+1 and receives a scalar reward r t . The process continues until the agent reaches a termi- nal state. We define as return R t = ∞ k=0 γ k r t+k , the total accumulated return from time step t with discount factor γ ∈ (0, 1]. The goal of the agent is to maximize the expected return from each state s t . The action value Q π (s, a) = E [R t |s t = s, a] is the expected return for selecting action a in state s and following policy π. The optimal value function Q * (s, a) = max π Q π (s, a) gives the maximum action value for state s and action a achievable by any policy. Similarly, the value of state s under policy π is defined as V π (s) = E [R t |s t = s] and is simply the expected return for following policy π from state s. In value-based model-free reinforcement learning meth- ods, the action value function is often modeled using a function approximator, such as a neural network. Let Q(s, a; θ) be an approximate action-value function with parameters θ. The updates to θ can be defined by a variety of reinforcement learning algorithms. A well known example of such an algorithm is Q-learning, which aims to directly approximate the optimal action value func- tion: Q * (s, a) ≈ Q(s, a; θ). In one-step Q-learning, the parameters θ of the action value function Q(s, a; θ) are learned by iteratively minimizing a sequence of loss functions, where the ith loss function defined asLwhere s ′ is the state encountered after state s. This standard formulation of the problem is called a Markov Decision Process. It assumes that the environment is Markovian, which means the transition to a state s t+1 is only conditioned by the {s t , a t } pair.Formally, a POMDP is described as a 6-tuple (S, A, P, R, ω, Z), where S, A, P, and R are, respec- tively, the states, actions, transition function P (S t+1 |S t , A t ), and reward function R : S × A → R of a Markov Decision Process (MDP). In addition, the agent has no longer access to the true system state but receives an observation instead. This observation is generated from the underlying system state according to the probability distribution z ∼ Z(s) = P (z t |s t ). The goal of the agent is to infer a policy π : Z 1:t → A t in order to maximize cumulative reward. The formalism of POMDP well captures the dynamics of many real world environments by explicitly acknowledging that the perception received by the agent offers only a partial glimpse of the underlying state. In realistic world environments it is not reasnoable to assume that the full state of the system can be provided to the agent or even determined. Consequently, the Markov property rarely holds in such observed environments.The resolution of partial observability, also called perceptual aliasing [WB91], is non-trivial and existing methods can roughly be divided into two classes. The first class of approaches explicitly maintain a belief state that corresponds to the distribution over the world states given the previous observations. Assuming a model-free hypothesis, i.e. no assumption taken over the transition and reward functions, a policy can be derived from this state estimation using reinforcement learning methods like value based methods, e.g. Q-Learning or policy based methods, e.g. policy-gradient approaches. Two major disadvantages can be mentioned: The first is the need for a model of Z(s) to support the state inference task. The second is the computational cost that is typically associated with the update of this belief state [KLC98, SPK13]. The second class of approaches learn to form and use memories based on interactions with the environment. These methods are challenging since it is a priori unknown which features of the observations will be relevant later, and associations may have to be formed over many steps. Here, having a differentiable mechanism to learn such dependencies from experience becomes desirable. For this reason, most model free approaches tend to assume full observability. In practice, partial observability is often solved by hand-crafting a sufficient state representation from observations. As an example, in video-games, one can estimate velocity from consecutive frames [MKS + 15a, DCH + 16].As mentioned before, the first attempts of Deep Q-Network, experimented on ATARI 2600 video- games, had no explicit mechanism for inferring the underlying state sequence of the POMDP, thus being effective only when a contiguous series of past observations reflect of the underlying system states [MKS + 15a]. In the general case, learning a Q-function : S × A → R from a fixed observation window can be arbitrarily bad since Q(z t−k:t , a t |θ) = Q(s t , a t |θ), where k is fixed. More recently, Deep Recurrent Q-Learning has been proposed [HS15]. This method uses a recurrent network, namely an Long Short Term Memory (LSTM) [HS97], to add a memorization capability to the previously proposed model. A drawback of this proposal comes from the necessity of selecting a priori, or using cross-validation, the dimension of the hidden and context vectors of the recurrent model which determine the memorization capacity. Another point of discussion might concern the experimental setting used in this last work. The authors propose to develop an artificial "Flickered" version of the Atari 2600 platform in order to mask parts or the entire current frame at a given period in order to force the model to memorize. In such a way, the performance of the model on an environment that has been transformed to a non-Markovian one can be measured. Finally, [OCSL16] is the closest reference to our work. The authors experiment the use of an off-the-shelf memory network as policy for the task of exploration and path-finding in a virtual 3D environment.In this paper, we make two propositions. First, we investigate the use of a gated attention mechanism coupled with a deep recurrent Q-Network. We suggest that such mechanism may allow the Q- network to better estimate the underlying system state, narrowing the gap between Q(z t−k:t , a t |θ) and Q(s t , a t |θ). Indeed, in the following sections we will show that attention enhanced deep Q- networks can better approximate actual Q-values from sequences of observations, leading to better policies in partially observed environments. As a second contribution, we present a simple simulated environment of stock trading for evaluating our proposed model. We compare it to fully connected neural networks and LSTM in the tasks of stock exchange and a simplified but realistic task of optimized execution that we will now briefly present.The field of algorithmic trading regroups a large family of methods that have been proposed to perform autonomous decision models over the global financial market. The discipline can be roughly decomposed into two categories. On the first hand, predictive methods with deterministic policies consist in learning indicators used as support for a deterministic, or stochastic but stationary, decision schema [Lev95, ZNG01]. These methods consist in learning actionable patterns used to trigger buying or selling actions based on the history of a identified set of trading signals or external macro- economical informations. On the other hand, policy learning has been investigated as a way to learn a investement and portfolio management policy directly from the stock market history and also macro-economical events [Neu95, Neu97, MS98]. More recently, the task of optimized execution has also been studied [NFK06]. In this context, the action space is reduced to just either selling or buying. Indeed, the actual policy been determined by a independent system, the optimized execution algorithm is in charge of applying an order to the market while leveraging on the constant fluctuation of the share prices in order to maximize the profitability of a chosen operation. In the context of this paper, we do not have the ambition to challenge highly a priori knowledge enriched and partially handcrafted portfolio management policies that are currently implemented in the real market place. However, we believe this execution context can be a novel and fruitful environment of experiment for conducting research on non-Markovian decision policy learning.In a model-free approach, the non-Markovian observation state transitions require the decision model to store observations resulting from the interaction with the environment in order to gather sufficient information to support decision. Recently, recurrent models like LSTM have been inves- tigated to incorporate such a memorization capability into the decision model for direct policy or value function learning [HS15, LC16]. A drawback of such an approach is the necessity of defining, as a hyper-parameter, the dimension of the hidden state vector of the network that limits the memory of the model. Furthermore, such a recurrent model does not explicitly learn to focus its attention to different parts of a growing memory when long temporal dependencies occur in observation space.As an alternative, attention-based models have already provided an encouraging alternative on sev- eral sequential decision tasks with immediate reward maximization like natural language translation [BCB14] or end-to-end dialog systems. In the former domain, two types of approaches have been investigated. The so-called sequence-to sequence model aims at memorizing the overall source sen- tence before deciding the target sentence words sequentially [SVL14]. The attention-based model aims at iteratively constructing a representation of an unbounded memory conditioned by the cur- rent state of the target sentence word generator [BCB15, PL16]. Motivated by the recent empirical success of the latter method, we further investigate such an approach based on the recently proposed Gated Memory Network model.As depicted in the next section, originally this line of research focused on text-based applications like natural language understanding, dialog management and machine reading. So, we propose to adapt and extend the use of such a model to policy learning.The End-to-End Memory Network architecture (MemN2N) [SSWF15a], consists of two main com- ponents: supporting memories and final answer prediction. Supporting memories are in turn com- prised of a set of input and output memory representations with memory cells. The input and output memory cells, denoted by m i and c i , are obtained by transforming the input observations x 1 , . . . , x n using two embedding matrices A and C , both of size d i For more difficult tasks requiring multiple supporting memories, the model can be extended to in- clude more than one set of input/output memories by stacking a number of memory layers. In this setting, each memory layer is named a hop and the (k + 1) th hop takes as input the output of the k{zi} Observations / Stock signals {zt=1, . . . , zt=T −1} hop:Lastly, the final step, the prediction of the answer to the question q, is performed byâbyˆbyâ = softmax(W (owherê a is the predicted answer distribution, W ∈ R |V |×d is a parameter matrix for the model to learn and K the total number of hops. As suggested in [PL16], Equation (2) can be considered as a form of residual with o k working as the residual function and u k the shortcut connection. However, as discussed in [SGS15], in contrast to the hard-wired skip connection in Residual Networks, one of the advantages of Highway Networks is the adaptive gating mechanism, capable of learning to dynamically control the information flow based on the current input. There- fore, we adopt the idea of the adaptive gating mechanism of Highway Networks and integrate it into MemN2N. The resulting model, named Gated End-to-End Memory Networks (GMemN2N) [PL16] and illustrated in Figure 1, is capable of dynamically conditioning the memory reading operation on the controller state u k at each hop. Concretely, we reformulate Equation (2) into:where W k T and b k are the hop-specific parameter matrix and bias term for the k th hop and T k (x) the transform gate for the k th hop and σ a sigmoidal activation function.In the case of policy learning, the memory cells are filled with past observations collected from past interactions with the environment, and the question input will carry current state information that are relevant to the agent and independent from the environment observations. In the context of stock trading and optimized execution, the memory blocks will carry the past values of the traded signal and the question block will carry the current budget and portfolio composition of the agent. Finally, assuming a discrete action set, the output of the model, the answer, will be the expected reward associated to each eligible action. Figure 1 summarizes the elements of this Gated End-to-End Memory Policy Network.A limitation of Memory Networks compared to other types of attention-based models, like those ap- plied to machine translation, is the necessity to encode temporal information into the memory blocks. Indeed, because of the commutative nature of Equation 1, any information regarding the order of the observations embedded in the memory blocks has to be encoded beforehand. In Dynamic Memory Network [KIO + 16], the hidden state of an LSTM is used to encode the values put into the memory blocks before computing the attention values over them. In the original end-to-end memory net- work [SSWF15b], the encoding is done using a deterministic function that transforms the sequence of word embeddings of each sentence before putting them into the memory blocks. We propose to embed the signal using a denoising and predictive neural auto-encoder. More specifically, the single hidden layer of the perceptron reconstructing the noisy input of the time frame is placed into the memory blocks. On one hand, the model is only a denoiser, but on the other hand, by adding output to the model, the neural network can predict future windows regarding the encoded time frame. This approach is related to the context-dependent word vectorization [MCCD13, PSM14].We consider policies represented as gated memory networks. The model builds a vector, i.e the con- troller state u, representing its latent state from the multiple attention-based readings of its memory blocks where the environment observations have been stored. The latent state begins with a fixed state u 0 . At each time-step t = 1, 2, . . . , n, the network takes as an input a series of observations, and computes its internal state according to a differentiable function F (z 1:n |θ f ) = u t and outputs a dis- tribution over actions a t according to a differentiable function G(u t |θ g ) = a t where θ = (θ f , θ g ). π θ (a t |o 1:t ) denotes the output of the memory network at time-step t. Past work has defined a principled method for updating the parameters θ of the policy π θ through reinforcement learning [Wil92, PS06] using stochastic gradient descent:While this update is unbiased, in practice it is known to suffer high variance and low converge rate. It has been shown [Wil92] that this update can be rewritten as, where b is a baseline, which can be an arbitrary function of states visited during an episode. Us- ing this general framework of policy-gradient learning via Gated Memory Network, we define our control model using the approach that have been described in the context of language modeling [SSWF15b]. In this application, a constant is defined as q and the network produces as an output a distribution over the vocabulary. This kind of approach can be put in parallel with the control model of Deep Q-Learning proposed in [MKS + 15b] where a Convolutional Neural Network takes as input a contiguous sliding window of video game screens and output the Q-values associated to a finite set of eligible actions. Finally, because of its stability in learning parametric policies, we use Asyn- chronous Deep Q-Learning as reinforcement learning algorithm which is described in Algorithm ?? as proposed in [MBM + 16].As an evaluation environment, we developed a simplified portfolio management platform. Following the settings proposed in [MS98], the decision space of trading consists in a set of three discrete actions A ∈ {Buy, Hold, Sell} assuming a fixed amount of stock exchanged for each action. The observation space Z ∈ R k is the current value of the k stocks considered for trading. For each transaction, a fixed transaction cost is associated. In a more realistic setting, the transaction cost is likely to be a function of the type and the amount of stocks involved at each decision step. In our experiments, we only consider the task of speculative trading which means that the reward, measured as the increase of budget at a given time step is the result of the evolution of the market shares. In a more realistic settings, dividends, which are the part of the companies benefice distributed to share holders, should also be considered as a potential source of income, especially in a multi-year scale and multi-stocks management settings.A second task that as been studied in the litterature is the optimized execution setting. It consists of either selling or buying a given amount of stock in a fixed amount of time as described in [NFK06]. For the optimized buying case, the goal consists in buying the desired amount of stock at the cheapest price over a given period of time. For the optimized selling case, the goal consists in following an acquisition strategy that allows us to sell at the higher possible price during the given period. Our simulation platform has been developed as a OpenAI Gym [BCP + 16] environment and is planned to be published as an open-source package. Our purpose is to encourage the research community of non-Markovian reinforcement learning to use such a framework as a reusable experimental testbed.During our experiments, indices have been studied as trading signals. The daily opening prices of a set of real indices have been chosen. However an other advantage of using stock exchange as a test-bed for non-Markovian control is the possibility to also generate such synthetic series. In comparison to other virtual environment, like First Person Shooter [KWR + 16] or Atari 2600 [BNVB13], the control of the required memory capacity to perform profitable control can be defined by estimating the Markovian order of the series. Indeed, in the context of games, the memory capacity can hardly be related from the partially observable maze or first person shooter as a function of the size of the maze. However, in the case of trading, the memory capacity requirement can be defined as the order of the time series. For our experiments, we choose to focus on 8 real indices taken from the main market places in US, Europe and Asia.Our Gated End-to-End Memory Policy Network takes as input the past observations of the traded series. At each time step, It computes the expected reward of each eligible actions. The model is optimized through policy gradient, prioritized experience replay [SQAS15] and double Q-learning in order to cope with inherent instability of such learning process. Beyond the stability and con- vergence rate compared to Q-Learning, such model allows one to implement a Boltzmann type of policy over the reward expectation using one forward pass of the model.Concernint the parameterization of our decision model. As suggested in [SSWF15a], 10% adjacent weight tying, and temporal encoding with 10% random noise is used. Learning rate η is initially assigned a value of 0.001 with exponential decay applied every 30 epochs by η/2 until 100 epochs are reached. Linear start is used in all our experiments as proposed by [SSWF15a]. With linear start, the softmax in each memory layer is removed and re-inserted after 30 epochs. Batch size is set to 32 and gradients with an ℓ 2 norm larger than 10 are divided by a scalar to have norm 10. All weights are initialized randomly from a Gaussian distribution with zero mean and σ = 0.1 except for the transform gate bias b k T which we empirically set the mean to 0.2. In all our experiments, we use the embedding size d = 20. As in [SSWF15a], since the memory-based models are sensitive to parameter initialization, we repeat each training 20 times and choose the best system based on the performance on the validation set. The temporal neural encoders are learnt individually over each training series and used in test to preprocess observation sequences before been placed into the memory block of the policy network. The hidden layer dimension of each encoder has been set by cross-validation to 25 and optimized using Adam [KB14]. Then, the baseline neural policy network is composed with two hidden layers of 30 hidden units with rectified linear activation and a linear output projection. The baseline LSTM model has a hidden representation of 50 dimensions. All the hyperparameters haven been estimated through cross-validation. Concerning the policy learning algorithm. The reward function is episodic. At the end of each episode, the agent receives a reward which is the difference between the budget at the end of the period and the initial budget. The network was trained using 200 consecutive days of daily opening values. The training phase consists in 10000 trading episodes over these sequences of values. The training on a given series represents approximatively one hour on one core of a NVIDIA Tesla P-100 GPU. In this experiment all policies are learnt independently from one series to another. The testing phase of each trading experiment is performed using 200 consecutive days of market. In the case of optimized trading, each testing corresponds to 100 roll-outs. The resulting policies follows a Bolzmann distribution over the reward predicted by the policy network. Finally, the update period of the Double Q-Learning mechanism is 100 action steps. Table 2 computes the profitability ratio which corresponds to the number of days, over the test pe- riod, where the agent is profitable. A trading day is qualified as profitable if the difference between the corresponding current budget and the initial budget of the agent is positive. Such evaluation makes sense as a speculative strategy where maximizing the amount of positive market exit oppor- tunities over a given period of time is excepted to be maximized. This first results confirms the utility of a control policy equipped with a memorization capability. Then, a control policy equipped with an attention mechanism as the one proposed in this work seems to be confirm.  The evaluation of the proposed policy over an optimized selling task is also depicted. In such setting, the set of authorized actions are reduced to A = {Hold, Sell}. The agent starts each episode with 50 stocks to sell in the trading period. The reward is the resulting accumulated budget at the end of the period. As for trading, the policies are evaluated of a testing series of 100 trading days. In such settings, the proposed policy show encouraging result that confirm the benefit of an attention based mechanism of memory management for learning differentiable policies in non- Markovian environment. For all experiments, the series absolute values are max-normalized in order to accelerate gradient descent and control gradient magnitude. Finally, the necessity of a memory in such task seems to be confirmed by the inferior performance of a memory-less fully connected layer model.In this paper, we have studied the question of non-Markovian decision processes and the use of an attention-based policy network called Gated End-to-End Memory Policy Network. The task of stock exchange and optimized execution have been used as experimental testbed to illustrate the capability of the model. In addition to the proposed the model, we think such a trading environment can produce fruitful research in the domain of non-Markovian control in the future. Indeed, the settings of stock exchange and revenue maximization allow to study the behavior of policy learn- ing algorithms and policy models with signals exhibiting different requirement of memorization. Furthermore, tasks of resource allocation and scheduling can be easily related to this formal set- ting. Finally, in comparison to the current results using parametric memories like Gated Rectified Units or Long Short Term Memory, we believe attention-based models, which have already demon- strated their advantages in the domain of sequence prediction in Natural Language Processing like machine translation and machine reading, can be of a first importance in the more general case of non-Markovian control. In the near future, we plan to release an open-source package of our OpenAI Gym Trade environment and the corresponding Gated Memory Policy Networks.
