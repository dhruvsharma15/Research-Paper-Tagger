Neural networks (NNs) are powerful approximators, which are built by in- terleaving linear layers with nonlinear mappings (generally called activation functions). The latter step is usually implemented using an element-wise, (sub-)differentiable, and fixed nonlinear function at every neuron. In partic- ular, the current consensus has shifted from the use of contractive mappings (e.g., sigmoids) to the use of piecewise-linear functions (e.g., rectified linear units, ReLUs (Glorot and Bengio, 2010)), allowing a more efficient flow of the backpropagated error ( Goodfellow et al., 2016). This relatively inflexi- ble architecture might help explaining the extreme redundancy found in the trained parameters of modern NNs (Denil et al., 2013).Designing ways to adapt the activation functions themselves, however, faces several challenges. On one hand, we can parameterize a known acti- vation function with a small number of trainable parameters, describing for example the slope of a particular linear segment ( He et al., 2015). While immediate to implement, this only results in a small increase in flexibility and a marginal improvement in performance in the general case (Agostinelli et al., 2014). On the other hand, a more interesting task is to devise a scheme allowing each activation function to model a large range of shapes, such as any smooth function defined over a subset of the real line. In this case, the inclusion of one (or more) hyper-parameters enables the user to trade off between greater flexibility and a larger number of parameters per-neuron. We refer to these schemes in general as non-parametric activation functions, since the number of (adaptable) parameters can potentially grow without bound.There are three main classes of non-parametric activation functions known in the literature: adaptive piecewise linear (APL) functions (Agostinelli et al., 2014), maxout networks ( Goodfellow et al., 2013), and spline activation func- tions ( Guarnieri et al., 1999). These are described more in depth in Section 5. In there, we also argue that none of these approaches is fully satisfac- tory, meaning that each of them loses one or more desirable properties, such as smoothness of the resulting functions in the APL and maxout cases (see Table 1 in Section 6 for a schematic comparison).In this paper, we propose a fourth class of non-parametric activation functions, which are based on a kernel representation of the function. In par- ticular, we define each activation function as a linear superposition of several kernel evaluations, where the dictionary of the expansion is fixed beforehand by sampling the real line. As we show later on, the resulting kernel activation functions (KAFs) have a number of desirable properties, including: (i) they can be computed cheaply using vector-matrix operations; (ii) they are linear with respect to the trainable parameters; (iii) they are smooth over their entire domain; (iv) using the Gaussian kernel, their parameters only have local effects on the resulting shapes; (v) the parameters can be regularized using any classical approach, including the possibility of enforcing sparseness through the use of 1 norms. To the best of our knowledge, none of the known methods possess all these properties simultaneously. We call a NN endowed with KAFs at every neuron a Kafnet.Importantly, framing our method as a kernel technique allows us to poten- tially leverage over a huge literature on kernel methods, either in statistics, machine learning ( Hofmann et al., 2008), and signal processing ( Liu et al., 2011). Here, we preliminarly demonstrate this by discussing several heuristics to choose the kernel hyper-parameter, along with techniques for initializing the trainable parameters. However, much more can be applied in this con- text, as we discuss in depth in the conclusive section. We also propose a bi-dimensional variant of our KAF, allowing the information from multiple linear projections to be nonlinearly combined in an adaptive fashion.In addition, we contend that one reason for the rareness of flexible activa- tion functions in practice can be found in the lack of a cohesive (introductory) treatment on the topic. To this end, a further aim of this paper is to provide a relatively comprehensive overview on the selection of a proper activation function. In particular, we divide the discussion on the state-of-the-art in three separate sections. Section 3 introduces the most common (fixed) acti- vation functions used in NNs, from the classical sigmoid function up to the recently developed self-normalizing unit ( Klambauer et al., 2017) and Swish function ( Ramachandran et al., 2017). Then, we describe in Section 4 how most of these functions can be efficiently parameterized by one or more adap- tive scalar values in order to enhance their flexibility. Finally, we introduce the three existing models for designing non-parametric activation functions in Section 5. For each of them, we briefly discuss relative strengths and draw-backs, which serve as a motivation for introducing the model subsequently.The rest of the paper is composed of four additional sections. Section 6 describes the proposed KAFs, together with several practical implementation guidelines regarding the selection of the dictionary and the choice of a proper initialization for the weights. For completeness, Section 7 briefly describes additional strategies to improve the activation functions, going beyond the addition of trainable parameters in the model. A large set of experiments is described in Section 8, and, finally, the main conclusions and a set of future lines of research are given in Section 9.We denote vectors using boldface lowercase letters, e.g., a; matrices are denoted by boldface uppercase letters, e.g., A. All vectors are assumed to be column vectors. The operator p is the standard p norm on an Euclidean space. For p = 2, it coincides with the Euclidean norm, while for p = 1 we obtain the Manhattan (or taxicab) norm defined for a generic vector v ∈ R B as 1 = B k=1 |v k |. Additional notations are introduced along the paper when required.We consider training a standard feedforward NN, whose l-th layer is de- scribed by the following equation:whereand b l ∈ R N l are adaptable weight matrices, and g l (·) is a nonlinear function, called activation function, which is applied element-wise. In a NN with L layers, x = h 0 denotes the input to the network, whilê y = h L denotes the final output.For training the network, we are provided with a set of, and we minimize a regularized cost function given by:where w ∈ R Q collects all the trainable parameters of the network, l(·, ·) is a loss function (e.g., the squared loss), r(·) is used to regularize the weights using, e.g., 2 or 1 penalties, and the regularization factor C &gt; 0 balances the two terms.In the following, we review common choices for the selection of g l , before describing methods to adapt them based on the training data. For readabil- ity, we will drop the subscript l, and we use the letter s to denote a single input to the function, which we call an activation. Note that, in most cases, the activation function g L for the last layer cannot be chosen freely, as it depends on the task and a proper scaling of the output. In particular, it is common to select g(s) = s for regression problems, and a sigmoid function for binary problems with y i ∈ {0, 1}:For multi-class problems with dummy encodings on the output, the softmax function generalizes the sigmoid and it ensures valid probability distributions in the output ( Bishop, 2006).We briefly review some common (fixed) activation functions for neural networks, that are the basis for the parametric ones in the next section. Before the current wave of deep learning, most activation functions used in NNs were of a 'squashing' type, i.e., they were monotonically non-decreasing functions satisfying: limwhere c can be either 0 or −1, depending on convention. Apart from the sigmoid in (3), another common choice is the hyperbolic tangent, defined as:Cybenko (1989) proved the universal approximation property for this class of functions, and his results were later extended to a larger class of functions in Hornik et al. (1989). In practice, squashing functions were found of lim- ited use in deep networks (where L is large), being prone to the problem of vanishing and exploding gradients, due to their bounded derivatives (Hochre-iter et al., 2001). A breakthrough in modern deep learning came from the introduction of the rectifier linear unit (ReLU) function, defined as:Despite being unbounded and introducing a point of non-differentiability, the ReLU has proven to be extremely effective for deep networks (Glorot and Bengio, 2010;Maas et al., 2013). The ReLU has two main advantages. First, its gradient is either 0 or 1, 1 making back-propagation particularly efficient. Secondly, its activations are sparse, which is beneficial from several points of views. A smoothed version of the ReLU, called softplus, is also introduced in Glorot et al. (2011): g(s) = log {1 + exp {s}} .Despite its lack of smoothness, ReLU functions are almost always preferred to the softplus in practice. One obvious problem of ReLUs is that, for a wrong initialization or an unfortunate weight update, its activation can get stuck in 0, irrespective of the input. This is referred to as the 'dying ReLU' condition. To circumvent this problem, Maas et al. (2013) introduced the leaky ReLU function, defined as:where the user-defined parameter α &gt; 0 is generally set to a small value, such as 0.01. While the resulting pattern of activations is not exactly sparse anymore, the parameters cannot get stuck in a poor region. (8) can also be written more compactly as g(s) = max {0, s} + α min {0, s}.Another problem of activation functions having only non-negative output values is that their mean value is always positive by definition. Motivated by an analogy with the natural gradient, Clevert et al. (2016) introduced the exponential linear unit (ELU) to renormalize the pattern of activations:where in this case α is generally chosen as 1. The ELU modifies the negative part of the ReLU with a function saturating at a user-defined value α. It is computationally efficient, being smooth (differently from the ReLU and the leaky ReLU), and with a gradient which is either 1 or g(s) + α for negative values of s.The recently introduced scaled ELU (SELU) generalizes the ELU to have further control over the range of activations ( Klambauer et al., 2017):where λ &gt; 1 is a second user-defined parameter. Particularly, it is shown in Klambauer et al. (2017) that for λ ≈ 1.6733 and α ≈ 1.0507, the successive application of (1) converges towards a fixed distribution with zero mean and unit variance, leading to a self-normalizing network behavior. Finally, Swish ( Ramachandran et al., 2017) is a recently proposed acti- vation somewhat inspired by the gating steps in a standard LSTM recurrent cell:where δ(s) is the sigmoid in (3).An immediate way to increase the flexibility of a NN is to parameterize one of the previously introduced activation functions with a fixed (small) number of adaptable parameters, such that each neuron can adapt its acti- vation function to a different shape. As long as the function remains differ- entiable with respect to these new parameters, it is possible to adapt them with any numerical optimization algorithm together with the linear weights and biases of the layer. Due to their fixed number of parameters and limited flexibility, we call these parametric activation functions.Historically, one of the first proposals in this sense was the generalized hyperbolic tangent (Chen and Chang, 1996), a tanh function parameterized by two additional positive scalar values a and b:Note that the parameters a, b are initialized randomly and are adapted in- dependently for every neuron. Specifically, a determines the range of the output (which is called the amplitude of the function), while b controls the slope of the curve. Trentin (2001) provides empirical evidence that learning the amplitude for each neuron is beneficial (either in terms of generaliza- tion error, or speed of convergence) with respect to having unit amplitude for all activation functions. Similar results were also obtained for recurrent networks (Goh and Mandic, 2003  (8), where the coefficient α is initialized at α = 0.25 everywhere and then adapted for every neuron. The resulting activation function is called parametric ReLU (PReLU), and it has a very simple derivative with respect to the new parameter:For a layer with N hidden neurons, this introduces only N additional param- eters, compared to 2N parameters for the generalized tanh. Importantly, in the case of p regularization, the user has to be careful not to regularize the α parameters, which would bias the optimization process towards classical ReLU / leaky ReLU activation functions. Similarly, Trottier et al. (2016) propose a modification of the ELU func- tion in (9) with an additional scalar parameter β, called parametric ELU (PELU):where both α and β are initialized randomly and adapted during the training process. Based on the analysis in Jin et al. (2016), there always exists a setting for the linear weights and α, β which avoids the vanishing gradient problem. Differently from the PReLU, however, the two parameters should be regularized in order to avoid a degenerate behavior with respect to the linear weights, where extremely small linear weights are coupled with very large values for the parameters of the activation functions. A more flexible proposal is the S-shaped ReLU (SReLU) ( Jin et al., 2016), which is parameterized by four scalar valuesThe SReLU is composed by three linear segments, the middle of which is the identity. Differently from the PReLU, however, the cut-off points between the three segments can also be adapted. Additionally, the function can have both convex and nonconvex shapes, depending on the orientation of the left and right segments, making it more flexible than previous proposals. Similar to the PReLU, the four parameters should not be regularized (Jin et al., 2016). Finally, a parametric version of the Swish function is the β-swish (Ra- machandran et al., 2017), which includes a tunable parameter β inside the self-gate:Intuitively, parametric activation functions have limited flexibility, re- sulting in mixed performance gains on average. Differently from parametric approaches, non-parametric activation functions allow to model a larger class of shapes (in the best case, any continuous segment), at the price of a larger number of adaptable parameters. As stated in the introduction, these meth- ods generally introduce a further global hyper-parameter allowing to balance the flexibility of the function, by varying the effective number of free param- eters, which can potentially grow without bound. Additionally, the methods can be grouped depending on whether each parameter has a local or global effect on the overall function, the former being a desirable characteristic.In this section, we describe three state-of-the-art approaches for imple- menting non-parametric activation functions: APL functions in Section 5.1, spline functions in Section 5.2, and maxout networks in Section 5.3.An APL function, introduced in Agostinelli et al. (2014), generalizes the SReLU function in (15) by summing multiple linear segments, where all slopes and cut-off points are learned under the constraint that the overall function is continuous:S is a hyper-parameter chosen by the user, while each APL is parameter- ized by 2S adaptable parameters. These parameters are randomly initialized for each neuron, and can be regularized with 2 regularization, similarly to the PELU, in order to avoid the coupling of very small linear weights and very large a i coefficients for the APL units.The APL unit cannot approximate any possible function. Its approxima- tion properties are described in the following theorem. , provided that h(s) satisfies the following two conditions: 1. There exists u ∈ R such that h(s) = s, ∀s ≥ u. 2. There exist two scalars, v, t ∈ R such that dh(s) s = t, ∀s &lt; v.The previous theorem implies that any piecewise-linear function can be ap- proximated, provided that its behavior is linear for very large, or small, s. A possible drawback of the APL activation function is that it introduces S + 1 points of non-differentiability for each neuron, which may damage the opti- mization algorithm. The next class of functions solves this problem, at the cost of a possibly larger number of parameters.An immediate way to exploit polynomial interpolation in NNs it to build the activation function over powers of the activation s (Piazza et al., 1992):where P is a hyper-parameter and we adapt the (P + 1) coefficients. Since a polynomial of degree P can pass exactly through P + 1 points, this polynomial activation function (PAF) can in theory approximate any smooth function. The drawback of this approach is that each parameter a i has a global influence on the overall shape, and the output of the function can easily grow too large or encounter numerical problems, particularly for large absolute values of s and large P .An improved way to use polynomial expansions is spline interpolation, giving rise to the spline activation function (SAF). The SAF was originally studied in Vecci et al. (1998); Guarnieri et al. (1999), and later re-introduced in a more modern context in Scardapane et al. (2016), following previous advances in nonlinear filtering ( Scarpiniti et al., 2013). In the sequel, we adopt the newer formulation.A SAF is described by a vector of T parameters, called knots, correspond- ing to a sampling of its y-values over an equispaced grid of T points over the x-axis, that are symmetrically chosen around the origin with sampling step ∆x. For any other value of s, the output value of the SAF is computed with spline interpolation over the closest knot and its P rightmost neigh- bors, where P is generally chosen equal to 3, giving rise to a cubic spline interpolation scheme. Specifically, denote by k the index of the closest knot, and by q k the vector comprising the corresponding knot and its P neighbors. We call this vector the span. We also define a new valuewhere ∆x is the user-defined sampling step. u defines a normalized abscissa value between the k-th knot and the (k + 1)-th one. The output of the SAF is then given by (Scarpiniti et al., 2013):where the vector u collects powers of u up to the order P :and B is the spline basis matrix, which defines the properties of the in- terpolation scheme (as shown later in Fig. 1b). For example, the popular Catmull-Rom basis for P = 3 is given by:The derivatives of the SAF can be computed in a similar way, both with respect to s and with respect to q k , e.g., see Scardapane et al. (2016). A visual example of the SAF output is given in Fig. 1. Each knot has only a limited local influence over the output, making their adaptation more stable. The resulting function is also smooth, and can in fact approximate any smooth function defined over a subset of the real line to a desired level of accuracy, provided that ∆x is chosen small enough. The drawback is that regularizing the resulting activation functions is harder to achieve, since p regularization cannot be applied directly to the values of the knots. In Guarnieri et al. (1999), this was solved by choosing a large ∆x, in turn severely limiting the flexibility of the interpolation scheme. A different proposal was made in Scardapane et al. (2016), where the vector q ∈ R T of SAF parameters is regularized by penalizing deviations from the values at initialization. Note that it is straightforward to initialize the SAF as any of the known fixed activation functions described before.Differently from the other functions described up to now, the maxout function introduced in Goodfellow et al. (2013) replaces an entire layer in (1). In particular, for each neuron, instead of computing a single dot product w T h to obtain the activation (where h is the input to the layer), we compute K different products with K separate weight vectors w 1 , . . . , w K and biases b 1 , . . . , b K , and take their maximum:where the activation function is now a function of a subset of the output of the previous layer. A NN having maxout neurons in all hidden layers is called a maxout network, and remains an universal approximator according to the following theorem.Theorem 2 (Theorem 4.3, ( Goodfellow et al., 2013)). Any continuous func- tion h(·) : R N 0 → R N L can be approximated arbitrarily well on a compact domain by a maxout network with two maxout hidden units, provided K is chosen sufficiently large.The advantage of the maxout function is that it is extremely easy to imple- ment using current linear algebra libraries. However, the resulting functions have several points of non-differentiability, similarly to the APL units. In addition, the number of resulting parameters is generally higher than with alternative formulations. In particular, by increasing K we multiply the orig- inal number of parameters by a corresponding factor, while other approaches contribute only linearly to this number. Additionally, we lose the possibility of plotting the resulting activation functions, unless the input to the maxout layer has less than 4 dimensions. An example with dimension 1 is shown in Fig. 2.In order to solve the smoothness problem, ( Zhang et al., 2014) introduced two smooth versions of the maxout neuron. The first one is the soft-maxout: The second one is the p -maxout, for a user-defined natural number p:i=1Closely related to the p -maxout neuron is the L p unit proposed in Gulcehre et al. (2013). Denoting for simplicity s i = w T i h + b i , the L p unit is defined as:where the K+1 parameters {c 1 , . . . , c K , p} are all learned via back-propagation.If we fix c i = 0, for p going to infinity the L p unit degenerates to a special case of the maxout neuron:In this section we describe the proposed KAF. Specifically, we model each activation function in terms of a kernel expansion over D terms as:whereare the mixing coefficients, {d i } i=1 are the called the dictionary elements, and κ(·, ·) : R × R → R is a 1D kernel function ( Hofmann et al., 2008). In kernel methods, the dictionary elements are generally selected from the training data. In a stochastic optimization setting, this means that D would grow linearly with the number of training iterations, unless some proper strategy for the selection of the dictionary is implemented (Liu et al., 2011; Van Vaerenbergh et al., 2012). To simplify our treatment, we consider a simplified case where the dictionary elements are fixed, and we only adapt the mixing coefficients. In particular, we sample D values over the x-axis, uniformly around zero, similar to the SAF method, and we leave D as a user-defined hyper-parameter. This has the additional benefit that the resulting model is linear in its adaptable parameters, and can be efficiently implemented for a mini-batch of training data using highly-vectorized linear algebra routines. Note that there is a vast literature on kernel methods with fixed dictionary elements, particularly in the field of Gaussian processes (Snelson and Ghahramani, 2006).The kernel function κ(·, ·) needs only respect the positive semi-definiteness property, i.e., for any possible choice ofFor our experiments, we use the 1D Gaussian kernel defined as:where γ ∈ R is called the kernel bandwidth, and its selection is discussed more at length below. Other choices, such as the polynomial kernel with p ∈ N, are also possible:By the properties of kernel methods, KAFs are equivalent to learning linear functions over a large number of nonlinear transformations of the original ac- tivation s, without having to explicitly compute such transformations. The Gaussian kernel has an additional benefit: thanks to its definition, the mix- ing coefficients have only a local effect over the shape of the output function (where the radius depends on γ, see below), which is advantageous during optimization. In addition, the expression in (28) with the Gaussian ker- nel can approximate any continuous function over a subset of the real line ( Micchelli et al., 2006). The expression resembles a one-dimensional radial basis function network, whose universal approximation properties are also well studied (Park and Sandberg, 1991). Below, we go more in depth over some additional considerations for implementing our KAF model. Note that the model has very simple derivatives for back-propagation:On the selection of the kernel bandwidth Selecting γ is crucial for the well-behavedness of the method, by acting indirectly on the effective number of adaptable parameters. In Fig. 3 we show some examples of functions obtained by fixing D = 20, randomly sampling the mixing coefficients, and only varying the kernel bandwidth, showing how γ acts on the smoothness of the overall functions.In the literature, many methods have been proposed to select the band- width parameter for performing kernel density estimation ( Jones et al., 1996). These methods include popular rules of thumb such as Scott (2015) or Silverman (1986).In the problem of kernel density estimation, the abscissa corresponds to a given dataset with an arbitrary distribution. In the proposed KAF scheme, the abscissa are chosen according to a grid, and as such the optimal band- width parameter depends uniquely on the grid resolution. Instead of leav- ing the bandwidth parameter γ as an additional hyper-parameter, we have empirically verified that the following rule of thumb represents a good com- promise between smoothness (to allow an accurate approximation of several initialization functions) and flexibility:where ∆ is the distance between the grid points. We also performed some experiments in which γ was adapted through back-propagation, though this did not provide any gain in accuracy.On the initialization of the mixing coefficients A random initialization of the mixing coefficients from a normal distri- bution, as in Fig. 3, provides good diversity for the optimization process. Nonetheless, a further advantage of our scheme is that we can initialize some (or all) of the KAFs to follow any know activation function, so as to guar- antee a certain desired behavior. Specifically, denote by t = [t 1 , . . . , t D ]T the vector of desired initial KAF values corresponding to the dictionary elementsT . We can initialize the mixing coefficientsT using kernel ridge regression:where K ∈ R D×D is the kernel matrix computed between t and d, and we add a diagonal term with ε &gt; 0 to avoid degenerate solutions with very large mixing coefficients. Two examples are shown in Fig. 4. In our experiments, we also consider a two-dimensional variant of the proposed KAF, that we denote as 2D-KAF. Roughly speaking, the 2D-KAF acts on a pair of activation values, instead of a single one, and learns a two- dimensional function to combine them. It can be seen as a generalization of a two-dimensional maxout neuron, which is instead constrained to output the maximum value among the two inputs.Similarly to before, we construct a dictionary d ∈ R D 2 ×2 by sampling a uniform grid over the 2D plane, by considering D positions uniformly spaced around 0 in both dimensions. We group the incoming activation values in pairs (assuming that the layer has even size), and for each possible pair of activations s = [s k , s k+1 ]T we output: i=1 . In this case, we consider the 2D Gaussian kernel:where we use the same rule of thumb in (34), multiplied by √ 2, to select γ. The increase in parameters is counter-balanced by two factors. Firstly, by grouping the activations we halve the size of the linear matrix in the subsequent layer. Secondly, we generally choose a smaller D with respect to the 1D case, i.e., we have found that values in [5,10] are enough to provide a good degree of flexibility. Table 1 provides a comparison of the two proposed KAF models to the three alternative non-parametric activation functions described before. We briefly mention here that a multidimensional variant of the SAF was explored in Solazzi and Uncini (2000).Many authors have considered ways of improving the performance of the classical activation functions, which do not necessarily require to adapt their shape via numerical optimization, or that require special care when imple- mented. For completeness, we briefly review them here before moving to the experimental section.As stated before, the problem of ReLU is that its gradient is zero outside the 'active' regime where s ≥ 0. To solve this, the randomized leaky ReLU ( Xu et al., 2015) considers a leaky ReLU like in (8), in which during training the parameter α is randomly sampled at every step in the uniform distribu- tion U(l, u), and the lower/upper bounds {l, u} are selected beforehand. To compensate with the stochasticity in training, in the test phase α is set equal to:which is equivalent to taking the average of all possible values seen during training. This is similar to the dropout technique ( Srivastava et al., 2014), which randomly deactivates some neurons during each step of training, and later rescales the weights during the test phase. More in general, several papers have developed stochastic versions of the classical artificial neurons, whose output depend on one or more random vari- ables sampled during their execution ( Bengio et al., 2013), under the idea that the resulting noise can help guide the optimization process towards better minima. Notably, this provides a link between classical NNs and other prob- abilistic methods, such as generative networks and networks trained using variational inference ( Bengio et al., 2014;Schulman et al., 2015). The main challenge is to design stochastic neurons that provide a simple mechanism for back-propagating the error through the random variables, without requiring   expensive sampling procedures, and with a minimal amount of interference over the network. As a representative example, the noisy activation func- tions proposed in Gulcehre et al. (2016) achieve this by combining activation functions with 'hard saturating' regimes (i.e., their value is exactly zero out- side a limited range) with random noise over the outputs, whose variance increases in the regime where the function saturates to avoid problems due to the sparse gradient terms. An example is given in Fig. 5.Another approach is to design vector-valued activation functions to maxi- mize parameter sharing. In the simplest case, the concatenated ReLU ( Shang et al., 2016) returns two output values by applying a ReLU function both on s and on −s. Similarly, the order statistic network ( Rennie et al., 2014) modifies a maxout neuron by returning the input activations in sorted order, instead of picking the highest value only. Multi-bias activation functions ( Li et al., 2016) compute several activations values by using different bias terms, and then apply the same activation function independently over each of the resulting values. The network-in-network ( Lin et al., 2013) model is a non- parametric approach specific to convolutional neural networks, wherein the nonlinear units are replaced with a fully connected NN.For specific tasks of audio modeling, some authors have proposed the use of Hermite polynomials for adapting the activation functions ( Siniscalchi et al., 2013;Siniscalchi and Salerno, 2017). Similarly to our proposed KAF, the functions are expressed as a weighted sum of several fixed nonlinear trans-formations of the activation values, i.e., the Hermite polynomials. However, the nonlinear transformations are computed through the use of a recurrence formula, thus highly increasing the computational load.In this section we provide a comprehensive evaluation of the proposed KAFs and 2D-KAFs when applied to several use cases. As a preliminary ex- periment, we begin by comparing multiple activation functions on a relatively small classification dataset (Sensorless) in Section 8.2, where we discuss sev- eral examples of the shapes that are generally obtained by the networks and initialization strategies. We then consider a large-scale dataset taken from Baldi et al. (2014) in Section 8.3, where we show that two layers of KAFs are able to significantly outperform a feedforward network with five hidden layers, even when considering parametric activation functions and state-of- the-art regularization techniques. In Section 8.4 we show that KAFs and 2D-KAF provide an increase in performance also when applied to convolu- tional layers on the CIFAR-10 dataset. Finally, we show in Section 8.5 that they have significantly faster training and higher cumulative reward for a set of reinforcement learning scenario using MuJoCo environments from the OpenAI Gym 3 .We provide an open-source library to replicate the exper- iments, with the implementation of KAFs and 2D-KAFs in three separate frameworks, i.e., AutoGrad 4 , TensorFlow 5 , and PyTorch 6 , which is publicly accessible on the web 7 .Unless noted otherwise, in all experiments we linearly preprocess the in- put features between -1 and +1, and we substitute eventual missing values with the median values computed from the corresponding feature columns. From the full dataset, we randomly keep a portion of the dataset for val- idation and another portion for test. All neural networks use a softmax activation function in their output layer, and they are trained by minimiz- ing the average cross-entropy on the training dataset, to which we add a small 2 -regularization term whose weight is selected in accordance to the literature. For optimization, we use the Adam algorithm ( Kingma and Ba, 2015) with mini-batches of 100 elements and default hyper-parameters. For each epoch we compute the accuracy over the validation set, and we stop training whenever the validation accuracy is not improving for 15 consecu- tive epochs. Experiments are performed using the PyTorch implementation on a machine with an Intel Xeon E5-2620 CPU, with 16 GB of RAM and a CUDA back-end employing an Nvidia Tesla K20c. All accuracy measures over the test set are computed by repeating the experiments for 5 different splits of the dataset (unless the splits are provided by the dataset itself) and initializations of the networks. Weights of the linear layers are always initialized using the so-called 'Uniform He' strategy, while additional param- eters introduced by parametric and non-parametric activation functions are initialized by following the guidelines of the original papers.We begin with an experiment on the 'Sensorless' dataset to investigate whether KAFs and 2D-KAFs can indeed provide improvements in accuracy with respect to other baselines, and for visualizing some of the common shapes that are obtained after training. The Sensorless dataset is a stan- dard benchmark for supervised techniques, composed of 58509 examples with 49 input features representing electric signals, that are used to predict one among 11 different classes representing operating conditions. We partition it using a random 15% for validation and another 15% for testing, and we use a small regularization factor of 10 −4 . In this dataset, we found that the best performing fixed activation func- tion is a simple hyperbolic tangent. In particular, a network with one hidden layers of 100 neurons achieves a test accuracy of 97.75%, while the best re- sult is obtained with a network of three hidden layers (each composed of 100 neurons), which achieves a test accuracy of 99.18%. Due to the simplicity of the dataset, we have not found improvements here by adding more layers or including dropout during training as in the following sections. The best per- forming parametric activation function is instead the PReLU, that improves the results by obtaining a 98.48% accuracy with a single hidden layer, and 99.30% with three hidden layers. For comparison, we train several feedforward networks with KAFs in the hidden layers, having a dictionary of D = 20 elements equispaced between −3.0 and 3.0, and initializing the linear coefficients from a normal distribution with mean 0 and variance 0.3. Using this setup, we already outperform all other baselines obtaining an accuracy of 99.04% with a single hidden layer, which improves to 99.80% when considering two hidden layers of KAFs.Although the dataset is relatively simple, the shapes we obtain are repre- sentative of all the experiments we performed, and we provide a selection in Fig. 6. Specifically, the initialization of the KAF is shown with a dashed red line, while the final KAF is shown with a solid green line. For understand- ing the behavior of the functions, we also plot the empirical distribution of the activations on the test set using a light blue in the background. Some shapes are similar to common activation functions discussed in Section 3, although they are shifted on the x-axis to correspond to the distribution of activation values. For example Fig. 6a is similar to an ELU, while Fig. 6d is similar to a standard saturating function. Also, while in the latter case the final shape is somewhat determined by the initialization, the final shapes in general tend to be independent from initialization, as in the case of Fig. 6a. Another common shape is that of a radial-basis function, as in Fig. 6f, which is similar to a Gaussian function centered on the mean of the empirical distribution. Shapes, however, can be vastly more complex than these. For example, in Fig. 6e we show a function which acts as a standard saturating function on the main part of the activations' distribution, while its right-tail tends to remove values larger than a given threshold, effectively acting as a sort of implicit regularizer. In Fig. 6b we show a KAF without an intu- itive shape, that selectively amplify (either positively or negatively) multiple regions of its activation space. Finally, in Fig. 6c we show an interesting pruning effect, where useless neurons correspond to activation functions that are practically zero everywhere. This, combined with the possibility of ap- plying 1 -regularization ( Scardapane et al., 2017) allows to obtain networks with a significant smaller number of effective parameters.Interestingly, the shapes obtained in Fig. 6 seem to be necessary for the high performance of the networks, and they are not an artifact of initializa- tion. Specifically, we obtain similar accuracies (and similar output functions) even when initializing all KAFs as close as possible to hyperbolic tangents, following the method described in Section 6, while we obtain a vastly inferior performance (in some cases even worse than the baseline), if we initialize the KAFs randomly and we prevent their adaptation. This (informally) points to the fact that their flexibility and adaptability seems to be an intrinsic com- ponent of their good performance in this experiment and in the following sections, an aspect that we will return to in our conclusive section.Results are also similar when using 2D-KAFs, that we initialize with D = 10 elements on each axis using the same strategy as for KAFs. In this scenario, they obtain a test accuracy of 98.90% with a single hidden layer, and an accuracy of 99.84% (thus improving over the KAF) for two hidden layers. Some examples of obtained shapes are provided in Fig. 7.In this section, we evaluate the algorithms on a realistic large-scale use case, the SUSY benchmark introduced in Baldi et al. (2014). The task is to predict the presence of super symmetric particles on a simulation of a col- lision experiment, starting from 18 features (both low-level and high-level) describing the simulation itself. The overall dataset is composed of five mil- lion examples, of which the last 500000 are used for test, and another 500000 for validation. The task is interesting for several reasons. Due to the na- ture of the data, even a tiny change in accuracy (measured in terms of area under the curve, AUC) is generally statistically significant. In the original paper, Baldi et al. (2014) showed the best AUC was obtained by a deep feed- forward network having five hidden layers, with significantly better results when compared to a shallow network. Surprisingly, Agostinelli et al. (2014) later showed that a shallow network is in fact sufficient, so long as it uses non-parametric activation functions (in that case, APL units).In order to replicate these results with our proposed methods, we con- sider a baseline network inspired to Baldi et al. (2014), having five hidden layers with 300 neurons each and ReLU activation functions, with dropout applied to the last two hidden layer with probability 0.5. For comparison, we also consider the same architecture, but we substitute ReLUs with ELU, SELU, and PReLU functions. For SELU, we also substitute the standard dropout with the customized version proposed in Klambauer et al. (2017). We compare with simpler networks composed of one or two hidden layers of 300 neurons, employing Maxout neurons (with K = 5), APL units (with S = 3 as proposed in Agostinelli et al. (2014)), and the proposed KAFs and 2D-KAFs, following the same random initializations as the previous section. Results, in terms of AUC and amount of trainable parameters, are given in Table 2.There are several clear results that emerge from the analysis of Table 2. First of all, the use of sophisticated activation functions (such as the SELU), or of parametric functions (such as the PReLU) can improve performance,  in some cases even significantly. However, these improvements still require several layers of depth, while they both fail to provide accurate results when experimenting with shallow networks. On the other hand, all non-parametric functions are able to achieve similar (or even superior) results, while only re- quiring one or two hidden layers of neurons. Among them, APL and Maxout achieve a similar AUC with one layer, but only APL is able to benefit from the addition of a second layer. Both KAF and 2D-KAF are able to signifi- cantly outperform all the competitors, and the overall best result is obtained by a 2D-KAF network with two hidden layers. This is obtained with a sig- nificant reduction in the number of trainable parameters, as also described more in depth in the following section.Although our focus has been on feedforward networks, an interesting ques- tion is whether the superior performance exhibited by KAFs and 2D-KAFs can also be obtained on different architectures, such as convolutional neural networks (CNNs). To investigate this, we train several CNNs on the CIFAR- 10 dataset, composed of 60000 images of size 32 × 32, belonging to 10 classes. Since our aim is only to compare different architectures for the convolutional kernels, we train simple CNNs made by stacking convolutional 'modules', each of which is composed by (a) a convolutive layer with 150 filters, with a filter size of 5 × 5 and a stride of 1; (b) a max-pooling operation over 3 × 3 windows with stride of 2; (c) a dropout layer with probability of 0.25. We consider CNNs with a minimum of 2 such modules and a maximum of 5, where the output of the last dropout operation is flattened before applying a linear projection and a softmax operation. Our training setup is equivalent to the previous sections.We consider different choices for the nonlinearity of the convolutional filters, using ELU as baseline, and our proposed KAFs and 2D-KAFs. In order to improve the gradient flow in the initial stages of training, KAFs in this case are initialized with the KRR strategy using ELU as the target. The results are shown in Fig. 8, where we show on the left the final test accuracy, and on the right the number of trainable parameters of the three architectures.Interestingly, both KAFs and 2D-KAFs are able to get significantly better results than the baseline, i.e., even 2 layers of convolutions are sufficient to surpass the accuracy obtained by an equivalent 5-layered network with the baseline activation functions. From Fig. 8b, we can see that this is obtained with a negligible increase in the number of trainable parameters for KAF, and with a significant decrease (roughly 50%) for 2D-KAF. The reason, as before, is that each nonlinearity for the 2D-KAF is merging information coming from two different convolutive filters, effectively halving the number of parameters required for the subsequent layer.Before concluding, we evaluate the performance of the proposed acti- vation functions when applied to a relatively more complex reinforcement learning scenario. In particular, we consider some representative MuJoCo environments from the OpenAI Gym platform, 8 where the task is to learn a policy to control highly nonlinear physical systems, including pendulums and bipedal robots. As a baseline, we use the open-source OpenAI imple- mentation of the proximal policy optimization algorithm ( Schulman et al., 2017), that learns a policy function by alternating between gathering new episodes of interactions with the environment, and building the policy it- self by optimizing a surrogate loss function. All hyper-parameters are taken directly from the original paper, without attempting a specific fine-tuning for our algorithm. The policy function for the baseline is implemented as a NN with two hidden layers, each of which has 64 hidden neurons with tanh nonlinearities, providing in output the mean of a Gaussian distribution that is used to select an action. For the comparison, we keep the overall setup fixed, but we replace the nonlinearities with KAF neurons, using the same initialization as the preceding sections.We plot the average cumulative reward obtained for every iteration of the algorithms on different environments in Fig. 9. We see that the policy networks implemented with the KAF functions consistently learn faster than the baseline with, in several cases, a consistent improvement with respect to the final reward.In this paper, after extensively reviewing known methods to adapt the activation functions in a neural network, we proposed a novel family of non-parametric functions, framed in a kernel expansion of their input value. We showed that these functions combine several advantages of previous ap- proaches, without introducing an excessive number of additional parameters. Furthermore, they are smooth over their entire domain, and their opera- tions can be implemented easily with a high degree of vectorization. Our experiments showed that networks trained with these activations can ob- tain a higher accuracy than competing approaches on a number of different benchmark and scenarios, including feedforward and convolutional neural networks.From our initial model, we made a number of design choices in this paper, which include the use of a fixed dictionary, of the Gaussian kernel, and of a hand-picked bandwidth for the kernel. However, many alternative choices are possible, such as the use of dictionary selection strategies, alternative kernels (e.g., periodic kernels), and several others. In this respect, one intriguing aspect of the proposed activation functions is that they provide a further link between neural networks and kernel methods, opening the door to a large number of variations of the described framework.
