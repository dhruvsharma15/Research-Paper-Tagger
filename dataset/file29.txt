Deep directed generative models are a powerful framework for modeling complex data distributions. Generative Adversarial Networks (GANs) [1] can implicitly learn the data generating distribution; more specifically, GAN can learn to sample from it. In order to do this, GAN trains a generator to mimic real samples, by learning a mapping from a latent space (where the samples are easily drawn) to the data space. Concurrently, a discriminator is trained to distinguish between generated and real samples. The key idea behind GAN is that if the discriminator finds it difficult to distinguish real from artificial samples, then the generator is likely to be a good approximation to the true data distribution.In its standard form, GAN only yields a one-way mapping, i.e., it lacks an inverse mapping mechanism (from data to latent space), preventing GAN from being able to do inference. The ability to compute a posterior distribution of the latent variable conditioned on a given observation may be important for data interpretation and for downstream applications (e.g., classification from the latent variable) [2,3,4,5,6,7]. Efforts have been made to simultaneously learn an efficient bidirectional model that can produce high-quality samples for both the latent and data spaces [3,4,8,9,10,11]. Among them, the recently proposed Adversarially Learned Inference (ALI) [4,10] casts the learning of such a bidirectional model in a GAN-like adversarial framework. Specifically, a discriminator is trained to distinguish between two joint distributions: that of the real data sample and its inferred latent code, and that of the real latent code and its generated data sample.While ALI is an inspiring and elegant approach, it tends to produce reconstructions that are not necessarily faithful reproductions of the inputs [4]. This is because ALI only seeks to match two joint distributions, but the dependency structure (correlation) between the two random variables (conditionals) within each joint is not specified or constrained. In practice, this results in solutions that satisfy ALI's objective and that are able to produce real-looking samples, but have difficulties reconstructing observed data [4]. ALI also has difficulty discovering the correct pairing relationship in domain transformation tasks [12,13,14].In this paper, (i) we first describe the non-identifiability issue of ALI. To solve this problem, we propose to regularize ALI using the framework of Conditional Entropy (CE), hence we call the proposed approach ALICE. (ii) Adversarial learning schemes are proposed to estimate the conditional entropy, for both unsupervised and supervised learning paradigms. (iii) We provide a unified view for a family of recently proposed GAN models from the perspective of joint distribution matching, including ALI [4,10], CycleGAN [12, 13, 14] and Conditional GAN [15]. (iv) Extensive experiments on synthetic and real data demonstrate that ALICE is significantly more stable to train than ALI, in that it consistently yields more viable solutions (good generation and good reconstruction), without being too sensitive to perturbations of the model architecture, i.e., hyperparameters. We also show that ALICE results in more faithful image reconstructions. (v) Further, our framework can leverage paired data (when available) for semi-supervised tasks. This is empirically demonstrated on the discovery of relationships for cross domain tasks based on image data.Consider two general marginal distributions q(x) and p(z) over x ∈ X and z ∈ Z. One domain can be inferred based on the other using conditional distributions, q(z|x) and p(x|z). Further, the combined structure of both domains is characterized by joint distributions q(x, z) = q(x)q(z|x) and p(x, z) = p(z)p(x|z).To generate samples from these random variables, adversarial methods [1] provide a sampling mechanism that only requires gradient backpropagation, without the need to specify the conditional densities. Specifically, instead of sampling directly from the desired conditional distribution, the random variable is generated as a deterministic transformation of two inputs, the variable in the source domain, and an independent noise, e.g., a Gaussian distribution. Without loss of generality, we use an universal distribution approximator specification [9], i.e., the sampling procedure for conditionals˜x conditionals˜ conditionals˜x ∼ p θ (x|z) and˜zand˜ and˜z ∼ q φ (z|x) is carried out through the following two generating processes: ˜ x = g θ (z, z ∼ p(z), ∼ N (0, I), and˜zand˜ and˜z = g φ (x, ζ), x ∼ q(x), ζ ∼ N (0, I),(1) where g θ (·) and g φ (·) are two generators, specified as neural networks with parameters θ and φ, respectively. In practice, the inputs of g θ (·) and g φ (·) are simple concatenations, [z and [x ζ], respectively. Note that (1) implies that p θ (x|z) and q φ (z|x) are parameterized by θ and φ respectively, hence the subscripts.The goal of GAN [1] is to match the marginal p θ (x) = p θ (x|z)p(z)dz to q(x). Note that q(x) denotes the true distribution of the data (from which we have samples) and p(z) is specified as a simple parametric distribution, e.g., isotropic Gaussian. In order to do the matching, GAN trains a ω-parameterized adversarial discriminator network, f ω (x), to distinguish between samples from p θ (x) and q(x). Formally, the minimax objective of GAN is given by the following expression: minwhere σ(·) is the sigmoid function. The following lemma characterizes the solutions of (2) in terms of marginals p θ (x) and q(x).The optimal decoder and discriminator, parameterized by {θ * , ω * }, correspond to a saddle point of the objective in (2), if and only if p θ * (x) = q(x).Alternatively, ALI [4] matches the joint distributions p θ (x, z) = p θ (x|z)p(z) and q φ (x, z) = q(x)q φ (z|x), using an adversarial discriminator network similar to (2), f ω (x, z), parameterized by ω. The minimax objective of ALI can be then written as minLemma 2 ( [4]) The optimum of the two generators and the discriminator with parameters {θ * , φ * , ω * } form a saddle point of the objective in (3), if and only if p θ * (x, z) = q φ * (x, z).From Lemma 2, if a solution of (3) is achieved, it is guaranteed that all marginals and conditional distributions of the pair {x, z} match. Note that this implies that q φ (z|x) and p θ (z|x) match; however, (3) imposes no restrictions on these two conditionals. This is key for the identifiability issues of ALI described below.The relationship (mapping) between random variables x and z is not specified or constrained by ALI. As a result, it is possible that the matched distribution π(x, z) p θ * (x, z) = q φ * (x, z) is undesirable for a given application. To illustrate this issue, Figure 1 shows all solutions (saddle points) to the ALI objective on a simple toy problem. The data and latent random variables can take two possible values, X = {x 1 , x 2 } and Z = {z 1 , z 2 }, respectively. In this case, their marginals q(x) and p(z) are known, i.e., q(x = x 1 ) = 0.5 and p(z = z 1 ) = 0.5. The matched joint distribution, π(x, z), can be repre- sented as a 2 × 2 contingency table. Figure 1(a) repre- sents all possible solutions of the ALI objective in (3), for any δ ∈ [0, 1]. Figures 1(b) and 1(c) represent oppo- site extreme solutions when δ = 1 and δ = 0, respec- tively. Note that although we can generate "realistic" values of x from any sample of p(z), for 0 &lt; δ &lt; 1, we will have poor reconstruction ability since the sequenceThe two (trivial) exceptions where the model can achieve perfect reconstruction correspond to δ = {1, 0}, and are illustrated in Figures 1(b) and 1(c), respectively. From this simple example, we see that due to the flexibility of the joint distribution, π(x, z), it is quite likely to obtain an undesirable solution to the ALI objective. For instance, i) one with poor reconstruction ability or ii) one where a single instance of z can potentially map to any possible value in X , e.g., in Figure 1(a) with δ = 0.5, z 1 can generate either x 1 or x 2 with equal probability.Many applications require meaningful mappings. Consider two scenarios:• A1: In unsupervised learning, one desirable property is cycle-consistency [12], meaning that the inferred z of a corresponding x, can reconstruct x itself with high probability. In Figure 1 this corresponds to either δ → 1 or δ → 0, as in Figures 1(b) and 1(c).• A2: In supervised learning, the pre-specified correspondence between samples imposes restrictions on the mapping between x and z, e.g., in image tagging, x are images and z are tags. In this case, paired samples from the desired joint distribution are usually available, thus we can leverage this supervised information to resolve the ambiguity between Figure 1(b) and (c). From our simple example in Figure 1, we see that in order to alleviate the identifiability issues associated with the solutions to the ALI objective, we have to impose constraints on the conditionals q φ (z|x) and p θ (z|x). Furthermore, to fully mitigate the identifiability issues we require supervision, i.e., paired samples from domains X and Z.To deal with the problem of undesirable but matched joint distributions, below we propose to use an information-theoretic measure to regularize ALI. This is done by controlling the "uncertainty" between pairs of random variables, i.e., x and z, using conditional entropies.Conditional Entropy (CE) is an information-theoretic measure that quantifies the uncertainty of random variable x when conditioned on z (or the other way around), under joint distribution π(x, z):The uncertainty of x given z is linked with H π (x|z); in fact, H π (x|z) = 0 if only if x is a deterministic mapping of z. Intuitively, by controlling the uncertainty of q φ (z|x) and p θ (z|x), we can restrict the solutions of the ALI objective to joint distributions whose mappings result in better reconstruction ability. Therefore, we propose to use the CE in (4), denoted as Lπ (z|x) (depending on the task; see below), as a regularization term in our framework, termed ALI with Conditional Entropy (ALICE), and defined as the following minimax objective: minL π CE (θ, φ) is dependent on the underlying distributions for the random variables, parametrized by (θ, φ), as made clearer below. Ideally, we could select the desirable solutions of (5) by evaluating their CE, once all the saddle points of the ALI objective have been identified. However, in practice, L π CE (θ, φ) is intractable because we do not have access to the saddle points beforehand. Below, we propose to approximate the CE in (5) during training for both unsupervised and supervised tasks. Since x and z are symmetric in terms of CE according to (4), we use x to derive our theoretical results. Similar arguments hold for z, as discussed in the Supplementary Material (SM).In the absence of explicit probability distributions needed for computing the CE, we can bound the CE using the criterion of cycle-consistency [12]. We denote the reconstruction of x asˆxasˆ asˆx, via generating procedure (cycle) ˆ x ∼ p θ (ˆ x|z), z ∼ q φ (z|x), x ∼ q(x). We desire that p θ (ˆ x|z) have high likelihood forˆxforˆ forˆx = x, for the x ∼ q(x) that begins the cycle x → z → ˆ x, and hence thatˆxthatˆ thatˆx be similar to the original x. Lemma 3 below shows that cycle-consistency is an upper bound of the conditional entropy in (4). Lemma 3 For joint distributions p θ (x, z) or q φ (x, z), we havewhere [·]. For the unsupervised case we want to leverage (6) to optimize the following upper bound of (5):Note that as ALI reaches its optimum, p θ (x, z) and (4), its upper bound, L Cycle (θ, φ), can be easily approximated via Monte Carlo simulation. Importantly, (7) can be readily added to ALI's objective without additional changes to the original training procedure.The cycle-consistency property has been previously leveraged in CycleGAN [12], DiscoGAN [13] and DualGAN [14]. However, in [12,13,14], cycle-consistency, L Cycle (θ, φ), is implemented via k losses, for k = 1, 2, and real-valued data such as images. As a consequence of an 2 -based pixel-wise loss, the generated samples tend to be blurry [8]. Recognizing this limitation, we further suggest to enforce cycle-consistency (for better reconstruction) using fully adversarial training (for better generation), as an alternative to L Cycle (θ, φ) in (7). Specifically, to reconstruct x, we specify an η-parameterized discriminator f η (x, ˆ x) to distinguish between x and its reconstructionˆxreconstructionˆ reconstructionˆx: minFinally, the fully adversarial training algorithm for unsupervised learning using the ALICE framework is the result of replacing L Cycle (θ, φ) with L A Cycle (θ, φ, η) in (7); thus, for fixed (θ, φ), we maximize wrt {ω, η}.The use of paired samples {x, ˆ x} in (8) is critical. It encourages the generators to mimic the reconstruction relationship implied in the first joint; on the contrary, the model may reduce to the basic GAN discussed in Section 3, and generate any realistic sample in X . The objective in (8) enjoys many theoretical properties of GAN. Particularly, Proposition 1 guarantees the existence of the optimal generator and discriminator.The proof is provided in the SM. Together with Lemma 2 and 3, we can also show that:, which indicates the conditionals are matched.(ii) On the contrary, the matched conditionals enforce H q φ (x|z) = 0, which indicates the corresponding mapping becomes deterministic.When the objective in (7) is optimized in an unsupervised way, the identifiability issues associated with ALI are largely reduced due to the cycle-consistency-enforcing bound in Lemma 3. This means that samples in the training data have been probabilistically "paired" with high certainty, by conditionals p θ (x|z) and p φ (z|x), though perhaps not in the desired configuration. In real- world applications, obtaining correctly paired data samples for the entire dataset is expensive or even impossible. However, in some situations obtaining paired data for a very small subset of the observations may be feasible. In such a case, we can leverage the small set of empirically paired samples, to further provide guidance on selecting the correct configuration. This suggests that ALICE is suitable for semi-supervised classification.For a paired sample drawn from empirical distributioñ π(x, z), its desirable joint distribution is well specified. Thus, one can directly approximate the CE aswhere the approximation (≈) arises from the fact that p θ (x|z) is an approximation tõ π(x|z). For the supervised case we leverage (9) to approximate (5) using the following minimax objective:Note that as ALI reaches its optimum, p θ (x, z) and (4) accordingly, thus (10) approaches (5) (ALICE).We can employ standard losses for supervised learning objectives to approximate L Map (θ) in (10), such as cross-entropy or k loss in (9). Alternatively, to also improve generation ability, we propose an adversarial learning scheme to directly match p θ (x|z) to the paired empirical conditional˜πconditional˜ conditional˜π(x|z), using conditional GAN [15] as an alternative to L Map (θ) in (10). The χ-parameterized discriminator f χ is used to distinguish the true pair {x, z} from the artificially generated one {ˆx{ˆx, z} (conditioned on z), usingThe fully adversarial training algorithm for supervised learning using the ALICE in (11) is the result of replacing (10), thus for fixed (θ, φ) we maximize wrt {ω, χ}.The proof is provided in the SM. Proposition 2 enforces that the generator will map to the correctly paired sample in the other space. Together with the theoretical result for ALI in Lemma 2, we haveCorollary 2 indicates that ALI's drawbacks associated with identifiability issues can be alleviated for the fully supervised learning scenario. Two conditional GANs can be used to boost the perfomance, each for one direction mapping. When tying the weights of discriminators of two conditional GANs, ALICE recovers Triangle GAN [16]. In practice, samples from the paired set˜πset˜ set˜π(x, z) often contain enough information to readily approximate the sufficient statistics of the entire dataset. In such case, we may use the following objective for semi-supervised learning:The first two terms operate on the entire set, while the last term only applies to the paired subset. Note that we can train (12) fully adversarially by replacing (8) and (11), respectively. In (12) each of the three terms are treated with equal weighting in the experiments if not specificially mentioned, but of course one may introduce additional hyperparameters to adjust the relative emphasis of each term.Connecting ALI and CycleGAN. We provide an information theoretical interpretation for cycle- consistency, and show that it is equivalent to controlling conditional entropies and matching con- ditional distributions. When cycle-consistency is satisfied, Corollary 1 shows that the conditionals are matched in CycleGAN. They also train additional discriminators to guarantee the matching of marginals for x and z using the original GAN objective in (2). This reveals the equivalence between ALI and CycleGAN, as the latter can also guarantee the matching of joint distributions p θ (x, z) and q φ (x, z). In practice, CycleGAN is easier to train, as it decomposes the joint distribution matching objective (as in ALI) into four subproblems. Our approach leverages a similar idea, and further improves it with adversarially learned cycle-consistency, when high quality samples are of interest. Stochastic Mapping vs. Deterministic Mapping. We propose to enforce the cycle-consistency in ALI for the case when two stochastic mappings are specified as in (1). When cycle-consistency is achieved, Corollary 1 shows that the bounded conditional entropy vanishes, and thus the corresponding mapping reduces to be deterministic. In the literture, one deterministic mapping has been empirically tested in ALI's framework [4], without explicitly specifying cycle-consistency. BiGAN [10] uses two deterministic mappings. In theory, deterministic mappings guarantee cycle-consistency in ALI's framework. However, to achieve this, the model has to fit a delta distribution (deterministic mapping) to another distribution in the sense of KL divergence (see Lemma 3). Due to the asymmetry of KL, the cost function will pay extremely low cost for generating fake-looking samples [17]. This explains the underfitting reasoning in [4] behind the subpar reconstruction ability of ALI. Therefore, in ALICE, we explicitly add a cycle-consistency regularization to accelerate and stabilize training.Conditional GANs as Joint Distribution Matching. Conditional GAN and its variants [15,18,19,20] have been widely used in supervised tasks. Our scheme to learn conditional entropy borrows the formulation of conditional GAN [15]. To the authors' knowledge, this is the first attempt to study the conditional GAN formulation as joint distribution matching problem. Moreover, we add the potential to leverage the well-defined distribution implied by paired data, to resolve the ambiguity issues of unsupervised ALI variants [4,10,12,13,14].The code to reproduce these experiments is at https://github.com/ChunyuanLI/ALICETo highlight the role of the CE regularization for unsupervised learning, we perform an experiment on a toy dataset. q(x) is a 2D Gaussian Mixture Model (GMM) with 5 mixture components, and p(z) is chosen as a standard Gaussian, N (0, I). Following [4], the covariance matrices and centroids are chosen such that the distribution exhibits severely separated modes, which makes it a relatively hard task despite its 2D nature. Following [21], to study stability, we run an exhaustive grid search over a set of architectural choices and hyper-parameters, 576 experiments for each method. We report Mean Squared Error (MSE) and inception score (denoted as ICP) [22] to quantitatively evaluate the performance of generative models. MSE is a proxy for reconstruction quality, while ICP reflects the plausibility and variety of sample generation. Lower MSE and higher ICP indicate better results. See SM for the details of the grid search and the calculation of ICP.We train on 2048 samples, and test on 1024 samples. The ground-truth test samples for x and z are shown in Figure 2(a) and (b), respectively. We compare ALICE, ALI and Denoising Auto-Encoders (DAEs) [23], and report the distribution of ICP and MSE values, for all (576) experiments in Figure 2 (c) and (d), respectively. For reference, samples drawn from the "oracle" (ground-truth) GMM yield ICP=4.977±0.016. ALICE yields an ICP larger than 4.5 in 77% of experiments, while ALI's ICP wildly varies across different runs. These results demonstrate that ALICE is more consistent and quantitatively reliable than ALI. The DAE yields the lowest MSE, as expected, but it also results in the weakest generation ability. The comparatively low MSE of ALICE demonstrates its acceptable reconstruction ability compared to DAE, though a very significantly improvement over ALI. Figure 3 shows the qualitative results on the test set. Since ALI's results vary largely from trial to trial, we present the one with highest ICP. In the figure, we color samples from different mixture components to highlight their correspondance between the ground truth, in Figure 2(a), and their reconstructions, in Figure 3 (first row, columns 2, 4 and 6, for ALICE, ALI and DAE, respectively). Importantly, though the reconstruction of ALI can recover the shape of manifold in x (Gaussian mixture), each individual reconstructed sample can be substantially far away from its "original" mixture component (note the highly mixed coloring), hence the poor MSE. This occurs because the adversarial training in ALI only requires that the generated samples look realistic, i.e., to be located near true samples in X , but the mapping between observed and latent spaces (x → z and z → x) is not specified. In the SM we also consider ALI with various combinations of stochastic/deterministic mappings, and conclude that models with deterministic mappings tend to have lower reconstruction ability but higher generation ability. In terms of the estimated latent space, z, in Figure 3 (first row, columns 1, 3 and 5, for ALICE, ALI and DAE, respectively), we see that ALICE results in a better latent representation, in the sense of mapping consistency (samples from different mixture components remain clustered) and distribution consistency (samples approximate a Gaussian distribution). The results for reconstruction of z and sampling of x are shown in the SM.In Figure 3 (second row), we also investigate latent space interpolation between a pair of test set examples. We use  Figure 3 shows that ALICE's interpolation is smooth and consistent with the ground-truth distributions. Interpolation using ALI results in realistic samples (within mixture components), but the transition is not order-wise consistent. DAEs provides smooth transitions, but the samples in the original space look unrealistic as some of them are located in low probability density regions of the true model. Adversarially learned reconstruction To demonstrate the effectiveness of our fully adversarial scheme in (8) (Joint A.) on real datasets, we use it in place of the 2 losses in DiscoGAN [13]. In practice, feature matching [22] is used to help the adversarial objective in (8) to reach its optimum.We also compared with a baseline scheme (Marginal A.) in [12], which adversarially discriminates between x and its reconstructionˆxreconstructionˆ reconstructionˆx.  The results are shown in Figure 4 (a). From top to bottom, each row shows ground-truth images, DiscoGAN (with Joint A., 2 loss and Marginal A. schemes, respectively) and BiGAN [10]. Note that BiGAN is the best ALI variant in our grid search compasion. The proposed Joint A. scheme can retain the same crispness characteristic to adversarially- trained models, while 2 tends to be blurry. Marginal A. provides realistic car images, but not faithful reproductions of the inputs. This explains the observations in [12] in terms of no performance gain. The BiGAN learns the shapes of cars, but misses the textures. This is a sign of underfitting, thus indicating BiGAN is not easy to train.Weak supervision The DiscoGAN and BiGAN are unsupervised methods, and exhibit very different cross-domain pairing configurations during different training epochs, which is indicative of non- identifiability issues. We leverage very weak supervision to help with convergence and guide the pairing. The results on shown in Figure 4 (b). We run each methods 5 times, the width of the colored lines reflect the standard deviation. We start with 1% true pairs for supervision, which yields significantly higher accuracy than DiscoGAN/BiGAN. We then provided 10% supervison in only 2 or 6 angles (of 11 total angles), which yields comparable angle prediction accuracy with full angle supervison in testing. This shows ALICE's ability in terms of zero-shot learning, i.e., predicting unseen pairs. In the SM, we show that enforcing different weak supervision strategies affects the final pairing configurations, i.e., we can leverage supervision to obtain the desirable joint distribution.Quantitative comparison To quantitatively assess the generated images, we use structural similarity (SSIM) [26], which is an established image quality metric that correlates well with human visual perception. SSIM values are between [0, 1]; higher is better. The SSIM of ALICE on prediction and reconstruction is shown in Figure 5 (a)(b) for the edge-to-shoe task. As a baseline, we set DiscoGAN with 2 -based supervision ( 2 -sup). BiGAN/ALI, highlighted with a circle is outperformed by ALICE in two aspects: (i) In the unpaired setting (0% supervision), cycle-consistency regularization (L Cycle ) shows significant performance gains, particularly on reconstruction. (ii) When supervision is leveraged (10%), SSIM is significantly increased on prediction. The adversarial-based supervision ( A -sup) shows higher prediction than 2 -sup. ALICE achieves very similar performance with the 50% and full supervision setup, indicating its advantage of in semi-supervised learning. Several generated edge images (with 50% supervision) are shown in Figure 5(c), A -sup tends to provide more details than 2 -sup. Both methods generate correct paired edges, and quality is higher than BiGAN and DiscoGAN. In the SM, we also report MSE metrics, and results on edge domain only, which are consistent with the results presented here.One-side cycle-consistency When uncertainty in one domain is desirable, we consider one-side cycle-consistency. This is demonstrated on the CelebA face dataset [27]. Each face is associated with a 40-dimensional attribute vector. The results are in the Figure 13 of SM. In the first task, we consider the images x are generated from a 128-dimensional Gaussian latent space z, and apply L Cycle on x.We compare ALICE and ALI on reconstruction in (a)(b). ALICE shows more faithful reproduction of the input subjects. In the second task, we consider z as the attribute space, from which the images x are generated. The mapping from x to z is then attribute classification. We only apply L Cycle on the attribute domain, and L A Map on both domains. When 10% paired samples are considered, the predicted attributes still reach 86% accuracy, which is comparable with the fully supervised case. To test the diversity on x, we first predict the attributes of a true face image, and then generated multiple images conditioned on the predicted attributes. Four examples are shown in (c).We have studied the problem of non-identifiability in bidirectional adversarial networks. A unified perspective of understanding various GAN models as joint matching is provided to tackle this problem. This insight enables us to propose ALICE (with both adversarial and non-adversarial solutions) to reduce the ambiguity and control the conditionals in unsupervised and semi-supervised learning. For future work, the proposed view can provide opportunities to leverage the advantages of each model, to advance joint-distribution modeling.Joint Distribution MatchingSince our paper constrain correlation of two random variables using information theoretical measures, we first review the related concepts. For any probability measure π on the random variables x and z, we have the following additive and subtractive relationships for various information measures, including Mutual Information (MI), Variation of Information (VI) and the Conditional Entropy (CE).The following shows how the negative log probability (NLL) of the reconstruction is related to variation of information and mutual information. On the support of (x, z), we denote q as the encoder probability measure, and p as the decoder probability measure. Note that the reconstruction loss for z can be writen as its log likelihood form as L R = −E z∼p(z),x∼p(x|z) [log q(z|x)].Lemma 4 For random variables x and z with two different probability measures, p(x, z) and q(x, z), we havewhere H p (z|x) is the conditional entropy. From lemma 4, we have Corollary 3 For random variables x and z with probability measure p(x, z), the mutual information between x and z can be written asGiven a simple prior p(z) such as isotropic Gaussian, H(z) is a constant.Corollary 4 For random variables x and z with probability measure p(x, z), the variation of information between x and z can be written asThe proof for cycle-consistency and conditional GAN using adversarial traning is shown below. It follows the proof of the original GAN paper: we first show the implication of optimal discriminator, and then show the corresponding optimal generator.In the unsupervised case, given data sample x, one desirable property is reconstruction. The following game learns to reconstruct:Proposition 3 For fixed (θ, φ), the optimal ω in (24) yieldsProof We start from a simple observation. Therefore, the objective in (24) can be expressed asNote thatThe expression in (26) is maximal as a function of f ω (x, ˆ x) if and only if the integrand is maximal for every (x, ˆ x). However, the problem max t a log(t) + b log(1 − t) attains its maximum at t = a a+b , showing thatFor the game in (24), for which (θ, φ) are optimized as to most confuse the discriminator, the optimal solution for the distribution parameters (θ, and therefore from (31)Similarly, we can show the cycle consistency property for reconstructing z asIn the supervised case, given the paired data sample π(x, z), the following game is used to condition- ally generate x [15]:To show the results, we need the following Lemma:Lemma 5 The optimial generator and discriminator, with parameters (θ * , ω * ), forms the saddle points of game in (33), if and only if p θ * (x|z) = π(x|z). Further, p θ * (x, z) = π(x, z) Proof For the observed paired data π(x, z), we have p(z) = π(z), where π(z) is marginal empirical distribution of z for the paired data.Also, π(˜ x|z) = δ(˜ x − x) wheñ x is paired with z in the dataset. We start from the observationTherefore, the objective in (33) can be expressed asThis integral is maximal as a function of f ω (x, z) if and only if the integrand is maximal for every (x, z). However, the problem max t a log(t) + b log(1 − t) attains its maximum at t = a a+b , showing thator equivalently, the optimum generator is p θ * (x|z) = π(x|z). Since q(x) = π(x), we further have p θ * (x, z) = π(x, z). Similarly, for conditional GAN of z, we can show that is q φ * (z|x) = π(z|x) and q φ * (x, z) = π(x, z) for the Combining them, we show thatThe 5-component Gaussian mixture model (GMM) in x is set with the means (0, 0), (2, 2), (−2, 2), (2, −2), (−2, −2), and standard derivation 0.2. The Isotropic Gaussian in z is set with mean (0, 0) and standard derivation 1.0.We consider various network architectures to compare the stability of the methods. The hyperpa- rameters includes: the number of layers and the number of neurons of the discriminator and two generators, and the update frenquency for discriminator and generator. The grid search specification is summarized in Table 1. Hence, the total number of experiments is 2 3 × 2 3 × 3 2 = 576.A generalized version of the inception score is calculated, ICP = E x KL(p(y)||p(y|x)), where x denotes a generated sample and y is the label predicted by a classifier that is trained off-line using the entire training set. It is also worth noting that although we inherit the name "inception score" from [22], our evaluation is not related to the "inception" model trained on ImageNet dataset. Our classifier is a regular 3-layer neural nets trained on the dataset of interest, which yields 100% classification accuracy on this toy dataset. C.2 Reconstruction of z and sampling for x We show the additional results for the econstruction of z and sampling for x in Figure 6. ALICE shows good sampling ability, as it reflects the Guassian characteristics for each of 5 components, while ALI's samples tends to be concentrated, reflected by the shrinked Guassian components. DAE learns an indentity mapping, and thus show weak generation ability.ALICE is a general CE-based framework to regularize the objectives of bidiretional adversarial training, in order to obtain desirable solutions. To clearly show the versatility of ALICE, we summarize its four variants, and test their effectivenss on toy datasets.In unsupervised learning, two forms of cycle-consistency/reconstruction are considered to bound CE:• Explicit cycle-consistency: Explicitly specified k -norm for reconstruction;• Implicit cycle-consistency: Implicitly learned reconstruction via adversarial trainingIn semi-supervised learning, the pairwise information is leveraged in two forms to approximate CE:• Explicit mapping: Explicitly specified k -norm mapping (e.g., standard supervised losses);• Implicit mapping: Implicitly learned mapping via adversarial trainingThe similarity/quality of the re- construction to the original sample is measured in terms of k metric. This is easy to implement and optimize. However, it may lead to visually low quality reconstruction in high dimensions.(ii) Implicit methods via adversarial training: it essentially requires the reconstruction to be close to the original sample in terms of 0 metric (see Section 3.3 of [10]: Adversarial feature learning). It theoretically guarantees perfect reconstruction, however, this is hard to achieve in practice, espcially in high dimension spaces.Results The effectivenss of these algorithms are demonstrated on toy data of low dimension in Fig- ure 7. The unsupervised variants are tested in the same toy dataset described above, the results are in Figure 7 (a)(b). For the supervised variants, we create a toy dataset, where z-domain is 2-component GMM, and x-domain is 5-component GMM. Since each domain is symmtric, ambiguity exists when Cycle-GAN variants attempt to discover the relationship of the two domains in pure unsupervised setting. Indeed, we observed random switching of the discoverd corresponded components in different runs of Cycle-GAN. By adding a tiny fraction of pairwise information (a cheap way to specify the desirable relationship ), we can easily learn the correct correspondences for the entire datasets. In We investigate the ALI model with different mappings:• ALI: two stochastic mappings;• ALI − : one stochastic mapping and one deterministic mapping;• BiGAN: two deterministic mappings.We plot the histogram of ICP and MSE in Fig. 8, and report the mean and standard derivation in Table 2. In Fig. 9, we compare their reconstruction and generation ability. Models with deterministic mapping have higher recontruction ability, while show lower sampling ability.Comparison on Reconstruction Please see row 1 and 2 in Fig. 9. For reconstruction, we start from one sample (red dot), and pass it through the cycle formed by the two mappings 100 times. The resulted reconstructions are shown as blue dots. The reconstructed samples tends to be concentrated with more deterministic mappings.     We investigate the effectiveness and impact of the proposed cycle-consistency regularizer (explicit 2 norm) on 3 datasets, including the toy dataset, MNIST and CIFAR-10. A large range of weighting hyperparameter λ is tested. The inception scores on toy and MNIST datasets are evaluted by the pre-trained "perfect" classifiers of these datasets, respectively, while inception scores on CIFAR is based on ImageNet. The results for different λ are shown in Figure 10, and the best performance is sumarized in Table 3.  E More Details on Real Data Experiments E.1 Car to Car Experiment Setup The dataset [24] consists of rendered images of 3D car models with varying azimuth angles at 15• intervals. 11 views of each car are used. The dataset is split into train set ( 169×11 = 1859 images) and test set ( 14×11 = 154 images), and further split the train set into two groups, each of which is used as A domain and B domain samples. To evaluate, we trained a regressor and a classifier that predict the azimuth angle using the train set. We map the car image from one domain to the other, and then reconstruct to the original domain. The cycle-consistency is evaluted as the prediction accuracy of the reconstructed images. Table 4 shows the MSE and prediction accuracy by leverage the supervision in different number of angles. To further demonstrate that we can easily control the correspondence configuration by designing the proper supervision, we use ALICE to enforce coherent supervsion and opposite supervision, respectively. Only 1% supervison information is used in each angle. We translated images in the test set using each of the three trained models, and azimuth angles were predicted using the regressor for both input and translated images. In Table 5, we show the cross domain relationship discovered by each method. X and Y axis indicates predicted angles of original and transformed cars, respectively. All three plots are results at the 10th epoch. Scatter points with supervision are more concentrated on the diagnals in the plots, which indicates higher prediction/correlation. The learning curves are shown in Table 5       The MSE results on cross-domain prediction and one-domain reconstruction are shown in Figure 12. Reconstruction results on the validation dataset of Celeba dataset are shown in Figure 13. ALI results are from the paper [4]. ALICE provides more faithful reconstruction to the input subjects. As a trade-off between theoretical optimum and practical convergence, we employ feature matching, and thus our results exhibits slight bluriness characteristic. We demonstrate the potential real applications of ALICE algorithms on the task of sketch to cartoon. We built a dataset by collecting frames from Disney's film Alice in Wonderland. A large image size 256 × 256 is considered. The training dataset consists of two domains: cartoon images and edges images, where the edges are created via holistically-nested edge detection [28] on their true cartoon images. The image content is about either of two characters in the film: Alice or White Rabbit. Therefore, each domain exhibits two modes. 52 images are collected in each domain. The one-to-one image correspondence between two domain is unknown, the goal is to efficiently generate realistic cartoon images for animation, based on the edges.CycleGAN is an unsupervised learning algorithm. Since we have shown its equivalence to ALI/BiGAN (see Related Work), and its superiority in terms of stability. We derive our weakly- supervised ALICE algorithm on this dataset as: (i) CycleGAN for the unpaired data, and (ii) explicit 1 loss and/or implicit conditional GAN loss for the paired samples. Note that only one pair is randomly chosen for each character. The total training iteration is 10K. We observed the results while training, and summarize in the following:• ALICE converges faster than CycleGAN: . The generated images after 6K iterations are show in Figure 14. CycleGAN generates images with mixed colors (e.g., , the clothes of Rabbit), while ALICE clearly paint colors in different regions.• ALICE enables desirable image generation (with better generalization): The gener- ated images after 10K iterations are show in Figure 15. We generated image based on slightly different edges: more background and details on the character. CycleGAN gets confused when identifying the character, thus inconsistently paint the dfferent colors to Rabbit, while explicit ALICE can generate coherent color.• ALICE improves image quality via weak supervision: Two sets of the generated image after 10K iterations are compared in Figure 16. ALICE can capture more visual details (e.g., the background flowers in (a)) and colors (e.g., the ambarella in (b)).  
