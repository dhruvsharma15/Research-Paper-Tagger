Recurrent neural networks (RNNs) have shown impres- sive results in modeling generation tasks that have a se- quential structured output form, such as machine transla- tion ( Sutskever et al., 2014;Bahdanau et al., 2015), caption generation (Karpathy &amp; Fei-Fei, 2015;Xu et al., 2015), and natural language generation Kiddon et al., 2016). These discriminative models are trained to learn only a conditional output distribution over strings and despite the sophisticated architectures and condition-Recent advances in neural variational inference Mnih &amp; Gregor, 2014) have sparked a series of latent variable models applied to NLP (Bowman et al., 2015;Serban et al., 2016;Cao &amp; Clark, 2017). For models with continuous latent variables, the reparameterisation trick ) is commonly used to build an unbiased and low-variance gra- dient estimator for updating the models. However, since a continuous latent space is hard to interpret, the major bene- fits of these models are the stochasticity and the regularisa- tion brought by the latent variable. In contrast, models with discrete latent variables are able to not only produce inter- pretable latent distributions but also provide a principled framework for semi-supervised learning ( ). This is critical for NLP tasks, especially where ad- ditional supervision and external knowledge can be utilized for bootstrapping (Faruqui et al., 2015;Kočisk´Kočisk´y et al., 2016). However, variational inference with discrete latent variables is relatively difficult due to the problem of high variance during sampling. Hence we in- troduce baselines, as in the REINFORCE (Williams, 1992) algorithm, to mitigate the high variance problem, and carry out efficient neural variational inference (Mnih &amp; Gregor, 2014) for the latent variable model. In the LIDM, the latent intention is inferred from user in- put utterances. Based on the dialogue context, the agent draws a sample as the intention which then guides the natu- ral language response generation. Firstly, in the framework of neural variational inference (Mnih &amp; Gregor, 2014), we construct an inference network to approximate the posterior distribution over the latent intention. Then, by sampling the intentions for each response, we are able to directly learn a basic intention distribution on a human-human dialogue corpus by optimising the variational lower bound. To fur- ther reduce the variance, we utilize a labeled subset of the corpus in which the labels of intentions are automatically generated by clustering. Then, the latent intention distri- bution can be learned in a semi-supervised fashion, where the learning signals are either from the direct supervision (labeled set) or the variational lower bound (unlabeled set).parse the input into actionable commands Q and access the KB to search for useful information in order to answer the query. Based on the search result, the model needs to sum- marise its findings and reply with an appropriate response m t in natural language.From the perspective of reinforcement learning, the latent intention distribution can be interpreted as the intrinsic pol- icy that reflects human decision-making under a particular conversational scenario. Based on the initial policy (la- tent intention distribution) learnt from the semi-supervised variational inference framework, the model can refine its strategy easily against alternative objectives using policy gradient-based reinforcement learning. This is somewhat analagous to the training process used in AlphaGo (Silver et al., 2016) for the game of Go. Based on LIDM, we show that different learning paradigms can be brought together under the same framework to bootstrap the development of a dialogue agent ( Li et al., 2016c;Weston, 2016).The LIDM is based on the end-to-end system architecture described in . It comprises three com- ponents: (1) Representation Construction; (2) Policy Net- work; and (3) Generator, as shown in Figure 1. To capture the user's intent and match it against the system's knowl- edge, a dialogue state vector s t = u t ⊕ b t ⊕ x t is derived from the user input u t and the knowledge base KB: u t is the distributed utterance representation, which is formed by encoding the user utterance 2 u t with a bidirectional LSTM (Hochreiter &amp; Schmidhuber, 1997) and concatenat- ing the final stage hidden states together,The belief vector b t , which is a concatenation of a set of probability distributions over domain specific slot-value pairs, is extracted by a set of pre-trained RNN-CNN belief trackers Mrkši´Mrkši´c et al., 2017), in which u t and m t−1 are processed by two different CNNs as shown in Figure 1,In summary, the contribution of this paper is two-fold: firstly, we show that the neural variational inference frame- work is able to discover discrete, interpretable intentions from data to form the decision-making basis of a dialogue agent; secondly, the agent is capable of revising its con- versational strategy based on an external reward within the same framework. This is important because it pro- vides a stepping stone towards building an autonomous di- alogue agent that can continuously improve itself through interaction with users. The experimental results demon- strate the effectiveness of our latent intention model which achieves state-of-the-art performance on both automatic corpus-based evaluation and human evaluation.where m t−1 is the preceding machine response and b t−1 is the preceding belief vector. They are included to model the current turn of the discourse and the long-term dialogue context, respectively. Based on the belief vector, a query Q is formed by taking the union of the maximum values of each slot. Q is then used to search the internal KB and re- turn a vector x t representing the degree of matching in the KB. This is produced by counting all the matching venues and re-structuring it into a six-bin one-hot vector. Among the three vectors that comprise the dialogue state s t , u t is completely trainable from data, b t is pre-trained using a separate objective function, and x t is produced by a dis- crete database accessing operation. For more details about the belief trackers and database operation refer to .Conditioning on the state s t , the policy network parame- terises the latent intention z t by a single layer MLP,Goal-oriented dialogue 1 ( Young et al., 2013) aims at build- ing models that can help users to complete certain tasks via natural language interaction. Given a user input utterance u t at turn t and a knowledge base (KB), the model needs towhere W 1 , b 1 , W 2 , b 2 are model parameters. Since π Θ (z t |s t ) is a discrete conditional probability distribution Figure 1. LIDM for Goal-oriented Dialogue Modeling based on dialogue state, we can also interpret the policy network here as a latent dialogue management component in the traditional POMDP-based framework ( Young et al., 2013;Gaši´Gaši´c et al., 2013). A latent intention z terised form with parameter set Θ,(n) t(or an action in the reinforcement learning literature) can then be sampled from the conditional distribution,This sampled intention (or action) z (n) t and the state vector s t can then be combined into a control vector d t , which is used to govern the generation of the system response based on a conditional LSTM language model, To carry out inference for the LIDM, we introduce an in- ference network q Φ (z t |s t , m t ) to approximate the posterior distribution p(z t |s t , m t ) so that we can optimise the varia- tional lower bound of the joint probability in a neural vari- ational inference framework ( . We can then derive the variational lower bound as,where b 3 and W 3∼5 are parameters, z t is the 1-hot repre- sentation of z (n) t , w t j is the last output token (i.e. a word, a delexicalised 2 slot name or a delexicalised 2 slot value), and h t where q Φ (z t ) is a shorthand for q Φ (z t |s t , m t ). Note that we use a modified version of the lower bound here by in- corporating a trade-off factor λ ( Higgins et al., 2017). The inference network q Φ (z t |s t , m t ) is then constructed by j−1 is the decoder's last hidden state. Note in Equa- tion 5 the degree of information flow from the state vector is controlled by a sigmoid gate whose input signal is the sam- pled intention zt . This prevents the decoder from over- fitting to the deterministic state information and forces it to take the sampled stochastic intention into account. The LIDM can then be formally written down in its parame-where o t is the joint representation, and both u t and m t are modeled by a bidirectional LSTM network. Although both q Φ (z t |s t , m t ) and π Θ (z t |s t ) are modelled as parameterised multinomial distributions, the approximation q Φ (z t |s t , m t ) only functions during inference by producing samples to compute the stochastic gradients, while π Θ (z t |s t ) is the generative distribution that generates the required samples for composing the machine response. and the gradient w.r.t. Φ can be rewritten asBased on the samples z (n) t ∼ q Φ (z t |s t , m t ), we use dif- ferent strategies to alternately optimise the parameters Θ and Φ against the variational lower bound (Equation 8). To do this, we further divide Θ into two sets Θ = {Θ 1 , Θ 2 }. Parameters Θ 1 on the decoder side are directly updated by back-propagating the gradients,n Despite the steps described above for reducing the variance, there remain two major difficulties in learning latent inten- tions in a completely unsupervised manner: (1) the high variance of the inference network prevents it from generat- ing sensible intention samples in the early stages of train- ing, and (2) the overly strong discriminative power of the LSTM language model is prone to the disconnection phe- nomenon between the LSTM decoder and the rest of the components whereby the decoder learns to ignore the sam- ples and focuses solely on optimising the language model. To ensure more stable training and prevent disconnection, a semi-supervised learning technique is introduced.Parameters Θ 2 in the generative network, however, are up- dated by minimising the KL divergence,zt where the entropy derivative ∂H[q Φ (z t |s t , m t )]/∂Θ 2 = 0 and therefore can be ignored. Finally, for the parameters Φ in the inference network, we firstly define the learning signal r(m t , zInferring the latent intentions underlying utterances is sim- ilar to an unsupervised clustering task. Standard clustering algorithms can therefore be used to pre-process the corpus and generate automatic labelsˆzlabelsˆ labelsˆz t for part of the training ex-Then when the model is trained on the unlabeled examples (m t , s t ) ∈ U, we optimise it against the modified variational lower bound given in Equa- tion 8Then the parameters Φ are updated by, However, when the model is updated based on examples from the labeled set (m t , s t , ˆ z t ) ∈ L, we treat the labeled intentionˆzintentionˆ intentionˆz t as an observed variable and train the model by maximising the joint log-likelihood,n However, this gradient estimator has a large variance be- cause the learning signal r(m t , zThe final joint objective function can then be written as L = αL 1 + L 2 , where α controls the trade-off between the supervised and unsupervised examples.(n) t , s t ) relies on samples from the proposal distribution q Φ (z t |s t , m t ). To reduce the variance during inference, we follow the REINFORCE al- gorithm Mnih &amp; Gregor, 2014) and introduce two baselines b and b(s t ), the centered learn- ing signal and input dependent baseline respectively to help reduce the variance. b is a learnable constant and b(s t ) = MLP(s t ). During training, the two baselines are updated by minimising the distance,One of the main purposes of learning interpretable, discrete latent intention inside a dialogue system is to be able to control and refine the model's behaviour with operational experience. The learnt generative network π Θ (z t |s t ) en- codes the policy discovered from the underlying data dis- tribution but this is not necessarily optimal for any specific task. Since π Θ (z t |s t ) is a parameterised policy network itself, any policy gradient-based reinforcement learning al- gorithm (Williams, 1992;Konda &amp; Tsitsiklis, 2003) can beused to fine-tune the initial policy against other objective functions that we are more interested in.Based on the initial policy π Θ (z t |s t ), we revisit the train- ing dialogues and update parameters based on the follow- ing strategy: when encountering unlabeled examples U at turn t the system samples an action from the learnt policy z  where the whole model is refined end-to-end using RL, updating only Θ effectively allows us to refine only the decision-making of the system and avoid the problem of over-fitting.We explored the properties of the LIDM model 3 using the CamRest676 corpus 4 collected by Wen et al (2017), in which the task of the system is to assist users to find a restaurant in the Cambridge, UK area. The corpus was col- lected based on a modified Wizard of Oz (Kelley, 1984) online data collection. Workers were recruited on Amazon Mechanical Turk and asked to complete a task by carrying out a conversation, alternating roles between a user and a wizard. There are three informable slots (food, pricerange, area) that users can use to constrain the search and six re- questable slots (address, phone, postcode plus the three informable slots) that the user can ask a value for once a restaurant has been offered. There are 676 dialogues in the dataset (including both finished and unfinished dialogues) and approximately 2750 conversational turns in total. The database contains 99 unique restaurants.size I set to 50, 70, and 100, respectively. The trade-off constants λ and α were both set to 0.1. To produce self- labeled response clusters for semi-supervised learning of the intentions, we firstly removed function words from all the responses and clustered them according to their content words. We then assigned the responses in the i-th frequent cluster to the i-th latent dimension as its supervised set. This results in about 35% (I = 50) to 43% (I = 100) labeled responses across the whole dataset. An example of the resulting seed set is shown in Table 1. During in- ference we carried out stochastic estimation by taking one sample for estimating the stochastic gradients. The model is trained by Adam (Kingma &amp; Ba, 2014) and tuned (early stopping, hyper-parameters) on the held-out validation set. We alternately optimised the generative model and the in- ference network by fixing the parameters of one while up- dating the parameters of the other.During reinforcement fine-tuning, we generated a sentence m t from the model to replace the ground truthˆmtruthˆ truthˆm t at each turn and define an immediate reward as whether m t can improve the dialogue success ( Su et al., 2015) relative tôtô m t , plus the sentence BLEU score (Auli &amp; Gao, 2014),  1 m t improves −1 m t degrades 0 otherwiseTo make a direct comparison with prior work we follow the same experimental setup as in Wen et al (2016; 2017). The corpus was partitioned into training, validation, and test sets in the ratio 3:1:1. The LSTM hidden layer sizes were set to 50, and the vocabulary size is around 500 after pre-processing, to remove rare words and words that can be delexicalised 2 . All the system components were trained jointly by fixing the pre-trained belief trackers and the dis- crete database operator with the model's latent intentionSuccess (    Table 2 presents the results of the corpus-based evaluation.The Ground Truth block shows the two metrics when we compute them on the human-authored responses. This sets a gold standard for the task. In the Published Models block, the results for the three baseline models were borrowed from Wen et al (2016), they are: (1) the vanilla neural di- alogue model (NDM), (2) NDM plus an attention mech- anism on the belief trackers, and (3) the attentive NDM with self-supervised sub-task neurons. The results of the LIDM model with and without RL fine-tuning are shown in the LIDM Models and the LIDM Models + RL blocks, respectively. As can be seen, the initial policy learned by fitting the latent intention to the underlying data distribu- tion yielded reasonably good results on BLEU but did not perform well on task success when compared to their deter- ministic counterparts (block 2 v.s. 3). This may be due to the fact that the variational lower bound of the dataset was optimised rather than task success during variational infer- ence. However, once RL was applied to optimise the suc- cess rate as part of the reward function (Equation 21) dur- ing the fine-tuning phase, the resulting LIDM+RL models outperformed the three baselines in terms of task success without significantly sacrificing BLEU (block 2 v.s. 4) 5 .success rate, the perceived comprehension ability and the naturalness of responses on a scale of 1 to 5. For each model, we collected 200 dialogues and averaged the scores. During human evaluation, we sampled from the top-5 in- tentions of the LIDM models and decoded a response based on the sample. The result is shown in Table 3. One interest- ing fact to note is that although the LIDM did not perform well on the corpus-based task success metric, the human judges rated its subjective success almost indistinguishably from the others. This discrepancy between the two exper- iments arises mainly from a flaw in the corpus-based suc- cess metric in that it favors greedy policies because the user side behaviours are fixed rather than interactional 6 . Despite the fact that LIDMs are considered only marginally better than NDM on subjective success, the LIDMs do outper- form NDM on both comprehension and naturalness scores. This is because the proposed LIDM models can better cap- ture multiple modes in the communicative intention and thereby respond more naturally by sampling from the la- tent intention variable.In order to assess the human perceived performance, we evaluated the three models (1) NDM, (2) LIDM, and (3) LIDM+RL by recruiting paid subjects on Amazon Me- chanical Turk. Each judge was asked to follow a task and carried out a conversation with the machine. At the end of each conversation the judges were asked to rate and com- pare the model's performance. We assessed the subjective Three example conversations are shown between a human judge and a machine, one from LIDM in Table 4 and two from LIDM+RL in Table 5, respectively. The results are displayed one exchange per block. Each induced latent in- tention is shown by a tuple (index, probability) followed by a decoded response, and the sample dialogues were pro- duced by following the responses highlighted in bold. As can be seen, the LIDM shown in Table 4 clearly has mul- tiple modes in the distribution over the learned intention latent variable, and what it represents can be easily inter- preted by the response generated. However, some inten- tions (such as intent 0) can result in very different responses under different dialogue states even though they were su- pervised by a small response set as shown in Table 1. This is mainly because of the variance introduced during vari- ational inference. Finally, when comparing Table 4 and  Table 5, we can observe the difference between the two di- alogue strategies: the LIDM, by inferring its policy from the supervised dataset, reflects the diverse set of modes in the underlying distribution; whereas the LIDM+RL, which Human : East side please Machine: ( 0 0.62) curry prince is a indian restaurant in the east side of town . would you like their telephone number ?(84 0.19) curry prince is an indian restaurant in the east of town and the number is 01223 566388 .(81 0.08) curry prince serves indian type of cuisine and is located in the east part of town .Human : yes , the address and phone number please Machine: ( 3 0.46) the address is 451 newmarket road fen ditton and the phone is 01223 566388 .(11 0.16) the address for curry prince is 451 newmarket road fen ditton and the phone number is 01223 566388 .( 0 0.15) curry prince is located at 451 newmarket road fen ditton and their phone is 01223 566388 . is there anything else i can help you with ?Human : Okay, thank you goodbye Machine: ( 0 0.57) thank you , goodbye .( 2 0.13) goodbye .( 1 0.11) you are welcome . goodbye .( 6 0.10) thank you for using the cambridge restaurant system . goodbye . Table 4. A sample dialogue from the LIDM, I=100 model, one exchange per block. Each latent intention is shown by a tuple (index, probability), followed by a decoded response. The sample dialogue was produced by following the responses highlighted in bold.refined its strategy using RL, exhibits a much greedier be- havior in achieving task success (e.g. in Table 5 in block 2 &amp; 4 the LIDM+RL agent provides the address and phone number even before the user asks). This is also supported by the human evaluation in Table 3 where LIDM+RL has much shorter dialogues on average compared to the other two models.explicitly models the hidden dialogue intentions underly- ing the user's utterances and allows the agent to directly learn a dialogue policy through interaction.Learning an end-to-end dialogue system is appealing but challenging because of the credit assignment problem. Dis- crete latent variable dialogue models such as LIDM are at- tractive because the latent variable can serve as an inter- face for decomposing the learning of language and the in- ternal dialogue decision-making. This decomposition can effectively help us resolve the credit assignment problem where different learning signals can be applied to differ- ent sub-modules to update the parameters. In variational inference for discrete latent variables, the latent distribu- tion is basically updated by the reward from the variational lower bound. While in reinforcement learning, the latent distribution (i.e. policy network) is updated by the rewards from dialogue success and sentence BLEU score. Hence, the latent variable bridges the different learning paradigms such as Bayesian learning and reinforcement learning and brings them together under the same framework. This framework provides a more robust neural network-based approach than previous approaches because it does not de- pend solely on sequence-to-sequence learning but insteadModeling chat-based dialogues ( Serban et al., 2015;Shang et al., 2015) as a sequence-to-sequence learning (Sutskever et al., 2014) problem is a common theme in the deep learn- ing community. Vinyals and Le (2015) has demonstrated a seq2seq-based model trained on a huge amount of conver- sation corpora which learns interesting replies conditioned on different user queries. However, due to an inability to model dialogue context, these models generally suffer from the generic response problem ( Li et al., 2016a;Serban et al., 2016). Several approaches have been proposed to mitigate this issue, such as modeling the persona ( Li et al., 2016b), reinforcement learning ( Li et al., 2016c), and intro- ducing continuous latent variables ( Serban et al., 2016;Cao &amp; Clark, 2017). While in our case, we not only make use of the latent variable to inject stochasticity for generating natural and diverse machine responses but also model the hidden dialogue intentions explicitly. This combines the merits of reinforcement learning and generative models. Oh no. How about Indian? Machine: (93 0.51) the gandhi is located at 72 regent street city centre and their telephone number is 01223 353942 .(72 0.21) the gandhi serves indian kind of food and is in the centre of town and is in the cheap price range . (12 0.12) the gandhi is a indian restaurant in the centre location and is in the cheap price range .Human : Okay, thank you goodbye Machine: ( 6 0.28) thank you for using the cambridge restaurant system . goodbye .   Table 4, the RL agent demonstrates a much greedier behavior toward task success. This can be seen in block 2 &amp; block 4 in which the agent provides the address and phone number even before the user asks. ment , and natural language genera- tion ( Wen et al., 2015). These system modules commu- nicate through a dialogue act formalism (Traum, 1999), which in effect constitute a fixed set of handcrafted inten- tions. This limits the ability of such systems to scale to more complex tasks. In contrast, the LIDM directly infers all underlying dialogue intentions from data and can handle intention distributions with long tails by measuring similar- ities against the existing ones during variational inference. Modeling of end-to-end goal-oriented dialogue systems has also been studied recently Bordes &amp; Weston, 2017), however, these models are typically de- terministic and rely on decoder supervision signals to fine- tune a large set of model parameters. supervision) as a simple way to generate automatic labels by heuristics is popular in many NLP tasks and has been applied to memory networks ( Hill et al., 2016) and neural dialogue systems  recently. Since there is no additional effort required in labeling, it can also be viewed as a method for bootstrapping.Much research has focused on combining different learn- ing paradigms and signals to bootstrap performance. For example, semi-supervised learning ( ) has been applied in the sample-based neural variational in- ference framework as a way to reduce sample variance. In practice, this relies on a discrete latent variable Kočisk´Kočisk´y et al., 2016) as the vehicle for the supervision labels. As in reinforcement learning, which has been a very common learning paradigm in dialogue sys- tems Su et al., 2016;Li et al., 2017), the policy is also parameterised by a discrete set of actions. As a consequence, the LIDM, which parameterises the in- tention space via a discrete latent variable, can automati- cally enjoy the benefit of bootstrapping from signals com- ing from different learning paradigms. In addition, self- supervised learning (Snow et al., 2004) (or distant, weak In this paper, we have proposed a framework for learning dialogue intentions via discrete latent variable models and introduced the Latent Intention Dialogue Model (LIDM) for goal-oriented dialogue modeling. We have shown that the LIDM can discover an effective initial policy from the underlying data distribution and is capable of revising its strategy based on an external reward using reinforcement learning. We believe this is a promising step forward for building autonomous dialogue agents since the learnt dis- crete latent variable interface enables the agent to perform learning using several differing paradigms. The experi- ments showed that the proposed LIDM is able to commu- nicate with human subjects and outperforms previous pub- lished results.
