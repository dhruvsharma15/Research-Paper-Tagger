When reading a document, short or long, humans have a mechanism that somehow allows them to remember the gist of what they have read so far. Consider the following example:"The U.S.presidential race isn't only drawing attention and controversy in the United States -it's being closely watched across the globe. But what does the rest of the world think about a campaign that has already thrown up one surprise after another? CNN asked 10 journalists for their take on the race so far, and what their country might be hoping for in America's next -"The missing word in the text above is easily predicted by any human to be either President or Commander in Chief or their synonyms. There have been various language models -from simple n- grams to the most recent RNN-based language models -that aim to solve this problem of predicting correctly the subsequent word in an observed sequence of words.A good language model should capture at least two important properties of natural language. The first one is correct syntax. In order to do prediction that enjoys this property, we often only need to consider a few preceding words. Therefore, correct syntax is more of a local property. Word order matters in this case. The second property is the semantic coherence of the prediction. To achieve this, we often need to consider many preceding words to understand the global semantic meaning of the sentence or document. The ordering of the words usually matters much less in this case.Because they only consider a fixed-size context window of preceding words, traditional n-gram and neural probabilistic language models ( Bengio et al., 2003) have difficulties in capturing global semantic information. To overcome this, RNN-based language models ( Mikolov et al., 2010; use hidden states to "remember" the history of a word sequence. However, none of these approaches explicitly model the two main properties of language mentioned above, correct syntax and semantic coherence. Previous work by Chelba and Jelinek (2000) and Gao et al. (2004) exploit syntactic or semantic parsers to capture long-range dependencies in language.In this paper, we propose TopicRNN, a RNN-based language model that is designed to directly capture long-range semantic dependencies via latent topics. These topics provide context to the RNN. Contextual RNNs have received a lot of attention ( Mikolov and Zweig, 2012;Ji et al., 2015;Lin et al., 2015;Ji et al., 2016;Ghosh et al., 2016). However, the models closest to ours are the contextual RNN model proposed by Mikolov and Zweig (2012) and its most recent extension to the long-short term memory (LSTM) architecture ( Ghosh et al., 2016). These models use pre-trained topic model features as an additional input to the hidden states and/or the output of the RNN. In contrast, TopicRNN does not require pre-trained topic model features and can be learned in an end-to-end fashion. We introduce an automatic way for handling stop words that topic models usually have difficulty dealing with. Under a comparable model size set up, TopicRNN achieves better perplexity scores than the contextual RNN model of Mikolov and Zweig (2012) on the Penn TreeBank dataset 1 . Moreover, TopicRNN can be used as an unsupervised feature extractor for downstream applications. For example, we derive document features of the IMDB movie review dataset using TopicRNN for sentiment classification. We reported an error rate of 6.28%. This is close to the state-of-the-art 5.91% ( Miyato et al., 2016) despite that we do not use the labels and adversarial training in the feature extraction stage.The remainder of the paper is organized as follows: Section 2 provides background on RNN-based language models and probabilistic topic models. Section 3 describes the TopicRNN network ar- chitecture, its generative process and how to perform inference for it. Section 4 presents per-word perplexity results on the Penn TreeBank dataset and the classification error rate on the IMDB 100K dataset. Finally, we conclude and provide future research directions in Section 5.We present the background necessary for building the TopicRNN model. We first review RNN-based language modeling, followed by a discussion on the construction of latent topic models.Language modeling is fundamental to many applications. Examples include speech recognition and machine translation. A language model is a probability distribution over a sequence of words in a predefined vocabulary. More formally, let V be a vocabulary set and y 1 , ..., y T a sequence of T words with each y t ∈ V . A language model measures the likelihood of a sequence through a joint probability distribution, T p(y 1 , ..., y T ) = p(y 1 ) p(y t |y 1:t−1 ).Traditional n-gram and feed-forward neural network language models ( Bengio et al., 2003) typically make Markov assumptions about the sequential dependencies between words, where the chain rule shown above limits conditioning to a fixed-size context window.RNN-based language models (Mikolov et al., 2011) sidestep this Markov assumption by defining the conditional probability of each word y t given all the previous words y 1:t−1 through a hidden state h t (typically via a softmax function): p(y t |y 1:t−1 ) p(y t |h t ),The function f (·) can either be a standard RNN cell or a more complex cell such as GRU ( Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997). The input and target words are related via the relation x t ≡ y t−1 . These RNN-based language models have been quite successful (Mikolov et al., 2011;Chelba et al., 2013;Jozefowicz et al., 2016).While in principle RNN-based models can "remember" arbitrarily long histories if provided enough capacity, in practice such large-scale neural networks can easily encounter difficulties during opti- mization ( Bengio et al., 1994;Pascanu et al., 2013;Sutskever, 2013) or overfitting issues ( Srivastava et al., 2014). Finding better ways to model long-range dependencies in language modeling is there- fore an open research challenge. As motivated in the introduction, much of the long-range depen- dency in language comes from semantic coherence, not from syntactic structure which is more of a local phenomenon. Therefore, models that can capture long-range semantic dependencies in lan- guage are complementary to RNNs. In the following section, we describe a family of such models called probabilistic topic models.Probabilistic topic models are a family of models that can be used to capture global semantic co- herency ( Blei and Lafferty, 2009). They provide a powerful tool for summarizing, organizing, and navigating document collections. One basic goal of such models is to find groups of words that tend to co-occur together in the same document. These groups of words are called topics and represent a probability distribution that puts most of its mass on this subset of the vocabulary. Documents are then represented as mixtures over these latent topics. Through posterior inference, the learned topics capture the semantic coherence of the words they cluster together (Mimno et al., 2011).The simplest topic model is latent Dirichlet allocation (LDA) ( Blei et al., 2003). It assumes K underlying topics β = {β 1 , . . . , β K } , each of which is a distribution over a fixed vocabulary. The generative process of LDA is as follows: First generate the K topics, β k ∼ iid Dirichlet(τ ). Then for each document containing words y 1:T , independently generate document-level variables and data:1. Draw a document-specific topic proportion vector θ ∼ Dirichlet(α).2. For the tth word in the document,Marginalizing each z t , we obtain the probability of y 1:T via a matrix factorization followed by an integration over the latent variable θ,In LDA the prior distribution on the topic proportions is a Dirichlet distribution; it can be replaced by many other distributions. For example, the correlated topic model ( Blei and Lafferty, 2006) uses a log-normal distribution. Most topic models are "bag of words" models in that word order is ignored. This makes it easier for topic models to capture global semantic information. However, this is also one of the reasons why topic models do not perform well on general-purpose language modeling applications such as word prediction. While bi-gram topic models have been proposed (Wallach, 2006), higher order models quickly become intractable.Another issue encountered by topic models is that they do not model stop words well. This is because stop words usually do not carry semantic meaning; their appearance is mainly to make the sentence more readable according to the grammar of the language. They also appear frequently in almost every document and can co-occur with almost any word 2 . In practice, these stop words are chosen using tf-idf (Blei and Lafferty, 2009).We next describe the proposed TopicRNN model. In TopicRNN, latent topic models are used to capture global semantic dependencies so that the RNN can focus its modeling capacity on the local dynamics of the sequences. With this joint modeling, we hope to achieve better overall performance on downstream applications.The model. TopicRNN is a generative model. For a document containing the words y 1:T ,3 θ ∼ N (0, I).2. Given word y 1:t−1 , for the tth word y t in the document,, with σ the sigmoid function.(c) Draw word y t ∼ p(y t |h t , θ, l t , B), whereThe stop word indicator l t controls how the topic vector θ affects the output. If l t = 1 (indicating y t is a stop word), the topic vector θ has no contribution to the output. Otherwise, we add a bias to favor those words that are more likely to appear when mixing with θ, as measured by the dot product between θ and the latent word vector b i for the ith vocabulary word. As we can see, the long- range semantic information captured by θ directly affects the output through an additive procedure. Unlike Mikolov and Zweig (2012), the contextual information is not passed to the hidden layer of the RNN. The main reason behind our choice of using the topic vector as bias instead of passing it into the hidden states of the RNN is because it enables us to have a clear separation of the contributions of global semantics and those of local dynamics. The global semantics come from the topics which are meaningful when stop words are excluded. However these stop words are needed for the local dynamics of the language model. We hence achieve this separation of global vs local via a binary decision model for the stop words. It is unclear how to achieve this if we pass the topics to the hidden states of the RNN. This is because the hidden states of the RNN will account for all words (including stop words) whereas the topics exclude stop words.We show the unrolled graphical representation of TopicRNN in Figure 1(a). We denote all model parameters as Θ = {Γ, V, B, W, W c } (see Appendix A.1 for more details). Parameter W c is for the inference network, which we will introduce below. The observations are the word sequences y 1:T and stop word indicators l 1:T . 4 The log marginal likelihood of the sequence y 1:T is T log p(y 1:T , l 1:T |h t ) = log p(θ)t=1Model inference. Direct optimization of Equation 2 is intractable so we use variational inference for approximating this marginal ( Jordan et al., 1999). Let q(θ) be the variational distribution on the marginalized variable θ. We construct the variational objective function, also called the evidence lower bound (ELBO), as follows:≤ log p(y 1:T , l 1:T |h t , Θ). Following the proposed variational autoencoder technique, we choose the form of q(θ) to be an inference network using a feed-forward neural network ( Kingma and Welling, 2013;Miao et al., 2015). Let X c ∈ N |Vc| + be the term-frequency representation of y 1:T excluding stop words (with V c the vocabulary size without the stop words). The variational autoencoder inference network q(θ|X c , W c ) with parameter W c is a feed-forward neural network with ReLU activation units that projects X c into a K-dimensional latent space. Specifically, we havewhere g(·) denotes the feed-forward neural network. The weight matrices W 1 , W 2 and biases a 1 , a 2 are shared across documents. Each document has its own µ(X c ) and σ(X c ) resulting in a unique distribution q(θ|X c ) for each document. The output of the inference network is a distribution on θ, which we regard as the summarization of the semantic information, similar to the topic proportions in latent topic models. We show the role of the inference network in Figure 1(b). During training, the parameters of the inference network and the model are jointly learned and updated via truncated backpropagation through time using the Adam algorithm (Kingma and Ba, 2014). We use stochastic samples from q(θ|X c ) and the reparameterization trick towards this end ( Kingma and Welling, 2013;Rezende et al., 2014).Generating sequential text and computing perplexity. Suppose we are given a word sequence y 1:t−1 , from which we have an initial estimation of q(θ|X c ). To generate the next word y t , we compute the probability distribution of y t given y 1:t−1 in an online fashion. We choose θ to be a point estimatê θ, the mean of its current distribution q(θ|X c ). Marginalizing over the stop word indicator l t which is unknown prior to observing y t , the approximate distribution of y t is p(y t |y 1:t−1 ) ≈ p(y t |h t , ˆ θ, l t )p(l t |h t ).The predicted word y t is a sample from this predictive distribution. We update q(θ|X c ) by including y t to X c if y t is not a stop word. However, updating q(θ|X c ) after each word prediction is expensive, so we use a sliding window as was done in Mikolov and Zweig (2012). To compute the perplexity, we use the approximate predictive distribution above. , not including the pre-training process, which might require more parameters than the additional W c in our complexity.We assess the performance of our proposed TopicRNN model on word prediction and sentiment analysis 5 . For word prediction we use the Penn TreeBank dataset, a standard benchmark for as- sessing new language models (Marcus et al., 1993). For sentiment analysis we use the IMDB 100k dataset (Maas et al., 2011), also a common benchmark dataset for this application 6 . We use RNN, LSTM, and GRU cells in our experiments leading to TopicRNN, TopicLSTM, and TopicGRU.  Figure 2: Inferred distributions using TopicGRU on three different documents. The content of these documents is added on the appendix. This shows that some of the topics are being picked up depending on the input document.We first tested TopicRNN on the word prediction task using the Penn Treebank (PTB) portion of the Wall Street Journal. We use the standard split, where sections 0-20 (930K tokens) are used for training, sections 21-22 (74K tokens) for validation, and sections 23-24 (82K tokens) for test- ing ( Mikolov et al., 2010). We use a vocabulary of size 10K that includes the special token unk for rare words and eos that indicates the end of a sentence. TopicRNN takes documents as inputs. We split the PTB data into blocks of 10 sentences to constitute documents as done by (Mikolov and Zweig, 2012). The inference network takes as input the bag-of-words representation of the input document. For that reason, the vocabulary size of the inference network is reduced to 9551 after excluding 449 pre-defined stop words.In order to compare with previous work on contextual RNNs we trained TopicRNN using different network sizes. We performed word prediction using a recurrent neural network with 10 neurons, Table 2: TopicRNN and its counterparts exhibit lower perplexity scores across different network sizes than reported in Mikolov and Zweig (2012). Table 2a shows per-word perplexity scores for 10 neurons. Table 2b and Table 2c  100 neurons and 300 neurons. For these experiments, we used a multilayer perceptron with 2 hidden layers and 200 hidden units per layer for the inference network. The number of topics was tuned depending on the size of the RNN. For 10 neurons we used 18 topics. For 100 and 300 neurons we found 50 topics to be optimal. We used the validation set to tune the hyperparameters of the model. We used a maximum of 15 epochs for the experiments and performed early stopping using the validation set. For comparison purposes we did not apply dropout and used 1 layer for the RNN and its counterparts in all the word prediction experiments as reported in Table 2. One epoch for 10 neurons takes 2.5 minutes. For 100 neurons, one epoch is completed in less than 4 minutes. Finally, for 300 neurons one epoch takes less than 6 minutes. These experiments were ran on Microsoft Azure NC12 that has 12 cores, 2 Tesla K80 GPUs, and 112 GB memory. First, we show five randomly drawn topics in Table 1. These results correspond to a network with 100 neurons. We also illustrate some inferred topic distributions for several documents from TopicGRU in Figure 2. Similar to standard topic models, these distributions are also relatively peaky.Next, we compare the performance of TopicRNN to our baseline contextual RNN using perplexity. Perplexity can be thought of as a measure of surprise for a language model. It is defined as the exponential of the average negative log likelihood.  Mikolov and Zweig (2012). Note that to compute these perplexity scores for word prediction we use a sliding window to compute θ as we move along the sequences. The topic vector θ that is used from the current batch of words is estimated from the previous batch of words. This enables fair comparison to previously reported results (Mikolov and Zweig, 2012).Another aspect of the TopicRNN model we studied is its capacity to generate coherent text. To do this, we randomly drew a document from the test set and used this document as seed input to the inference network to compute θ. Our expectation is that the topics contained in this seed document are reflected in the generated text. Table 3 shows generated text from models learned on the PTB and IMDB datasets. See Appendix A.3 for more examples. Table 3: Generated text using the TopicRNN model on the PTB (top) and IMDB (bottom).they believe that they had senior damages to guarantee and frustration of unk stations eos the rush to minimum effect in composite trading the compound base inflated rate before the common charter 's report eos wells fargo inc. unk of state control funds without openly scheduling the university 's exchange rate has been downgraded it 's unk said eos the united cancer &amp; began critical increasing rate of N N at N N to N N are less for the country to trade rate for more than three months $ N workers were mixed eos lee is head to be watched unk month she eos but the acting surprisingly nothing is very good eos i cant believe that he can unk to a role eos may appear of for the stupid killer really to help with unk unk unk if you wan na go to it fell to the plot clearly eos it gets clear of this movie 70 are so bad mexico direction regarding those films eos then go as unk 's walk and after unk to see him try to unk before that unk with this film 11.11% WRRBM ( Dahl et al., 2012) 12.58% WRRBM + BoW (bnc) ( Dahl et al., 2012) 10.77% MNB-uni ( Wang &amp; Manning, 2012) 16.45% MNB-bi ( Wang &amp; Manning, 2012) 13.41% SVM-uni ( Wang &amp; Manning, 2012) 13.05% SVM-bi ( Wang &amp; Manning, 2012) 10.84% NBSVM-uni ( Wang &amp; Manning, 2012) 11.71% seq2-bown-CNN (Johnson &amp; Zhang, 2014) 14.70% NBSVM-bi ( Wang &amp; Manning, 2012) 8.78% Paragraph Vector (Le &amp; Mikolov, 2014) 7.42% SA-LSTM with joint training (Dai &amp; Le, 2015) 14.70% LSTM with tuning and dropout (Dai &amp; Le, 2015) 13.50% LSTM initialized with word2vec embeddings (Dai &amp; Le, 2015) 10.00% SA-LSTM with linear gain (Dai &amp; Le, 2015) 9.17% LM-TM (Dai &amp; Le, 2015) 7.64% SA-LSTM (Dai &amp; Le, 2015) 7.24% Virtual Adversarial ( Miyato et al. 2016) 5.91% TopicRNN 6.28%We performed sentiment analysis using TopicRNN as a feature extractor on the IMDB 100K dataset. This data consists of 100,000 movie reviews from the Internet Movie Database (IMDB) website. The data is split into 75% for training and 25% for testing. Among the 75K training reviews, 50K are unlabelled and 25K are labelled as carrying either a positive or a negative sentiment. All 25K test reviews are labelled. We trained TopicRNN on 65K random training reviews and used the remaining 10K reviews for validation. To learn a classifier, we passed the 25K labelled training reviews through the learned TopicRNN model. We then concatenated the output of the inference network and the last state of the RNN for each of these 25K reviews to compute the feature vectors. We then used these feature vectors to train a neural network with one hidden layer, 50 hidden units, and a sigmoid activation function to predict sentiment, exactly as done in Le and Mikolov (2014).To train the TopicRNN model, we used a vocabulary of size 5,000 and mapped all other words to the unk token. We took out 439 stop words to create the input of the inference network. We used 500 units and 2 layers for the inference network, and used 2 layers and 300 units per-layer for the Figure 3: Clusters of a sample of 10000 movie reviews from the IMDB 100K dataset using Top- icRNN as feature extractor. We used K-Means to cluster the feature vectors. We then used PCA to reduce the dimension to two for visualization purposes. red is a negative review and green is a positive review.RNN. We chose a step size of 5 and defined 200 topics. We did not use any regularization such as dropout. We trained the model for 13 epochs and used the validation set to tune the hyperparameters of the model and track perplexity for early stopping. This experiment took close to 78 hours on a MacBook pro quad-core with 16GHz of RAM. See Appendix A.4 for the visualization of some of the topics learned from this data. Table 4 summarizes sentiment classification results from TopicRNN and other methods. Our error rate is 6.28%. 8 This is close to the state-of-the-art 5.91% ( Miyato et al., 2016) despite that we do not use the labels and adversarial training in the feature extraction stage. Our approach is most similar to Le and Mikolov (2014), where the features were extracted in a unsupervised way and then a one-layer neural net was trained for classification. Figure 3 shows the ability of TopicRNN to cluster documents using the feature vectors as created during the sentiment analysis task. Reviews with positive sentiment are coloured in green while reviews carrying negative sentiment are shown in red. This shows that TopicRNN can be used as an unsupervised feature extractor for downstream applications. Table 3 shows generated text from models learned on the PTB and IMDB datasets. See Appendix A.3 for more examples. The overall generated text from IMDB encodes a negative sentiment.In this paper we introduced TopicRNN, a RNN-based language model that combines RNNs and latent topics to capture local (syntactic) and global (semantic) dependencies between words. The global dependencies as captured by the latent topics serve as contextual bias to an RNN-based lan- guage model. This contextual information is learned jointly with the RNN parameters by maxi- mizing the evidence lower bound of variational inference. TopicRNN yields competitive per-word perplexity on the Penn Treebank dataset compared to previous contextual RNN models. We have reported a competitive classification error rate for sentiment analysis on the IMDB 100K dataset. We have also illustrated the capacity of TopicRNN to generate sensible topics and text. In future work, we will study the performance of TopicRNN when stop words are dynamically discovered during training. We will also extend TopicRNN to other applications where capturing context is important such as in dialog modeling. If successful, this will allow us to have a model that performs well across different natural language processing applications.We use the following notation: C is the vocabulary size (including stop words), H is the number of hidden units of the RNN, K is the number of topics, and E is the dimension of the inference network hidden layer. Table 5 gives the dimension of each of the parameters of the TopicRNN model (ignoring the biases).  'u.s.', 'demand', 'swing', 'in', 'the', 'territory', "'s", 'favor', 'they', 'argue', 'local', 'businessmen', 'will', 'probably', 'overcome', 'their', 'N', 'worries', 'and', 'continue', 'doing', 'business', 'as', 'usual', 'but', 'economic', 'arguments', 'however', 'solid', 'wo', "n't", 'necessarily', '&lt;unk&gt;', 'hong', 'kong', "'s", 'N', 'million', 'people', 'many', 'are', 'refugees', 'having', 'fled', 'china', "'s", '&lt;unk&gt;', 'cycles', 'of', 'political', 'repression', 'and', 'poverty', 'since', 'the', 'communist', 'party', 'took', 'power', 'in', 'N', 'as', 'a', 'result', 'many', 'of', 'those', 'now', 'planning', 'to', 'leave', 'hong', 'kong', 'ca', "n't", 'easily', 'be', '&lt;unk&gt;', 'by', '&lt;unk&gt;', 'improvements', 'in', 'the', 'colony', "'s", 'political', 'and', 'economic', 'climate'Figure on the middle: 'it', 'said', 'the', 'man', ' whom', 'it', 'did', 'not', 'name', 'had', 'been', 'found', 'to', 'have', 'the', 'disease', 'after', 'hospital', 'tests', 'once', 'the', 'disease', 'was', 'confirmed', 'all', 'the', 'man', "'s", 'associates', 'and', 'family', 'were', 'tested', 'but', 'none', 'have', 'so', 'far', 'been', 'found', 'to', 'have', 'aids', 'the', 'newspaper', 'said', 'the', 'man', 'had', 'for', 'a', 'long', 'time', 'had', 'a', 'chaotic', 'sex', 'life', 'including', 'relations', 'with', 'foreign', 'men', 'the', 'newspaper', 'said', 'the', 'polish', 'government', 'increased', 'home', 'electricity', 'charges', 'by', 'N', 'N', 'and', 'doubled', 'gas', 'prices', 'the', 'official', 'news', 'agency', '&lt;unk&gt;', 'said', 'the', 'increases', 'were', 'intended', 'to', 'bring', '&lt;unk&gt;', 'low', 'energy', 'charges', 'into', 'line', 'with', 'production', 'costs', 'and', 'compensate', 'for', 'a', 'rise', 'in', 'coal', 'prices', 'in', '&lt;unk&gt;', 'news', 'south', 'korea', 'in', 'establishing', 'diplomatic', 'ties', 'with', 'poland', 'yesterday', 'announced', '$', 'N', 'million', 'in', 'loans', 'to', 'the', 'financially', 'strapped', 'warsaw', 'government', 'in', 'a', 'victory', 'for', 'environmentalists', 'hungary', "'s", 'parliament', 'terminated', 'a', 'multibillion-dollar', 'river', '&lt;unk&gt;', 'dam', 'being', 'built', 'by', '&lt;unk&gt;', 'firms', 'the', '&lt;unk&gt;', 'dam', 'was', 'designed', 'to', 'be', '&lt;unk&gt;', 'with', 'another', 'dam', 'now', 'nearly', 'complete', 'N', 'miles', '&lt;unk&gt;', 'in', 'czechoslovakia', 'in', 'ending', 'hungary', "'s", 'part', 'of', 'the', 'project', 'parliament', 'authorized', 'prime', 'minister', '&lt;unk&gt;', '&lt;unk&gt;', 'to', 'modify', 'a', 'N', 'agreement', 'with', 'czechoslovakia', 'which', 'still', 'wants', 'the', 'dam', 'to', 'be', 'built', 'mr.', '&lt;unk&gt;', 'said', 'in', 'parliament', 'that', 'czechoslovakia', 'and', 'hungary', 'would', 'suffer', 'environmental', 'damage', 'if', 'the', '&lt;unk&gt;', '&lt;unk&gt;', 'were', 'built', 'as', 'planned' Figure on the right: 'in', 'hartford', 'conn.', 'the', 'charter', 'oak', 'bridge', ' will', 'soon', 'be', 'replaced', 'the', '&lt;unk&gt;', '&lt;unk&gt;', 'from', 'its', '&lt;unk&gt;', '&lt;unk&gt;', 'to', 'a', 'park', '&lt;unk&gt;', 'are', 'possible', 'citizens', 'in', 'peninsula', 'ohio', 'upset', 'over', 'changes', 'to', 'a', 'bridge', 'negotiated', 'a', 'deal', 'the', 'bottom', 'half', 'of', 'the', '&lt;unk&gt;', 'will', 'be', 'type', 'f', 'while', 'the', 'top', 'half', 'will', 'have', 'the', 'old', 'bridge', "'s", '&lt;unk&gt;', 'pattern', 'similarly', 'highway', 'engineers', 'agreed', 'to', 'keep', 'the', 'old', '&lt;unk&gt;', 'on', 'the', 'key', 'bridge', 'in', 'washington', 'd.c.', 'as', 'long', 'as', 'they', 'could', 'install', 'a', 'crash', 'barrier', 'between', 'the', 'sidewalk', 'and', 'the', 'road', '&lt;unk&gt;', '&lt;unk&gt;', 'drink', 'carrier', 'competes', 'with', '&lt;unk&gt;', '&lt;unk&gt;', '&lt;unk&gt;', 'just', 'got', 'easier', 'or', 'so', 'claims', '&lt;unk&gt;', 'corp.', 'the', 'maker', 'of', 'the', '&lt;unk&gt;', 'the', 'chicago', 'company', "'s", 'beverage', 'carrier', 'meant', 'to', 'replace', '&lt;unk&gt;', '&lt;unk&gt;', 'at', '&lt;unk&gt;', 'stands', 'and', 'fast-food', 'outlets', 'resembles', 'the', 'plastic', '&lt;unk&gt;', 'used', 'on', '&lt;unk&gt;', 'of', 'beer', 'only', 'the', '&lt;unk&gt;', 'hang', 'from', 'a', '&lt;unk&gt;', 'of', '&lt;unk&gt;', 'the', 'new', 'carrier', 'can', '&lt;unk&gt;', 'as', 'many', 'as', 'four', '&lt;unk&gt;', 'at', 'once', 'inventor', '&lt;unk&gt;', 'marvin', 'says', 'his', 'design', 'virtually', '&lt;unk&gt;', '&lt;unk&gt;' A.3 MORE GENERATED TEXT FROM THE MODEL:We illustrate below some generated text resulting from training TopicRNN on the PTB dataset. Here we used 50 neurons and 100 topics:Text1: but the refcorp bond fund might have been unk and unk of the point rate eos house in national unk wall restraint in the property pension fund sold willing to zenith was guaranteed by $ N million at short-term rates maturities around unk products eos deposit posted yields slightly Text2: it had happened by the treasury 's clinical fund month were under national disap- pear institutions but secretary nicholas instruments succeed eos and investors age far compound
