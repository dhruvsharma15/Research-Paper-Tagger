2. We employ convolutional architectures to encode the image content, represent the question, and learn the inter- actions between the image and question representations, which are jointly learned to produce the answer condi- tioning on the image and question.how many leftover donuts is the red bicycle holding Recently, the visual Turing test, an open domain task of question answering based on real-world images, has been proposed to resemble the famous Turing test. In ( Gao et al. 2015) a human judge will be presented with an image, a question, and the answer to the question by the computational models or human annotators. Based on the answer, the human judge needs to determine whether the answer is given by a human (i.e. pass the test) or a machine (i.e. fail the test). Geman et al. (Geman et al. 2015) proposed to produce a stochastic sequence of binary questions from a given test image, where the answer to the question is limited to yes/no. Malinowski et al. (Malinowski and Fritz 2014b;Malinowski and Fritz 2015) further discussed the associated challenges and issues with regard to visual Turing test, such as the vision and language representations, the common sense knowledge, as well as the evaluation. The image QA task, resembling the visual Turing test, is then proposed. Malinowski et al. (Malinowski and Fritz 2014a) proposed a multi-world approach that conducts the semantic parsing of question and segmentation of image to produce the answer. Deep neural networks are also em- ployed for the image QA task, which is more related to our research work. The work by (Malinowski, Rohrbach, and Fritz 2015;Gao et al. 2015) formulates the image QA task as a generation problem. Malinowski et al.'s model (Malinowski, Rohrbach, and Fritz 2015), namely the Neural- Image-QA, feeds the image representation from CNN and the question into the long-short term memory (LSTM) to produce the answer. This model ignores the different characteristics of questions and answers. Compared with the questions, the answers tend to be short, such as one single word denoting the object category, color, number, and so on. The deep neural network in ( Gao et al. 2015), inspired by the multimodal recurrent neural networks model ( ), used two LSTMs for the representations of question and answer, respec- tively. In (Ren, Kiros, and Zemel 2015), the image QA task is formulated as a classification problem, and the so- called visual semantic embedding (VSE) model is proposed. LSTM is employed to jointly model the image and ques- tion by treating the image as an independent word, and appending it to the question at the beginning or ending position. As such, the joint representation of image and question is learned, which is further used for classification. However, simply treating the image as an individual word cannot help effectively exploit the complicated relations between the image and question. Thus, the accuracy of the answer prediction may not be ensured. In order to cope with these drawbacks, we proposed to employ an end-to- end convolutional architectures for the image QA to capture the complicated inter-modal relationships as well as the representations of image and question. Experimental results demonstrate that the convolutional architectures can can achieve better performance for the image QA task.For image QA, the problem is to predict the answer a given the question q and the related image I:where Ω is the set containing all the answers. θ denotes all the parameters for performing image QA. In order to make a reliable prediction of the answer, the question q and image I need to be adequately represented. Based on their representations, the relations between the two multimodal inputs are further learned to produce the answer. In this paper, the ability of CNN is exploited for not only modeling image and sentence individually, but also capturing the relations and interactions between them. As illustrated in Figure 2, our proposed CNN framwork for image QA consists of three individual CNNs: one im- age CNN encoding the image content, one sentence CNN generating the question representation, one multimodal con- volution layer fusing the image and question representations together and generate the joint representation. Finally, the joint representation is fed into a softmax layer to produce the answer. The three CNNs and softmax layer are fully coupled for our proposed end-to-end image QA framework, with all the parameters (three CNNs and softmax) jointly learned in an end-to-end fashion.There are many research papers employing CNNs to gen- erate image representations, which achieve the state-of- the-art performances on image recognition (Simonyan and Zisserman 2014;Szegedy et al. 2015). In this paper, we employ the work (Simonyan and Zisserman 2014)   where s rp defines the size of local "receptive field" for convolution. " concatenates the s rp vectors into a long vector. In this paper, s rp is chosen as 3 for the convolution process. The parameters within the convolution unit are shared for the whole question with a window covering 3 semantic components sliding from the beginning to the end. The input of the first convolution layer for the sentence CNN is the word embeddings of the question:where ν i wd is the word embedding of the i th word in the question.Max-pooling With the convolution process, the sequential s rp semantic components are composed to a higher semantic representation. However, these compositions may not be the meaningful representations, such as "is on the" of the question in Figure 3. The max-pooling process following each convolution process is performed:In this paper, CNN is employed to model the question for image QA. As most convolution models ( Lecun and Bengio 1995;Kalchbrenner, Grefenstette, and Blunsom 2014), we consider the convolution unit with a local "receptive field" and shared weights to capture the rich structures and compo- sition properties between consecutive words. The sentence CNN for generating the question representation is illustrated in Figure 3. For a given question with each word represented as the word embedding ( ), the sentence CNN with several layers of convolution and max-pooling is performed to generate the question representation ν qt .Firstly, together with the stride as two, the max-pooling process shrinks half of the representation, which can quickly make the sentence representation. Most importantly, the max-pooling process can select the meaningful composi- tions while filter out the unreliable ones. As such, the meaningful composition "of the chair" is more likely to be pooled out, compared with the composition "front of the".Convolution For a sequential input ν, the convolution unit for feature map of type-f on the th layer isThe convolution and max-pooling processes exploit and summarize the local relation signals between consecutive words. More layers of convolution and max-pooling can help to summarize the local interactions between words at larger scales and finally reach the whole representation of the question. In this paper, we employ three layers of convolution and max-pooling to generate the question representation ν qt .where w ( ) are the parameters for the f feature map on the th layer, σ is the nonlinear activation function, and ν i ( denotes the segment of ( th layer for the convolution at location i , which is defined as follows. Configurations and Training is performed, which is expected to capture the interactions and relations between the two multimodal inputs.where ν in mm is the input of the multimodal convolution unit. ν After the mutlimodal convolution layer, the multimodal representation ν mm jointly modeling the image and question is obtained. ν mm is then fed into a softmax layer as shown in Figure 2, which produces the answer to the given image and question pair.Three layers of convolution and max-pooling are employed for the sentence CNN. The numbers of the feature maps for the three convolution layers are 300, 400, and 400, respectively. The sentence CNN is designed on a fixed archi- tecture, which needs to be set to accommodate the maximum length of the questions. In this paper, the maximum length of the question is chosen as 38. The word embeddings are obtained by the skip-gram model ( ) with the dimension as 50. We use the VGG (Simonyan and Zisserman 2014) network as the image CNN. The dimension of ν im is set as 400. The multimodal CNN takes the image and sentence representations as the input and generate the joint representation with the number of feature maps as 400.The proposed CNN model is trained with stochastic gradi- ent descent with mini batches of 100 for optimization, where the negative log likelihood is chosen as the loss. During the training process, all the parameters are tuned, including the parameters of nonlinear image mapping, image CNN, sentence CNN, multimodal convolution layer, and softmax layer. Moreover, the word embeddings are also fine-tuned. In order to prevent overfitting, dropout (with probability 0.1) is used. In this section, we firstly introduce the configurations of our CNN model for image QA and how we train the proposed CNN model. Afterwards, the public image QA datasets and evaluation measurements are introduced. Finally, the experimental results are presented and analyzed.One straightforward way for evaluating image QA is to uti- lize accuracy, which measures the proportion of the correctly answered testing questions to the total testing questions. Besides accuracy, Wu-Palmer similarity (WUPS) ( Wu and Palmer 1994;Malinowski and Fritz 2014a) is also used to measure the performances of different models on the image QA task. WUPS calculates the similarity between two words based on their common subsequence in a taxonomy tree.  Competitor Models We compare our models with re- cently developed models for the image QA task, specifically the multi-world approach (Malinowski and Fritz 2014a), the VSE model (Ren, Kiros, and Zemel 2015), and the Neural-Image-QA approach (Malinowski, Rohrbach, and Fritz 2015).Performances on Image QA The performances of our proposed CNN model on the DAQUAR-All, DAQUAR- Reduced, and COCO-QA datasets are illustrated in Table  1, 2, and 3, respectively. For DAQUAR-All and DAQUAR- Reduced datasets with multiple words as the answer to the question, we treat the answer comprising multiple words as an individual class for training and testing. For the DAQUAR-All dataset, we evaluate the perfor- mances of different image QA models on the full set ("mul- tiple words"). The answer to the image and question pair may be a single word or multiple words. Same as the work (Malinowski, Rohrbach, and Fritz 2015), a subset containing the samples with only a single word as the answer is created and employed for comparison ("single word"). Our proposed CNN model significantly outperforms the multi- world approach and Neural-Image-QA in terms of accuracy, WUPS@0.0, and WUPS@0.9. Specifically, our proposed CNN model achieves over 20% improvement compared to Neural-Image-QA in terms of accuracy on both "multiple words" and "single word". The results, shown in Table. 1, demonstrate that our CNN model can more accurately model the image and question as well as their interactions, thus yields better performances for the image QA task. Moreover, the language approach (Malinowski, Rohrbach, and Fritz 2015), which only resorts to the question performs inferiorly to the approaches that jointly model the image and question. The image component is thus of great help to the image QA task. One can also see that the performances on "multiple words" are generally inferior to those on "single word".For the DAQUAR-Reduced dataset, besides the Neural- Image-QA approach, the VSE model is also compared on "single word". Moreover, some of the methods introduced in (Ren, Kiros, and Zemel 2015) are also reported and compared. GUESS is the model which randomly outputs the answer according to the question type. BOW treats each word of the question equally and sums all the word vectors to predict the answer by logistic regression. LSTM is performed only on the question without considering the image, which is similar to the language approach (Malinowski, Rohrbach, and Fritz 2015). IMG+BOW performs the multinomial logistic regression based on the image feature and a BOW vector obtained by summing all the word vectors of the question. VIS+LSTM and 2-VIS+BLSTM are two versions of the VSE model. VIS+LSTM has only a single LSTM to encode the image and question in one direction, while 2-VIS+BLSTM uses a bidirectional LSTM to encode the image and question along with both directions to fully exploit the interactions between image and each word of the question. It can be observed that 2-VIS+BLSTM outperforms VIS+LSTM with a big margin. The same ob- servation can also be found on the COCO-QA dataset, as shown in Table 3, demonstrating that the bidirectional LSTM can more accurately model the interactions between image and question than the single LSTM. Our proposed CNN model significantly outperforms the competitor mod- els. More specifically, for the case of "single word", our proposed CNN achieves nearly 20% improvement in terms of accuracy over the best competitor model 2-VIS+BLSTM.For the COCO-QA dataset, IMG+BOW outperforms VIS+LSTM and 2-VIS+BLSTM, demonstrating that the Moreover, we examine the language modeling ability of the sentence CNN as follows. The words of the test questions are randomly reshuffled. Then the reformulated questions are sent to the sentence CNN to check whether the sentence CNN can still generate reliable question represen- tations and make accurate answer predictions. For randomly reshuffled questions, the results on COCO-QA dataset are 40.74, 53.06, and 80.41 for the accuracy, WUPS@0.9, and WUPS@0.0, respectively, which are significantly inferior to that of natural-language like questions. The result indicates that the sentence CNN possesses the ability of modeling natural questions. The sentence CNN uses the convolution process to compose and summarize the neighboring words. And the reliable ones with higher semantic meanings will be pooled and composed further to reach the final sentence representation. As such, the sentence CNN can compose the natural-language like questions to reliable high semantic representations.In this paper, we proposed one CNN model to address the image QA problem. The proposed CNN model relies on convolutional architectures to generate the image represen- tation, compose consecutive words to the question repre- sentation, and learn the interactions and relations between the image and question for the answer prediction. Experi- mental results on public image QA datasets demonstrate the superiority of our proposed model over the state-of-the-art methods.
