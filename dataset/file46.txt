The number of articles published every year keeps increasing ( Druss and Marcus, 2005;Larsen and Von Ins, 2010) and well over 50 million schol- arly articles have been published so far (Jinha, 2010). While this repository of human knowledge contains invaluable information, it has become in- creasingly difficult to take advantage of all avail- able information due to its sheer amount.One challenge is that the knowledge present in scholarly articles is mostly unstructured. One approach to organize this knowledge is to clas- sify each sentence (Kim et al., 2011;Amini et al., 2012;Hassanzadeh et al., 2014;. Another approach is to extract entities and relations between them, which is the focus of the ScienceIE shared task at SemEval2017(Augenstein et al., 2017.Relation extraction can be seen as a process comprising two steps that can be done jointly (Li and Ji, 2014) or separately: first, entities of inter- est need to be identified, then the relation among each possible set of entities has to be determined. * These authors contributed equally to this work. In this work, we concentrate on the second step (often referred to as relation extraction or classi- fication) and on binary relations, i.e. relations be- tween two entities. Extracted relations can be used for a variety of tasks such as question-answering systems (Ravichandran and Hovy, 2002), ontol- ogy extension ( Schutz and Buitelaar, 2005), and clinical trials ( Frunza and Inkpen, 2011).In this paper, we describe the system that we submitted for the ScienceIE shared task. Our sys- tem is based on convolutional neural networks and ranked first for relation extraction (subtask C).Existing systems for relation extraction can be classified into five categories (Zettlemoyer, 2013): systems based on hand-built patterns (Yangarber and Grishman, 1998), bootstrapping meth- ods (Brin, 1998), unsupervised methods (Gonzalez and Turmo, 2009), distant supervision ( Snow et al., 2004), and supervised methods. We focus on supervised methods, as the ScienceIE shared task provides a labeled training set.Supervised methods for relation extrac- tion commonly employ support vector ma- chines ( Uzuner et al., 2010Uzuner et al., , 2011Minard et al., 2011;GuoDong et al., 2005), na¨ıvena¨ıve Bayes ( Zayaraz and Kumara, 2015), maximum entropy (Sun and Grishman, 2012), or conditional random fields ( Sutton and McCallum, 2006). These meth- ods require the practitioner to handcraft features, such as surface features, lexical features, syntactic features ( Grouin et al., 2010) or features derived from existing ontologies ( Rink et al., 2011). The use of kernels based on dependency trees has also been explored ( Bunescu and Mooney, 2005;Culotta and Sorensen, 2004;Zhou et al., 2007).More recently, a few studies have investigated the use of artificial neural networks for relation ex- traction (Socher et al., 2012;Nguyen and Grishman, 2015;Hashimoto et al., 2013). Our approach follows this line of work.   Our model for relation extraction comprises three parts: preprocessing, convolutional neural net- work (CNN), and postprocessing.Second, the CNN takes each preprocessed sen- tence as input, and predicts the relation between the two entities. The CNN architecture, illustrated in Figure 1, consists of four main layers, similar to the one used in text classification (Collobert et al., 2011;Kim, 2014;Gehrmann et al., 2017).The preprocessing step takes as input each raw text (i.e., in ScienceIE, a paragraph of a scientific arti- cle) as well as the location of all entities present in the text, and output several examples. Each ex- ample is represented as a list of tokens, each with four features: the relative positions of the two en- tity mentions, and their entity types and part-of- speech (POS) tags. Figure 1 shows an example from the ScienceIE corpus in the table on the left. Sentence and token boundaries as well as POS tags are detected using the Stanford CoreNLP toolkit ( , and every pair of entity mentions of the same type within each sen- tence boundary are considered to be of a potential relation. We also remove any references (e.g. [1,2]) that are irrelevant to the task, and ensure that the sentences are not too long by eliminating the tokens before the beginning of the first entity men- tion and after the end of the second entity mention.1. the embedding layer converts each feature (word, relative positions 1 / 2, type of entity, and POS tag) into an embedding vector via a lookup table and concatenates them. 2. the convolutional layer with ReLU activation transforms the embeddings into feature maps by sliding filters over the tokens. 3. the max pooling layer takes the most effec- tive feature in each feature map by applying the max operator. 4. the fully connected layer with softmax activa- tion outputs the probability of each relation.Finally, the postprocessing step uses the rules in Table 1 to correct the relations detected by the CNN, or to detect additional relations. These rules were developed from the examples in the training set, to be consistent with common sense.  "rel": relation, "arg": argument. "Syn", "Hypo", "Hyper", and "None" refers to the "Synonym-of", "Hyponym-of", "Hypernym-of", and "None' relations. Note that the "Hypernym-of" relation is the reverse of the "Hyponym-of" relation, introduced in addition to the relations annotated for the dataset.During training, the objective is to maximize the log probability of the correct relation type. The model is trained using stochastic gradient descent with minibatch of size 16, updating all parameters, i.e., token embeddings, feature embeddings, CNN filter weights, and fully connected layer weights, at each gradient descent step. For regularization, dropout is applied before the fully connected layer, and early stop with a patience of 10 epochs is used based on the development set. The token embeddings are initialized using publicly available 1 pre-trained token embeddings, namely GloVe ( Pennington et al., 2014) trained on Wikipedia and Gigaword 5 (Parker et al., 2011). The feature embeddings and the other parameters of the neural network are initialized randomly.To deal with class imbalance, we upsampled the synonym and hyponym classes by duplicating the examples in the positive classes so that the upsam- pling ratio, i.e., the ratio of the number of positive examples in each class to that of the negative ex- amples, is at least 0.5. Without the upsampling, it was impossible to train the model. a sentence are annotated as either "Synonym-of", "Hyponym-of", or "None". Table 3 shows the number of examples for each relation class.Train Dev Test   We evaluate our model on the ScienceIE dataset ( Augenstein et al., 2017), which consists of 500 journal articles evenly distributed among the domains of computer science, material sci- ences and physics. Three types of entities are an- notated: process, task, and material. The relation between each pair of entity of the same type within  Table 5: Results for various ordering strategies on the development set of the ScienceIE dataset, averaged over 10 runs each. 3 "corr. w/ n. s.": correct order with negative sampling. Hyp+Syn is obtained by merging the output of the best hyponym classifier and that of the best synonym classifier.the sentence "A dog is an animal." If we set "dog" be the first argument and "animal" the second, then the corresponding relation is "Hyponym-of"; however, if we reverse the argument order, then the "Hyponym-of" relation does not hold any more. Therefore, it is crucial to ensure that 1) the CNN is provided with the information about the argu- ment order, and 2) it is able to utilize the given information efficiently. In our work, the former point is addressed by providing the CNN with the two relative position features compared to the first and the second argument of the relation respec- tively. In order to certify the latter point, we ex- perimented with four strategies for argument or- dering, outlined in Table 2. over, ordering the argument as the order of ap- pearance in the text and training once per relation (i.e., fixed order) is as efficient as training each re- lation as two examples in two possible argument ordering, one with the original relation class and the other with the reverse relation class (i.e., any order), despite the small size of the dataset. Table 5 shows the results from experimenting with various argument ordering strategies. The correct order strategy performed the worst, but the nega- tive sampling improved over it slightly, while the fixed order and any order strategies performed the best. The latter two strategies performed almost equally well in terms of micro-averaged F1-score. This implies that for relation extraction it may be advantageous to use both the original relation classes as well as their "reverse" relation classes for training, instead of using only the original rela- tion classes with the "correct" argument ordering (with or without the negative sampling). More- The difference in performance between the cor- rect order versus the fixed or any order strategies is more prominent for the "Hyponym-of" relation than for the "Synonym-of" relation. This is ex- pected, since the argument ordering strategy is different only for the order-sensitive "Hyponym- of" relation. It is somewhat surprising though, that the correct order strategy performs worse then the other strategies even for order-insensitive "Synonym-of" relation. This may be due to the fact that the model does not see any training exam- ples with the reversed argument ordering for the "Synonym-of" relation. In comparison, the nega- tive sampling strategy, which learns from both the original and reversed argument ordering for the "Synonym-of" relation, the performance is com- parable to the two best performing strategies.We have also experimented with different evalu- ation strategies for the models trained with the any order and fixed order strategies. When the model is trained with the any order strategy, the choice of the evaluation strategy does not impact the per- formance. In contrast, when the model is trained with the fixed order strategy, it performs better if the same strategy is used for evaluation. This may be the reason that the model trained with the cor- rect order strategy does not perform as well, since it has to be evaluated with a different strategy from training, namely the any order strategy, as we do not know the correct ordering of arguments for ex- amples in the test set. We have also tried training binary classifiers for the "Hyponym-of" and the "Synonym-of" rela- tions separately and then merging the outputs of the best classifiers for each relations. While the bi- nary classifiers individually performed better than the multi-way classifier for each corresponding re- lation class, the overall performance based on the micro-averaged F1-score did not improve over the multi-way classifier after merging the outputs of the hyponym and the synonym classifiers. Based on the results from the argument or- dering strategy experiments, we submitted the model trained using the fixed order strategy, which ranked number one in the challenge. The result is shown in Figure 3: Impact of bracket deletion and sen- tence cutting. "w+rp+et+pos" represents the CNN model trained using all features with both bracket deletion and sentence cutting during preprocess- ing. "-bracket deletion" is the same model trained only without bracket deletion, and "-sentence cut- ting" just without sentence cutting. To quantify the importance of various features of our model, we trained the model by gradually adding more features one by one, from word em- beddings, relative positions, and entity types to POS tags in order. The results on the importance of the features as well as postprocessing are shown in Figure 2. Adding the relative position features improved the performance the most, while adding the entity type improved it the least. Figure 3 quantifies the impact of two prepro- cessing steps, deleting brackets and cutting sen- tences, introduced to compensate for the small dataset size. Cutting the sentence before the first entity and after the second entity resulted in a dra- matic impact on the performance, while deleting brackets (i.e., removing the reference marks) im- prove the performance modestly. This implies that the text between the two entities contains most of the information about the relation between them.In this article we have presented an ANN-based approach to relation extraction, which ranked first in the SemEval-2017 task 10 (ScienceIE) for re- lation extraction in scientific articles (subtask C). We have experimented with various strategies to incorporate argument ordering for ordering- sensitive relations, showing that an efficient strat- egy is to fix the arguments ordering as appears on the text by introducing reverse relations. We have also demonstrated that cutting the sentence before the first entity and after the second entity is effec- tive for small datasets.
