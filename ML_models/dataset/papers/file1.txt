Although convolutional neural networks (CNNs) and re- current neural networks (RNNs) have been successfully applied to various image and natural language processing tasks (cf. [1,2,3,4]), these breakthroughs only slowly translate to multimodal tasks such as visual question an- swering (VQA) where the model needs to create a joint un- derstanding of the image and question. Such multimodal tasks require joint visual and textual representations.Since global features can hardly answer questions about certain local parts of the input, attention mechanisms have been extensively used in VQA recently [5,6,7,8,9,10,11,12]. It attempts to make the model predict based on spatial or lingual context. However, most attention mech- anisms used in VQA models are rather simple, consisting of two convolutional layers followed by a softmax to gen- erate the attention weights which are summed over the im- age features. These shallow attention mechanisms may fail to select the relevant information from the joint representa- tion of the question and image. Creating attention for com- plex questions, particularly sequential or relational reason- ing questions, requires processing information in a sequen- tial manner which recurrent layers are better suited due to their ability to capture relevant information over an input sequence.In this paper, we propose a RNN-based joint represen- tation to generate visual and textual attention. We argue that embedding a RNN in the joint representation helps the model process information in a sequential manner and de- termine what is relevant to solve the task. We refer to the combination of RNN embedding and attention as Recurrent Textual Attention Unit (RTAU) and Recurrent Visual Atten- tion Unit (RVAU) respective of their purpose. Furthermore, we employ these units in a fairly simple network, referred to as Dual Recurrent Attention Units (DRAU) network, and show improved results over several baselines. Finally, we enhance state-of-the-art models by replacing the model's default attention mechanism with RVAU.Our main contributions are the following:• We introduce a novel approach to generate soft atten- tion. To the best of our knowledge, this is the first at- tempt to generate attention maps using recurrent neural networks. We provide quantitative and qualitative re- sults showing performance improvements over the de-fault attention used in most VQA models.• Our attention modules are modular, thus, they can sub- stitute existing attention mechanisms in most models fairly easily. We show that state-of-the-art models with RVAU "plugged-in" perform consistently better than their vanilla counterparts.joint loss minimization to train the units. However during testing, they use the first answering unit which was trained from other units through backpropagation.This section discusses common methods that have been explored in the past for VQA.Bilinear representations Fukui et al. [7] use compact bi- linear pooling to attend over the image features and com- bine it with the language representation. The basic concept behind compact bilinear pooling is approximating the outer product by randomly projecting the embeddings to a higher dimensional space using Count Sketch projection [13] and then exploiting Fast Fourier Transforms to compute an effi- cient convolution. An ensemble model using MCB won first place in VQA (1.0) 2016 challenge. Kim et al. [5] argues that compact bilinear pooling is still expensive to compute and shows that it can be replaced by element-wise prod- uct (Hadamard product) and a linear mapping (i.e. fully- connected layer) which gives a lower dimensional repre- sentation and also improves the model accuracy. Recently, Ben-younes et al. [14] proposed using Tucker decomposi- tion [15] with a low-rank matrix constraint as a bilinear representation. They propose this fusion scheme in an ar- chitecture they refer to as MUTAN which as of this writing is the current state-of-the-art on the VQA 1.0 dataset.Notable mentions Kazemi and Elqursh [18] show that a simple model can get state-of-the-art results with proper training parameters. Wu et al. [19] construct a textual rep- resentation of the semantic content of an image and merges it with textual information sourced from a knowledge base. Ray et al. [20] introduce a task of identifying relevant ques- tions for VQA. Kim et al. [21] apply residual learning tech- niques to VQA and propose a novel attention image atten- tion visualization method using backpropagation.We propose our method in this section. Figure 1 illus- trates the flow of information in the DRAU model. Given an image and question, we create the input representations v and q. Next, these features are combined by 1 × 1 con- volutions into two separate branches. Then, the branches are passed to an RTAU and RVAU. Finally, the branches are combined using a fusion operation and fed to the final clas- sifier. The full architecture of the network is depicted in Figure 2.Attention-based Closely related to our work, Lu et al. [9] were the first to feature a co-attention mechanism that ap- plies attention to both the question and image. Nam et al.[6] use a Dual Attention Network (DAN) that employs at- tention on both text and visual features iteratively to predict the result. The goal behind this is to allow the image and question attentions to iteratively guide each other in a syn- ergistic manner.Image representation We use the 152-layer "ResNet" pretrained CNN from He et al. [1] to extract image features. Similar to [7,6], we resize the images to 448 × 448 and extract the last layer before the final pooling layer (res5c) with size 2048 × 14 × 14. Finally, we use l 2 normaliza- tion on all dimensions. Recently, Anderson et al. [22] have shown that object-level features can provide a significant performance uplift compared to global-level features from pretrained CNNs. Therefore, we experiment with replacing the ResNet features with FRCNN [23] features with a fixed number of proposals per image (K = 36).RNNs for VQA Using recurrent neural networks (RNNs) for VQA has been explored in the past. Xiong et al. [16] build upon the dynamic memory network from Kumar and Varaiya [17] and proposes DMN+. DMN+ uses episodic modules which contain attention-based Gated Recurrent Units (GRUs). Note that this is not the same as what we propose; Xiong et al. generate soft attention using convolu- tional layers and then uses it to substitute the update gate of the GRU. In contrast, our approach uses the recurrent layers to generate the attention. Noh and Han [8] propose recur- rent answering units in which each unit is a complete mod- ule that can answer a question about an image. They use Question representation We use a fairly similar repre- sentation as [7]. In short, the question is tokenized and en- coded using an embedding layer followed by a tanh acti- vation. We also exploit pretrained GloVe vectors [24] and concatenate them with the output of the embedding layer. The concatenated vector is fed to a two-layer unidirectional LSTM that contains 1024 hidden states each. In contrast to Fukui et al., we use all the hidden states of both LSTMs rather than concatenating the final states to represent the fi- nal question representation.We apply multiple 1 × 1 convolution layers in the net- work for mainly two reasons. First, they learn weights from the image and question representations in the early layers. This is important especially for the image representation, since it was originally trained for a different task. Second, they are used to generate a common representation size. To obtain a joint representation, we apply 1 × 1 convolu- tions followed by PReLU activations [1] on both the image and question representations. Through empirical evidence, PReLU activations were found to reduce training time sig- nificantly and improve performance compared to ReLU and tanh activations. We provide these results in Section 4.Next, we use the attention weights to compute a weighted average of the image and question features.where f n is the input representation and att a,n is the atten- tion applied on the input. Finally, the attention maps are fed into a fully-connected layer followed by a PReLU activa- tion. Figure 3 illustrates the structure of a RAU.The result from the above-mentioned layers is concate- nated and fed to two separate recurrent attention units (RAU). Each RAU starts with another 1 × 1 convolution and PReLU activation:where W out is a weight vector of the fully connected layer and y att,n is the output of each RAU.where W a is the 1 × 1 convolution weights, x is the input to the RAU, and c a is the output of the first PReLU. Furthermore, we feed the previous output into an unidi- rectional LSTM:A fusion operation is used to merge the textual and visual branches. For DRAU, we experiment with using element- wise multiplication (Hadamard product) and MCB [7,25]. The result of the fusion is given to a many-class classifier using the top 3000 frequent answers. We use a single-layer softmax with cross-entropy loss. This can be written as:where h a,n is the hidden state at time n.To generate the attention weights, we feed all the hidden states of the previous LSTM to a 1 × 1 convolution layer followed by a softmax function. The 1×1 convolution layer could be interpreted as the number of glimpses the model sees.where y text and y vis are the outputs of the RAUs, W ans represents the weights of the multi-way classifier, and P a is the probability of the top 3000 frequent answers.The final answerâanswerˆanswerâ is chosen according to the following:where W g is the glimpses' weights and W att,n is the atten- tion weight vector.Experiments are performed on the VQA 1.0 and 2.0 datasets [26,27] We evaluate our results on the validation, test-dev, test- std splits of each dataset. Models evaluated on the vali- dation set use train and Visual Genome for training. For the other splits, we include the validation set in the training data. However, the models using FRCNN features do not use data augmentation with Visual Genome.To train our model, we use Adam [29] for optimization with β 1 = 0.9, β 2 = 0.999, and an initial learning rate of = 7 × 10 −4 . The final model is trained with a small batch size of 32 for 400K iterations. We did not fully ex- plore tuning the batch size which explains the relatively high number of training iterations. Dropout (p = 0.3) is applied after each LSTM and after the fusion operation. All weights are initialized as described in [30] except LSTM layers which use an uniform weight distribution. The pre- trained ResNet was fixed during training due to the massive computational overhead of fine-tuning the network for the VQA task. While VQA datasets provide 10 answers per image-question pair, we sample one answer randomly for each training iteration.model used the same question representation described in [7] and passed the output to a softmax 3000-way classifica- tion layer. The goal of this architecture was to assess the extent of the language bias present in VQA.The second baseline is a simple joint representation of the image features and the language representation. The representations were combined using the compact bilinear pooling from [25]. We chose this method specifically be- cause it was shown to be effective by Fukui et al. [7]. The main objective of this model is to measure how a robust pooling method of multimodal features would perform on its own without a deep architecture or attention. We refer to this model as Simple MCB.For the last baseline, we substituted the compact bilin- ear pooling from Simple MCB with an LSTM consisting of hidden states equal to the image size. A 1 × 1 convolu- tional layer followed by a tanh activation were used on the image features prior to the LSTM, while the question repre- sentation was replicated to have a common embedding size for both representations This model is referred to as Joint LSTM.We begin by testing our baseline models on the VQA 1.0 validation set. As shown in Table 1, the language-only baseline model managed to get 48.3% overall. More im- pressively, it scored 78.56% on Yes/No questions. The Sim- ple MCB model further improves the overall performance, although little improvement is gained in the binary Yes/No tasks. Replacing MCB with our basic Joint LSTM embed- ding improves performance across the board.During early experiments, the VQA 2.0 dataset was not yet released. Thus, the baselines and early models were evaluated on the VQA 1.0 dataset. While building the fi- nal model, several parameters were changed, mainly, the learning rate, activation functions, dropout value, and other modifications which we discuss in this section.Baselines We started by designing three baseline archi- tectures. The first baseline produced predictions solely from the question while totally ignoring the image. The Modifications to the Joint LSTM Model We test sev- eral variations of the Joint LSTM baseline which are high- lighted in Table 2. Using PReLU activations has helped in two ways. First, it reduced time for convergence from 240K iterations to 120K. Second, the overall accuracy has improved, especially in the Other category. The next modi- fications were inspired by the results from [18]. We experi- mented with appending positional features which can be de- scribed as the coordinates of each pixel to the depth/feature dimension of the image representation. When unnormalized with respect to the other features, it worsened results sig- nificantly, dropping the overall accuracy by over 2 points.  After the release of VQA 2.0, we shifted our empirical evaluation towards the newer dataset. First, we retrain and retest our best performing VQA 1.0 model Joint LSTM as well as several improvements and modifications.Since VQA 2.0 was built to reduce the language prior and bias inherent in VQA, the accuracy of Joint LSTM drops significantly as shown in Table 3. Note that all the mod- els that were trained so far do not have explicit visual or textual attention implemented. Our first network with ex- plicit visual attention, RVAU, shows an accuracy jump by almost 3 points compared to the Joint LSTM model. This result highlights the importance of attention for good per- formance in VQA. Training the RVAU network as a multi- label task (RVAU multilabel ), i.e. using all available annotations at each training iteration, drops the accuracy horribly. This is the biggest drop in performance so far. This might be caused by the variety of annotations in VQA for each ques- tion which makes the task for optimizing all answers at once much harder.The addition of RTAU marks the cre- ation of our DRAU network. The DRAU model shows fa- vorable improvements over the RVAU model. Adding tex- tual attention improves overall accuracy by 0.56 points. Substituting the PReLU activations with ReLU (DRAU ReLU ) massively drops performance. While further training might have helped the model improve, PReLU offers much faster convergence. Increasing the value of the dropout layer after the fusion operation (DRAU high final dropout ) improves perfor- mance by 0.13 points, in contrast to the results of the Joint LSTM model on VQA 1.0. Note that on the VQA 1.0 tests, we changed the values of all layers that we apply dropout on, but here we only change the last one after the fusion operation. Totally removing this dropout layer worsens ac- curacy. This suggests that the optimal dropout value should be tuned per-layer.We test a few variations of DRAU on the test-dev set. We can observe that VQA benefits from more training data; the same DRAU network performs better (62.24% vs. 59.58%) thanks to the additional data. Most of the literature resize the original ResNet features from 224 × 224 to 448 × 448. To test the effect of this scaling, we train a DRAU variant with the original ResNet size (DRAU small ). Reducing the image feature size from 2048 × 14 × 14 to 2048 × 7 × 7 adversely affects accuracy as shown in Table 4. Adding more glimpses significantly reduces the model's accuracy (DRAU glimpses = 4 ). A cause of this performance drop could be related to the fact that LSTMs process the input in a one- dimensional fashion and thus decide that each input is either relevant or non-relevant. This might explain why the at- tention maps of DRAU separate the objects from the back- ground in two glimpses as we will mention in Section 5. 2D Grid LSTMs [31]   Table 6 shows a comparison between DRAU and other state-of-the-art models. Excluding model ensem- bles, DRAU performs favorably against other models. To the best of our knowledge, [5] has the best single model performance of 65.07% on the test-std split which is very close our best model (65.03%). Small modifications or hy- perparameter tuning could push our model further. Finally, the FRCNN image features boosts the model's performance close to the state-of-the-art ensemble model. VQA 2.0 Our model DRAU MCB fusion landed the 8th place in the VQA 2.0 Test-standard task. 4 . Currently, all reported submissions that outperform our single model use model ensembles. Using FRCNN features boosted the model's performance to outperform some of the ensemble models (66.85%). The first place submission [22] reports using an ensemble of 30 models. In their report, the best single model that uses FRCNN features achieves 65.67% on the test-standard split which is outperformed by our best single model DRAU FRCNN features . Table 5. Results of state-of-the-art models with RVAU.gested by [22]. This change provides a significant perfor- mance increase of 3.04 points (DRAU FRCNN features ).To verify the effectiveness of the recurrent attention units, we replace the attention layers in MCB and MUTAN [14] with RVAU.For MCB [7] we remove all the layers after the first MCB operation until the first 2048-d output and replace them with RVAU. Due to GPU memory constraints, we reduced the size of each hidden unit in RVAU's LSTM from 2048 to 1024. In the same setting, RVAU significantly helps im- prove the original MCB model's accuracy as shown in Ta- ble 5. The most noticeable performance boost can be seen in the number category, which supports our hypothesis that recurrent layers are more suited for sequential reasoning.Furthermore, we test RVAU in the MUTAN model [14]. The authors use a multimodal vector with dimension size of 510 for the joint representations. For coherence, we change the usual dimension size in RVAU to 510. At the time of this writing, the authors have not released results on VQA 2.0 using a single model rather than a model ensemble. There- fore, we train a single-model MUTAN using the authors' implementation. 2 The story does not change here, RVAU improves the model's overall accuracy.In this section, we provide qualitative results that high- light the effect of the recurrent layers compared to the MCB model.The strength of RAUs is notable in tasks that require se- quentially processing the image or relational/multi-step rea- soning. In the same setting, DRAU outperforms MCB in counting questions. This is validated in a subset of the val- idation split questions in the VQA 2.0 dataset as shown in Figure 4. Figure 5 shows some qualitative results between DRAU and MCB. For fair comparison we compare the first attention map of MCB with the second attention map of our model. We do so because the authors of MCB [7] visualize the first map in their work 5 . Furthermore, the first glimpse of our model seems to be the complement of the second attention, i.e. the model separates the background and the target object(s) into separate attention maps. We have not tested the visual effect of more than two glimpses on our model. In Figure 5, it is clear that the recurrence helps the model attend to multiple targets as apparent in the difference of the attention maps between the two models. DRAU seems to also know how to count the right object(s). The top right ex- ample in Figure 5 illustrates that DRAU is not easily fooled by counting whatever object is present in the image but rather the object that is needed to answer the question. This    property also translates to questions that require relational reasoning. The second column in Figure 5 demonstrates how DRAU can attend the location required to answer the question based on the textual and visual attention maps.We proposed an architecture for VQA with a novel at- tention unit, termed the Recurrent Attention Unit (RAU). The recurrent layers help guide the textual and visual atten- tion since the network can reason relations between several parts of the image and question. We provided quantitative and qualitative results indicating the usefulness of a recur- rent attention mechanism. Our DRAU model showed im- proved performance in tasks requiring sequential/complex reasoning such as counting or relational reasoning over [7], the winners of the VQA 2016 challenge. In VQA 1.0, we achieved near state-of-the-art results for single model per- formance with our DRAU network (65.03 vs. 65.07 [5]  of the state-of-the-art 5-model ensemble MUTAN [14]. Fi- nally, we demonstrated that substituting the visual attention mechanism in other networks, MCB [7] and MUTAN [14], consistently improves their performance. In future work we will investigate implicit recurrent attention mechanism us- ing recently proposed explanation methods [37,38]. 
