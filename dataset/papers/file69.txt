Recently the deep neural network (DNN)-based acoustic models (AMs) greatly improved automatic speech recognition (ASR) accu- racy on many tasks [1,2,3,4]. Further improvements were reported by using more advanced models such as convolutional neural net- works (CNNs) [5] and long short-term memory (LSTM) recurrent neural networks (RNNs) [6,7,8].Although these new techniques help to decrease the word error rate (WER) on distant speech recognition (DSR) [9], DSR remains a challenging task due to the reverberation and overlapping acoustic signals, even with sophisticated front-end processing techniques [10,11,12] and multi-pass decoding schemes.In this paper, we explore more advanced back-end techniques for DSR. It is reported [8] that deep LSTM (DLSTM) RNNs help improve generalization and often outperform single-layer LSTM RNNs. However, DLSTM RNNs are harder to train and slower to converge. In this paper, we extend DLSTM RNNs by introducing a gated direct connection between memory cells of adjacent layers. These direct links, called highway connections, provide a path for information to flow between layers more directly without decay. It alleviates the gradient vanishing problem and enables DLSTM RNNs training with virtually arbitrary depth. Here, we refer to an LSTM RNN with highway connections as HLSTM RNN.To further improve the performance, we also introduce the latency-controlled bidirectional LSTM (LC-BLSTM) RNNs. In the LC-BLSTM RNNs, the past history is fully exploited similar to that in the unidirectional LSTM RNNs. However, unlike the standard BLSTM RNNs which can start model evaluation only af- ter seeing the whole utterance, the LC-BLSTM RNNs only look ahead for a fixed number of frames which limits the latency. The LC-BLSTM can be much more efficiently trained than the standard BLSTM without performance loss. It also trains and decodes faster than context-sensitive-chunk BLSTMs [13] which can only access limited past and future context.Our study is conducted on the AMI single distant microphone (SDM) setup. We compare the standard (B)LSTM RNNs and high- way (B)LSTM RNNs with 3 and 8 layers. We show that the highway (B)LSTM RNNs with dropout applied to the highway connection significantly outperform the standard (B)LSTM RNNs. The high way connection helps to train deeper networks better and the high- way (B)LSTM RNNs seem to benefit more from sequence discrim- inative training. Overall, our proposed model decreased WER by 15.7% over DNNs, 14.4% over CNNs [14], and 5.3% over DLSTM RNNs relatively. To our best knowledge, the 43.9/47.7% WER we achieved on the AMI (SDM) dev and eval sets is the best results reported on this task.1 . The rest of the paper is organized as follows. In Section 2 we briefly discuss related work. In Section 3 we introduce standard DL- STM RNNs, BLSTM RNNs, and the highway (B)LSTM RNNs. In Section 4 we describe LC-BLSTM and the way to train such mod- els efficiently with both frame and sequence discriminative criteria. We summarize the experimental setup in Section 5 and report exper- imental results in Section 6. Conclusions are reiterated in Section 7.2. RELATED WORK * Part of the work reported here was carried out during the 2015 Jelinek Memorial Summer Workshop on Speech and Language Technologies at the University of Washington, Seattle, and was supported by Johns Hopkins Uni- versity via NSF Grant No IIS 1005411, and gifts from Google, Microsoft Research, Amazon, Mitsubishi Electric, and MERL.After developing the highway LSTMs independently we noticed that similar work has been done in [16,17,18]. All of these works share the same idea of adding gated linear connections between different layers. The highway networks proposed in [16] adaptively carry some dimensions of the input directly to the output so that infor- mation can flow across layers much more easily. However, their formulation is different from ours and their focus is DNN. The work in [17] share the same idea and model structure. [18] is more general and uses a generic form. However, their task is on text e.g. machine translation while our focus is distant speech recognition. In addi- tion, we used dropout as the way to control the highway connections which turns out to be critical for DSR. The LSTM RNN was initially proposed in [19] to solve the gradient diminishing problem in RNNs. It introduces a linear dependence between ct, the memory cell state at time t, and ct−1, the same cell's state at t − 1. is the weight matrix connecting the carry gate to the input of this layer. w (L+1) cd is a weight vector from the carry gate to the past cell state in the current layer. w (L+1) ld iteratively from t = 1 to T , where σ( ˙ ) is the logistic sigmoid func- tion, it, ft, ot, ct and mt are vectors to represent values at time t of the input gate, forget gate, output gate, cell activation, and cell output activation respectively. denotes element-wise product of vectors. W * are the weight matrices connecting different gates, and b * are the corresponding bias vectors. All the matrices are full except the matrices Wci, W cf , Wco from the cell to gate vector which is di- agonal.is a weight vector connecting the carry gate to the lower layer mem- ory cell. d (l+1) is the carry gate activation vectors at layer l + 1. Using the carry gate, an HLSTM RNN computes the cell state at layer (l + 1) according toDeep LSTM RNNs are formed by stacking multiple layers of LSTM cells. Specifically, the output of the lower layer LSTM cells y l t is fed to the upper layer as input x l+1 t. Although each LSTM layer is deep in time since it can be unrolled in time to become a feed- forward neural network in which each layer shares the same weights, deep LSTM RNNs still outperform single-layer LSTM RNNs signif- icantly. It is conjectured [8] that DLSTM RNNs can make better use of parameters by distributing them over the space through multiple layers. Note that in the conventional DLSTM RNNs the interaction between cells in different layers must go through the output-input connection.while all other equations are the same as that in the standard LSTM RNNs as described in Eq. (1),(2),(4), and (5).Thus, depending on the output of the carry gates, the highway connection can smoothly vary its behavior between that of a plain LSTM layer or simply passes its cell memory from previous layer. The highway connection between cells in different layers makes in- fluence from cells in one layer to the other more direct and can al- leviate the gradient vanishing problem when training deeper LSTM RNNs.The unidirectional LSTM RNNs we described above can only ex- ploit past history. In speech recognition, however, future contexts also carry information and should be utilized to further enhance acoustic models. The bidirectional RNNs take advantage of both past and future contexts by processing the data in both directions with two separate hidden layers. It is shown in [6,7,13] that bidi- rectional LSTN RNNs can indeed improve the speech recognition results. In this study, we also extend the HLSTM RNNs from unidi- rection to bidirection. Note that the backward layer follows the same equations used in the forward layer except that t − 1 is replaced by t + 1 to exploit future frames and the model operates from t = T to 1. The output of the forward and backward layers are concatenated to form the input to the next layer.Nowadays, GPUs are widely used in deep learning by leverag- ing massive parallel computations via mini-batch based training. For unidirectional RNN models, to better utilize the parallelization power of the GPU card, in [15], multiple sequences (e.g., 40) are often packed into the same mini-batch. Truncated BPTT is usually performed for parameters updating, therefore, only a small segment (e.g., 20 frames) of each sequence has to be packed into the mini- batch. However, when applied to sequence level training (BLSTM or sequence training), GPU's limited memory restricts the number of sequences that can be packed into a mini-batch, especially for LVCSR tasks with long training sequences and large model sizes. One alternative way to speed up is using asynchronous SGD based on a GPU/CPU farm [20]. In this section, we are more focused on fully utilizing the parallelization power of a single GPU Card. The algorithms proposed here can also be applied to a multi-GPU setup.To increase the number of sequences that can be fit into a mini-batch, we propose a two-forward-pass solution for sequence discriminative training on top of recurrent neural networks. The basic idea is pretty straightforward. For sequence training of recurrent models, we use the same mini-batch packaging method as that in the cross-entropy training case, i.e., we pack multiple sequences (e.g., 40) into the same mini-batch, each with a small chunk (e.g., 20 frames). Then in the first forward pass, we collect log-likelihood of frames in the mini-batch, and put those into a pool, without updating the model. We do this until we have collected the log-likelihood for a certain number of sequences. At this point, we are able to compute the error signal for each of the sequence in the pool. We then roll back the mini-batches that we have just computed error signals for, and start the second forward pass, this time we update the model using the error signals from the pool. With this two-forward-pass solution, we are able to pack far more sequences in the same mini-batch, e.g., 40∼60, thus leading to much faster training.To speed up the training of bi-direcctional RNNs, the Context- sensitive-chunk BPTT (CSC-BPTT) is proposed in [13]. In this method, a sequence is firstly split into chunks of fixed length Nc. Then N l past frames and Nr future frames are concatenated before and after each chunk as the left and right context, respectively. The appended frames are only used to provide context information and do not generate error signals during training. Since each trunk can be independently drawn and trained, they can be stacked to form large minibatches to speed up training. Unfortunately, the model trained with CSC-BPTT is no longer the true bidirectional RNN since the history it can exploit is limited by the left and right context concatenated to the chunk. It also in- troduces additional computation cost during decoding since both the left and right contexts need to be recomputed for each chunk.To solve the problems in the CSC-BPTT we propose the latency- controlled bi-directional RNNs. Different from the CSC-BPTT, in our new model we carry the whole past history while still using a truncated future context. Instead of concatenating and computing N l left contextual frames for each chunk we directly carry over the left contextual information from the previous chunk of the same ut- terance. For every chunk, both training and decoding computational cost is reduced by a factor of We evaluated our models on the AMI meeting corpus [21]. The AMI corpus comprises around 100 hours of meeting recordings, recorded in instrumented meeting rooms. Multiple microphones were used, including individual headset microphones (IHM), lapel microphones, and one or more microphone arrays. In this work, we use the single distant microphone (SDM) condition for our experiments. Our systems are trained and tested using the split recommended in the corpus release: a training set of 80 hours, a development set and a test set each of 9 hours. For our training, we use all the segments provided by the corpus, including those with overlapped speech. Our models are evaluated on the evaluation set only. NIST's asclite tool [22] is used for scoring.. Moreover, loading the history from previous mini-batch instead of a fixed contextual win- dows makes the context exact when compared to the uni-directional model. Note that the standard BLSTM RNNs come with significant latency since the model can only be evaluated after seeing the whole utterance. In the latency-controlled BLSTM RNNs the latency is limited to Nr which can be set by the users. In our experiments, we process 40 utterances in parallel which is 10 times faster than pro- cessing the whole utterances without performance loss. Compared to the CSC BPTT our approach is 1.5 times faster and often leads to better accuracy.Kaldi [23] is used for feature extraction, early stage triphone training as well as decoding. A maximum likelihood acoustic training recipe is used to trains a GMM-HMM triphone system. Forced alignment is performed on the training data by this triphone system to generate labels for further neural network training.The Computational Network Toolkit (CNTK) [15] is used for neural network training. We start off by training a 6-layer DNN, with 2, 048 sigmoid units per layer. 40-dimensional filterbank fea- tures, together with their corresponding delta and delta-delta features are used as raw feature vectors. For our DNN training we concate- nated 15 frames of raw feature vectors, which leads to a dimension of 1, 800. This DNN again is used to force align the training data to generate labels for further LSTM training.Our (H)LSTM models, unless explicitly stated otherwise, are added with a projection layer on top of each layer's output, as pro- posed in [8], and are trained with 80-dimensional log Mel filterbank (FBANK) features. For LSTMP models, each hidden layer consists of 1024 memory cells together with a 512-node projection layer. For the BLSTMP models, each hidden layer consists of 1024 memory cells (512 for forward and 512 for backward) with a 300-node pro- jection layer. Their highway companions share the same network structure, except the additional highway connections.All models are randomly initialized without either generative or discriminative pretraining [24]. A validation set is used to control the learning rate which will be halved when no gain is observed. To train the unidirectional model, the truncated back-propagation- through-time (BPTT) [25] is used to update the model parameters. Each BPTT segment contains 20 frames and process 40 utterances simultaneously. To train the latency-controlled bidirectional model, we set Nc = 22 and Nr = 21 and also process 40 utterances si- multaneously. A start learning rate of 0.2 per minibatch is used and then the learning rate scheduler takes action. For frame level cross-entropy training, L2 constraint regularization [26] is used. For sequence training, L2 constraint regularization is also applied when- ever it is used in the corresponding cross-entropy trained model. We use a fixed per sample learning rate of 1e − 5 for DNN sequence training, and 2e − 6 for LSTM sequence training.  Table 3. Comparison of shallow and deep networksThe performance of various models are evaluated using word error rate (WER) in percent below. All the experiments are conducted on AMI SDM1 eval set, if not specified otherwise. Since we do not exclude the overlapping speech segments during model training, in addition to results on the full eval set, we also show results on a subset that only contains the non-overlapping speech segments as [14].When a network goes deeper, the training usually becomes dif- ficult. Table 3 compares the performance of shallow and deep net- works. From the table we can see that for a normal LSTMP network, when it goes from 3 layers to 8 layers, the recognition performance degrades dramatically. For the highway network, however, the WER only increase a little bit. The table suggests that the highway con- nection between LSTM layers allows the network to go much deeper than the normal LSTM networks.  Table 1 gives WER performance of the 3-layer LSTMP and BLSTMP RNNs, as well as their highway versions. The perfor- mance of the DNN network is also listed for comparison. From the table, it's clear that the highway version of the LSTM RNNs con- sistently outperform their non-highway companions, though with a small margin. We perform sequence discriminative training for the networks discussed in Section 4.2. Detailed results are shown in Table 4. The table suggests that introducing the highway connection between LSTMP layers is beneficial to sequence discriminative training. For example, without the highway connection, sequence training on top of the 3-layer LSTMP network brings WER from 50.7 down to 49.3, a relative improvement of only 3% on this particular task. After in- troducing the highway connection and dropout, the improvement is from 50.4 to 47.7, 5% relatively. The relative improvement is even larger on the non-overlapping segment subset, which is roughly 7%. The results suggest that sequence training is beneficial from both highway connection and a deeper structure. Table 2. Performance of highway (B)LSTMP RNNs with dropout Dropout is applied to the highway connection to control its flow: a high dropout rate essentially turns off the highway connection, and a small dropout rate, on the other hand, keeps the connection alive. In our experiments, for early training stages, we use a small dropout rate of 0.1, and increase it to 0.8 after 5 epochs of training. Per- formance of highway (B)LSTMP networks with dropout is shown in Table 2, as we can see, dropout helps to further bring down the WER for highway networks.We presented a novel highway LSTM network, and applied it to a far-field speech recognition task. Experimental results suggest that this type of network consistently outperforms the normal (B)LSTMP networks, especially when dropout is applied to the highway con- nection to control the connection's on/off state. Further experiments also suggest that the highway connection allows the network to go much deeper, and to get larger benefit from sequence discriminative training.
