Recurrent Neural Networks (RNNs) are powerful models which are naturally suited to processing sequential data. They can maintain a hidden state which encodes informa- tion about previous elements in the sequence. For a classi- cal version of RNN (Elman, 1990), at every timestep, the hidden state is updated as a function of both the input and the current hidden state. In theory, this recursive procedure allows these models to store complex signals for arbitrarily long timescales.However, in practice RNNs are considered difficult to train due to the so-called vanishing and exploding gradient prob- lems ( Bengio et al., 1994). These problems arise when the spectral norm of the transition matrix is significantly differ- ent than 1, or due to the non-linear transition functions. If the spectral norm of the transition matrix is greater than 1, the gradients will grow exponentially in magnitude during backpropagation, which is known as the exploding gradi- ent problem. If the spectral norm is less than 1, the gradi- ents will vanish exponentially quickly, which is known as the vanishing gradient problem. Recently, a simple strat- egy of clipping gradients has been introduced, and has proved effective in addressing the exploding gradient prob- lem (Mikolov, 2012). The problem of vanishing gradients has shown itself to be more difficult, and various strategies have been proposed over the years to address it. One very successful approach, known as Long Short-Term Memory (LSTM) units (Hochreiter &amp; Schmidhuber, 1997), has been to modify the architecture of the hidden units by introduc- ing gates which explicitly control the flow of information as a function of both the state and the input. Specifically, the signal stored in a hidden unit must be explicitly erased by a forget gate and is otherwise stored indefinitely. This allows information to be carried over long periods of time. LSTMs have become very successful in applications to lan- guage modeling, machine translation, and speech recogni- tion ( Zaremba et al., 2014;Sutskever et al., 2014;Graves et al., 2013). Other methods have been proposed to deal with learning long-term dependencies, such as adding a separate contextual memory ( Mikolov et al., 2015), sta- bilizing activations (Krueger &amp; Mimesevic, 2015) or us- ing more sophisticated optimization schemes (Martens &amp; Sutskever, 2011). Two recent methods propose to directly address the vanishing gradient problem by either initializ- ing or parameterizing the transition matrix with orthogonal or unitary matrices ( Arjovsky et al., 2015;Le et al., 2015 These works have used a set of synthetic problems (orig- inally outlined in (Hochreiter &amp; Schmidhuber, 1997) or variants thereof) for testing the ability of methods to learn long-term dependencies. These synthetic problems are designed to be pathologically difficult, and require mod- els to store information over very long timescales (hun- dreds of timesteps). Different approaches have solved these problems to varying degrees of success. In (Martens &amp; Sutskever, 2011), the authors report that their Hessian- Free optimization based method solves the addition task for T = 200 timesteps. The authors of (Krueger &amp; Mimesevic, 2015) reported that their method beat the chance base- line for the adding task in 8/9 cases for T = 400. In ( Le et al., 2015), the IRNN is reported to solve the addition task for T = 300. The method proposed in ( Arjovsky et al., 2015) is able to solve the copy task for up to T = 500 timesteps, and is able to completely solve the addition task for up to T = 400 timesteps, and partially solves it for T = 750.An sRNN (Elman, 1990) where M is the out- put dimension), a d × N encoder matrix U (where N is the input dimension), and a bias b. If either the output or input is categorical, M (respectively N ) is the number of classes, and we use a one-hot representation. As the sRNN ingests a sequence, it keeps running updates to a hidden state h, and using the hidden state and the decoder matrix, produces outputs y:In this work we analyze these "long-memory" tasks, and construct explicit RNN solutions for them. The solutions illuminate both the tasks, and provide a theoretical justifica- tion for the success of recent approaches using orthogonal initializations of, or unitary constraints on, the transition matrix of the RNN. In particular, we show that a classical Elman RNN with no transition non-linearity and random orthogonal initialization is with high probability close to our explicit RNN solution to the sequence memorization task, and the same network architecture with identity ini- tialization is close to the explicit solution to the addition task. We verify experimentally that initializing correctly (i.e. random orthogonal or identity) is critical for success on these tasks. Finally, we show how l 2 pooling can be used to allow a model to "choose" between a random-orthogonal or identity-like memory. where x t , y t , h t are the input, output and hidden state re- spectively at time t. While there have been great improve- ments in the training of sRNNs since their introduction, and while they have been shown to be powerful models in tasks such as language modeling (Mikolov, 2012), it can still be difficult to train generic sRNNs to use information about inputs from hundreds of timesteps previous for comput- ing the current output ( Bengio et al., 1994;Pascanu et al., 2013).There are several other works which have studied the prop- erties of orthogonal matrices in relation to neural networks. The work of ( Saxe et al., 2013) gives exact solutions to the learning dynamics of deep linear networks, and based on this analysis, suggests an orthogonal initialization scheme to accelerate learning. The authors of (White et al., 2004) and ( Ganguli et al., 2008) study the ability of linear RNNs (with orthogonal and generic transition matrices, respec- tively) to store scalar sequences in their hidden state, and show that the memory capacity scales with the number of hidden units. Our work complements theirs by providing a related analysis for discrete input sequences.In the following, we will use a simplification of the sRNNs that makes them in some sense less powerful models, but makes it easier to train them to solve simple long-memory tasks. Namely, by placing the non-linearity between the input and hidden state, rather than between the hidden state and output, we obtain RNNs with linear transitions (or, in the case of categorical inputs, not using a non-linearity at all). We call these LT-RNNs. The update equations are then:Finally, note that by appropriately scaling the weights and biases, an sRNN can be made to approximate a LT-RNN, but not the other way around (and of course the optimiza- tion may never find this scaling).We review some recurrent neural network (RNN) archi- tectures for processing sequential data, and discuss the modifications we use for the long memory problems. We fix the following notation: input sequences are denoted x 0 , x 1 , ..., x t , ..., and output sequences are denoted by y 0 , y 1 , ..., y t , ... .The LSTM of (Hochreiter &amp; Schmidhuber, 1997) is an ar- chitecture designed to improve upon the sRNN with the introduction of simple memory cells with a gating archi- tecture. In this work we use the architecture originally proposed in (Hochreiter &amp; Schmidhuber, 1997). For each memory cell, the network computes the output of four gates: an update gate, input gate, forget gate and output gate. The outputs of these gates are:This task tests the network's ability to recall information seen many time steps previously. We follow the same setup as ( Arjovsky et al., 2015), which we briefly outline here.The cell state is then updated as a function of the input and the previous state:i=1 be a set of K symbols, and pick numbers S and T . The input consists of a T + 2S length vector of categories, starting with S entries sampled uniformly fromFinally, the hidden state is computed as a function of the cell state and the output gate:A relatively common variation of the original LSTM involves adding so-called "peephole connections" (Gers et al., 2003) which allows information to flow from the cell state to the various gates. This variant was originally de- signed to measure or generate precise time intervals, and has proven successful for speech recognition and sequence generation (Graves, 2013;Graves et al., 2013).i=1 which are the sequence to be remembered. The next T − 1 inputs are set to a K+1 , which is a blank cate- gory. The following (single) input is a K+2 , which repre- sents a delimiter indicating that the network should output the initial S entries of the input. The last S inputs are set to a K+1 . The required output sequence consists of T + S entries of a K+1 , followed by the first S entries of the input sequence in exactly the same order. The task is to mini- mize the average cross-entropy of the predictions at each time step, which amounts to remembering a categorical se- quence of length S for T time steps.We can write out an LT-RNN solution for this problem. We will write out descriptions for U, V, W from equation (2). Note that since the inputs are categorical, we assume that no non-linearity is used. Fix a number d. For each j in {1, ..., d}, pick a random integer l j drawn uniformly from {1, ..., T + S}, and let Below we will consider LT-RNNs initialized with either random orthogonal transition matrices, or identity transi- tions, and we will see that there is a large difference in behavior between these initializations. However, we can set up an architecture where a random orthogonal initial- ization behaves much closer to an identity initialization by using an l 2 pooling layer at the output. If we feed both the pooled and unpooled hidden layer to the decoder, the model can choose whether it wants an identity-like or random- orthogonal like representation. We fix a pool size k, and then the update equations for this model are:Now define Q, and then V from (2) bywhere if h is theSo V is a (2d + 1) × (2d + 1) block diagonal matrix. Note that iterating Q "spins" each of the Q i at different rates, but they all synchronize at multiples of S +T . Thus Q acts as a "clock" with period S+T . Now set˜Uset˜ set˜U to be a 2d×K matrix with columns sampled uniformly from the unit sphere, and form U by appending two zero columns tõ U and then one extra row, with −1/S for each entry between 1 and K, −1 for the K + 1 entry, and T + S + 1 for the K + 2 entry. Schematically,3. TasksIn this section we describe tasks from (Hochreiter &amp; Schmidhuber, 1997;Arjovsky et al., 2015;Le et al., 2015) which involve dependencies over very long timescales which are designed to be pathologically hard for the sRNN.Finally, set W = U T , except scale the K + 1 column by S + 1, zero out the K + 2 column, and also zero out the entries below˜Ubelow˜ below˜U .This givesNow we will show how the RNN operates, starting with a high-level overview. The last dimension of the hidden state divides the state space into two regions, one where the model outputs the blank symbol and the other where it outputs one of the first K symbols in the dictionary. The model begins in the first region and remains there until it encounters the delimiter symbol, which sends it into the second. Now, the symbols in the input sequence are all encoded in the hidden state and a rotation is applied with each timestep. A key result is that rotation by powers of Q "hides" a symbol encoded in the hidden state, i.e. decor- relates its current representation from its original one. Due to the periodicity of Q, after T + S timesteps, the different symbols in the input sequence will surface from the hidden state one at a time, in the order in which they were seen, while the other symbols in the sequence, whose represen- tations have rotations applied to them, remain hidden. This causes the output units of each of the symbols to fire in the correct order. • The sum continues to cycle, giving i j as output for each following j up to S.We now briefly argue that uWe now give a more precise description, for which we need a little more notation. Denote by˜hby˜ by˜h the first 2d coordinates of h, and by h 2d+1 the last coordinate, and denote by u j the jth column of˜Uof˜ of˜U . Then the RNN works as follows, initialized with hidden state 0:is small when d is large enough w.r.t. S and K. We will repeatedly use that the variance of a sum of independent, mean-zero ran- dom variables grows as the sum of the variances. Denote by u ji the pair of coordinates of the jth column u j of˜Uof˜ of˜U corresponding to the i th block; since u are uniform on the sphere, we expect • After the first S inputs, we have i u ji has mean zero, and since• For the next T inputs, only h 2d+1 changes, increment- ing by −1 at each step. Note that W K+1 has so far been the best match to h because it has large negative last component.Moreover, since the Q p u j are uniform on the sphere,• At time T + S when the a K+2 token is seen, h 2d+1 is set positive, which ensures the blank symbol will not be output.for j = j. Similarly, we expectThus we can fix a small number say = .1, and choose d large enough so that with high probability |wWe argue below that if d is large enough w.r.t. S and K, with high probability, u T i1 S j=2 Q 1−j u ij is small, and so multiplication with W has max value at i 1 .i u i = 1. Fi- nally, there is a weak dependence on K here; for fixed and K it is exponentially unlikely (in d) that the nearest neighbor i = i 1 is close enough to u i to interfere.This solution mechanism suggests that a random orthogo- nal matrix (chosen, for example, via QR decomposition of a Gaussian matrix) is a good starting point for solving this task. The construction above is invariant to rotations; and we can always find a basis so that a given orthogonal ma- trix has the block form above in that basis. Thus all that is necessary is for the descent to nudge the eigenvalues of the orthogonal matrix to be S + T roots of unity, and then it already has the basic form of the construction above. This also gives a good explanation for the performance of the models used for the copy problem in ( Arjovsky et al., 2015) is added to the hidden state, asThis mechanism has been known (at least implicitly, al- though we don't know if it has been written down explicitly before) at least since (Hochreiter &amp; Schmidhuber, 1997), and it can be seen as a very simple LSTM model, with the following gates:Finally, note that although we used the setup of (Arjovsky et al., 2015), the construction can be modified to solve problems 2a and 2b in (Hochreiter &amp; Schmidhuber, 1997)and no non-linearity in Equation (5).Since the construction of the copy mechanism is random- ized, we provide an experiment to show how the solution degrades as a function of K (the dictionary size) and S (the length of the sequence to be remembered). There is not a strong dependence on T (the length of time to remember the sequence). Figure 1 shows the number of successes over 500 runs with d = 128.Note that the solution mechanism for the copy problem above depends on having a fixed location for regurgitating the input. In the experiments below, we also discuss a vari- ant of the copy task, where the symbol to indicate that the memorized sequence must be output is randomly located in S + 1, S + T ; this can be considered a variant of task 2c in (Hochreiter &amp; Schmidhuber, 1997). We do not know a bounded in T explicit LT-RNN or sRNN solution for this variable length problem (although the above solution using a multiplicative RNN instead of an sRNN, and keeping dNote that the 1 × 1 matrix V in the mechanism for the adding problem is the "identity". We can build a more re- dundant solution by using a larger identity matrix. We can describe the identity using the same block structure as the matrix Q defined for the copy task; namely each l j = 0.On the other hand, the Q for the copy task acts as a "clock" that synchronizes after a fixed number of steps T + S. It is important for the mechanism we described that the clock looks random at any time between 1 and S + T . For exam- ple, if we had instead used the same l j in each block Q j , the mechanism would not succeed. The transition matrices for the addition task and the copy task are thus opposites in the sense that for addition, all the l j are the same (i.e. a δ mass on the unit circle), and for copy, the l j are as uni- formly distributed on the unit circle as possible.2 extra hidden variables to track the power of V solves it).The adding problem requires the network to remember two marked numbers in a long sequence and add them. Specif- ically, the input consists of a two dimensional sequenceis uniformly sam- pled between 0 and 1, and the second coordinate is 0 at each j save two; in these two entries,In the experiments below, we will show that it is hard for an LT-RNN to learn the adding task when its transition ma- trix is initialized as a random orthogonal matrix but easy when initialized with the identity, and vice-versa for the copy task. One way to get a "unified" solution is to use l 2 pooling, as in 6. Then when initialized with a matrix with l j distributed uniformly, the decoder can choose to use the pooled hiddens (which through away the phase, and so ap- pear identity-like) for the adding task, or use the raw hid- dens, which are clock-like.  For each task and timescale, we trained 8 LT-ORNNs and 8 LT-IRNNs with different random seeds. The transforma- tion matrices for all models were intialized using a Gaus- sian distribution with mean 0 and variance 1/ √ n (where n is the number of incoming connections to each hidden unit). For LT-ORNNs, we then projected the transition ma- trix to its nearest orthogonal matrix by setting its singular values to 1. exploded whenever the largest singular value of the tran- sition matrix became much greater than 1. Therefore, we adopted a simple activation clipping strategy where we rescaled activations to to have magnitude l whenever their magnitude exceeded l. In our experiments we chose l = 1000.In all experiments, we used RMSProp to train our net- works with a fixed learning rate and a decay rate of 0.9. In preliminary experiments we tried different learning rates in {1, 10 −1 , 10 −2 , 10 −3 , 10 −4 , 10 −5 } and chose the largest one for which the loss did not diverge, for the LT-RNN's we used 10 −4 .We also included LSTMs in all our experiments as a base- line. We used the same method as for LT-RNN to pick the learning rate, and ended up with 10 −3 . Figure 2 shows the results on the copy task for the LSTM, LT-ORNN and LT-IRNN. All networks are trained with 80 hidden units. We see that the LSTM has difficulty beat- ing the baseline performance of only outputting the empty symbol; however it does eventually converge to the solution (this is not shown in the figure). However, the LT-ORNN solves the task almost immediately. We note that this be- havior is similar to that of the uRNN in ( Arjovsky et al., 2015), which is paramaterized in a way that makes it easy to recover the explicit solution described above. The LT- IRNN is never able to find the solution.For all experiments, we normalized the gradients with re- spect to hidden activations by 1/T , where T denotes the number of timesteps. In preliminary experiments, we also found that for LT-RNN models the activations frequently  Figure 4. Results for copy and addition task with pooling architectures. Note that the LSTM will eventually solve the copy task, but the LT-IRNN will not. the problem whereas the LT-ORNN is only able to solve it after a very long time, or not at all. The LSTM is also able to easily solve the task, which is consistent with the origi- nal work of (Hochreiter &amp; Schmidhuber, 1997) where the authors report solving the task for up to 1000 timesteps. We note that this LSTM baseline differs from that of ( Arjovsky et al., 2015;Le et al., 2015) where it is reported to have more difficulty solving the addition task. We hypothesize that this difference is due to the use of different variants of the LSTM architecture such as peephole connections. cally, at every iteration we applied one step of stochastic gradient descent to minimize the loss ||V T V − I||, eval- uated at m random points on the unit sphere. Note that this requires O(md 2 ) operations and a regular update re- quires O(T md 2 ) operations, so adding this soft constraint has negligible computational overhead. In our experiments we set m = 50, which was the same at the minibatch size.We next ran a series of experiments to examine the effect of feeding pooled outputs to the decoder, to see if we could obtain good performance on both the copy and addition tasks with a single architecture and initialization. In these experiments, we added a soft penalty on the transition ma- trix V to keep it orthogonal throughout training. Specifi- In all pooling experiments we used a pool size and stride of 2. The results are shown in Figure 4. The LT-ORNN with pooling is easily able to solve the copy task for both timescales, and approximately solves the addition task for both timescales as well, even though convergence is slower than the LT-IRNN. Its success on the copy task is not sur- prising, since by zeroing out the matrix W P in Equation 6 it can solve the problem with the same solution as the reg- ular LT-ORNN. The good performance on the adding task is somewhat more interesting. To gain insight into how the network stores information in a stable manner while having  an (approximately) orthogonal transition matrix, we plotted the activations of its hidden states over time as it processes an input sequence. This is displayed in Figure 6. We ob- serve relatively constant activations until the first marked number is encountered, which triggers oscillatory patterns along certain dimensions. When the second marked num- ber is seen, existing oscillations are amplified and new ones emerge. This suggests that the network stores informa- tion stably through the radius of its hidden state's rotations along different 2-dimensional subspaces. The information is then recovered as the phase is discarded though the pool- ing operation. Thus the model can have "uniform" clock- like oscillations that are perceived as δ-like after the pool- ing.Having seen the stark impact of initialization on the per- formance of LT-IRNNs and LT-ORNNs for the copy and addition task, and its mitigation through the addition of a pooling layer, we then tested all the models on a problem for which we did not have a (roughly fixed size) solution mechanism, namely the variable length copy task. Fig- ure 5 shows the performance of an LT-IRNN, LT-ORNN, LT-ORNN with l 2 pooling, and LSTM (each with 80 hid- den units) on the variable length copy task with T = 100 timesteps. Even though the number of timesteps is signif- icantly less than in other tasks, none of the LT-RNNs are able to beat the chance baseline, whereas the LSTM is able to solve the task even though its convergence is slow. This experiment is a classic example of how a detail of construc- tion of a synthetic benchmark can favor a model in a way that fails to generalize to other tasks.In this work, we analyzed two standard synthetic long-term memory problems and provided explicit RNN solutions for them. We found that the (fixed length T ) copy problem can be solved using an RNN with a transition matrix that is a T + S root of the identity matrix I, and whose eigenvalues are well distributed on the unit circle, and we remarked that random orthogonal matrices almost satisfy this description. We also saw that the addition problem can be solved with I as a transition matrix. We showed that correspondingly, initializing with I allows a linear-transition RNN to easily be optimized for solving the addition task, and initializing with a random orthogonal matrix allows easy optimization for the copy task; but that flipping these leads to poor re- sults. This suggests an optimization difficulty in transition- ing between oscillatory and steady dynamics, which can be mitigated by adding an l 2 pooling layer that allows the model to easily choose between the two regimes. Finally, our experiment with the variable length copy task illustrates that although synthetic benchmarks can be useful for eval- uating specific capabilities of a given model, success does not necessarily generalize across different tasks, and novel model architectures should be evaluated on a broad set of benchmarks as well as natural data.
