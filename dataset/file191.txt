Deep convolutional networks have, in recent times, achieved near-human performance on an array of visual, auditory, and other cognitive tasks [1,2]. The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3,4,5,6,7,8].Binary convolutional networks that use binary convolutional kernels, and binary neuron activations are ideally suited to be run on low-power neuromorphic architectures that use spike-based communication [9]. In addition, storage and computational efficiency may be gained by using structured matrices in convolutional layers. Using structured matrices in the fully connected layers (also known as linear layers) of deep networks has been studied in the literature [10,11,12] with the objective of reducing the number of learned parameters. The main idea behind these approaches is to restrict the connectivity matrix of a (fully connected) layer to be from a known family of matrices, which are parametrised by a few variables and adapt the backpropagation to update those variables. On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13,14,15,7,16,17,18,19,20,21] -mostly after training.In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9,22,23,24,25,26,21]. By connecting the efficient weight representation schema used in neuromorphic architectures [9,21] with block Toeplitz matrices that arise in discrete convolution, we identify a family of convolution kernels that are naturally hardware efficient. The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27,28,29,30] and thereby harvesting the best that a particular architecture offers.We incorporate learning structured convolutional matrices into traditional stochastic gradient descent [31] so that the trained inference networks are hardware-ready. Furthermore, we exploit a known equivalence between stochastic bounded rectified linear units and deterministic threshold neurons and propose a novel approach to training networks with binary neuron activations. This procedure allows us to obtain accurate gradient estimates during backward step, and speed-up convergence of training and enriches the library of tools available to train low-precision deep neural networks. We evaluate our system on Cifar10 data set and compare against best energy vs accuracy numbers reported on currently available hardware [3] -our approach reduces the number of TrueNorth cores required to achieve 87.5% on Cifar10 from 31872 TrueNoth cores in [3] to 13216 cores.We begin the next section by discussing binary convolutional networks and their suitability for neuromorphic hardware and introduce the weight representation mechanism used in TrueNorth architecture [9]. In Section 3, we discuss the structure of discrete convolution matrices and present our main result connecting that structure with the weight representation in TrueNorth -thereby identifying the family of structured convolution matrices that efficiently map to that architecture. Section 4 outlines our methodology for training networks to use only structured convolution kernels, and discusses how the connection between noisy ReLUs and binary neurons is exploited during training. We summarize our experimental results in Section 5 followed by concluding remarks in Section 6.Binary convolutional networks [5,3] are a particularly elegant instance of low-precision computing targeted toward object recognition tasks. Motivated by the possibility of deploying state-of-the- art image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32,4,5,8,6,3]. Such networks use t´1, 1u-valued (or, t0, 1u) neurons throughout the network and thus best suited to be deployed on neuromorphic hardware using spiking neurons. Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22,23,24,25,26,21].A TrueNorth chip consists of a network of neurosynaptic cores with programmable connectivity, synapses, and neuron parameters [9]. It is a multicore array where each core consists of 256 input lines, 256 neurons, and a 256ˆ256256ˆ256ˆ256 synaptic crossbar array. Each input line in a core can connect to one neuron on any core through a spike router, and that input line is accessible to all of the 256 neurons on that core through the crossbar, thereby resulting in block-wise connectivity. All communication to-, from-, and within-chip is performed using spikes. TrueNorth neurons use a variant of an integrate- and-fire model with 23 configurable parameters [33] where a neuron's state variable updates each tick (typically at 1000 ticks per second, though higher rates are possible). Synapses have individually configurable on/off states and have a strength assigned by look-up table. Specifically, each neuron has a 4-entry table of 8-bit signed-integers, each input line to a core is assigned an input-type of 1, 2, 3 or 4, and each synapse then determines its strength by using the input-type on its source side to index into the table of the neuron on its target side. In summary, for non-negative integers L, N ď 256, the crossbar weight matrix is factored into three components: a) a L ˆ N binary (t0, 1u-valued) connectivity matrix C, (b) an input-type vector g of length L over the integers t1, 2, 3, 4u, and (c) a set of strength-value functions s j : t1, 2, 3, 4u Ñ t´255, . . . , 255u, j " 1, 2, . . . , N -one for each of the N neurons on a core. If we extend functions s j to operate on vectors element-wise, then the L ˆ N weight matrix M of a core can be written aswhere ˝ denotes the Hadamard product 1 between matrices.To account for the fact that a TrueNorth neuron can connect to at most 256 input features, as in [3], network structure in this paper constrained by partitioning features in each layer into one or more 1 Which is simply the element-wise product between matrices equally sized groups. Neurons in a group connect to all the input features in that group, and output features from different groups are generated from disjoint set of input features 2 . The number of groups in a layer is chosen such that the total filter size (rowsˆcolumnsˆfeaturesrowsˆrowsˆcolumnsrowsˆcolumnsˆrowsˆcolumnsˆfeatures) as well as the number of output features of each group is less than or equal to 256. To further promote sparsity in connections, we allow the convolutional filters to be t´1, 0, 1u-valued. The number of neurons that spike in response to an input image influence the energy utilization as well as the number of classification per unit time that the underlying hardware can support. With this in mind, we strive to train networks with sparse binary activations. All the networks studied in this work use t0, 1u-valued neurons where average fraction of 1's is less than 20%. All of the above considerations result in networks that are sparse in connectivity, weights, and activations.When discrete convolution is written as a matrix multiplication, the resulting 'convolution matrix' has a block Toeplitz structure 3 [34]. By establishing the connection between the weight representation in (1) and Toeplitz matrices, we identify a set of convolution kernels (symmetric kernels 4 ) that efficiently map to the TrueNorth architecture. We need a few more notations before giving a formal statement of this connection.For an arbitrary matrix H, we denote its i-th column by H i and its pi, jq-th element by H i,j . If h is an arbitrary scalar-valued function on real numbers, we extend it to operate on matrices by applying it element-wise. For two functions h 1 and h 2 , h 1 ¨ h 2 denotes function composition. Let X ˇ K denote the 2-D convolution 5 between an n ˆ n data matrix X and an l ˆ l convolution kernel matrix K. For any matrix Y , let vecpY q denote the vectorization (i.e., concatenation of columns) of Y . Fact 1 (See for example, [34]): If W denotes the convolution matrix such thatthen W is a block Toeplitz matrix of the form, where submatrix Wi "0 denotes the zero matrices of appropriate dimension, and for i " 1, 2, . . . , l, W i is an n ˆ pní`1qpnípní`pní`1q Toeplitz matrix constructed from the i-th column of the convolution kernel K. Such an n 2 ˆ npní`1qnpnínpní` npní`1q matrix W will be referred to as the block Toeplitz convolution matrix for K, and we write W pKq to make its dependence on K explicit. The sparsity and circulant nature of convolution matrices are fully exploited in most approaches to accelerate training and inference [35,36,37].Suppose that the dimension of the data matrix X is such that n 2 ď 256. Ideally, for every choice of kernel K the corresponding convolution matrix W pKq can be represented in the form of (1) for some choice of type vector g, a set of strength functions, and a binary connectivity matrix C. However, the representation in (1) requires the input-type vector g to be common to all the columns of the weight matrix. Hence, it is conceivable that not every block Toeplitz convolution matrix W pKq can be represented in that form.  is an one-to-one relationship between the set of convolution kernels and the set of block Toeplitz matrices of the form in (3). Theorem 3.1 asserts that only the set of block Toeplitz matrices associated with symmetric kernels characterized by the theorem can be represented by (1). The characterization also suggests a constructive procedure to generate all symmetric kernels. The proof of the theorem is outlined in the appendix. Example 3, also included in the appendix, explains the problem of relating the weight representation in (1) and convolution matrices in more detail.Theorem 3.1. Let n and l be non-negative integers such that L " n 2 ď 256, N " n ´ l ` 1 ď 256 and let K be an l ˆ l kernel matrix. There exist g, C, and s 1 , . . . , s N as in (1) such that M pg, C, ts i u N i"1 q " W pKq if K satisfiesfor some choice of 1. commutative permutations σ 1 , and σ 2 on t1, 2, 3, 4u2. seed element ρ P t1, 2, 3, 4uIf K contains at least four distinct entries and no zeros, then the conditions on K are also necessary.We refer to the convolution kernels identified in Theorem 3.1 as symmetric kernels and use the notation sympf, ρ, σ 1 , σ 2 , Bq to refer to a particular instance. The following two examples use the ordered-pair notation to define the functions σ 1 ,σ 2 , and f and show how some of the well-known convolutional operators are in the family of symmetric kernels identified by Theorem 3.1., andwhich is a widely used approximation to the spatial Laplacian operator., and B "K is the vertical Prewitt operator and the corresponding horizontal operator also may be constructed similarly.Notice that the set of symmetric kernels include instances from the set of separable (e.g. Prewitt), as well as nonseparable (e.g. Laplacian) matrices. A simple search reveals that there are 120 unique pairs σ 1 and σ 2 of commutative permutations on the set of four elements that can be used in Equation 4. Moreover, since we only consider t´1, 0, 1u-valued convolution kernels in the work, the function f in (4) is restricted to be t´1, 1u-valued, thus there are 16 possible choices for f . Since there are 4 possible choices for ρ and the components of B are independent, there are a total of 2We have limited the discussion thus far in this section to 2-D convolutional matrices for ease of presentation. However, recall that deep convolutional networks use 3-D matrices as convolutional kernels and we will now extend our definition of symmetric kernels to cover this important case. An l ˆ l ˆ m symmetric kernel is defined by A straight-forward counting argument as before reveals that there are 2 m l 2 ˆ16ˆ120ˆ4 m symmetric kernels of dimension lˆlˆm. Suppose that l " 3, and m " 8, there are about 10 30 t´1, 0, 1u-valued kernels to choose from!In a typical deep learning setting, the goal is to learn a set of kernels for every convolutional layer in order to minimize some loss function using a gradient-based optimization method. In our case, we want the kernels to be from the class of symmetric kernels that efficiently map to TrueNorth architecture. Our strategy is as follows: At first, we allow every kernel K in the network to be learned without any constraints. Next, because our goal is to generate a trained network that maps to TrueNorth, once the performance on the validation data saturates, the unconstrained real-valued kernel K is replaced by a suitable member of the symmetric kernels as described below. If K P R lˆlˆm denotes the unconstrained convolution kernel, then let  [38,39,40] to speed-up the search in (6). When all the weights of a layer are converted to symmetric kernels , the network is trained for a at least one epoch before the next layer weights are replaced.Figure 2: (a) Noisy ReLU, (b) Threshold neuron.Rectified linear unites (ReLUs) are the typical form of non-linearity used in deep networks. Our approach seeks to use a binary activations obtained by threshold functions because they map to neuromorphic harware with spike-based communication infrastruture efficiently. Our strategy is to begin the training with ReLU activations and to make them increasingly noisy so that they can be replaced with binary activations in the end. Using stochastic neuron activation functions has been explored in the literature for various purposes [41,42,43,44]. An early attempt at using noisy ReLU with the objective of obtaining binary activations was reported in [45] for single-layer networks.It is well-known in information theory [46,47] that the best way to communicate using an amplitude- bounded channel in the presence of additive noise of the form shown in Figure 2 is to use a discrete set of input values for x, and that for an appropriate choice of noise variance, binary signaling is optimal. Moreover, unlike in information theory where the noise distribution and variance are presupposed, we have the freedom to choose and vary them throughout the training period. We seek to utilize this connection to train a binary convolutional network using noisy ReLUs.During training, we use the bounded noisy ReLU shown in Figure 2 (a) with a zero mean uniform distributed random variable distributed in the range rás the noise source. The range of the random variable is slowly increased from 0 to T {2. Towards the end of the training period, noisy ReLUs are replaced by the threshold neuron shown in Figure 2 (b) one layer at a time for fine tuning the network. We use the gradient of the ReLU saturating at T during the backward step throughout the training.The networks are trained on GPUs using MatConvNet deep learning libraries [48]. The stochastic gradient descent was used with dropout [49] layers, momentum (0.9), weight decay (10´6 10´6 ), and batch normalization [50]. The parameters learned through training are mapped to hardware using reusable, composable network description functions called corelets [51]. The corelets created for this work automatically compile the learned network parameters, which are independent of any neuromorphic platform, into an platform-specific hardware configuration file that can directly program TrueNorth chips.The proposed method was evaluated using the image recognition benchmark dataset CIFAR-10 ( Figure 3). The CIFAR-10 dataset [52] consists of color natural images, 32 x 32 pixels in size, in 10 classes, with 50000 training images and 10000 test images. We evaluated the representation power of symmetric kernels by designing two convolution networks using one and four TrueNorth chips and trained for CIFAR10 dataset. Recent results [5,8,3] as well as our own experiments seem to suggest that pooling layers are unsuitable for networks with binary neuron activations. So our networks are built by stacking four sets of convolution layers and each set contains four convolution layers. The smaller network is described in Table 1 while the large network we used was obtained simply by increasing the number of features in each layer. The final set of output features are divided uniformly among the classes and the evaluation is performed at 1 classification per hardware tick.  Table 1: Structure of 1-chip symmetric kernel convolution network. The network consists of four sets and each set contains four convolution layers with varying patch size, stride and groups.To characterize performance, the trained 1-Chip convolution networks were deployed on a IBM TrueNorth NS1e board packed with 4096 neuromorphic cores and run to measure the classification accuracy and throughput. The 4-Chip networks were run on simulation [53], and classification accuracies were measured for the dataset. The results are shown in Table 2   By using symmetric kernels, we have been able to deploy a network with twice as many features on the same number of cores as was possible using the approach described in [3]. Thus, we have been able to obtain better accuracy for the same amount of hardware even though the set of allowed convolutional kernels are a strict subset of the set of t´1, 0, 1u-valued kernels allowed in [3]. With multi-chip networks, we achieved near state-of-the-art accuracy with significantly less number of TrueNorth cores, improving the energy efficiency by more than two-fold.Our study shows that convolutional networks built using only structured kernels are surprisingly rich and maintain their representational power, and that backpropagation can be adapted to learn in the presence of such constraints. Furthermore, our investigation suggests that co-designing algorithms and architecture may offer a successful strategy toward even more energy efficient deployment platform for deep learning. " B i´k`1j´ì1 if k ď i ď pk`2qpk`pk`2q and l ď j ď pì 2q 0 otherwise.Now define C r (the r-th column of C) to be vecpHq, and the r-th strength function to beBy iterating over all pairs k and l, the matrix C and all the strength functions are completely defined. Let k, l P t1, 2, . . . , 14u, and r " pí 1q¨14`k1q¨1q¨141q¨14`1q¨14`k. For g and C and s 1 , . . . , s 196 defined above, we havèwhere paq follows since M pg, C, ts i u 196 196 i"1 q r denotes the r-th column of M pg, C, ts i u i"1 q and its r-th column is simply the element-wise product between s r pgq and C r ; pbq follows from the fact that C r (defined through H in (8)) masks all components of X that do not contribute to the pk, lq-th convolution result and the all-one row and column vectors simply sum all elements of the 3 ˆ 3 matrix between them. Now using (9) andwhere paq follows by using ρ " 1 and applying σ 1 and σ 2 from the example, pbq is obtained by applying f defined in the example. Substituting (11) in (10), we getM pg, C, ts i uwhere paq follows from the definition of convolution, pbq follows since r " pí 1q¨14`k1q¨1q¨141q¨14`1q¨14`k, and r-th element of the row-vector vecpXq t W pKq corresponds to pk, lq-th convolution result, pcq follows since (12) holds for each r P t1, 2, . . . , 196u, pdq follows since X is arbitrary in (13). The last equality completes the proof of first part of the theorem applied to Example 1.The following example shows the construction of the input-type vector g, the binary connectivity matrix C, and and strength functions in (1) in much more detail for concreteness.Example 3. Consider convolving a 4 ˆ 4 matrix X with a 3 ˆ 3 kernels K, and denote the resulting 2 ˆ 2 matrix by Y . The kernel is placed on top of the corresponding elements of matrix X needed to compute Y below:  X2;1 X2;2 X2;3 X2;4 X2;1 X2;2 X2;3 X2;4X3;1 X3;2 X3;3 X3;4 X3;1 X3;2 X3;3 X3;4This can be expressed as matrix multiplication in the following form:Now consider performing the above matrix multiplication using a TrueNorth core. Naturally, the input vector vecpXq is routed to 16 of the input lines of a TrueNorth core in the order shown in (14), and the two matrices on either side of the Hadamard product in (14) are implemented using the crossbar by selecting 1q an appropriate binary connectivity matrix C, 2q a set of four strength functions s 1 , s 2 , s 3 , and s 4 , and 3q a 4 ˆ 4 matrix G such that G i,j defines the input-type for X i,j so that by defining g " vecpGq we can get¨˛  which is sufficient to be able to compute the aforementioned convolution.The obvious choice for C is the binary matrix on the RHS of (15), and the remainder of the discussion focuses on the choice of the type vector and strength functions. Since the strength function is common to all the rows of a particular column, two rows with distinct non-zero entries in a column must be assigned distinct types. To illustrate this further, let us consider the following kernelNow let us rewrite the matrix on the RHS of (15) for this example and color the rows of the resulting matrix such that any two rows of the first column with distinct non-zero elements have different colors:The rows where the first column is 0 are free to choose a color based on subsequent columns so they remain uncolored for now. However, even though the colors so far have been chosen considering only the first column, it is important that no two rows of a subsequent column get the same color but contain distinct non-zero entries. In that case, the coloring scheme is said to be in 'conflict', and a conflict- free coloring scheme may not exist. Notice that the current row coloring does not contain any such conflicts so we can proceed with the remaining columns one at a time (starting position of the row- color indicates which column was under consideration while a row was assigned a particular color):pi, jq location of the input incident ÐÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝ on the corresponding row as in (14) / where the type-vector g simply indicates the color index of the corresponding row (green -1, red -2, blue -3, yellow -4). The above coloring scheme represents a valid coloring solution that is conflict-free. For i " 1, 2, 3, 4, the strength function s i is defined by letting s i pcq to be the unique non-zero value associated with color index c in column i and we have a solution that satisfies (15).In general, suppose that the kernel K satisfies K i,j " f pσ pρqqq for some function f , ρ P t1, 2, 3, 4u, and two commuting permutations σ 1 and σ 2 on t1, 2, 3, 4u. We will now illustrate the idea behind why the above coloring procedure will never run into conflicts for such a K. For simplicity, assume that the function f is invertible. Since such a K has exactly four distinct entries, rows of the matrix whose first column contains a non-zero entry can be colored with four distinct colors without any conflict in that column. Assume that after such a coloring based on just the non-zero entries of the first column, two rows have been already assigned the same color as shown below:The two rows with the same color implyùñ f pσ 1 pσ 2 pρqqq " f pσ 2 2 1 pσ 2 pρqqq ùñ σ 1 pσ 2 pρqq " σ 2 2 1 pσ 2 pρqq ùñ ρ " σ 1 pσ 2 pρqq. Now let us use the last equality to show that this coloring choice does not lead to a conflict in any of the three remaining columns: K 2,3 " σ 1 pσ 2 2 pρqq " σ 2 pσ 1 pσ 2 pρqqq " σ 2 pρq " K 1,2 K 3,2 " σ 2 1 pσ 2 pρqq " σ 1 pσ 1 pσ 2 pρqqq " σ 1 pρq " K 2,1 K 2,2 " σ 1 pσ 2 pρqq " ρ " K 1,1which is precisely what we needed. Similar analysis can be used to show that all the rows can be colored without conflicts for such a matrix K. In fact, by defining the 4 ˆ 4 matrix G by G i,j " σ i´1 1 pσ j´1 2 pρqq and letting g " vecpGq, we can use g to obtain a conflict-free coloring solution! Outline of the proof of Theorem 3.1. Extending the arguments in Remark 1 in a straightforward manner proves the first part of the theorem. We will now outline the proof for the second half the theorem assuming n " 16, l " 3 and the proof for general case is a straightforward extension of arguments here. Assume that K contains at least four distinct non-zero entries as in the statement of the theorem. Suppose that there exist some g, C, and ts i u N N i"1 such that M pg, C, ts i u i"1 q " W pKq, we will now construct f , ρ, σ 1 , and σ 2 such thatpρqqq. Let k, l P t1, 2, . . . , 14u, and r " pí 1q¨14`k1q¨1q¨141q¨14`1q¨14`k. Let X denote the 16ˆ1616ˆ16ˆ16 input matrix. As in Remark 1, the pk, lq-th convolution result is computed using the r-th column of M pg, C, ts i u N i"1 q by vecpXq t M pg, C, ts i u N i"1 q r " vecpXq t ps r pgq ˝ C r q .Recall that the range of s r cannot contain zero (since its range must contain all unique entries of K - of which there are four non-zero elements), consequently s r pgq cannot not contain any zero entries. The pk, lq-th convolution only depends on tX i,j : k ď i ď pk`2qpk`pk`2q, l ď j ď pì 2qu. Furthermore, since the theorem statement also requires that K does not contain zeros, the pk, lq-th convolution depends on all of tX i,j : k ď i ď pk`2qpk`pk`2q, l ď j ď pì 2qu non-trivially. Hence, if H denotes the 16ˆ1616ˆ16ˆ16 matrix constructed such that C r " vecpHq), then H must satisfy H i,j " " 1 if k ď i ď pk`2qpk`pk`2q and l ď j ď pì 2q 0 otherwise (17) which is merely stating that the binary vector C r masks out all the rows on which the pk, lq-th convolution does not depend on. Now let G denote the 16ˆ1616ˆ16ˆ16 matrix obtained by reshaping g so that the input-type of X i,j is G i,j (i.e, g " vecpGq). We have vecpXq t ps r pgq ˝ C r q ff paq " r1 1 1s˜«pbq " r1 1 1s˜«where paq follows from (17) and the fact that g " vecpGq, pbq follows from the assumption that vecpXq t ps r pgq ˝ C r q is equal to the pk, lq-th convolution result. Since the input matrix X is arbitrary, by combining (18) and (19) we getFinally, we have
