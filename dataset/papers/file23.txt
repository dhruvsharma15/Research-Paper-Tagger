Convolutional neural networks (CNNs) have become deeper and larger to pursue increasingly better performance on classification and recognition tasks [22,18,40,12,46]. Looking at the successes of deep learning in computer vison, we find that a large amount of training or pre- training data is essential in training deep neural networks.Large-scale image datasets, such as the ImageNet ILSVRC dataset [36], Places [50], and MS COCO [23], have led to a series of breakthroughs in visual recognition, including image classification [24], object detection [10], and seman- tic segmentation [25]. Many other related visual tasks have benefited from these breakthroughs.Nonetheless, researchers face a dilemma when using deep convolutional neural networks to perform visual tasks that do not have sufficient training data. Training a deep network with insufficient data might even give rise to infe- rior performance in comparison to traditional classifiers fed with handcrafted features. Fine-grained classification prob- lems, such as Oxford Flowers 102 [29] and Stanford Dogs 120 [19], are such examples. The number of training sam- ples in these datasets is far from being enough for training large-scale deep neural networks, and the networks would become overfit quickly.Solving the overfitting problem for deep convolutional neural networks on learning tasks without sufficient train- ing data is challenging [39]. Transfer learning techniques that apply knowledge learnt from one task to other related tasks have been proven helpful [30]. In the context of deep learning, fine-tuning a deep network pre-trained on the Im- ageNet or Places dataset is a common strategy to learn task- specific deep features.This strategy is considered a simple transfer learning technique for deep learning. However, since the ratio between the number of learnable parameters and the number of training samples still remains the same, fine-tuning needs to be terminated after a relatively small number of iterations; otherwise, overfitting still occurs.In this paper, we attempt to tackle the problem of training deep neural networks for learning tasks that have insuffi- cient training data. We adopt the source-target joint training methodology [45] when fine-tuning deep neural networks. The original learning task without sufficient training data is called the target learning task, T t . To boost its perfor- mance, the target learning task is teamed up with another learning task with rich training data. The latter is called the source learning task, T s . Suppose the source learning task has a large-scale training set D s , and the target learn- ing task has a small-scale training set D t . Since the target learning task is likely a specialized task, we envisage the image signals in its dataset possess certain unique low-level characteristics (e.g. fur textures in Stanford Dogs 120 [19]), and the learned kernels in the convolutional layers of a deep network need to grasp such characteristics in order to gen- erate highly discriminative features. Thus supplying suffi- cient training images with similar low-level characteristics becomes the most important mission of the source learn- ing task. Our core idea is to identify a subset of training images from D s whose low-level characteristics are simi- lar to those from D t , and then jointly fine-tune a shared set of convolutional layers for both source and target learning tasks. The source learning task is fine-tuned using the se- lected training images only. Hence, this process is called selective joint fine-tuning. The rationale behind this is that the unique low-level characteristics of the images from D t might be overwhelmed if all images from D s were taken as training samples for the source learning task.How do we select images from D s that share similar low-level characteristics as those from D t ? Since kernels followed with nonlinear activation in a deep convolutional neural network (CNN) are actually nonlinear spatial filters, to find sufficient data for training high-quality kernels, we use the responses from existing linear or nonlinear filter banks to define similarity in low-level characteristics. Ga- bor filters [28] form an example of a linear filter bank, and the complete set of kernels from certain layers of a pre- trained CNN form an example of a nonlinear filter bank. We use histograms of filter bank responses as image descriptors to search for images with similar low-level characteristics.The motivation behind selecting images according to their low-level characteristics is two fold. First, low-level characteristics are extracted by kernels in the lower con- volutional layers of a deep network. These lower convo- lutional layers form the foundation of an entire network, and the quality of features extracted by these layers deter- mines the quality of features at higher levels of the deep net- work. Sufficient training images sharing similar low-level characteristics could strength the kernels in these layers. Second, images with similar low-level characteristics could have very different high-level semantic contents. Therefore, searching for images using low-level characteristics has less restrictions and can return much more training images than using high-level semantic contents.The above source-target selective joint fine-tuning scheme is expected to benefit the target learning task in two different ways. First, since convolutional layers are shared between the two learning tasks, the selected training sam- ples for the source learning task prevent the deep network from overfitting quickly. Second, since the selected train- ing samples for the source learning task share similar low- level characteristics as those from the target learning task, kernels in their shared convolutional layers can be trained more robustly to generate highly discriminative features for the target learning task.The proposed source-target selective joint fine-tuning scheme is easy to implement. Experimental results demon- strate state-of-the-art performance on multiple visual clas- sification tasks with much less training samples than what is required by recent deep learning architectures. These vi- sual classification tasks include fine-grained classification on Stanford Dogs 120 [19] and Oxford Flowers 102 [29], image classification on Caltech 256 [11], and scene classi- fication on MIT Indoor 67 [33].In summary, this paper has the following contributions:• We introduce a new deep transfer learning scheme, called selective joint fine-tuning, for improving the performance of deep learning tasks with insufficient training data. It is an important step forward in the context of the widely adopted strategy of fine-tuning a pre-trained deep neural network.• We develop a novel pipeline for implementing this deep transfer learning scheme. Specifically, we compute descrip- tors from linear or nonlinear filter bank responses on train- ing images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task.• Experiments demonstrate that our deep transfer learning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning.Multi-Task Learning. Multi-task learning (MTL) ob- tains shared feature representations or classifiers for related tasks [3]. In comparison to learning individual tasks in- dependently, features and classifiers learned with MTL of- ten have better generalization capability. In deep learning, faster RCNN [34] jointly learns object locations and labels using shared convolutional layers but different loss func- tions for these two tasks. In [7], the same multi-scale con- volutional architecture was used to predict depth, surface normals and semantic labels. This indicates that convolu- tional neural networks can be adapted to different tasks eas- ily. While previous work [8,34] attempts to find a shared feature space that benefits multiple learning tasks, the pro- posed joint training scheme in this paper focuses on learning a shared feature space that improves the performance of the target learning task only.Feature Extraction and Fine-tuning. Off-the-shelf CNN features [37,5] have been proven to be powerful in vari- ous computer vision problems. Pre-training convolutional neural networks on ImageNet [36] or Places [50]   target learning task. Fine-tuning pre-trained models [10] has become a commonly used method to learn task-specific features. The transfer ability of different convolutional lay- ers in CNNs has been investigated in [48]. However, for tasks that do not have sufficient training data, overfitting occurs quickly during fine-tuning. The proposed pipeline in this paper not only alleviates overfitting, but also attempts to find a more discriminative feature space for the target learning task.to form a training set. In our method, we search for near- est neighbors in a large-scale labeled dataset using low- level features instead of high-level semantic information. It has been shown in [27] that low-level features computed in the bottom layers of a CNN encode very rich information, which can completely reconstruct the original image. Our experimental results show that nearest neighbor search us- ing low-level features can outperform that using high-level semantic information as in [21].Transfer Learning. Different from MTL, transfer learn- ing (or domain adaptation) [30] applies knowledge learnt in one domain to other related tasks. Domain adaptation al- gorithms can be divided into three categories, including in- stance adaption [16,1], feature adaption [26,41], and model adaption [6]. Hong et al. [15] transferred rich semantic in- formation from source categories to target categories via the attention model. Tzeng et al.[41] performed feature adaptation using a shared convolutional neural network by transferring the class relationship in the source domain to the target domain. To make our pipeline more flexible, this paper does not assume the source and target label spaces are the same as in [41]. Different from the work in [1] which randomly resamples training classes or images in the source domain, this paper conducts a special type of transfer learning by selecting source training samples that are near- est neighbors of samples in the target domain in the space of certain low-level image descriptor.3. Selective Joint Fine-tuning 3.1. Overview Fig. 1 shows the overall pipeline for our proposed source-target selective joint fine-tuning scheme. Given a target learning task T t that has insufficient training data, we perform selective joint fine-tuning as follows. The en- tire training dataset associated with the target learning task is called the target domain. The source domain is defined similarly. Krause et al. [21] directly performed Google image search using keywords associated with categories from the target domain, and download a noisy collection of images , should be large enough to train a deep con- volutional neural network from scratch. Ideally, these train- ing images should present diversified low-level character- istics. That is, running a filter bank on them give rise to as diversified responses as possible. There exist a few large-scale visual recognition datasets that can serve as the source domain, including ImageNet ILSVRC dataset [36], Places [50], and MS COCO [23].Source Domain Training Images : In our selective joint fine-tuning, we do not use all images in the source domain as training images. Instead, for each image from the target domain, we search a certain number of images with simi- lar low-level characteristics from the source domain. Only images returned from these searches are used as training images for the source learning task in selective joint fine- tuning. We apply a filter bank to all images in both source domain and target domain. Histograms of filter bank re- sponses are used as image descriptors during search. We associate an adaptive number of source domain images with each target domain image. Hard training samples in the tar- get domain might be associated with a larger number of source domain images. Two filter banks are used in our experiments. One is the Gabor filter bank, and the other consists of kernels in the convolutional layers of AlexNet pre-trained on ImageNet [22].CNN Architecture : Almost any existing deep convolu- tional neural network, such as AlexNet [22], VGGNet [18], and ResidualNet [12], can be used in our selective joint fine- tuning. We use the 152-layer residual network with identity mappings [13] as the CNN architecture in our experiments. The entire residual network is shared by the source and tar- get learning tasks. An extra output layer is added on top of the residual network for each of the two learning tasks. This output layer is not shared because the two learning tasks may not share the same label space. The residual network is pre-trained either on ImageNet or Places.Source-Target Joint Fine-tuning : Each task uses its own cost function during selective joint fine-tuning, and ev- ery training image only contributes to the cost function cor- responding to the domain it comes from. The source do- main images selected by the aforementioned searches are used as training images for the source learning task only while the entire target domain is used as the training set for the target learning task only. Since the residual network (with all its convolutional layers) is shared by these two learning tasks, it is fine-tuned by both training sets. And the output layers on top of the residual network are fine-tuned by its corresponding training set only. Thus we conduct end-to-end joint fine-tuning to minimize the original loss functions of the source learning task and the target learning task simultaneously.Filter Bank We use the responses to a filter bank to de- scribe the low-level characteristics of an image. The first filter bank we use is the Gabor filter bank. Gabor filters are commonly used for feature description, especially texture description [28]. Gabor filter responses are powerful low- level features for image and pattern analysis. We use the parameter setting in [28] as a reference. For each of the real and imaginary parts, we use 24 convolutional kernels with 4 scales and 6 orientations. Thus there are 48 Gabor filters in total.Kernels in a deep convolutional neural network are ac- tually spatial filters. When there is nonlinear activation fol- lowing a kernel, the combination of the kernel and nonlinear activation is essentially a nonlinear filter. A deep CNN can extract low/middle/high level features at different convolu- tional layers [48]. Convolutional layers close to the input data focus on extract low-level features while those further away from the input extract middle-and high-level features. In fact, a subset of the kernels in the first convolutional layer of AlexNet trained on ImageNet exhibit oriented stripes, similar to Gabor filters [22]. When trained on a large-scale diverse dataset, such as ImageNet, such kernels can be used for describing generic low-level image characteristics. In practice, we use all kernels (and their following nonlinear activation) from the first and second convolutional layers of AlexNet pre-trained on ImageNet as our second choice of a filter bank. Image Descriptor Let C i (m, n) denote the response map to the i-th convolutional kernel or Gabor filter in our filter bank, and φ i its histogram. To obtain more discriminative histogram features, we first obtain the upper bound h i is divided into a set of small bins. We adaptively set the width of ev- ery histogram bin so that each of them contains a roughly equal percentage of pixels. In this manner, we can avoid a large percentage of pixels falling into the same bin. We con- catenate the histograms of all filter response maps to form a feature vector, where C is the number of classes, p m i,c is the probability that the i-th training sample belongs to the c-th class after a softmax layer in the m-th iteration.Training samples that have high classification uncer- tainty are considered hard training samples. In the next iteration, we increase the number of nearest neighbors of the hard training samples as in Eq.  i is the number of nearest neighbors in the m- th iteration. By changing the number of nearest neighbors for samples in the target domain, the subset of the source domain used as training data evolves over iterations, which in turn gradually changes the feature representation learned in the deep network. In the above equation, we typically set δ = 0.1, σ 0 = 4K 0 and σ 1 = 2K 0 , where K 0 is the initial number of nearest neighbors for all samples in the target domain. In our experiments, we stop after five iterations.In Table 1, we compare the effectiveness of Gabor fil- ters and various combinations of kernels from AlexNet in our selective joint fine-tuning. In this experiment, we use the 50-layer residual network [12] with half of the convolu- tional kernels in the original architecture.any source datasets as our baseline. Note that the network architecture we use is different from those used in most published methods for the datasets we run experiments on, and many existing methods adopt sophisticated parts mod- els and feature encodings. The performance of such meth- ods are still included in this paper to indicate that our simple holistic method without incorporating parts models and fea- ture encodings is capable of achieving state-of-the-art per- formance.We use the pre-trained model released in [12] to initial- ize the residual network. During selective joint fine-turning, source and target samples are mixed together in each mini- batch. Once the data has passed the average pooling layer in the residual network, we split the source and target sam- ples, and send them to their corresponding softmax classi- fier layer respectively. Both the source and target classifiers are initialized randomly.We run all our experiments on a TITAN X GPU with 12GB memory. All training data is augmented as in [31] first, and we follow the training and testing settings in [12]. Every mini-batch can include 20 224×224 images using a modified implementation of the residual network. We in- clude randomly chosen samples from the target domain in a mini-batch. Then for each of the chosen target sample, we further include one of its retrieved nearest neighbors from the source domain in the same mini-batch. We set the iter size to 10 for each iteration in Caffe [17]. The momen- tum parameter is set to 0.9 and the weight decay is 0.0001 in SGD. During selective joint fine-tuning, the learning rate starts from 0.01 and is divided by 10 after every 2400−5000 iterations in all the experiments. Most of the experiments can finish in 16000 iterations.In all experiments, we use the 152-layer residual network with identity mappings [13] as the deep convolutional archi- tecture, and conventional fine-tuning performed on a pre- trained network with the same architecture without using We use the ImageNet ILSVRC 2012 training set [36] as the source domain for Stanford Dogs [19], Oxford Flow- ers [29], and Caltech 256 [11], and the combination of the ImageNet and Places 205 [50] training sets as the source domain for MIT Indoor 67 [33]. Fig. 2 shows the retrieved 1-st, 10-th, 20-th, 30-th, and 40-th nearest neighbors from ImageNet [36] or Places [50]. It can be observed that cor- responding source and target images share similar colors, local patterns and global structures. Since low-level filter bank responses do not encode strong semantic information, the 50 nearest neighbors from a target domain include im- ages from various and sometimes completely unrelated cat- egories. We find out experimentally that there should be at least 200,000 retrieved images from the source domain. Too few source images give rise to overfitting quickly. Therefore, the initial number of retrieved nearest neighbors (K 0 ) for each target training sample is set to meet this requirement. On the other hand, a surprising result is that setting K 0 too large would make the performance of the target learning task drop significantly. In our experiments, we set K 0 to dif- ferent values for Stanford Dogs (K 0 = 100), Oxford Flow- ers (K 0 = 300), Caltech 256 (K 0 = 50 − 100), and MIT Indoor 67 (K 0 = 100). Since there exists much overlap among the nearest neighbors of different target samples, the retrieved images typically do not cover the entire ImageNet or Places datasets. formation during selective joint fine-tuning, and use the commonly used mean class accuracy to evaluate the per- formance as in [11].As shown in Table 2, the mean class accuracy achieved by fine-tuning the residual network using the training sam- ples of this dataset only and without a source domain is 80.4%. It shows that the 152-layer residual network [12,13] pre-trained on the ImageNet dataset [36] has a strong gen- eralization capability on this fine-grained classification task. Using the entire ImageNet dataset during regular joint fine- tuning can improve the performance by 5.1%. When we finally perform our proposed selective joint fine-tuning us- ing a subset of source domain images retrieved using his- tograms of low-level convolutional features, the perfor- mance is further improved to 90.2%, which is 9.8% higher than the performance of conventional fine-tuning without a source domain and 4.3% higher than the result reported in [21], which expands the original target training set using Google image search. This comparison demonstrates that selective joint fine-tuning can significantly outperform con- ventional fine-tuning.Stanford Dogs 120. Stanford Dogs 120 [19] contains 120 categories of dogs. There are 12000 images for training, and 8580 images for testing. We do not use the parts in- Oxford Flowers 102. Oxford Flowers 102 [29] consists of 102 flower categories. 1020 images are used for training, 1020 for validation, and 6149 images are used for testing. HAR-CNN [44] 49.4 Local Alignment [9] 57.0 Multi scale metric learning [32] 70.3 MagNet [35] 75.1 Web Data + Original Data [21] 85.9that Simon et al. [38] used the validation set in this dataset as additional training data. To verify the effectiveness of our joint fine-tuning strategy, we have also conducted ex- periments using this training setting and our result from a single network outperforms that of [38] by 1.7%.Training from scratch using target domain only 53.  There are only 10 training images in each category. As shown in Table 3, the mean class accuracy achieved by conventional fine-tuning using the training samples of this dataset only and without a source domain is 92.3%. Se- lective joint fine-tuning further improves the performance to 94.7%, 3.3% higher than previous best result from a single network [35]. To compare with previous state-of-the-art re- sults obtained using an ensemble of different networks, we also average the performance of multiple models obtained during iterative source image retrieval for hard training sam- ples in the target domain. Experiments show that the perfor- mance of our ensemble model is 95.8%, 1.3% higher than previous best ensemble performance reported in [20]. Note Caltech 256. Caltech 256 [11] has 256 object categories and 1 background cluster class. In every category, there are at least 80 images used for training, validation and test- ing. Researchers typically report results with the number of training samples per class falling between 5 and 60. We follow the testing procedure in [42] to compare with state- of-the-art results.We conduct four experiments with the number of train- ing samples per class set to 15, 30, 45 and 60, respectively. According to Table 4, in comparison to conventional fine- tuning without using a source domain, selective joint fine- tuning improves classification accuracy in all four experi- ments, and the degree of improvement varies between 2.6% and 4.1%. Performance improvement due to selective joint fine-tuning is more obvious when a smaller number of target training image per class are used. This is because limited di- versity in the target training data imposes a greater need to seek help from the source domain. In most of these exper- iments, the classification performance of our selective joint fine-tuning is also significantly better than previous state- of-the-art results.Method mean Acc(%)MPP [47] 91.3 Multi-model Feature Concat [1] 91.3 MagNet [35] 91.4 VGG-19 + GoogleNet + AlexNet [20] 94.5Training from scratch using target domain only 58.   Table 5, the mean class accuracy of se- lective joint fine-tuning with ImageNet as the source do- main is 82.8%, 1.1% higher than that of conventional fine- tuning without using a source domain. Since ImageNet is an object-centric dataset while MIT Indoor 67 is a scene dataset, it is hard for training images in the target domain to retrieve source domain images with similar low-level char- acteristics. But source images retrieved from ImageNet still prevent the network from overfitting too heavily and help achieve a performance gain. When the Places dataset serves as the source domain, the mean class accuracy reaches 85.8%, which is 4.1% higher than the performance of fine- tuning without a source domain and 4.8% higher than pre- vious best result from a single network [4]  [47] 80.8 VGG-19 + FV [4] 81.0 VGG-19 + GoogleNet [20] 84.7 Multi scale + multi model ensemble [14] 86.0  source domain based on both ImageNet and Places does not further improve the performance. Once averaging the out- put from the networks jointly fine-tuned with Places and the hybrid source domain, we obtain a classification accu- racy 0.9% higher than previous best result from an ensemble model [14].helpful. Third, instead of using a subset of retrieved train- ing images, we use the same number of randomly chosen training images from the source domain. Again, the perfor- mance drops by 4.7% and 1.5% respectively. Fourth, to val- idate the effectiveness of iteratively increasing the number of retrieved images for hard training samples in the target domain, we turn off this feature and only use the same num- ber (K 0 ) of retrieved images for all training samples in the target domain. The performance drops by 1.9% and 0.5% respectively. This indicates that our adaptive scheme for hard samples is useful in improving the performance. Fifth, we use convolutional kernels in the two bottom layers of a pre-trained AlexNet as our filter bank. If we replace this fil- ter bank with the Gabor filter bank, the overall performance drops by 2.7% and 0.9% respectively, which indicates a fil- ter bank learned from a diverse dataset could be more pow- erful than an analytically defined one. Finally, if we per- form conventional fine-tuning without using a source do- main, the performance drop becomes quite significant and reaches 9.8% and 2.4% respectively.We perform an ablation study on both Stanford Dogs 120 [19] and Oxford Flowers 102 [29] by replacing or remov- ing a single component from our pipeline. First, instead of fine-tuning, we perform training from scratch in two set- tings, one using the target domain only and the other using selective joint training. Tables 2 and 3 show that while se- lective joint training obviously improves the performance, it is still inferior than fine-tuning pretrained networks. This is because we only subsample a relatively small percentage (20-30%) of the source data, which is still insufficient to train deep networks from scratch. Second, instead of using a subset of retrieved training images from the source domain, we simply use all training images in the source domain. Joint fine-tuning with the entire source domain decrease the performance by 4.6% and 1.3% respectively. This demon- strates that using more training data from the source do- main is not always better. On the contrary, using less but more relevant data from the source domain is actually more In this paper, we address deep learning tasks with insuffi- cient training data by introducing a new deep transfer learn- ing scheme called selective joint fine-tuning, which per- forms a target learning task with insufficient training data simultaneously with another source learning task with abun- dant training data. Different from previous work which di- rectly adds extra training data to the target learning task, our scheme borrows samples from a large-scale labeled dataset for the source learning task, and do not require addi- tional labeling effort beyond the existing datasets. Experi- ments show that our deep transfer learning scheme achieves state-of-the-art performance on multiple visual classifica- tion tasks with insufficient training data for deep networks. Nevertheless, how to find the most suitable source domain for a specific target learning task remains an open problem for future investigation.
