The development of the variational autoencoder framework ( Kingma and Welling, 2014;Rezende et al., 2014) has paved the way for learning large- scale, directed latent variable models. This has led to significant progress in a diverse set of machine learning applications, ranging from computer vi- sion ( Gregor et al., 2015;Larsen et al., 2016) to natural language processing tasks (Mnih and Gregor, 2014;Miao et al., 2016;Bowman et al., 2015; Nevertheless, the majority of current models as- sume a simple prior in the form of a multivariate Gaussian distribution in order to maintain mathe- matical and computational tractability. This is of- ten a highly restrictive and unrealistic assumption to impose on the structure of the latent variables. First, it imposes a strong uni-modal structure on the latent variable space; latent variable samples from the generating model (prior distribution) all cluster around a single mean. Second, it forces the latent variables to follow a perfectly symmet- ric distribution with constant kurtosis; this makes it difficult to represent asymmetric or rarely occur- ring factors. Such constraints on the latent vari- ables increase pressure on the down-stream gen- erative model, which in turn is forced to carefully partition the probability mass for each latent factor throughout its intermediate layers. For complex, multi-modal distributions -such as the distribu- tion over topics in a text corpus, or natural lan- guage responses in a dialogue system -the uni- modal Gaussian prior inhibits the model's ability to extract and represent important latent structure in the data. In order to learn more expressive latent variable models, we therefore need more flexible, yet tractable, priors.prior distribution based on the piecewise constant distribution. We derive an analytical, tractable form that is applicable to the variational autoen- coder framework and propose a differentiable parametrization for it. We then evaluate the ef- fectiveness of the distribution when utilized both as a prior and as approximate posterior across variational architectures in two natural language processing tasks: document modeling and natu- ral language generation for dialogue. We show that the piecewise constant distribution is able to capture elements of a target distribution that can- not be captured by simpler priors -such as the uni-modal Gaussian. We demonstrate state-of- the-art results on three document modeling tasks, and show improvements on a dialogue natural lan- guage generation. Finally, we illustrate qualita- tively how the piecewise constant distribution rep- resents multi-modal latent structure in the data.ables Srivastava et al., 2013;Larochelle and Lauly, 2012;Uria et al., 2014;Lauly et al., 2016;Bornschein and Bengio, 2015;Mnih and Gregor, 2014). The success of such dis- crete latent variable models -which are able to partition probability mass into separate regions - serves as one of our main motivations for investi- gating models with more flexible continuous latent variables for document modeling. More recently, Miao et al. (2016) proposed to use continuous la- tent variables for document modeling.The idea of using an artificial neural network to approximate an inference model dates back to the early work of Hinton and colleagues (Hinton and Zemel, 1994;Hinton et al., 1995;Dayan and Hinton, 1996). Researchers later proposed Markov chain Monte Carlo methods (MCMC) (Neal, 1992), which do not scale well and mix slowly, as well as variational approaches which require a tractable, factored distribution to approximate the true posterior distribution ( Jordan et al., 1999). Others have since proposed using feed-forward in- ference models to initialize the mean-field infer- ence algorithm for training Boltzmann architectures (Salakhutdinov and Larochelle, 2010;Ororbia II et al., 2015). Recently, the variational autoencoder framework (VAE) was proposed by Kingma and Welling (2014) and Rezende et al. (2014), closely related to the method proposed by Mnih and Gregor (2014). This framework allows the joint training of an inference network and a di- rected generative model, maximizing a variational lower-bound on the data log-likelihood and facil- itating exact sampling of the variational posterior. Our work extends this framework.With respect to document modeling, neural ar- chitectures have been shown to outperform well- established topic models such as Latent Dirich- let Allocation (LDA) (Hofmann, 1999;Blei et al., 2003). Researchers have successfully proposed several models involving discrete latent vari- Researchers have also investigated latent vari- able models for dialogue modeling and dialogue natural language generation ( Bangalore et al., 2008;Crook et al., 2009;Zhai and Williams, 2014). The success of discrete latent variable models in this task also motivates our investi- gation of more flexible continuous latent vari- ables.Closely related to our proposed ap- proach is the Variational Hierarchical Recur- rent Encoder-Decoder (VHRED, described below) ( Serban et al., 2017b), a neural architecture with latent multivariate Gaussian variables. In parallel with our work, Zhao et al. (2017) has also pro- posed a latent variable model for dialogue mod- eling with the specific goal of generating diverse natural language responses.Researchers have explored more flexible dis- tributions for the latent variables in VAEs, such as autoregressive distributions, hierarchical prob- abilistic models and approximations based on MCMC sampling (Rezende et al., 2014;Rezende and Mohamed, 2015;Kingma et al., 2016;Ranganath et al., 2016;Maaløe et al., 2016;Salimans et al., 2015;Burda et al., 2016;Chen et al., 2017;Ruiz et al., 2016). These are all complimentary to our approach; it is possible to combine them with the piecewise constant latent variables. In parallel to our work, multiple research groups have also proposed VAEs with discrete latent variables ( Maddison et al., 2017;Jang et al., 2017;Rolfe, 2017;Johnson et al., 2016). This is a promising line of research, however these approaches often require approximations which may be inaccurate when applied to larger scale tasks, such as docu- ment modeling or natural language generation. Fi- nally, discrete latent variables may be inappropri- ate for certain natural language processing tasks.We start by introducing the neural variational learning framework. We focus on modeling dis- crete output variables (e.g. words) in the context of natural language processing applications. How- ever, the framework can easily be adapted to han- dle continuous output variables.Let w 1 , . . . , w N be a sequence of N tokens (words) conditioned on a continuous latent vari- able z. Further, let c be an additional observed variable which conditions both z and w 1 , . . . , w N . Then, the distribution over words is:Gaussian distribution and f might be defined as f θ ( = µ + σ where µ and σ are in the param- eter set θ. In this case, z is able to represent any Gaussian with mean µ and variance σ 2 .Model parameters are learned by maximizing the variational lower-bound in eq. (1) using gra- dient descent, where the expectation is computed using samples from the approximate posterior.The majority of work on VAEs propose to parametrize z as multivariate Gaussian distrib- tions. However, this unrealistic assumption may critically hurt the expressiveness of the latent vari- able model. See Appendix A for a detailed dis- cussion. This motivates the proposed piecewise constant latent variable distribution.where θ are the model parameters. The model first generates the higher-level, continuous latent vari- able z conditioned on c. Given z and c, it then gen- erates the word sequence w 1 , . . . , w N . For unsu- pervised modeling of documents, the c is excluded and the words are assumed to be independent of each other, when conditioned on z:We propose to learn latent variables by parametriz- ing z using a piecewise constant probability den- sity function (PDF). This should allow z to rep- resent complex aspects of the data distribution in latent variable space, such as non-smooth regions of probability mass and multiple modes.Let n ∈ N be the number of piecewise constant components. We assume z is drawn from PDF: Model parameters can be learned using the varia- tional lower-bound ( Kingma and Welling, 2014):where 1 (x) is the indicator function, which is one when x is true and otherwise zero. The distribu- tion parameters are a i &gt; 0, for i = 1, . . . , n. The normalization constant is:where we note that Q ψ (z|w 1 , . . . , w N , c) is the approximation to the intractable, true posterior P θ (z|w 1 , . . . , w N , c). Q is called the encoder, or sometimes the recognition model or inference model, and it is parametrized by ψ. The distri- bution P θ (z|c) is the prior model for z, where the only available information is c. The VAE framework further employs the re-parametrization trick, which allows one to move the derivative of the lower-bound inside the expectation. To ac- complish this, z is parametrized as a transforma- tion of a fixed, parameter-free random distribu- tion z = f θ ( where is drawn from a ran- dom distribution. Here, f is a transformation of parametrized by θ, such that f θ ( ∼ P θ (z|c). For example, might be drawn from a standard It is straightforward to show that a piecewise con- stant distribution with more than n &gt; 2 pieces is capable of representing a bi-modal distribution. When n &gt; 2, a vector z of piecewise constant variables can represent a probability density with 2 |z| modes. Figure 1 illustrates how these variables help model complex, multi-modal distributions.In order to compute the variational bound, we need to draw samples from the piecewise constant distribution using its inverse cumulative distribu- tion function (CDF). Further, we need to compute the KL divergence between the prior and posterior. The inverse CDF and KL divergence quantities are both derived in Appendix B. During training we must compute derivatives of the variational bound in eq. (1). These expressions involve derivatives generation this might, for example, be produced by an LSTM encoder applied to the dialogue his- tory), which is shared across all latent variable dimensions. The matrices H  Figure 1: Joint density plot of a pair of Gaussian and piecewise constant variables. The horizontal axis corresponds to z 1 , which is a univariate Gaus- sian variable. The vertical axis corresponds to z 2 , which is a piecewise constant variable.are learnable parameters. For the posterior distribution, previous work has shown it is better to parametrize the posterior distribution as a linear interpolation of the prior distribution mean and variance and a new estimate of the mean and variance based on the observation x ( Fraccaro et al., 2016). The interpolation is controlled by a gating mechanism, allowing the model to turn on/off latent dimensions:of indicator functions, which have derivatives zero everywhere except for the changing points where the derivative is undefined. However, the proba- bility of sampling the value exactly at its changing point is effectively zero. Thus, we fix these deriva- tives to zero. Similar approximations are used in training networks with rectified linear units.where Enc(c, x) is an embedding of both c and x. The matrices H The interpolation mechanism is controlled by α µ and α σ , which are initialized to zero (i.e. initial- ized such that the posterior is equal to the prior).In this section, we develop the parametrization of both the Gaussian variable and our proposed piecewise constant latent variable.Let x be the current output sequence, which the model must generate (e.g. w 1 , . . . , w N ). Let c be the observed conditioning information. If the task contains additional conditioning information this will be embedded by c. For example, for dialogue natural language generation c represents an em- bedding of the dialogue history, while for docu- ment modeling c = ∅.We parametrize the piecewise prior parameters us- ing an exponential function applied to a linear transformation of the conditioning information: Let µ prior and σ 2,prior be the prior mean and vari- ance, and let µ post and σ 2,post be the approximate posterior mean and variance. For Gaussian la- tent variables, the prior distribution mean and vari- ances are encoded using linear transformations of a hidden state. In particular, the prior distribu- tion covariance is encoded as a diagonal covari- ance matrix using a softplus function:We now introduce two classes of VAEs. The mod- els are extended by incorporating the Gaussian and piecewise latent variable parametrizations.where Enc(c) is an embedding of the conditioning information c (e.g. for dialogue natural languageThe neural variational document model (NVDM) model has previously been proposed for document modeling (Mnih and Gregor, 2014;Miao et al., 2016), where the latent variables are Gaussian.Since the original NVDM uses Gaussian latent variables, we will refer to it as G-NVDM. We pro- pose two novel models building on G-NVDM. The first model we propose uses piecewise constant la- tent variables instead of Gaussian latent variables.We refer to this model as P-NVDM. The second model we propose uses a combination of Gaus- sian and piecewise constant latent variables. The models sample the Gaussian and piecewise con- stant latent variables independently and then con- catenates them together into one vector. We refer to this model as H-NVDM.Let V be the vocabulary of document words. Let W represent a document matrix, where row w i is the 1-of-|V | binary encoding of the i'th word in the document. Each model has an encoder com- ponent Enc(W ), which compresses a document vector into a continuous distributed representa- tion upon which the approximate posterior is built. For document modeling, word order information is not taken into account and no additional condi- tioning information is available. Therefore, each model uses a bag-of-words encoder, defined as a multi-layer perceptron (MLP) Enc(c = ∅, x) = Enc(x). Based on preliminary experiments, we choose the encoder to be a two-layered MLP with parametrized rectified linear activation functions (we omit these parameters for simplicity). For the approximate posterior, each model has the param- eter matrix W and Gregor (2014) and Miao et al. (2016). We learn the prior mean and variance, while these were fixed to a standard Gaussian in previous work. This increases the flexibility of the model and makes optimization easier. In addition, we use a gating mechanism for the approximate pos- terior of the Gaussian variables. This gating mech- anism allows the model to turn off latent vari- able (i.e. fix the approximate posterior to equal the prior for specific latent variables) when computing the final posterior parameters. Furthermore, Miao et al. (2016) alternated between optimizing the ap- proximate posterior parameters and the generative model parameters, while we optimize all parame- ters simultaneously. for the Gaussian means and variances. We initialize the bias parameters to zero in order to start with centered Gaussian and piecewise constant priors. The encoder will adapt these priors as learning progresses, using the gating mechanism to turn on/off latent dimensions.Let z be the vector of latent variables sampled according to the approximate posterior distribu- tion. Given z, the decoder Dec(w, z) outputs a distribution over words in the document:The variational hierarchical recurrent encoder- decoder (VHRED) model has previously been pro- posed for dialogue modeling and natural language generation ( Serban et al., 2017bSerban et al., , 2016b). The model decomposes dialogues using a two-level hi- erarchy: sequences of utterances (e.g. sentences), and sub-sequences of tokens (e.g. words). Let w n be the n'th utterance in a dialogue with N utter- ances. Let w n,m be the m'th word in the n'th utter- ance from vocabulary V given as a 1-of-|V | binary encoding. Let M n be the number of words in the n'th utterance. For each utterance n = 1, . . . , N , the model generates a latent variable z n . Condi- tioned on this latent variable, the model then gen- erates the next utterance:where R is a parameter matrix and b is a parameter vector corresponding to the bias for each word to be learned. This output probability distribution is combined with the KL divergences to compute the lower-bound in eq. (1). See Appendix C. Our baseline model G-NVDM is an improve- ment over the original NVDM proposed by Mnih where θ are the model parameters. VHRED con- sists of three RNN modules: an encoder RNN, a context RNN and a decoder RNN. The en- coder RNN computes an embedding for each ut- terance. This embedding is fed into the context RNN, which computes a hidden state summariz- ing the dialogue context before utterance n: h con n−1 . This state represents the additional conditioning information, which is used to compute the prior distribution over z n :where f prior is a PDF parametrized by both θ and h con n−1 . A sample is drawn from this distribution:. This sample is given as input to the decoder RNN, which then computes the out- put probabilities of the words in the next utterance. The model is trained by maximizing the varia- tional lower-bound, which factorizes into indepen- dent terms for each sub-sequence (utterance):where distribution Q ψ is the approximate posterior distribution with parameters ψ, computed simi- larly as the prior distribution but further condi- tioned on the encoder RNN hidden state of the next utterance. The original VHRED model ( Serban et al., 2017b) used Gaussian latent variables. We re- fer to this model as G-VHRED. The first model we propose uses piecewise constant latent vari- ables instead of Gaussian latent variables. We re- fer to this model as P-VHRED. The second model we propose takes advantage of the representation power of both Gaussian and piecewise constant la- tent variables. This model samples both a Gaus- sian latent variable z Table 1: Test perplexities on three document mod- eling tasks: 20-NewGroup (20-NG), Reuters cor- pus (RCV1) and CADE12 (CADE). Perplexities were calculated using 10 samples to estimate the variational lower-bound. The H-NVDM models perform best across all three datasets.gaussian n and a piecewise la- tent variable z piecewise n independently conditioned on the context RNN hidden state:where f prior, gaussian and f prior, piecewise are PDFs parametrized by independent subsets of parame- ters θ. We refer to this model as H-VHRED.We evaluate the proposed models on two types of natural language processing tasks: document modeling and dialogue natural language genera- tion. All models are trained with back-propagation using the variational lower-bound on the log- likelihood or the exact log-likelihood. We use the first-order gradient descent optimizer Adam ( Kingma and Ba, 2015) with gradient clipping ( Pascanu et al., 2012Tasks We use three different datasets for docu- ment modeling experiments. First, we use the 20 News-Groups (20-NG) dataset ). Second, we use the Reuters corpus (RCV1-V2), using a version that con- tained a selected 5,000 term vocabulary. As in previous work Larochelle and Lauly, 2012), we transform the original word frequencies using the equation log(1 + TF), where TF is the original word fre- quency. Third, to test our document models on text from a non-English language, we use the Brazilian Portuguese CADE12 dataset (Cardoso-Cachopo, 2007). For all datasets, we track the validation bound on a subset of 100 vectors randomly drawn from each training corpus.Training All models were trained using mini- batches with 100 examples each. A learning rate of 0.002 was used. Model selection and early stop- ping were conducted using the validation lower- bound, estimated using five stochastic samples per validation example. Inference networks used 100 units in each hidden layer for 20-NG and CADE, and 100 for RCV1. We experimented with both 50 and 100 latent random variables for each class of models, and found that 50 latent variables per- formed best on the validation set. For H-NVDM we vary the number of components used in the PDF, investigating the effect that 3 and 5 pieces had on the final quality of the model. The number environment project  science  project  gov  built  flight  major  high  lab  based  technology  mission  earth  world  launch  include  form  field  science  scale  working  nasa  sun  build  systems  special  gov  technical  area   Table 2: Word query similarity test on 20 News- Groups: for the query 'space", we retrieve the top 10 nearest words in word embedding space based on Euclidean distance. H-NVDM-5 asso- ciates multiple meanings to the query, while G- NVDM only associates the most frequent meaning. Results In Table 1, we report the test docu- ment perplexity: exp(− 1 D 1 n Ln log P θ (x n ). We use the variational lower-bound as an approxima- tion based on 10 samples, as was done in (Mnih and Gregor, 2014). First, we note that the best baseline model (i.e. the NVDM) is more competi- tive when both the prior and posterior models are learnt together (i.e. the G-NVDM), as opposed to the fixed prior of ( Miao et al., 2016). Next, we observe that integrating our proposed piecewise variables yields even better results in our docu- ment modeling experiments, substantially improv- ing over the baselines. More importantly, in the 20-NG and Reuters datasets, increasing the num- ber of pieces from 3 to 5 further reduces perplex- ity. Thus, we have achieved a new state-of-the- art perplexity on 20 News-Groups task and -to the best of our knowledge -better perplexities on the CADE12 and RCV1 tasks compared to us- ing a state-of-the-art model like the G-NVDM. We also evaluated the converged models using an non- parametric inference procedure, where a separate approximate posterior is learned for each test ex- ample in order to tighten the variational lower- bound. H-NVDM also performed best in this eval- uation across all three datasets, which confirms that the performance improvement is due to the piecewise components. See appendix for details.In Table 2, we examine the top ten highest ranked words given the query term "space", using the decoder parameter matrix. The piecewise vari- ables appear to have a significant effect on what is uncovered by the model.In the case of "space", the hybrid with 5 pieces seems to value two senses of the word-one related to "outer space" (e.g., "sun", "world", etc.) and another related to the dimen- sions of depth, height, and width within which things may exist and move (e.g., "area", "form", "scale", etc.). On the other hand, G-NVDM ap- pears to only capture the "outer space" sense of as a conversation between a user and a technical support assistant -the model must generate the next appropriate response in the dialogue. For ex- ample, when it is the turn of the technical support assistant, the model must generate an appropriate response helping the user resolve their problem.  Figure 2, both G-NVDM and H-NVDM-5 learn representations which disentangle the topic clusters on 20-NG. However, G-NVDM appears to have more dis- persed clusters and more outliers (i.e. data points in the periphery) compared to H-NVDM-5. Al- though it is difficult to draw conclusions based on these plots, these findings could potentially be ex- plained by the Gaussian latent variables fitting the latent factors poorly.Task We evaluate VHRED on a natural language generation task, where the goal is to generate re- sponses in a dialogue. This is a difficult prob- lem, which has been extensively studied in the recent literature (Ritter et al., 2011;Lowe et al., 2015;Sordoni et al., 2015;Li et al., 2016;Serban et al., 2016b,a). Dialogue response generation has recently gained a significant amount of atten- tion from industry, with high-profile projects such as Google SmartReply ( Kannan et al., 2016) and Microsoft Xiaoice ( Markoff and Mozur, 2015). Even more recently, Amazon has announced the Alexa Prize Challenge for the research community with the goal of developing a natural and engaging chatbot system (Farber, 2016).We evaluate on the technical support response generation task for the Ubuntu operating system. We use the well-known Ubuntu Dialogue Corpus ( Lowe et al., 2015Lowe et al., , 2017, which consists of about 1/2 million natural language dialogues extracted from the #Ubuntu Internet Relayed Chat (IRC) channel. The technical problems discussed span a wide range of software-related and hardware- related issues. Given a dialogue history -suchWe evaluate the models using the activity-and entity-based metrics designed specifically for the Ubuntu domain ( Serban et al., 2017a). These metrics compare the activities and entities in the model generated responses with those of the ref- erence responses; activities are verbs referring to high-level actions (e.g. download, install, unzip) and entities are nouns referring to technical ob- jects (e.g. Firefox, GNOME). The more activities and entities a model response overlaps with the reference response (e.g. expert response) the more likely the response will lead to a solution.Training The models were trained to maxi- mize the log-likelihood of training examples us- ing a learning rate of 0.0002 and mini-batches of size 80. We use a variant of truncated back- propagation. We terminate the training procedure for each model using early stopping, estimated using one stochastic sample per validation exam- ple. We evaluate the models by generating dia- logue responses: conditioned on a dialogue con- text, we fix the model latent variables to their me- dian values and then generate the response using a beam search with size 5. We select model hyper- parameters based on the validation set using the F1 activity metric, as described earlier.It is often difficult to train generative models for language with stochastic latent variables (Bowman et al., 2015;Serban et al., 2017b). For the latent variable models, we therefore experiment with reweighing the KL divergence terms in the variational lower-bound with values 0.25, 0.50, 0.75 and 1.0. In addition to this, we linearly in- crease the KL divergence weights starting from zero to their final value over the first 75000 train- ing batches. Finally, we weaken the decoder RNN by randomly replacing words inputted to the de- coder RNN with the unknown token with 25% probability. These steps are important for effec- tively training the models, and the latter two have been used in previous work by Bowman et al. (2015) and Serban et al. (2017b). HRED (Baseline): We compare to the HRED model ( Serban et al., 2016b): a sequence-to- sequence model, shown to outperform other es- tablished models on this task, such as the LSTM RNN language model ( Serban et al., 2017a). The HRED model's encoder RNN uses a bidirectional GRU RNN encoder, where the forward and back- ward RNNs each have 1000 hidden units. The context RNN is a GRU encoder with 1000 hidden units, and the decoder RNN is an LSTM decoder with 2000 hidden units. 2 The encoder and con- text RNNs both use layer normalization ( Ba et al., 2016). 3 We also experiment with an additional rectified linear layer applied on the inputs to the decoder RNN. As with other hyper-parameters, we choose whether to include this additional layer based on the validation set performance. HRED, as well as all other models, use a word embedding dimensionality of size 400.G-HRED: We compare to G-VHRED, which is VHRED with Gaussian latent variables ( Serban et al., 2017b). G-VHRED uses the same hyper- parameters for the encoder, context and decoder RNNs as the HRED model. The model has 100 Gaussian latent variables per utterance. P-HRED: The first model we propose is P- VHRED, which is VHRED model with piecewise constant latent variables. We use n = 3 number of pieces for each latent variable. P-VHRED also uses the same hyper parameters for the encoder, context and decoder RNNs as the HRED model. Similar to G-VHRED, P-VHRED has 100 piece- wise constant latent variables per utterance.H-HRED: The second model we propose is H- VHRED, which has 100 piecewise constant (with n = 3 pieces per variable) and 100 Gaussian la- tent variables per utterance. H-VHRED also uses the same hyper-parameters for the encoder, con- text and decoder RNNs as HRED.Results: The results are given in Table 3. All latent variable models outperform HRED w.r.t. both activities and entities. This strongly suggests that the high-level concepts represented by the latent variables help generate meaningful, goal- directed responses. Furthermore, each type of latent variable appears to help with a different aspects of the generation task. G-VHRED per- forms best w.r.t. activities (e.g. download, install and so on), which occur frequently in the dataset. This suggests that the Gaussian latent variables learn useful latent representations for frequent ac- tions. On the other hand, H-VHRED performs best w.r.t. entities (e.g. Firefox, GNOME), which are often much rarer and mutually exclusive in the dataset. This suggests that the combination of Gaussian and piecewise latent variables help learn useful representations for entities, which could not be learned by Gaussian latent variables alone. We further conducted a qualitative analysis of the model responses, which supports these conclu- sions. See Appendix G. 4 In this paper, we have sought to learn rich and flexible multi-modal representations of latent vari- ables for complex natural language processing tasks. We have proposed the piecewise constant distribution for the variational autoencoder frame- work. We have derived closed-form expressions for the necessary quantities required for in the au- toencoder framework, and proposed an efficient, differentiable implementation of it. We have in- corporated the proposed piecewise constant dis- tribution into two model classes -NVDM and VHRED -and evaluated the proposed models on document modeling and dialogue modeling tasks. We have achieved state-of-the-art results on three document modeling tasks, and have demonstrated substantial improvements on a dialogue modeling task. Overall, the results highlight the benefits of incorporating the flexible, multi-modal piece- wise constant distribution into variational autoen- coders. Future work should explore other natural language processing tasks, where the data is likely to arise from complex, multi-modal latent factors.2 Since training lasted between 1-3 weeks for each model, we had to fix the number of hidden units during preliminary experiments on the training and validation datasets. 3 We did not apply layer normalization to the decoder RNN, because several of our colleagues have found that this may hurt the performance of generative language models.The authors acknowledge NSERC, Canada Re- search Chairs, CIFAR, IBM Research, Nuance Foundation and Microsoft Maluuba for fund- ing.Alexander G. Ororbia II was funded by a NACME-Sloan scholarship. The authors thank Hugo Larochelle for sharing the News- Group 20 dataset.The authors thank Lau- rent Charlin, Sungjin Ahn, and Ryan Lowe for constructive feedback. This research was en- abled in part by support provided by Calcul Qubec (www.calculquebec.ca) and Com- pute Canada (www.computecanada.ca).The majority of work on VAEs propose to parametrize z -both the prior and approximate posterior (encoder) -as a multivariate Gaussian variable. However, the multivariate Gaussian is a uni-modal distribution and can therefore only rep- resent one mode in latent space. Furthermore, the multivariate Gaussian is perfectly symmetric with a constant kurtosis. These properties are problem- atic if the latent variables we aim to represent are inherently multi-modal, or if the latent variables follow complex, non-linear probability manifolds (e.g. asymmetric distributions or heavy-tailed dis- tributions). For example. the frequency of topics in news articles could be represented by a continu- ous probability distribution, where each topic has its own island of probability mass; sports and pol- itics topics might each be clustered on their own separate island of probability mass with zero or lit- tle mass in between them. Due to its uni-modal na- ture, the Gaussian distribution can never represent such probability distributions. As another exam- ple, ambiguity and uncertainty in natural language conversations could similarly be represented by is- lands of probability mass; given the question How do I install Ubuntu on my laptop?, a model might assign positive probability mass to specific, un- ambiguous entities like Ubuntu 4.10 and to well- defined procedures like installation using a DVD.In particular, certain entities like Ubuntu 4.10 are now outdated -these entities occur rarely in prac- tice and should be considered rare events. When modeling such complex, multi-modal latent dis- tributions, the mapping from multivariate Gaus- sian latent variables to outputs -i.e. the condi- tional distribution P θ (w n |z) -has to be highly non-linear in order to compensate for the simplis- tic Gaussian distribution and capture the natural latent factors in an intermediate layer of the model. However, it is difficult to learn such non-linear mappings when using the variational bound in eq.(1), as it incurs additional variance from sampling the latent variable z. Consequently, such models are likely to converge on solutions that do not cap- ture salient aspects of the latent variables, which in turn leads to a poor fit of the output distribution.To train the model using the re-parametrization trick, we need to generate z = f ( where ∼ Uniform(0, 1). To do so, we employ inverse trans- form sampling (Devroye, 1986), which requires finding the inverse of the cumulative distribution function (CDF). We derive the CDF of eq. (2):Next, we derive its inverse:Armed with the inverse CDF, we can now draw a sample z:In addition to sampling, we need to compute the Kullback-Leibler (KL) divergence between the prior and approximate posterior distributions of the piecewise constant variables. We assume both the prior and the posterior are piecewise constant distributions. We use the prior superscript to de- note prior parameters and the post superscript to denote posterior parameters (encoder model pa- rameters). The KL divergence between the prior and posterior can be computed using a sum of in- tegrals, where each integral inside the sum corre- sponds to one constant segment:In order to improve training, we further trans- form the piecewise constant latent variables to lie within the interval [−1, 1] after sampling: z = 2z − 1. This ensures the input to the decoder RNN has mean zero initially.The KL-terms may be interpreted as regularizers of the parameter updates for the encoder model ( Kingma and Welling, 2014). These terms encour- age the posterior distributions to be similar to their corresponding prior distributions, by limiting the amount of information the encoder model trans- mits regarding the output.The complete NVDM architecture is defined as:As described in the model section, the probability distribution of the generative model factorizes as:where ⊗ is the Hadamard product, •• is an operator that combines the Gaussian and the Piecewise variables and Dec(w, z) is the decoder model. 5 As a result of using the re-parametrization trick and choice of prior, we calculate the latent variable z through the two samples, 0 and 1 . f (•) is a non-linear activation function, which was the parametrized linear rectifier (with a learnable "leak" parameters) for the 20 News-Groups ex- periments and the softsign function, or f (v) = v/(1 + |v|), for Reuters and CADE. The decoder model Dec(z) outputs a probability distribution over words conditioned on z. In this case, we de- fine g(•) as the softmax function (omitting the bias term c for clarity) computed as:where θ are the model parameters. VHRED uses three RNN modules: an encoder RNN, a context RNN and a decoder RNN. First, each utterance is encoded into a vector by the encoder RNN:where f enc θ is either a GRU or a bidirectional GRU function. The last hidden state of the encoder RNN is given as input to the context RNN. The context RNN uses this state to updates its internal hidden state: con con con con enc The decoder's output is used to calculate the first term in the variational lower-bound: log P θ (W |z). The prior and posterior distributions are used to compute the KL term in the variational lower- bound. The lower-bound is:where f con θ is a GRU function taking as input two vectors. This state conditions the prior distribution over z n :where the KL term is the sum of the Gaussian and piecewise KL-divergence measures:where f prior is a PDF parametrized by both θ and h con n−1 . Next, a sample is drawn from this distribu- tion: z n ∼ P θ (z n |w &lt;n ). The sample and context state are given as input to the decoder RNN: where f dec θ is the LSTM gating function taking as input four vectors. The output distribution is com- puted by passing h dec n,m through an MLP f mlp θ , an affine transformation and a softmax function:and found that models with n = 3 were easier to train. Therefore, we use n = 3 pieces for both P-VHRED and H-VHRED. For all models, we compute the log-likelihood and variational lower-bound costs starting from the second utterance in each dialogue.where O ∈ R |V |×d is the word embedding matrix for the output distribution with embedding dimen- sionality d ∈ N.As mentioned in the model section, the approxi- mate posterior is conditioned on the encoder RNN state of the next utterance:where f post is a PDF parametrized by ψ and h enc n,Mn (i.e. the future state of the encoder RNN after pro- cessing w n ).For the Gaussian latent variables, we use the interpolation gating mechanism described in the main text for the approximate posterior. We ex- perimented with other mechanisms for controlling the gating variables, such as defining α µ and α σ to be a linear function of the encoder. However, this did not improve performance in our preliminary experiments.Piecewise Constant Variable Interpolation We conducted initial experiments with the interpola- tion gating mechanism for the approximate pos- terior of the piecewise constant latent variables. However, we found that this did not improve per- formance.Dialogue Modeling We use the Ubuntu Di- alogue Corpus v2.0 extracted January, 2016: http://cs.mcgill.ca/ ˜ jpineau/ datasets/ubuntu-corpus-1.0/.For the HRED model we found that an addi- tional rectified linear units layer decreased perfor- mance on the validation set according to the activ- ity F1 metric. Hence we test HRED without the rectified linear units layer. On the other hand, for all VHRED models we found that the additional rectified linear units layer improved performance on the validation set. For P-VHRED, we found that a final weight of one for the KL divergence terms performed best on the validation set. For G-VHRED and H-VHRED, reweighing the KL di- vergence terms with a final value 0.25 performed best on the validation set. We conducted prelimi- nary experiments with n = 3 and n = 5 pieces, Iterative Inference For the document modeling experiments, our results and conclusions depend on how tight the variational lower-bound is. As such, it is in theory possible that some of our mod- els are performing much better than reported by the variational lower-bound on the test set. There- fore, we use a non-parametric iterative inference procedure to tighten the variational lower-bound, which aims to learn a separate approximate poste- rior for each test example. The iterative inference procedure consists of simple stochastic gradient descent (no more than 100 steps), with a learning rate of 0.1 and the same gradient rescaling used in training. For 20 News-Groups, the iterative in- ference procedure is stopped on a test example if the bound does not improve over 10 iterations. For Reuters and CADE, the iterative inference proce- dure is stopped if the bound does not improve over 5 iterations. During iterative inference the param- eters of the model, as the well as the generated prior, are all fixed. Only the gradients of the varia- tional lower-bound with respect to generated pos- terior model parameters (i.e. the mean and vari- ance of the Gaussian variables, and the piecewise components, a i ) are used to update the posterior model for each document (using a freshly drawn sample for each inference iteration step).Note, this form of inference is expensive and re- quires additional meta-parameters (e.g. a step-size and an early-stopping criterion). We remark that a simpler, and more accurate, approach to inference might perhaps be to use importance sampling.The results based on iterative inference are re- ported in Table 5. As Section 6.1, we find that H-NVDM outperforms the G-NVDM model. This confirms our previous conclusions.In our current examples, it appears that the H- NVDM with 5 pieces returns more general words. For example, as evidenced in Table 4, in the case of "government", the baseline seems to value the plural form of the word (which is largely based on morphology) while the hybrid model actually Gaussian variables to capture other aspects of the data, as we found was the case with names (e.g., "jesus", "kent", etc.). pulls out meaningful terms such as "federal", "pol- icy", and "administration". Approximate Posterior Analysis We present an additional analysis of the approximate poste- rior on 20 News-Groups, in order to understand what the models are capturing. For a test example, we calculate the squared norm of the gradient of the KL terms w.r.t. the word embedding inputted to the approximate posterior model. The higher the squared norm of the gradients of a word is, the more influence it will have on the posterior approximation (encoder model). For every test example, we count the top 5 words with highest squared gradients separately for the multivariate Gaussian and piecewise constant latent variables. 6 The results shown in Table 6, illustrate how the piecewise variables capture different aspects of the document data. The Gaussian variables were orig- inally were sensitive to some of the words in the table. However, in the hybrid model, nearly all of the temporal words that the Gaussian variables were once more sensitive to now more strongly affect the piecewise variables, which themselves also capture all of the words that were origi- nally missed This shift in responsibility indicates that the piecewise constant variables are better equipped to handle certain latent factors. This ef- fect appears to be particularly strong in the case of certain nationality-based adjectives (e.g., "ameri- can", "israeli", etc.). While the G-NVDM could model multi-modality in the data to some degree, this work would be primarily done in the model's decoder. In the H-NVDM, the piecewise vari- ables provide an explicit mechanism for captur- ing modes in the unknown target distribution, so it makes sense that the model would learn to use the piecewise variables instead, thus freeing up the Ubuntu Experiments We present test examples -dialogue context and model responses gener- ated using beam search -for the Ubuntu models in Table 7. The examples qualitatively illustrate the differences between models. First, we observe that HRED tends to generate highly generic re- sponses compared to all the latent variable mod- els. This supports the quantitative results reported in the main text, and suggests that modeling the latent factors through latent variables is critical for this task. Next, we observe that H-VHRED tends to generate relevant entities and commands -such as mount command, xserver-xorg, static ip address and pulseaudio in examples 1-4. On the other hand, G-VHRED tends to be better at gen- erating appropriate verbs -such as list, install, pastebin and reboot in examples 1-3 and example 5. Qualitatively, P-VHRED model appears to per- form somewhat worse than both G-VHRED and H-VHRED. This suggests that the Gaussian latent variables are important for the Ubuntu task, and therefore that the best performance may be ob- tained by combining both Gaussian and piecewise latent variables together in the H-VHRED model.Twitter Experiments We also conducted a di- alogue modeling experiment on a Twitter corpus, extracted from based on public Twitter conversa- tions ( Ritter et al., 2011). The dataset is split into training, validation, and test sets, containing re- spectively 749,060, 93,633 and 9,399 dialogues each. On average, each dialogue contains about 6 utterances (dialogue turns) and about 94 words. We pre-processed the tweets using byte-pair en- coding ( Sennrich et al., 2016) with a vocabulary consisting of 5000 sub-words.We trained our models with a learning rate of 0.0002 and mini-batches of size 40 or 80. 7 As for the Ubuntu experiments, we used a variant of truncated back-propagation and apply gradient clipping. We experiment with G-VHRED and H- VHRED. Similar to ( Serban et al., 2017b), we use a bidirectional GRU RNN encoder, where the for- ward and backward RNNs each have 1000 hid- Table 5: Comparative test perplexities on various document datasets (50 latent variables). Note that document probabilities were calculated using 10 samples to estimate the variational lower-bound. den units. We experiment with context RNN en- coders with 500 and 1000 hidden units, and find that that 1000 hidden units reach better perfor- mance w.r.t. the variational lower-bound on the validation set. The encoder and context RNNs use layer normalization ( Ba et al., 2016). We exper- iment with decoder RNNs with 1000, 2000 and 4000 hidden units (LSTM cells), and find that 2000 hidden units reach better performance. For the G-VHRED model, we experiment with latent multivariate Gaussian variables with 100 and 300 dimensions, and find that 100 dimensions reach better performance. For the H-VHRED model, we experiment with latent multivariate Gaussian and piecewise constant variables each with 100 and 300 dimensions, and find that 100 dimensions reach better performance. We drop words in the decoder with a fixed drop rate of 25% and multi- ply the KL terms in the variational lower-bound by a scalar, which starts at zero and linearly increases to 1 over the first 60,000 training batches. Note, unlike the Ubuntu experiments, the final weight of the KL divergence is exactly one (hence the bound is tight). ents w.r.t. each word embedding. The higher the sum of the squared gradients of a word is, the more influence it will have on the posterior approxima- tion (encoder model). For every test dialogue, we count the top 5 words with highest squared gradi- ents separately for the multivariate Gaussian and piecewise constant latent variables. 8 The results are shown in Table 8. The piece- wise constant latent variables clearly capture dif- ferent aspects of the dialogue compared to the Gaussian latent variables. The piecewise constant variable approximate posterior encodes words re- lated to time (e.g. weekdays and times of day) and events (e.g. parties, concerts, Easter). On the other hand, the Gaussian variable approximate posterior encodes words related to sentiment (e.g. laugh- ter and appreciation) and acronyms, punctuation marks and emoticons (i.e. smilies). We also con- duct a similar analysis on the document models evaluated in Sub-section 6.1, the results of which may be found in the Appendix.Our hypothesis is that the piecewise constant la- tent variables are able to capture multi-modal as- pects of the dialogue. Therefore, we evaluate the models by analyzing what information they have learned to represent in the latent variables. For each test dialogue with n utterances, we condition each model on the first n − 1 utterances and com- pute the latent posterior distributions using all n utterances. We then compute the gradients of the KL terms of the multivariate Gaussian and piece- wise constant latent variables w.r.t. each word in the dialogue. Since the words vectors are dis- crete, we compute the sum of the squared gradi-
