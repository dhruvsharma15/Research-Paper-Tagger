Automated story generation is the problem of automat- ically selecting a sequence of events, actions, or words that can be told as a story. To date, most story genera- tion systems have used symbolic planning (Meehan 1976;Lebowitz 1987;Pérez y Pérez and Sharples 2001;Porteous and Cavazza 2009;Riedl and Young 2010) or case-based reasoning (Gervás et al. 2005). While these automated story generation systems were able to produce impressive results, they rely on a human-knowledge engineer to provide sym- bolic domain models that indicated legal characters, actions, and knowledge about when character actions can and cannot be performed; these systems are limited to only telling sto- ries about topics that are covered by the domain knowledge. Consequently, it is difficult to determine whether the quality of the stories produced by these systems is a result of the algorithm or good knowledge engineering.Open story generation ( Li et al. 2013) is the problem of automatically generating a story about any topic with- out a priori manual domain knowledge engineering. Open story generation requires an intelligent system to either learn a domain model from available data ( Li et al. 2013;Roemmele et al. 2017) or to reuse data and knowledge avail- able from a corpus (Swanson and Gordon 2012).In this paper, we explore the use of recurrent encoder-decoder neural networks (e.g., Sequence2Sequence (Sutskever, Vinyals, and Le 2014)) for open story genera- tion. An encoder-decoder RNN is trained to predict the next token(s) in a sequence, given one or more input tokens. The network architecture and set of weights θ comprise a genera- tive model capturing and generalizing over patters observed in the training data. For open story generation, we must train the network on a dataset that encompasses as many story topics as possible. For this work, we use a corpus of movie plot summaries extracted from Wikipedia (Bamman, O'Connor, and Smith 2014) under the premise that the set of movies plots on Wikipedia covers the range of topics that people want to tell stories about.In narratological terms, an event is a unit of story featuring a world state change (Prince 1987). Textual story corpora, including Wikipedia movie plot corpora, is comprised of unstructured textual sentences. One benefit to dealing with movie plots is its clarity of events that occur, although this is often to the expense of more creative language. Even so, character-or word-level analysis of these sentences would fail to capture the interplay between the words that make up the meaning behind the sentence. Character-and word- level recurrent neural networks can learn to create gram- matically correct sentences but often fail to produce coher- ent narratives beyond a couple of sentences. On the other hand, sentence-level events would be too unique from each other to find any real relationship between them. Even with a large corpus of stories, we would most likely have sequences of sentences that would only ever be seen once. For exam- ple, "Old ranch-hand Frank Osorio travels from Patagonia to Buenos Aires to bring the news of his daughter's demise to his granddaughter Alina." occurs only once in our corpus, so we have only ever seen one example of what is likely to occur before and after it (if anything). Due to event sparsity, we are likely to have poor predictive ability.In order to help maintain a coherent story, one can provide an event representation that is expressive enough to preserve the semantic meaning of sentences in a story corpus while also reducing the sparsity of events (i.e. increasing the po- tential overlap of events across stories and the number of examples of events the learner observes). In this paper, we have developed an event representation that aids in the pro- cess of automated, open story generation. The insight is that if one can extract some basic semantic information from the sentences of preexisting stories, one can learn the skeletons of what "good" stories are supposed to be like. Then, using these templates, the system will be able to generate novel sequences of events that would resemble a decent story.The first contribution of our paper is thus an event repre- sentation and a proposed recurrent encoder-decoder neural network for story generation called event2event. We evaluate our event representation against the naive baseline sentence representation and a number of alternative representations.In event2event, a textual story corpus is preprocessed- sentences are translated into our event representation by ex- tracting the core semantic information from each sentence. Event preprocessing is a linear-time algorithm using a num- ber of natural language processing techniques. The pro- cessed text is then used to train the neural network. How- ever, event preprocessing is a lossy process and the resultant events are not human-readable. To address this, we present a story generation pipeline in which a second neural net- work, event2sentence, translates abstract events back into natural language sentences. The event2sentence network is an encoder-decoder network trained to fill in the missing de- tails necessary for the abstract events to be human-readable.Our second contribution is the overall story generation pipeline in which subsequent events of a story are generated via an event2event network and then translated into natural language using an event2sentence network. We present an evaluation of event2sentence on different event representa- tions and draw conclusions about the effect of representa- tions on the ability to produce readable stories.The remainder of the paper is organized as follows. First, we discuss related work on automated story generation, fol- lowed by an introduction of our event representation. Then we introduce our event2event network and provide an evalu- ation of the event representation in the context of story gen- eration. We show how the event representation can be used in event2sentence to generate a human-readable sentences from events. We end with a discussion of future work and conclusions about these experiments and how our event rep- resentation and event-to-sentence model will fit into our final system. reasoning to identify relevant existing story content in on- line blogs. The Scheherazade system ( Li et al. 2013) uses a crowdsourced corpus of example stories to learn a domain model from which to generate novel stories.Recurrent neural networks can theoretically learn to pre- dict the probability of the next character, word, or sentence in a story. Roemmele and Gordon (Roemmele et al. 2017) use a Long Short-Term Memory (LSTM) network (Hochre- iter and Schmidhuber 1997) to generate stories. They use Skip-thought vectors ( Kiros et al. 2015) to encode sentences and a technique similar to word2vec (Mikolov et al. 2013) to embedded entire sentences into 4,800-dimensional space. They trained their network on the BookCorpus dataset. Khalifa et al. (2017) argue that stories are better generated using recurrent neural networks trained on highly special- ized textual corpora, such as the body of works from a sin- gle, prolific author. However, such a technique is not capable of open story generation.Based off of the theory of script learning (Schank and Abelson 1977), Chambers and Jurafsky (2008) learn causal chains that revolve around a protagonist. They developed a representation that took note of the event/verb that oc- curred and the type of dependency that connected the event to the protagonist (e.g. was the protagonist the object of this event?).Pichotta and Mooney (2016a) developed a 5-tuple event representation of (v, e s , e o , e p , p), where v is the verb, p is a preposition, and e s , e o , and e p are nouns representing the subject, direction object, and prepositional object, re- spectively. Our representation was inspired by this work, al- though we use a slightly different representation. Because it was a paper on script learning, they did not need to convert the event representations back into natural language.Related to automated story generation, the story cloze test (Mostafazadeh et al. 2016) is the task of choosing between two given endings to a story. The story cloze test transforms story generation into a classification problem: a 4-sentence story is given along with two alternative sentences that can be the 5th sentence. State-of-the art story cloze test tech- niques use a combination of word embeddings, sentiment analysis, and stylistic features ( Mostafazadeh et al. 2017).Automated Story Generation has been a research problem of interest since nearly the inception of artificial intelligence. Early attempts relied on symbolic planning (Meehan 1976;Lebowitz 1987;Pérez y Pérez and Sharples 2001;Riedl and Young 2010) or case-based reasoning using ontologies (Gervás et al. 2005). These techniques could only generate stories for predetermined and well-defined domains of char- acters, places, and actions. The creativity of these systems conflated the robustness of manually-engineered knowledge and algorithm suitability.Recently, machine learning has been used to attempt to learn the domain model from which stories can be created or to identify segments of story content available in an ex- isting repository to assemble stories. The SayAnthing sys- tem (Swanson and Gordon 2012) uses textual case-based Automated story generation can be formalized as follows: given a sequence of events, sample from the probability dis- tribution over successor events. That is, simple automated story generation can be expressed as a process whereby the next event is computed by sampling or maximizing P r θ (e t+1 |e t−k , ..., e t−1 , e t ) where θ is the set of parameters of a generative domain model, e i is the event at time i, and where k indicates the size of a sliding window of context, or history.In our work, the probability distribution is produced by a recurrent encoder-decoder network with parameters θ. In this section, we consider what the level of abstraction for the inputs into the network should be such that it produces the best predictive power while retaining semantic knowledge. Event sparsity results in a situation where all event succes- sors have a low probability of occurrence, potentially within a margin of error. In this situation, story generation devolves to a random generation process.Following Pichotta and Mooney (2016a), we developed a 4-tuple event representation v, o, m where v is a verb, s is the subject of the verb, o is the object of the verb, and m is the modifier or "wildcard", which can be a propositional ob- ject, indirect object, causal complement (e.g., in "I was glad that he drove," "drove" is the causal complement to "glad."), or any other dependency unclassifiable to Stanford's depen- dency parser. All words were stemmed. Events are created by first extracting dependencies with Stanford's CoreNLP ( Manning et al. 2014) and locating the appropriate depen- dencies mentioned above. If the object or modifier cannot be identified, we insert the placeholder EmptyP arameter, which we will refer to as ∅ in this paper.Our event translation process can either extract a single event from a sentence or multiple events per sentence. If we were to extract multiple events, it is because there are verbal or sentential conjunctions in the sentence. Consider the sentence "John and Mary went to the store," our al- gorithm would extract two events: go, store, ∅∅ and go, store, ∅∅. The average number of events per sen- tence was 2.69.Our experiments below used a corpus of movie plots from Wikipedia (Bamman, O'Connor, and Smith 2014), which we cleaned to any remove extraneous Wikipedia syntax, such as links for which actors played which characters. This corpus contains 42,170 stories with the average number of sentences per story being 14.515.The simplest form of our event representation is achieved by extracting the verb, subject, object, and modifier term from each sentence. However, there are variations on the event representation that increase the level of abstraction (and thus decrease sparsity) and help the encoder-decoder network predict successor events. We enumerate some of the possible variations below.had the numbering reset after every input-output pair (i.e. every line of data; consistent across two sentences)-or, continued NEs.• Adding Genre Information. We did topic modeling on the entire corpus using Python's Latent Dirichlet Analy- sis 2 set for discovering 100 different categories. We took this categorization as a type of emergent genre classifica- tion. Some clusters had a clear pattern, e.g., "job company work money business". Others were less clear. Each clus- ter was given a unique genre number which was added to the event representation to create a 5-tuple v, o, m, g where s, v, o, and m are defined as above and g is the genre cluster number.We note that other event representations can exist, includ- ing representations that incorporate more information as in (Pichotta and Mooney 2016a). The experiments in the next section show how different representations affect the ability of a recurrent neural network to predict story continuations.The event2event network is a recurrent multi-layer encoder- decoder network based on (Sutskever, Vinyals, and Le 2014). Unless otherwise stated in experiments below, our event2event network is trained with input x = w . The experiments described below seek to determine how different event representations affected event2event predic- tions of the successor event in a story. We evaluated each event representation using two metrics. Perplexity is the measure of how "surprised" a model is by a training set. Here we use it to gain a sense of how well the probabilistic model we have trained can predict the data. Specifically, we built the model using an n-gram length of 1:• Generalized. Each element in the event tuple undergoes further abstraction. Named entities were identified (cf. (Finkel, Grenager, and Manning 2005)), and "PERSON" names were replaced with the tag &lt;NE&gt;n, where n indi- cates the n-th named entity in the sentence. Other named entities were labeled as their NER category (e.g. LOCA- TION, ORGANIZATION, etc.). The rest of the nouns were replaced by the WordNet (Miller 1995) synset two levels up in the inherited hypernym hierarchy, giving us a general category (e.g. self-propelled vehicle.n.01 vs the original word "car" (car.n.01)), while avoiding labeling it too generally (e.g. entity.n.01). Verbs were replaced by VerbNet (Schuler 2005) version 3.2.4 1 frames (e.g. "arrived" becomes "escape-51.1", "transferring" becomes "contribute-13.2-2").where x is a token in the text, and• Named Entity Numbering. There were two ways of numbering the named entities (i.e. people's names) that we experimented with. One way had the named entity numbering reset with every sentence (consistent within sentence)-or, sentence NEs, our "default". The other wayThe larger the unigram perplexity, the less likely a model is to produce the next unigram in a test dataset. The second metric is BLEU score, which compares the similarity between the generated output and the "ground truth" by looking at n-gram precision. The neural network architecture we use was initially envisioned for machine translation purposes, where BLEU is a common evaluation metric. Specifically, we use an n-gram length of 4 and so the score takes into account all n-gram overlaps between the generated and expected output where n varies from 1 to 4 ( Papineni et al. 2002).We use a greedy decoder to produce the final sequence by taking the token with the highest probability at each step.w wherê W is the generated token appended to the hypothesis, S is the input sequence, and w represents the possible output tokens.6. Generalized Bigram. This experiment tests whether RNN history aids in predicting the next event. We modified event2event to give it the event bigram e n−1 , e n and to predict e n+1 , e n+2 . We believe that this experiment could generalize to cases with a e n−k , ..., e n history.For each experiment, we trained a sequence-to-sequence re- current neural net (Sutskever, Vinyals, and Le 2014) us- ing Tensorflow ( Abadi et al. 2015). Each network was trained with the same parameters (0.5 learning rate, 0.99 learning rate decay, 5.0 maximum gradient, 64 batch size, 1024 model layer size, and 4 layers), varying only the in- put/output, the bucket size, the number of epochs and the vocabulary. The neural nets were trained until the decrease in overall loss was less than 5% per epoch. This took be- tween 40 to 60 epochs for all experiments. The data was split into 80% training, 10% validation, and 10% test data. All reported results were evaluated using the the held-out test data.We evaluated 11 versions of our event representation against a sentence-level baseline. Numbers below corre- spond to rows in results Table 1. 7. Generalized Bigram, Continued &lt;NE&gt;s. This experi- ment has the same continued NE numbering as experi- ment #4 had but we trained event2event with event bi- grams.8. Generalized Bigram + Genre. This is a combination of the ideas from experiments #5 and #6: generalized events in event bigrams and with genre added.The following three experiments investigate extracting more than one event per sentence in the story corpus when possible; the prior experiments only use the first event per sentence in the original corpus.9. Generalized Multiple, Sequential. When a sentence yields more than one event, e 1 n , e 2 n , ... where n is the nth sen- tence and e i 0. Original Sentences. As our baseline, we evaluated how well an original sentence can predict its following original sentence within a story.n is the ith event created from the nth sen- tence, we train the neural network as if each event occurs in sequence, i.e., e 1 n predicts e 2 n , e 2 n predicts e 3 n , etc. The last event from sentence n predicts the first event from sentence n + 1.1. Original Words Baseline. We took the most basic, 4-word event representation: v, o, m with no abstraction and using original named entity names.10. Generalized Multiple, Any Order. Here we gave the RNN all orderings of the events produced by a single sentence paired, in turn, with all orderings of each event of the fol- lowing sentence.2. Original Words with &lt;NE&gt;s. Identical to the previous experiment except entity names that were classified as "PERSON" through NER were substituted with &lt;NE&gt;n.3. Generalized. The same 4-word event structure except with named entities replaced and all other words generalized through WordNet or VerbNet, following the procedure de- scribed earlier.11. Generalized Multiple, All to All. In this experiment, we took all of the events produced by a single sentence to- gether as the input, with all of the events produced by its following sentence together as output. For example, if sentence i produced events e To avoid an overwhelming number of experiments, the next set of experiments used the "winner" of the first set of experiments. Subsequent experiments used variations of the generalized event representation (#3), which showed drasti- cally lower perplexity scores. 4. Generalized, Continued &lt;NE&gt;s. This experiment mir- rors the previous with the exception of the number of the &lt;NE&gt;s. In the previous experiment, the num- bers restarted after every event. Here, the numbers con- tinue across input and output. So if event 1 mentioned "Kendall" and event 2 (which follows event 1 in the story) mentioned "Kendall", then both would have the same number for this character.5. Generalized + Genre. This is the same event structure as experiment #3 with the exception of an additional, 5 th pa- rameter in the event: genre. The genre number was used in training for event2event but removed from inputs and out- puts before testing; it artificially inflated BLEU scores be- cause it was easy for the network to guess the genre num- ber as the genre number was weighted equally to other words.The results from the experiments outlined above can be found in Table 1.The original word events had similar perplexity to orig- inal sentences. This parallels similar observations made by Pichotta and Mooney (Pichotta and Mooney 2016b). Delet- ing words did little to improve the predictive ability of our event2event network. However, perplexity improved signif- icantly once character names were replaced by generalized &lt;NE&gt;tags, followed by generalizing other words and verbs.Overall, the generalized events had much better perplex- ity scores, and making them into bigrams-incorporating history-improved the BLEU scores to nearly those of the original word events. Adding in genre information improved perplexity.The best perplexity was achieved when multiple general- ized events were created from sentences as long as all of the events were fed in at the same time (i.e. no order was being forced upon the events that came from the same sentence). The training data was set up to encourage the neural network to correlate all of the events in one sentence with all of the events from the next sentence.Although the events with the original words (with or with- out character names) performed better in terms of BLEU score, it is our belief that BLEU is not the most appropriate metric for event generation because it emphasizes the recre- ation of the input. Overall, BLEU scores are very low for all experiments, attesting to the inappropriateness of the metric. Perplexity is a more appropriate metric for event generation because it correlates with the ability for a model to predict the entire test dataset. Borrowing heavily from the field of language modeling, the recurrent neural network approach to story generation is a prediction problem.Our intuition that the generalized events would perform better in generating successive events bears out in the data. However, greater generalization makes it harder to return events to natural language sentences. We also see that the BLEU scores for the bigram experiments are generally higher than the others. This shows that history matters and that the additional context provided increases the number of n-gram overlaps between the generated and expected out- puts.The movie plots corpus contains numerous sentences that can be interpreted as describing multiple events. Naive implementation of multiple events hurt perplexity because there is no implicit order of events generated from the same sentence; they are not necessarily sequential. When we al- low multiple events from sentences to be followed by all of the events from a subsequent sentence, perplexity improves. Unfortunately, events are not human-readable and must be converted to natural language sentences. Since the conver- sion from sentences to (multiple) events for event2event is a linear and lossy process, the translation of events back to sentences is non-trivial as it requires adding details back in. For example, the event characterize-29.2, male.n.02, feeling.n.01 could, hypothetically, have come from the sentence "Her brother praised the boy for his em- pathy." In actuality, this event came from the sentence "His uncle however regards him with disgust." Complicating the situation, the event2event encoder- decoder network is not guaranteed to produce an event that has ever been seen in the training story corpus. Furthermore, our experiments with event representations for event2event indicate that greater generalization lends to better story gen- eration. However, the greater the generalization, the harder it is to translate an event back into a natural language sentence.In this section we introduce event2sentence, a neural network designed to translate an event into natural lan- guage. The event2event network takes an input event e n = n , v n , o n , v n and samples from a distribution over possi- ble successor events e n+1 = n+1 , v n+1 , o n+1 , m n+1 As before, we use a recurrent encoder-decoder network based on (Sutskever, Vinyals, and Le 2014). The event2sentence network is trained on parallel corpora of sentences from a story corpus and the corresponding events. In that sense, event2sentence is attempting to learn to reverse the lossy event creation process.We envision event2event and event2sentence working together as illustrated in Figure 1. First, a sentence- provided by a human-is turned into one or more events. The event2event network generates one or more successive events. The event2sentence network translates the events back into natural language and presents it to the human reader. The dashed lines and boxes represent future work for filling in story specifics. To continue story generation, event n+1 can be fed back into event2event; the sentence generation is purely for human consumption.The event2sentence experiments in the next section inves- tigate how well different event representations can be "trans- lated" back into natural language sentences.The setup for this set of experiments is almost identical to that of the event2event experiments, with the main difference being that we used PyTorch 3 which more easily lent itself to implementing beam search. The LSTM RNN networks in these experiments use beam search instead of greedy search to aid in finding a more optimal solution while decoding.The beam search decoder works by maintaining a num- ber of partial hypotheses at each step (known as the beam width or B, where B=5). Each of these hypotheses is a po- tential prefix of a sentence. At each step, the B tokens with the highest probabilities in the distribution are used to ex- pand the partial hypotheses. This continues until the end-of- sentence tag is reached.The input for these experiments was the events of a par- ticular representation and the output was a newly-generated sentence based on the input event(s). The models in these experiments were trained on the events paired with the sen- tences they were "eventified" from. In a complete story gen- eration system, the output of the event2event network feeds into the event2sentence network. Examples of this can be seen in Table 3. However, we tested the event2sentence net- work on the same events extracted from the original sen- tences as were used for event2event in order to conduct controlled experiments and compute perplexity and BLEU scores.To test event2sentence with an event representation that used the original words is relatively straight forward. Ex- perimenting on translating generalized events to natural lan- guage sentences was more challenging since we would be forcing the neural net to guess character names, nouns, and verbs.We devised an alternative approach for generalized event2sentence whereby sentences were first partially even- tified. That is, we trained event2sentence on generalized sen- tences where the "PERSON" named entities were replaced by &lt;NE&gt;tags, other named entities were replaced by their NER category, and the remaining nouns were replaced with WordNet synsets. The verbs were left alone since they often do not have to be consistent across sentences within a story. The intuition here is that the character names and particulars of objects and places are highly mutable and do not affect the overall flow of a story as long as they remain consistent.Below, we show an example of a sentence and its partially generalized counterpart. The original sentence Although splitting and pruning the sentences should bring most sentences down to a single event, this isn't always the case. Thus, we ran an event2sentence experiment where we extracted all of the events from the S+P sentences.The remaining craft launches a Buzz droid at the ARC 1 7 0 which lands near the Clone Trooper rear gunner who uses a can of Buzz Spray to dislodge the robot.would be partially generalized toThe results of our event2sentence experiments are shown in Table 2. Although generalizing sentences improves perplex- ity drastically, splitting and pruning sentences yields better BLEU scores when the original words are kept. In the case of event2sentence, BLEU scores make more sense as a met- ric since the task is a translation task. Perplexity in these experiments appears to correspond to vocabulary size. Generalized events with full-length generalized sentences have better BLEU scores than when the original words are used. However, when we work with S+P sentences, the pat- tern flips. We believe that because both S+P and word gen- eralizing methods reduce sparsity of events, when they are combined too much information is lost and the neural net- work struggles to find any distinguishing patterns. Table 3 shows examples from the entire pipeline as it cur- rently exists, that is from one sentence to the next sentence without slot filling (See Figure 1). To get a full sense of how the generalized sentences would read, imagine adding char- acter names and other details as if one were completing a Mad-Libs game.The remaining activity.n.01 launches a happening.n.01 droid at the ORGANIZATION 1 7 0 which property.n.01 near the person.n.01 enlisted person.n.01 rear skilled worker.n.01 who uses a instrumentality.n.03 of happening.n.01 chemical.n.01 to dislodge the device.n.01We also looked at whether event2sentence performance would be improved if we used multiple events per sentence (when possible) instead of the default single event per sen- tence. Alternatively, we automatically split and prune (S+P) sentences; removing prepositional phrases, splitting senten- tial phrases on conjunctions, and, when it does not start with a pronoun (e.g. who), splitting S' (read: S-bar) from its orig- inal sentence and removing the first word. This would allow us to evaluate sentences that would have fewer (ideally one) events extracted from each. For example, The question remains how to determine exactly what char- acter names and noun details to use in place for the &lt;NE&gt;s and WordNet placeholders. In Figure 1, we propose the addi- tion of Working Memory and Long-Term Memory modules. The Working Memory module would retain the character names and nouns in a lookup table that were removed dur- ing the eventification process. After a partially generalized sentence is produced by event2sentence, the system can use the Working Memory lookup table to fill character names and nouns back into the placeholders. The intuition is that from one event to the next, many of the details-especially character names-are likely to be reused.In stories it is common to see a form of turn-taking be- tween characters. For example the two events "John hits An- drew" &amp; "Andrew runs away from John" followed by "John chases Andrew" illustrate the turn-taking pattern. If John was always used as the first named entity, the meaning of the example would be significantly altered. The continuous numbering of named entities (event2event experiment #7) is designed to assist event bigrams with maintaining turn- taking patterns.There are times when the Working Memory will not be able to fill named entity and WordNet synset placeholder slots because the most recent event bigram does not contain the element necessary for reuse. The Long-Term Memory maintains a history of all named entities and nouns that have ever been used in the story and information about how long ago they were last used. See (Martin, Harrison, and Riedl 2016) for a cognitively-plausible event-based memory that can be used to compute the salience of entities in a story. The underlying assumption is that stories are more likely to reuse existing entities and concepts than introduce new entities and concepts.Our model of automated story generation as prediction of successor events is simplistic; it assumes that stories can be generated by a language model that captures generalized patterns of event co-occurrence. Story generation can also be formalized as a planning problem, taking into account communicative goals. In storytelling, a communicative goal can be to tell a story about a particular topic, to include a theme, or to end the story in a particular way. In future work, we plan to replace the event2event network with a reinforce- ment learning process that can perform lookahead to analyze whether potential successor events are likely to lead to com- municative intent being met.grams did not significantly harm the generative model and will likely help with coherence as they incorporate more his- tory into the process, although story coherence is difficult to measure and was not evaluated in our experiments.Although generalization of events away from natural lan- guage appears to help with event successor generation, it poses the problem of making story content unreadable. We introduced a second neural network, event2sentence, that learns to translate events with generalized or original words back into natural language. This is important because it is possible for event2event to generate events that have never occurred (or have occurred rarely) in a story training cor- pus. We maintain that being able to recover human-readable sentences from generalized events is valuable since our event2event experiments show use that they are preferred, and it is necessary to be able to fill in specifics later for dy- namic storytelling. We present a proposed pipeline architec- ture for filling in missing details in automatically generated partially generalized sentences.The pursuit of automated story generation is nearly as old as the field of artificial intelligence itself. Whereas prior ef- forts saw success with hand-authored domain knowledge, machine learning techniques and neural networks provide a path forward toward the vision of open story generation, the ability for a computational system to create stories about any conceivable topic without human intervention other than providing a comprehensive corpus of story texts.
