Gated Autoencoders (GAEs) are a class of unsupervised neural network models especially suited for learning relations between data instances. This capability is useful for tasks like facial expression modeling [Susskind et al., 2011], motion learning [Michalski et al., 2014], and perspective-invariant object recognition [Memisevic, 2012]. Another non-trivial task to which GAEs lend themselves is analogy-making, in which the objective is to produce a data instance X given the three instances A, B, C, and the query "X is to A as B is to C".The aptness of GAEs to model such relations stems from three-way multiplicative (or "gating") connections between two input sources and a hidden mapping layer. The advantage of this type of connections over additive connections (as used commonly in traditional neural networks) is that it allows for more independence between learned representations of content and transformation, respectively [Memisevic, 2013].In other words, an important factor contributing to the successful use of GAEs in relation learning tasks is that they learn representations of relations (we will refer to these representations as mapping codes, see Section 3.1) that are to some degree invariant to the content in the inputs. Given this observation, it is not far-fetched to ask whether models that are better at learning content-invariant mapping codes are also better at performing relation learning tasks.In this paper we propose a novel training objective for GAEs, by extending the classical training objective with a content-invariance regularization (CIR) term that penalizes what we call the cross- reconstruction error (see Section 3.2) between tuples of input pairs with similar mapping codes. This approach leverages the fact that the standard GAE loss tends to encode relations between the inputs in the mapping codes, even if they are only weakly content-invariant. Note that this bootstrapping procedure is completely unsupervised, and does not require labeled input pairs, where the type of transformation is known in advance.We show experimentally that this approach leads to stronger content-invariance in representations of both 2D and 3D rotations of images, compared to standard GAEs. First we compare the cross- reconstruction error of tuples of input image pairs conveying the same rotation angles. We then assess the content-invariance of the mapping space in terms of cluster separation (see Section 4.3.1). Thirdly, we use the mapping codes as inputs in a k-nearest neighbor classification task. Furthermore, the effect of CIR is shown qualitatively using analogy making examples of rotated MNIST and NORB data. Lastly and importantly, we find that CIR does not hinder the standard GAE loss. To the contrary, it is often improved by the proposed regularization term.The paper is organized as follows. Section 2 gives an overview on the usage of the GAE and discusses related models. In Section 3 the GAE is described and the CIR is introduced. The experiment setup including data preparation, training details and evaluation is described in Section 4. Results are presented and discussed in Section 5 and Section 6 gives a conclusion.GAEs utilize multiplicative interactions to learn correlations between or within data instances. This principle was first used in cross-correlation models for motion in vision by Adelson and Bergen [1985] and later for binocular disparity by Sanger [1988]; Qian [1994]; Fleet et al. [1996], based on hand-crafted Gabor filters [Memisevic, 2013].Using multiplicative interactions in combination with filter learning, higher-order neural networks- computing sums over products-were proposed by Rumelhart et al. [1986]; Giles and Maxwell [1987]; Smolensky [1990], all of which were not directly related to vision.Tenenbaum and Freeman [2000] introduced bi-linear models for separating person and pose of face images. Bi-linear models are two-factor models whose outputs are linear in either factor when the other is held constant, a property which also applies to the GAE. Olshausen et al. [2007] proposed another variant of a bi-linear model in order to learn objects and their optical flow. Due to its similar architecture, the Gated Boltzmann Machine (GBM) Hinton, 2007, 2010] can be seen as a direct predecessor of the GAE. GBMs were applied to image pairs for learning transformations, for modeling facial expression [Susskind et al., 2011], and for disentangling facial pose and expression [Reed et al., 2014]. The GAE was introduced by Memisevic [2011] as a derivative of the GBM, as standard learning criteria became applicable through the development of Denoising Autoencoders [Vincent et al., 2010].While the before-mentioned contributions were mostly used to relate different data instances, a series of publications exist which aim to capture within-image correlations based on the GBM [Hinton and others, 2010;Courville et al., 2011], on the GAE [Memisevic, 2011] and on other architectures [Wainwright and Simoncelli, 1999;Hoyer and Hyvärinen, 2002;Hyvärinen and Hoyer, 2000;Karklin and Lewicki, 2006]. These models are closely related to energy models [Hinton, 1981], based on the idea to square filter responses in a network.GAEs were further used for analogy-making on rotated 3D objects and to learn transformation- invariant representations for classification tasks [Memisevic and Exarchakis, 2013;Memisevic, 2012], for parent-offspring resemblance [Dehghan et al., 2014], and for prediction of accelerated motion by stacking more layers in order to learn higher-order derivatives [Michalski et al., 2014].In other domains, the GAE and some variants have been applied in linguistics to learn to negate adjectives [ Rimell et al., 2017], to activity recognition with the Kinekt sensor [Mocanu et al., 2015], to robotics to learn to write numbers [Droniou et al., 2014] and to learning multi-modal mappings between action, sound, and visual stimuli [Droniou et al., 2015].Cheung et al. [2014] extended the common Autoencoder with a penalty term to disentangle object classes and their variations. In Section 3.1, the GAE-as used in our experiments-is described, and Section 3.2 introduces the Content-Invariance Regularization (CIR).Given a set of data pairs (X, Y) with pairs x i , y i ∈ R P , i = 0 . . . N , where each part of pair i is a transformed version of the other. A Gated Autoencoder (GAE) can learn such transformations, representing them in so-called mapping units as mapping codes m i ∈ R L , computed aswhere U, V ∈ R P ×O and W ∈ R O×L are weight matrices, σ is a non-linearity, the operator · denotes the element-wise multiplication, and P is a band-diagonal matrix (see Figure 1 for a depiction of the GAE):In a trained GAE, U and V contain a set of filter pairs constituting the eigenvectors of the training data before and after a transformation. Projecting inputs x and y onto the filter pairs results in a decomposition of the data into subspaces. When U contains the real parts and V contains the imaginary parts of the eigenvectors, a specific transformation between x and y amounts to a number of concurrent subspace rotations in the complex plane. Therefore, the element-wise (i.e. pairwise) multiplication of filter responses in Equation 1 results in rotation detectors which we refer to as "factors".Pooling two neighboring factors with P is an extension to the common GAE (as proposed by Memisevic and Exarchakis [2013]), explicitly separating within-subspace pooling from across- subspace pooling (in the common GAE performed by W only), reducing the complexity in learning W. Using P forces all four corresponding filters to remain in the same subspace, where the two filters within each weight matrix are approximately in quadrature. As a result, the factors after pooling representing relative rotation angles are less sensitive to absolute starting angles. Matrix W pools across subspaces, causing mapping units to learn common transformations in the data. For more details on that, please refer to Memisevic [2012].An instance can be reconstructed given the other instance and a mapping code as˜x as˜ A common way to train a GAE [Memisevic, 2013;Michalski et al., 2014] is to minimize the symmetric reconstruction errorThe symmetric reconstruction error cost function (cf. Equation 5) penalizes the reconstruction error in both the forward and backward transformations for each training pair. However, this does not guarantee a mapping m i , encoding a specific transformation, results in minimal reconstruction error for all pairs exhibiting that transformation. If we would know the transformation exhibited by pairs in the training data, this objective could be easily enforced by selecting pairs with the same transformation (in case of discrete transformation types), or similar transformations (in case of continuous transformation types), and minimizing the reconstruction error of one pair given the mapping code of the other pair. We call this the cross-reconstruction error. In the absence of this knowledge, we don't know a priori which pairs should be selected for minimizing the cross- reconstruction error.However, if we assume that variation in transformation accounts for more variance in the mapping space than variation in content does, it follows that codes that are close in the mapping space are more likely to encode similar transformations than similar content. In that case, reducing the cross- reconstruction error between pairs that are close to each other in the mapping space will increase the content-invariance of the mapping codes. This assumption can of course not be expected to hold in the initial stages of training a GAE, but becomes increasingly likely as the symmetric reconstruction error (the standard GAE loss) decreases during training.This leads us to the following formulation of content-invariance regularization (CIR). Let M ∈ R N ×L denote the mapping codes computed for training pairs (X, Y), such that x i , y i is the data pair yielding code m i . For a given pair i, let m (1) . . . m (N −1) be a reordering of M/m i such thatThen, given a randomly chosen random code m j from the k 1 nearest neighbors m (1) . . . m (k) of m i , x i and y i are cross-reconstructed asand yrespectively. The symmetric cross-reconstruction error is the error between the known instances and their cross-reconstructions:Finally, the symmetric cross-reconstruction error is linearly interpolated with the original symmetric reconstruction error (cf. Equation 5) aswhere the scalar λ controls the strength of the regularization, and is intended as increasing from 0 to 1 over the course of training, according to some scheme. We will return to this issue in Section 4.2.The invariance-regularization is tested quantitatively and qualitatively, on different data sets and for different combinations of training and evaluation data. Section 4.1 introduces the used datasets and data preparation, Section 4.2 gives training details, and the evaluation of the method is described in Section 4.2.The majority of tests is performed on rotated MNIST digits [LeCun et al., 2010]. We use the train set, validation set, and test set split of the original MNIST data set, and thereof we create different trans- formation sets. The MNISTR20 transformation set includes pairs of {−180, −160, −140, . . . , 160} degree rotation angle, resulting in 18 discrete transformation classes. For the MNISTR20/10 set, all angles of the MNISTR20 set are increased by 10 degrees, in order to obtain the complementary classes {−170, −150, . . . , 170}. The MNISTR1 transformation set include pairs whose mutual rotations exhibit all possible discrete angles of [−180, 179] degrees, resulting in 360 classes.For the experiments on rotated 3D objects, the train set of the small NORB dataset 2 is used, which consists of videos of rotating objects in 5 categories (e.g. four-legged animals, cars, planes), with 9 objects in each category. For training the first eight objects, and for testing the ninth objects in each category are used. From the videos, pairwise frames with an azimuth angle of [-20, 20] degrees, with fixed elevation of 30 degrees and fixed lighting condition are extracted.All datasets are contrast-normalized to zero mean and unit variance, a crucial preparation for usage with the GAE [Memisevic and Exarchakis, 2013].The CIR takes a bootstrapping approach relying on a pre-training of the model. Therefore, the strength λ and the number of nearest neighbors k to swap mapping codes with (cf. Section 3.2) should be minimal at the beginning of the training. A scheme which works well for the MNIST data is a linear increase of the parameters during training, where we increase both parameters to their maximum values λ = 1 and k = 10 over 3000 epochs. For the NORB dataset, a stepwise increase of the CIR parameters has shown to work best.The GAE is trained using stochastic gradient descent, where 50% of the input units are turned off at random. Enforcing sparsity on the mapping units and on the factors, as well as weight regularization is crucial for a good performance of the model. A further improvement is reached by limiting the weight norms and by clipping the gradients during training. A cost term is used which penalizes the deviation of the norm of each input filter to the average filter norm, as well as the deviation from zero mean.For all experiments, the GAE is trained on different GAE Data sets, and tested using different Eval Data sets. The Mean Symmetric Reconstruction Error (MSRE) and the Mean Symmetric Cross- Reconstruction Error (MSCRE) are measured by reconstructing the test set of the respective Eval Data using the GAE trained on the training set of the respective GAE Data. The Davies-Bouldin Index (DBI) (see Section 4.3.1) and the Rotation Error (see Section 4.3.2) are evaluated based on the mapping codes inferred by projecting the test set of the respective Eval Data in the mapping space of the GAE trained on the training set of the respective GAE Data.The Davies-Bouldin Index (DB) [Davies and Bouldin, 1979] is a metric for the quality of the clustering of data points. It is commonly used to evaluate clustering algorithms and their parameters. For example, in k-means clustering [Hartigan, 1975], the DB can be used to determine k. DB amounts to the average ratio between the scatter within clusters and the distance between cluster centers. It is minimal, when clusters are of minimal expansion and of maximal mutual distance. When content information does not contribute to the variance in the content-invariant representations of transformations, resulting clusters are of smaller expansion. Therefore, we use the DB as a metric for evaluating the quality of the representations learned by the GAE.The error of the GAEs predicted rotation angles for the Eval Data is evaluated as follows. A K-Nearest Neighbor (KNN) classifier is used to predict the discrete rotation angles for the test set pairs of the Eval Data. From these predictions, the mean distances to the closest true rotation angles is calculated. For example, when the predicted rotation angle for a test pair is −175 degrees, and the true angle is 170 degrees, that pair contributes an error of 15 degrees.  Table 1: Results for the Gated Autoencoder with (GAE+CIR) and without (GAE) content-invariance regularization, on different datasets. Section 4.1 describes the referenced datasets, and Section 4.3 describes the evaluation criteria. The column GAE Data specifies the dataset used to train the GAE, and the column Eval Data shows the dataset used for evaluation.The quantitative results from the experiments are shown in Table 1. The GAE with the content- invariance regularization (CIR) clearly outperforms the GAE without CIR on almost all combinations of GAE Data and Eval Data. In particular, it is interesting to note that the MSRE is also reduced (in all but one case) when using the GAE with CIR. This is an unexpected result, since the training of the GAE without CIR optimizes solely MSRE, rather than MSRE and MSCRE jointly. This suggest that by reducing content-variance in the mapping space, CIR facilitates generalization over transformations. The better MSCRE on the MNISTr20/10 Eval Data set supports this assumption, because none of these rotation angles were in the training set. This comes at the cost of slightly higher MSRE on that set indicating that a higher generalization on transformations results in lower specificity on content types. The lower DBI values of GAE+CIR with respect to GAE show that the clusters formed by pairs with identical transformations (e.g. all instances of numbers rotated by 60 degrees) are more compact in the GAE+CIR. Figure 2 shows analogy making examples for the MNISTr20 dataset. The better quality of the analogies using the invariance-regularization indicates that the mappings learned by the GAE better represent the content-invariant transformations. The fourth group shows an ambiguous transformation, as the "1" in the input may represent both a rotation by 0 and by 180 degrees. This ambiguity is visible in the transformation results: they show two superimposed rotations (particularly noticeable by the two loops in the reconstruction of the "2" in the first row). While the GAE+CIR can obviously not resolve this ambiguity, its reconstruction is less noisy. Figure 3 shows analogy making examples for the NORB dataset similar to Memisevic and Exarchakis [2013], who used a variant of the standard GAE with concatenated video frames, tied weights and "gating noise". Even with this variant, it was not always possible to apply transformations inferred from image pairs in an analogy making task (some inferred rotations lead to noisy non-transformed copies of the source images and, remarkably, the identity transformation sometimes leads to a rotation in the analogy renderings). As our experiments show, with the vanilla GAE [Memisevic, 2011], it is almost impossible to solve this task. Only source images very similar to the query source can be approximately transformed. The CIR mitigates this problem, making it possible to learn content-invariant transformations with the vanilla GAE.  The empirical results reported above show that while GAEs are able to learn mapping codes to transformations that are to some degree content-invariant (especially in 2D image rotations), the content-invariance of the mapping codes can be substantially improved by including a regularization term that penalizes the cross-reconstruction error.Although this approach principally requires knowledge of the transformations exhibited by data pairs (cross-reconstruction error should only be penalized for pairs with similar transformations), we show that this knowledge is not necessary in practice, and that a bootstrapping approach in which the influence of the regularization term is incrementally increased during training allows it to work in unsupervised settings, by relying on the assumption that similar mapping codes represent similar transformations.Interestingly, the results also show that in many cases training with content-invariance regularization also leads to lower standard GAE loss (symmetric reconstruction error), despite the fact that the content-invariance regularization competes with the reconstruction error during training.In this paper we have limited our experiments to learning 2D and 3D object rotations in images. The regularization method is obviously not limited to this type of data and transformations, and future work should assess the merits of the proposed method on other data, and using different transformations.
