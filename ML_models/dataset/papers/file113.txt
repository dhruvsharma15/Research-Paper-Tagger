are two important research areas in speech processing. Tradi- tionally, these two tasks are treated independently and studied by two independent communities, although some researchers indeed work on both areas. Unfortunately, this is not the way that human processes speech signals: we always deci- pher speech content and other meta information together and simultaneously, including languages, speaker characteristics, emotions, etc. This 'multi-task decoding' is based on two foundations: (1) all these human capabilities share the same signal processing pipeline in our aural system, and (2) they are mutually beneficial as the success of one task promotes others' in real life. Therefore, we believe that multiple tasks in speech processing should be performed by a unified artificial intelligence system. This paper focuses on speech and speaker recognition, and demonstrates that these two tasks can be solved by a single unified model.In fact, the relevance of speech and speaker recognition has been recognized by researchers for a long time. On one hand, these two tasks share many common techniques, from the MFCC feature extraction to the HMM modeling; and on the other hand, researchers in both areas have been used to learning from each other. For instance, the success of deep neural networks (DNNs) in speech recognition [1], [2] has motivated the neural model in speaker recognition [3], [4]. Additionally, researchers also know for a long time that employing the knowledge provided by one area often helps improve the other. For instance, i-vectors produced by speaker recognition have been used to improve speech recognition [5], and phone posteriors derived from speech recognition have been utilized to improve speaker recognition [6], [7]. More- over, the combination of these two systems has already gained attention. For instance, speech and speaker joint inference was proposed in [8], and an LSTM-based multi-task model was proposed in [9]. Although highly interesting, all the above research can not be considered as multi-task learning, and the speech and speaker recognition systems are designed, trained and executed independently.The development of deep learning techniques in speech processing provides new hope for multi-task learning. Since 2011, deep recurrent neural networks (RNNs) have become the new state-of-the-art architectures in speech recognition [10], [11], and recently, the same architecture has gained much success in speaker recognition, at least in text-dependent con- ditions [12]. In both the two tasks, deep learning delivers two main advantages: first, the structural depth (multiple layers) extracts task-oriented features, and second, the temporal depth (recurrent connections) accumulates dynamic evidence. Due to the similarity in the model structure, a simple question rises that can we use a single model to perform the two tasks together?Indeed, this 'multi-task learning' has been known working well to boost correlated tasks [13]. For example, in multilin- gual speech recognition, it has been known that sharing low- level layers of DNNs can improve performance on each lan- guage [14]. And in another experiment, phone and grapheme recognition were treated as two correlated tasks [15]. The central idea of multi-task learning in the deep learning era is that correlated tasks can share the same feature extraction, and so the low-level layers of DNNs for these tasks can be shared. However, this feature-sharing architecture does not apply to speech and speaker recognition. This is because these two tasks are actually 'negatively correlated': speech recognition requires features involving as much as content information, with speaker variance removed; while speaker recognition requires features involving as much as speaker information, with linguistic content removed. For these tasks, feature sharing is certainly not applicable. Unfortunately, many tasks are negatively correlated, e.g., language identification and speaker recognition, emotion recognition and speech recognition. Finding a multi-task learning approach that can  Fig. 1. Multi-task recurrent learning for ASR and SRE. F (t) denotes primary features (e.g., Fbanks), P (t) denotes phone identities (e.g., phone posteriors, high-level representations for phones), S(t) denotes speaker identities (e.g., speaker posteriors, high-level representations for speakers). The associated computation is as follows:deal with negatively-correlated tasks is therefore highly desir- able.This paper presents a novel recurrent architecture that can be used to learn negatively-correlated tasks simultaneously. The basic idea is to use the output of one task as part of the input of others. It would be ideal if the output of one task can provide information for others immediately, but this is not feasible in implementation. Therefore the output of one task at the previous time step is used to provide information for others at the current time step. This leads to an inter-task recurrent structure that is similar to conventional RNNs, though the recurrent connections link different tasks. We employed this multi-task recurrent learning to speech and speaker recognition and observed promising results. The idea is illustrated in Fig. 1. We note that a similar multi-task architecture was recently proposed in [9]. The difference is that they focus on speaker adaptation for ASR, while we demonstrated improvement on both ASR and SRE tasks with the joint learning.The rest of the paper is organized as follows: Section II presents the model architecture, and Section III reports the ex- periments. The conclusions plus the future work are presented in Section IV.In the above equations, the W terms denote weight matrices and those associated with cells were set to be diagonal in our implementation. The b terms denote bias vectors. x t and y t are the input and output symbols respectively; i t , f t , o t represent respectively the input, forget and output gates; c t is the cell and m t is the cell output. r t and p t are two output components derived from m t , where r t is recurrent and fed to the next time step, while p t is not recurrent and contributes to the present output only. σ(·) is the logistic sigmoid function, and g(·) and h(·) are non-linear activation functions, often chosen to be hyperbolic. denotes the element-wise multiplication.We start from the single-task models for ASR and SRE. As mentioned, the state-of-the-art architecture for ASR is the re- current neural network, especially the long short-term memory (LSTM) [10]. This model also delivers good performance in SRE [12]. We therefore choose LSTM to build the baseline single-task systems. Particularly, the modified LSTM structure proposed in [11] is used. The network structure is shown in Fig. 2.The basic idea of the multi-task recurrent model, as shown in Fig. 1, is to use the output of one task at the current frame as an auxiliary information to supervise other tasks when processing the next frame. When this idea is materialized as a computational model, there are many alternatives that need to be carefully investigated. In this study, we use the recurrent LSTM model shown in the previous section to build the ASR component and the SRE component, as shown in Fig. 3. These two components are identical in structure and accept the same input signal. The only difference is that they are trained with different targets, one for phone discrimination and the other for speaker discrimination. Most importantly, there are some inter-task recurrent links that combine the two components as a single network, as shown by the dash lines in Fig. 3.Besides the model structure, a bunch of design options need to be chosen. The first question is where the recurrent informa- tion should be extracted. For example, it can be extracted from the cell c t or cell output m t , or from the output component r t or p t , or even from the output y t . Another question is which computation block will receive the recurrent information. It can be simply the input variable x t , but can also be the input gate i t , the output gate o t , the forget gate f t or the non-linear function g(·). Actually, augmenting the recurrent information to x t is equal to feed the information to i t , o t , f t and g(·) simultaneously. Note that a weight matrix is introduced as an extra free parameter for each recurrent information feedback. Moreover, the component that the information is extracted from is not necessarily the same for different tasks, nor is the component that receives the information. However in this study, we simply consider the symmetric structure. and the computation for SRE is as follows:  The proposed method was tested with the WSJ database, which has been labelled with both word transcripts and speaker identities. We first present the ASR and SRE baselines and then report the multi-task model. All the experiments were conducted with the Kaldi toolkit [16].  • Test set: This set involves three datasets (dev93, eval92 and eval93). It consists of 27 speakers and 1, 049 utter- ances. This dataset was used to evaluate the performance of both ASR and SRE. For SRE, the evaluation consists of 21, 350 target trials and 528, 326 non-target trials, constructed based on the test set.With all the above alternatives, the multi-task recurrent model is rather flexible. The structure shown in Fig. 3 is just one simple example, where the recurrent information is extracted from both the recurrent projection r t and the nonrecurrent projection p t , and the information is applied to the non-linear function g(·). We use the superscript a and s to denote the ASR and SRE tasks respectively. The computation for ASR can be expressed as follows: The ASR system was built largely following the Kaldi WSJ s5 nnet3 recipe, except that we used a single LSTM layer for simplicity. The dimension of the cell was 1, 024, and the dimensions of the recurrent and nonrecurrent projections were set to 256. The target delay was 5 frames. The natural stochastic gradient descent (NSGD) algorithm was employed to train the model [17]. The input feature was the 40- dimensional Fbanks, with a symmetric 2-frame window to splice neighboring frames. The output layer consisted of 3, 419 units, equal to the total number of pdfs in the conventional GMM system that was trained to bootstrap the LSTM model. The baseline performance is reported in Table I.   We built two SRE baseline systems: one is an i-vector system and the other is an 'r-vector' system that is based on the recurrent LSTM model.For the i-vector system, the acoustic feature was 39- dimensional MFCCs. The number of Gaussian components of the UBM was 1, 024, and the dimension of i-vectors was 200. For the r-vector system, the architecture was similar to the one used by the LSTM-based ASR baseline, except that the dimension of the cell was 512, and the dimensions of the recurrent and nonrecurrent projections were set to 128. Additionally, there was no target delay. The input of the r- vector system was the same as ASR system, and the output was corresponding to the 282 speakers in the training set. Similar to the work in [3], [4], the speaker vector ('r-vector') was derived from the output of the recurrent and nonrecurrent projections, by averaging the output of all the frames. The dimension was 256.The baseline performance is reported in Table II. It can be observed that the i-vector system generally outperforms the r- vector system. Particularly, the discriminative methods (LDA and PLDA) offer much more significant improvement for the i- vector system than for the r-vector system. This observation is consistent with the results reported in [4], and can be attributed to the fact that the r-vector model has already been learned 'discriminatively' with the LSTM structure. For this reason, we only consider the simple cosine kernel when scoring r- vectors in the following experiments.Feedback ASR SRE Info.partner task. Involving more information from the nonrecurrent projection does not offer consistent benefit. This observation, however, is only based on the present experiments. With more data, it is likely that more information leads to additional gains.For the recurrent information 'receiver', i.e., the component that receives the recurrent information, it seems that for ASR the input gate and the activation function are equally effective, while the output gate seems not so appropriate. For SRE, all results seem good. Again, these observations are just based on a relative small database; with more data, the performance with different configurations may become distinguishable. We report a novel multi-task recurrent learning architecture that can jointly train multiple negatively-correlated tasks. Primary results on the WSJ database demonstrated that the presented method can learn speech and speaker models simul- taneously and improve the performance on both tasks. Future work involves analyzing more factors such as target delay, exploiting partially labelled data, and applying the approach to other negatively-correlated tasks.Due to the flexibility of the multi-task recurrent LSTM structure, it is not possible to evaluate all the configurations. We chose some typical ones and report the results in Table III. We just show the ASR results on the combined dataset mentioned before. Note that the last configure, where the recurrent information is fed to all the gates and the non-linear activation g(·), is equal to augmenting the information to the input variable x.From the results shown in Table III, we first observe that the multi-task recurrent model consistently improves performance on both ASR and SRE, no matter where the recurrent infor- mation is extracted and where it applies. Most interestingly, on the SRE task, the multi-task system can obtain equal or even better performance than the i-vector/PLDA system. This is the first time that the two negatively-correlated tasks are learned jointly in a unified framework and boost each other.For the recurrent information, it looks like the recurrent pro- jection r t is sufficient to provide valuable supervision for the
