Figure 1: A label corruption matrix C (top left) and three matrix estimates for a corrupted CIFAR-10 dataset. Entry C ij is the probability that a label of class i is corrupted to class j, or C ij = p(˜ y = j|y = i). Our estimate matches the true corruption matrix closer than the confusion matrix and the Forward method. Further comparisons and method descriptions are in Section 4.3.Robustness to label noise is set to become an increasingly important property of supervised learning models. With the advent of deep learning, the need for more labeled data makes it inevitable that not all examples will have high- quality labels. This is especially true of data sources that admit automatic label extraction, such as web crawling for images, and tasks for which high-quality labels are expen- sive to produce, such as semantic segmentation or parsing. Additionally, label corruption may arise in data poisoning ( Li et al., 2016;Steinhardt et al., 2017). Both natural and malicious label corruption are known to sharply degrade the performance of classification systems ( Zhu &amp; Wu, 2004).how much can be gained from access to a small set of examples where labels are considered gold standard. This scenario is realistic, as it is usually the case that a number of trusted examples have been gathered in the validation and test sets, and that more could be gathered if necessary.We consider the scenario where we have access to a large set of examples with potentially corrupted labels and determine * Equal contribution 1 University of Chicago 2 Foundational Re- search Institute 3 Toyota Technological Institute at Chicago. Corre- spondence to: Mantas Mazeika &lt;mantas@uchicago.edu&gt;.To leverage the additional information from trusted labels, we propose a new loss correction and empirically verify it on a number of vision and natural language datasets with label corruption. Specifically, we demonstrate recovery from extremely high levels of label noise, including the dire case when the untrusted data has a majority of its labels corrupted. Such severe corruption can occur in adversarial situations like data poisoning, or when the number of classes is large. In comparison to loss corrections that do not employ trusted data ( Patrini et al., 2016), our method is significantly moreThe methods of (Mnih &amp; Hinton, 2012), (Larsen et al., 1998), (Patrini et al., 2016) and (Sukhbaatar et al., 2014) allow for label noise robustness by modifying the model's architecture or by implementing a loss correction. Unlike (Mnih &amp; Hinton, 2012) who focus on binary classification of aerial images and ( Larsen et al., 1998) who assume the labels are symmetric (i.e., that the noise and the labels are independent), ( Patrini et al., 2016) and (Sukhbaatar et al., 2014) consider label noise in the multi-class problem setting with asymmetric labels.In leveraging trusted data, we focus our investigation on the stochastic matrix correction approach used by (Sukhbaatar et al., 2014;Patrini et al., 2016). In this approach, a stochas- tic matrix is applied to the softmax output of a classifier, and the resulting new softmax output is trained to match the noisy labeling. If the stochastic matrix is engineered so as to approximate the label noising procedure, this approach can bring the original output close to the distribution of clean labels, under moderate assumptions.In ( Sukhbaatar et al., 2014), the authors introduce a stochas- tic matrix measuring label corruption, note its inability to be calculated without access to the true labels, and propose a method of forward loss correction. Forward loss correction adds a linear layer to the end of the model and the loss is adjusted accordingly to incorporate learning about the label noise. In the work of ( Patrini et al., 2016), they also make use of the forward loss correction mechanism, and propose an estimate of the label corruption estimation matrix which relies on strong assumptions and no clean labels.We explore two avenues of utilizing the trusted dataset to improve this approach. The first involves directly using the trusted data while training the final classifier. As this could be applied to existing stochastic matrix correction methods, we run ablation studies to demonstrate its effect. The second avenue involves using the additional information conferred by the clean labels to obtain a better matrix to use with the approach. As a first approximation, one could use a normalized confusion matrix of a classifier trained on the untrusted dataset and evaluated on the trusted dataset. We demonstrate, however, that this does not work as well as the estimate used by our method, which we now describe. Contra (Sukhbaatar et al., 2014;Patrini et al., 2016), we make the assumption that during training the model has ac- cess to a small set of clean labels and use this to create our label noise correction. This assumption has been leveraged by others for the purpose of label noise robustness, most no- tably (Veit et al., 2017;Li et al., 2017;Xiao et al., 2015), and tenuously relates our work to the field of semi-supervised learning ( Zhu, 2005;Chapelle et al., 2010). In ( Veit et al., 2017), human-verified labels are used to train a label clean- ing network by estimating the residuals between the noisy and clean labels in a multi-label classification setting. In the multi-class setting that we focus on in this work, ( Li et al., 2017) propose distilling the predictions of a model trained on clean labels into a second network trained on the noisy Our method makes use of D to estimate the K × K ma- trix of corruption probabilities C ij = p(˜ y = j | y = i). Once this estimate is obtained, we use it to train a modified classifier from which we recover an estimate of the desired conditional distribution p(y | x). We call this method the Gold Loss Correction (GLC), so named because we make use of trusted or gold standard labels.To estimate the probabilities p(˜ y | y), we make use of the identityThe left hand side of the equality can be approximated by training a neural network on D. Let˜θLet˜ Let˜θ be the parameters of this network, and letˆpletˆ letˆp(˜ y | x; ˜ θ) be its softmax output vector. Given an example x and its true one-hot label y, the term on the right reduces to p(˜ y | y, x) = p(˜ y | x). In the case where˜ywhere˜ where˜y is conditionally independent of x given y, this further reduces to p(˜ y | x) = p(˜ y | y). In the case where˜ywhere˜ where˜y is not conditionally independent of x given y, we can still approximate p(˜ y | y). We know We find using˜susing˜ using˜s to work well in practice, even for some singular corruption matrices. We can further improve on this method by using the data in the trusted set to train the corrected classifier. On examples from the trusted set encountered during training, we temporarily set C to the identity matrix to turn off the correction. This has the ef- fect of allowing our label correction to handle a degree of instance-dependency in the label noise . A summary of our method is in the algorithm below.. This forces p(˜ y | y, x)p(x | y) = p(˜ y | y)p(x | ˜ y, y).forIntegrating over all x gives us 7:num_examples += 1end for 10:= p(˜ y | y).We approximate the integral on the left with the expectation of p(˜ y | y, x) over the empirical distribution of x given y. We empirically demonstrate GLC on a variety of datasets and architectures under several types of label noise.This is how we estimate our corruption matrix for GLC. The second equality comes from noting that if y is known, the preceding discussion implies p(˜ y | x) = p(˜ y | y, x). This approximation relies onˆponˆ onˆp(˜ y | x) being a good estimate of p(˜ y | x) and on the number of trusted examples of each class.Generating Corrupted Labels. Suppose our dataset has t + u examples. We sample a set of t datapoints D, and the remaining u examples form D, which we probabilistically corrupt according to a true corruption matrix C. Note that we do not have knowledge of which of our u untrusted exam- ples are corrupted. We only know that they are potentially corrupted.Now with C, we follow the method of ( Sukhbaatar et al., 2014;Patrini et al., 2016) to train a corrected classifier. Given the K × 1 softmax output s of our classifier, we define the new outputs as˜sas˜ as˜s := Cs. We then reinitialize θ and train the modeî p(˜ s | x; θ) on the noisy labels with cross- entropy loss. If˜yIf˜ If˜y is conditionally independent of x given y, and if C is nonsingular, then it follows from the invertibility ofTo generate the untrusted labels from the true labels in D, we first obtain a corruption matrix C. Then, for an example with true label i, we sample the corrupted label from the categorical distribution parameterized by the ith row of C.Comparing Loss Correction Methods. The GLC differs from previous loss corrections for label noise in that it rea- sonably assumes access to a high-quality annotation source. Therefore, to compare to other loss correction methods, we ask how each method performs when starting from the same dataset with the same label noise. In other words, the only additional information our method uses is knowledge of which examples are trusted, and which are potentially C is a perfect estimate of C. corrupted.for a vocab size of 5,000.MNIST. The MNIST dataset contains 28 × 28 grayscale images of the digits 0-9. The training set has 50,000 images and the test set has 10,000 images. For preprocessing, we rescale the pixels to a unit range.We train an LSTM with 64 hidden dimensions on this data. We train using the Adam optimizer (Kingma &amp; Ba, 2014) for 3 epochs with batch size 64 and the suggested learning rate of 0.001. For regularization, we use dropout ( Srivastava et al., 2014) on the linear output layer with a keep probability of 0.8.We train a 2-layer fully connected network each with dimen- sion 256. The network is again optimized with Adam for 10 epochs, all while using batches of size 32 and a learning rate of 0.001. For regularization, we use 2 weight decay on all layers with λ = 1 × 10 −6 .CIFAR. The two CIFAR datasets contain 32×32×3 color images. CIFAR-10 has ten classes, and CIFAR-100 has 100 classes. CIFAR-100 has 20 "superclasses" which partition its 100 classes into 20 semantically similar sets. We use these superclasses for hierarchical noise. Both datasets have 50,000 training images and 10,000 testing images. For both datasets, we train a Wide Residual Network (Zagoruyko &amp; Komodakis, 2016) of depth 40. We train for 75 epochs using a widening factor of 2 and stochastic gradient descent with restarts (Loshchilov &amp; Hutter, 2016).Twitter. The Twitter Part of Speech dataset (Gimpel et al., 2011) contains 1,827 tweets annotated with 25 POS tags. The training set has 1,000 tweets, and the test set has 500. We use pretrained 50-dimensional word vectors, and for each token, we concatenate word vectors in a fixed window centered on the token. These form our training and test set. We use a window size of 3, and train a 1-layer fully connected network with hidden size 256, and use the non- linearity from (Hendrycks &amp; Gimpel, 2016). We train using the Adam optimizer for 15 epochs with batch size 64 and learning rate 0.001. For regularization, we use 2 weight decay with λ = 5 × 10 −5 on all but the linear output layer.IMDB. use 2 weight decay with λ = 1 × 10 −4 on the output layer.Forward Loss Correction. The forward correction method from (Patrini et al., 2016) also obtains C by train- ing a classifier on the noisy labels, and using the resulting softmax probabilities. However, this method does not make use of a trusted fraction of the training data. Instead, it uses the argmax at the 97 th percentile of softmax probabilities for a given class as a heuristic for detecting an example that is truly a member of said class. As in the original paper, we replace this with the argmax over all softmax probabilities for a given class on CIFAR-100 experiments. The estimate and using this network to provide soft targets for the un- trusted data. In this way, labels are "distilled" from a neural network. If the classifier's decisions for untrusted inputs are less reliable than the original noisy labels, then the net- work's utility is limited. Thus, to obtain a reliable neural network, a large trusted dataset is necessary. A new classi- fier is trained using labels that are a convex combination of the soft targets and the original untrusted labels.C is then used to train a corrected classifier in the same way as GLC.Forward Gold. To examine the effect of training on trusted labels as done by GLC, we augment the Forward method by replacing its C estimate with the identity on trusted examples. We refer to the resulting method as For- ward Gold, which can be seen as an intermediate method between Forward and GLC.Corruption-Generating Matrices. We consider three types of corruption matrices: corrupting uniformly to all classes, i.e. C ij = 1/K, flipping a label to a different class, and corrupting uniformly to classes which are semantically similar. In order to create a uniform corruption at different strengths, we take a convex combination of an identity ma- trix and the matrix 11 T /K. We refer to the coefficient of 11T /K as the corruption strength for a "uniform" corrup- tion. A "flip" corruption at strength m involves, for each row, giving an off-diagonal column probability mass m and the entries along the diagonal probability mass 1 − m. Fi-  nally, a more realistic corruption is hierarchical corruption. For this corruption, we apply uniform corruption only to semantically similar classes; for example, "bed" may be corrupted to "couch" but not "beaver" in CIFAR-100. For CIFAR-100, examples are deemed semantically similar if they share the same "superclass" or coarse label specified by the dataset creators.Experiments and Analysis of Results. We train the mod- els described in Section 4.2 under uniform, label-flipping, and hierarchical label corruptions at various fractions of trusted data in the dataset. To assess the performance of GLC, we compare it to other loss correction methods and two baselines: one where we train a network only on trusted data without any label corrections, and one where the net- work trains on all data without any label corrections. Ad- ditionally, we report results on a variant of GLC that uses normalized confusion matrices, which we elaborate on in the discussion. We record errors on the test sets at the cor- ruption strengths {0, 0.1, . . . , 1.0}. Since we compute the model's accuracy at numerous corruption strengths, CIFAR experiments involves training over 500 Wide Residual Net- works. In Tables 4 and 5, we report the area under the error curves across corruption strengths {0, 0.1, . . . , 1.0} for all baselines and corrections. A sample of error curves are displayed in Figure 2.Across all experiments, GLC obtains better area under the error curve than the Forward and Distillation methods. The rankings of the other methods and baselines are mixed. On MNIST, training on the trusted data alone outperforms all methods save for GLC and Confusion Matrix, but performs significantly worse on CIFAR-100, even with large trusted fractions. Interestingly, Forward Gold performs worse than Forward on several datasets. We did not observe the same behavior when turning off the corresponding component of GLC, and believe it may be due to variance introduced during training by the difference in signal provided by the Forward method's C estimate and the clean labels. The GLC provides a superior C estimate, and thus may be better able to leverage training on the clean labels. Additional results on SVHN, are in the supplementary materials.Our next benchmark for GLC is to use noisy labels obtained from a weak classifier. This models the scenario of label noise arising from a classification system weaker than one's own, but with access to information about the true labels that one wishes to transfer to one's own system. For example, scraping image labels from surrounding text on web pages provides a valuable signal, but these labels would train a sub-par classifier without correcting the label noise.  Weak Classifier Label Generation. To obtain the labels, we train 40-layer Wide Residual Networks on CIFAR-10 and CIFAR-100 with clean labels for ten epochs each. Then, we sample from their softmax distributions with a tempera- ture of 5, and fix the resulting labels. This results in noisy labels which we use in place of the labels obtained through the uniform, flip, and hierarchical corruption methods. The weak classifiers obtain accuracies of 40% on CIFAR-10 and 7% on CIFAR-100. Despite the presence of highly corrupted labels, we are able to significantly recover per- formance with the use of a trusted set. Note that unlike the previous corruption methods, weak classifier labels have only one corruption strength. Thus, performance is mea- sured in percent error rather than area under the error curve. Results are displayed in Table 6.K classes, a confusion matrix requires at least K 2 trusted examples to estimate all entries of C, whereas GLC requires only K trusted examples.Another problem with using confusion matrices is that nor- malized confusion matrices give a biased estimate of C in the limit, due to using an argmax over class scores rather than randomly sampling a class. This leads to vastly overes- timating the value in the dominant entry of each row, as can be seen in Figure 1. Correspondingly, we found GLC out- performs confusion matrices by a significant margin across nearly all experiments, with a smaller gap in performance on datasets where K, the number of classes, is smaller. Re- sults are displayed in the main tables. We also found that smoothing the normalized confusion matrices was necessary to stabilize training on CIFAR-100.Analysis of Results. Overall, GLC outperforms all other methods in the weak classifier label experiments. The Distil- lation method performs better than GLC by a small margin at the highest trusted fraction, but performs worse at lower trusted fractions, indicating that GLC enjoys superior data efficiency. This is highlighted by GLC attaining a 26.94% error rate on CIFAR-10 with a trusted fraction of 1%, down from the original error rate of 60%. It should be noted, how- ever, that training with no correction attains 28.32% error on this experiment, suggesting that the weak classifier labels have low bias. The improvement conferred by GLC is more significant at higher trusted fractions.Data Efficiency. We have seen that GLC works for small trusted fractions, and we further corroborate its data effi- ciency by turning to the Clothing1M dataset ( Xiao et al., 2015). Clothing1M is a massive dataset with both human- annotated and noisy labels, which we use to compare the data efficiency of GLC to that of Distillation when very few trusted labels are present. The Clothing1M dataset consists of 1 million noisily labeled clothing images obtained by crawling online marketplaces. 50,000 images have human- annotated examples, from which we take subsamples as our trusted set.Confusion Matrices. An intuitively reasonable alterna- tive to GLC is to estimate C by a confusion matrix. To do this, one would train a classifier on the untrusted examples, obtain its confusion matrix on the trusted examples, row- normalize the matrix, and then train a corrected classifier as in GLC. However, GLC is a far more data-efficient and lower-variance method of estimating C. In particular, for For both GLC and Distillation, we first fine-tune a pre- trained 34-layer ResNet on untrusted training examples for four epochs, and use this to estimate our corruption ma- trix. Thereafter, we fine-tune the network for four more epochs on the combined trusted and untrusted sets using the respective method. During fine tuning, we freeze the first seven layers, and train using gradient descent with Nesterov momentum and a cosine learning rate schedule. For prepro- cessing, we randomly crop to a resolution of 224 × 224, and use mirroring. We also upsample the trusted dataset, finding this to give better performance for both methods. 2. As demonstrated by (Guo et al., 2017), modern deep neural network classifiers tend to have overconfident softmax distributions. We found this to be the case with ourˆpourˆ ourˆp(˜ y | x) estimate, despite the higher entropy of the noisy labels, and used the temperature scaling confidence calibration method proposed in the paper to calibratê p(˜ y | x). 3. Suppose we know the base rates of corrupted labels˜blabels˜labels˜b, where˜bwhere˜where˜b i = p(˜ y = i), and the base rate of true labels b of the trusted set. If we posit that C 0 corrupted the labels, then we should have bT . Thus, we may obtain a superior estimate of the corruption matrix by computing a new estimate C = argmin T As shown in Figure 3, GLC outperforms Distillation by a large margin, especially at lower numbers of trusted ex- amples. This is because Distillation requires fine-tuning a classifier on the trusted data alone, which generalizes poorly with very few examples. By contrast, estimating the C matrix can be done with very few examples. Correspond- ingly, we find that our advantage decreases as the number of trusted examples increases.With more trusted labels, performance on Clothing1M satu- rates as evident in Figure 3. We consider the extreme and train on the entire trusted set for Clothing1M. We fine-tune a pre-trained 50-layer ResNeXt (Xie et al., 2016) on un- trusted training examples to estimate our corruption matrix. Then, we fine-tune the ResNeXt on all training examples. During fine-tuning, we use gradient descent with Nesterov momentum. During the first two epochs, we tune only the output layer with a learning rate of 10 −2 . Thereafter, we tune the whole network at a learning rate of 10 −3 for two epochs, and for another two epochs at 10 −4 . Then we apply our loss correction. Now, we fine-tune the entire network at a learning rate of 10 −3 for two epochs, continue training at 10 −4 , and early-stop based upon the validation set. In a previous work, ( Xiao et al., 2015) obtain 78.24% in this setting. However, our method obtains a state-of-the-art ac- curacy of 80.67%, while with this procedure the Forward method only obtains 79.03% accuracy.We found that using the true corruption matrix as our C provides a benefit of 0.96 percentage points in area under the error curve, but neither the confidence calibration nor the base rate incorporation was able to change the performance from the original GLC. This indicates that GLC is robust to the use of uncalibrated networks for estimating C, and that improving its performance may be difficult without directly improving the performance of the neural network used to estimatê p(y | x).Better Performance for Worst-Case Corruption. The uniform corruption that we use in experiments is an exam- ple of worst-case corruption in the sense that the mutual information betweeñ y and y is zero when the corruption strength equals 1.0. We found that training on the trusted dataset only resulted in superior performance at this cor- ruption setting, especially on Twitter. This indicates that it may be possible to devise a re-weighting of the loss on trusted and untrusted examples using information theoretic measures obtained from C that would improve performance in worst-case regimes.Improving C Estimation. For some datasets, the classifierˆp classifierˆ classifierˆp(˜ y | x) may be a poor estimate of p(˜ y | x), presenting a bottleneck in the estimation of C for GLC. To see the extent to which this could impact performance, and whether simple methods for improvingˆpimprovingˆ improvingˆp(˜ y | x) could help, we ran several variants of GLC experiment on CIFAR-100 under the label flipping corruption at a trusted fraction of 5/100 which we now describe. For all variants, we averaged the area under In this work we have shown the impact of having a small set of trusted examples on classifier label robustness. We proposed the Gold Loss Correction (GLC), a method for handling label noise. This method leverages the assumption that the model has access to a small set of correct labels to yield accurate estimates of the noise distribution. In our experiments, GLC surpasses previous label robustness methods across various natural language processing and vision domains which we showed by considering several corruptions and numerous strengths. Consequently, GLC is a powerful, data-efficient label corruption correction.     
