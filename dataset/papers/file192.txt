Many machine learning algorithms assume that the training and test data are independent and identically distributed (i.i.d.). However, this assumption rarely holds in practice as the data is likely to change over time and space. Even though state-of-the-art Deep Convolutional Neural Network features are invari- ant to low level cues to some degree [2,3,4], Donahue et al. [5] showed that they still are susceptible to domain shift. Instead of collecting labelled data and train- ing a new classifier for every possible scenario, unsupervised domain adaptation methods [6,7,8,9,10,1] try to compensate for the degradation in performance by transferring knowledge from labelled source domains to unlabelled target do- mains. A recently proposed CORAL method [1] aligns the second-order statistics of the source and target distributions with a linear transformation. Even though it is "frustratingly easy", it works well for unsupervised domain adaptation. However, it relies on a linear transformation and is not end-to-end: it needs to first extract features, apply the transformation, and then train an SVM classifier in a separate step.In this work, we extend CORAL to incorporate it directly into deep networks by constructing a differentiable loss function that minimizes the difference be- tween source and target correlations-the CORAL loss. Compared to CORAL, our proposed Deep CORAL approach learns a non-linear transformation that is more powerful and also works seamlessly with deep CNNs. We evaluate our method on standard benchmark datasets and show state-of-the-art performance.Previous techniques for unsupervised adaptation consisted of re-weighting the training point losses to more closely reflect those in the test distribution [11,12] or finding a transformation in a lower-dimensional manifold that brings the source and target subspaces closer together. Re-weighting based approaches often as- sume a restricted form of domain shift-selection bias-and are thus not appli- cable to more general scenarios. Geodesic methods [13,7] bridge the source and target domain by projecting source and target onto points along a geodesic path [13], or finding a closed-form linear map that transforms source points to target [7]. [14,8] align the subspaces by computing the linear map that min- imizes the Frobenius norm of the difference between the top n eigenvectors. In contrast, CORAL [1] minimizes domain shift by aligning the second-order statistics of source and target distributions.Adaptive deep neural networks have recently been explored for unsupervised adaptation. DLID [15] trains a joint source and target CNN architecture with two adaptation layers. DDC [16] applies a single linear kernel to one layer to minimize Maximum Mean Discrepancy (MMD) while DAN [17] minimizes MMD with multiple kernels applied to multiple layers. ReverseGrad [18] adds a binary classifier to explicitly confuse the two domains.Our proposed Deep CORAL approach is similar to DDC, DAN, and Re- verseGrad in the sense that a new loss (CORAL loss) is added to minimize the difference in learned feature covariances across domains, which is similar to minimizing MMD with a polynomial kernel. However, it is more powerful than DDC (which aligns sample means only), much simpler to optimize than DAN and ReverseGrad, and can be integrated into different layers or architectures seamlessly.We address the unsupervised domain adaptation scenario where there are no labelled training data in the target domain, and propose to leverage both the deep features pre-trained on a large generic domain (such as Imagenet [19]) and the labelled source data. In the meantime, we also want the final learned features to work well on the target domain. The first goal can be achieved by initializing the network parameters from the generic pre-trained network and fine-tuning it on the labelled source data. For the second goal, we propose to minimize the difference in second-order statistics between the source and target feature activations, i.e. the CORAL loss. Figure 1 shows a sample Deep CORAL architecture using our proposed correlation alignment layer for deep domain adaptation. We refer to Deep CORAL as any deep network incorporating the CORAL loss for domain adaptation. For generalization and simplicity, here we apply the CORAL loss to the f c8 layer of AlexNet [20]. Integrating it to other layers or network architectures should be straight- forward.We first describe the CORAL loss between two domains for a single feature layer.Suppose we are given source-domain training exampleswith labels L S = {y i }, i ∈ {1, ..., L}, and unlabeled target data D T = {u i }, u ∈ R d . Suppose the number of source and target data are n S and n T respectively. Here both x and u are the d-dimensional deep layer activations φ(I) of input I that we are trying to learn. Suppose D ij ij S (D T ) indicates the j-th dimension of the i-th source (target) data example and C S (C T ) denote the feature covariance matrices.We define the CORAL loss as the distance between the second-order statistics (covariances) of the source and target features:where · 2 F denotes the squared matrix Frobenius norm. The covariance matri- ces of the source and target data are given by:where 1 is a column vector with all elements equal to 1. The gradient with respect to the input features can be calculated using the chain rule:We use batch covariances and the network parameters are shared between two networks.We describe our method by taking a multi-class classification problem as the running example. As mentioned before, the final deep features need to be both discriminative enough to train strong classifier and invariant to the difference between source and target domains. Minimizing the classification loss itself is likely to lead to overfitting to the source domain, causing reduced performance on the target domain. On the other hand, minimizing the CORAL loss alone might lead to degenerated features. For example, the network could project all of the source and target data to a single point, making the CORAL loss trivially zero. However, no strong classifier can be constructed on these features. Joint training with both the classification loss and CORAL loss is likely to learn features that work well on the target domain:where t denotes the number of CORAL loss layers in a deep network and λ is a weight that trades off the adaptation with classification accuracy on the source domain. As we show below, these two losses play counterparts and reach an equilibrium at the end of training, where the final features are expected to work well on the target domain.We evaluate our method on a standard domain adaptation benchmark -the Office dataset [6]. The Office dataset contains 31 object categories from an office environment in 3 image domains: Amazon, DSLR, and W ebcam. We follow the standard protocol of [7,17,5,16,18] and use all the labelled source data and all the target data without labels. Since there are 3 domains, we conduct experiments on all 6 shifts, taking one domain as the source and another as the target.In this experiment, we apply the CORAL loss to the last classification layer as it is the most general case-most deep classifier architectures (e.g., convolutional, recurrent) contain a fully connected layer for classification. Applying the CORAL loss to other layers or other network architectures should be straightforward.The dimension of last fully connected layer (f c8) was set to the number of categories (31) and initialized with N (0, 0.005). The learning rate of f c8 is set to 10 times the other layers as it was training from scratch. We initialized the other layers with the parameters pre-trained on ImageNet [19] and kept the original layer-wise parameter settings. In the training phase, we set the batch size to 128, base learning rate to 10 −3 , weight decay to 5 × 10 −4 , and momentum to 0.9. The weight of the CORAL loss (λ) is set in such way that at the end of training the classification loss and CORAL loss are roughly the same. It seems be a reasonable choice as we want to have a feature representation that is both discriminative and also minimizes the distance between the source and target domains. We used Caffe [21] and BVLC Reference CaffeNet for all of our experiments.We compare to 7 recently published methods: CNN [20] (no adaptation), GFK [7], SA [8], TCA [22], CORAL [1], DDC [16], DAN [17]. GFK, SA, and TCA are manifold based methods that project the source and target distributions into a lower-dimensional manifold and are not end-to-end deep methods. DDC adds a domain confusion loss to AlexNet and fine-tunes it on both the source and target domain. DAN is similar to DDC but utilizes a multi-kernel selection method for better mean embedding matching and adapts in multiple layers. For direct comparison, DAN in this paper uses the hidden layer f c8. For GFK, SA, TCA, and CORAL, we use the f c7 feature fine-tuned on the source domain (F T 7 in [1]) as it achieves better performance than generic pre-trained features, and train a linear SVM [8,1]. To have a fair comparison, we use accuracies reported by other authors with exactly the same setting or conduct experiments using the source code provided by the authors.From Table 1 we can see that Deep CORAL (D-CORAL) achieves better average performance than CORAL and the other 6 baseline methods. In three 3 out of 6 shifts, it achieves the highest accuracy. For the other 3 shifts, the margin between D-CORAL and the best baseline method is very small ( 0.7).  To get a better understanding of Deep CORAL, we generate three plots for domain shift A→W. In Figure 2(a) we show the training (source) and testing (target) accuracies for training with vs. without CORAL loss. We can clearly see that adding the CORAL loss helps achieve much better performance on the target domain while maintaining strong classification accuracy on the source domain. In Figure 2(b) we visualize both the classification loss and the CORAL loss for training w/ CORAL loss. As the last fully connected layer is randomly initialized with N (0, 0.005), in the beginning the CORAL loss is very small while the classification loss is very large. After training for a few hundred iterations, these two losses are about the same. In Figure 2(c) we show the CORAL distance between the domains for training w/o CORAL loss (setting the weight to 0). We can see that the distance is getting much larger ( 100 times larger compared to training w/ CORAL loss). Comparing Figure 2(b) and Figure 2(c), we can see that even though the CORAL loss is not always decreasing during training, if we set its weight to 0, the distance between source and target domains becomes much larger. This is reasonable as fine-tuning without domain adaptation is likely to overfit the features to the source domain. Our CORAL loss constrains the distance between source and target domain during the fine-tuning process and helps to maintain an equilibrium where the final features work well on the target domain.In this work, we extended CORAL, a simple yet effective unsupervised do- main adaptation method, to perform end-to-end adaptation in deep neural net- works. Experiments on standard benchmark datasets show state-of-the-art per- formance. Deep CORAL works seamlessly with deep networks and can be easily integrated into different layers or network architectures.
