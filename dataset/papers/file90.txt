Building machines that are able to recognize speech represents a fundamental step towards flexible and natural human-machine interfaces. A primary role for improving such a technology is played by deep learning [1], which has recently contributed to significantly outperform previous GMM/HMM speech recog- nizers [2]. During the last years, deep learning has been rapidly evolving, progressively offering more powerful and robust tech- niques, including effective regularization methods [3,4], im- proved optimization algorithms [5], as well as better network architectures. The early deep learning works in speech recog- nition were mainly based on standard multilayer perceptrons (MLPs) [6,7], while recent systems benefit from more ad- vanced architectures, such as Convolutional Neural Networks (CNNs) [8] and Time Delay Neural Network (TDNN) [9,10].Nevertheless, since speech is inherently a sequential sig- nal, it would be natural to address it with Recurrent Neural Networks (RNNs), which are potentially able to properly cap- ture long-term dependencies. Several works have already high- lighted the effectiveness of RNNs in various speech processing tasks such as speech recognition [11,12,13,14,15], speech enhancement [16], speech separation [17,18] as well as speech activity detection [19]. Recent results in the newborn field of end-to-end speech recognition [20,21] have also shown that RNNs are promising candidates for replacing traditional Hidden Markov Models (HMMs) in DNN/HMM speech recognizers.Training RNNs, however, can be complicated by vanish- ing and exploding gradients, which might impair learning long- term dependencies [22]. Although exploding gradients can ef- fectively be tackled with simple clipping strategies [23], the vanishing gradient problem requires special architectures to be properly addressed. A common approach relies on the so- called gated RNNs, whose core idea is to introduce a gating mechanism for better controlling the flow of the information through the various time-steps. Within this family of archi- tectures, vanishing gradient issues are mitigated by creating ef- fective "shortcuts", in which the gradients can bypass multiple temporal steps.The most popular gated RNNs are Long Short-Term Mem- ory networks (LSTMs) [24], which often achieve state-of-the- art performance in several machine learning tasks. LSTMs rely on a network design consisting of memory cells which are con- trolled by forget, input, and output gates. Despite their effec- tiveness, such a sophisticated gating mechanism might result in an overly complex model that can be tricky to implement efficiently. On the other hand, computational efficiency is a crucial issue for RNNs and considerable research efforts have recently been devoted to the development of alternative archi- tectures [25,26,27]. With this purpose, an attempt to simplify LSTMs has led to a novel model called Gated Recurrent Unit (GRU) [28,29], which is based on just two multiplicative gates. Despite the adoption of a simplified gating mechanism, some works in the literature agree that GRUs and LSTMs provide a comparable performance in different machine learning tasks [29,26,30].This work continues these efforts by further revising GRUs and proposing an architecture potentially more suitable for speech recognition. The contribution of this paper is twofold: First, we propose to remove the reset gate from the network de- sign. Similarly to [31], we found that removing the reset gate does not affect the system performance, since we observed a certain redundancy in the role played by the update and reset gates. Second, we propose to replace hyperbolic tangent (tanh) with Rectified Linear Units (ReLU) activations in the state up- date equation. In the past, such a non-linearity has been avoided for RNNs due to the numerical instabilities caused by the un- boundedness of ReLU activations. However, when coupling our ReLU-based GRU architecture with batch normalization [4], we did not experience such numerical issues. This allows us to take advantage of ReLU neurons, which have been proven effective at further alleviating the vanishing gradient problem as well as speeding up network training.Our results, obtained on different tasks, input features, and noisy conditions show that, in our implementation, the revised architecture reduces the per-epoch training wall-clock time with more than 30%, while improving the recognition performance over all the experimental conditions considered in this work.The GRU architecture is defined by the following equations:In particular, zt and rt are vectors corresponding to the up- date and reset gates respectively, while ht represents the state vector for the current time frame t. The activations of both gates are element-wise logistic sigmoid functions σ(·), which constrain zt and rt to take values ranging from 0 and 1. The candidate state ht is processed with a hyperbolic tangent. The network is fed by the current input vector xt (e.g., a vector with speech features), while the parameters of the model are the ma- trices Wz, Wr, W h (the feed-forward connections) and Uz, Ur, U h (the recurrent weights). The architecture finally includes trainable bias vectors bz, br and b h , which are added before the non linearities.As shown in Eq. 1d, the current state vector ht is a linear in- terpolation between the previous activation ht−1 and the current candidate state ht. The weighting factors are set by the update gate zt, that decides how much the units will update their acti- vations. Note that such a linear interpolation, which is similar to that adopted for LSTMs [24], is the key component for learn- ing long-term dependencies. For instance, if zt is close to one, the previous state is kept unaltered and can remain unchanged for an arbitrary number of time steps. On the other hand, if zt is close to zero, the network tends to favor the candidate state Figure 1: Average activations of update and reset gates for a GRU trained on TIMIT in a chunk of the utterance "sx403" of speaker "faks0". ht, which depends more heavily on the current input and on the closer hidden states. The candidate state ht, also depends on the reset gate rt, which allows the model to delete the past memory by forgetting the previously computed states.The model proposed in this paper is a revised version of the GRUs described above. The main changes concern reset gate and ReLU activations, as outlined in the next sub-sections.Moreover, we believe that a certain redundancy in the acti- vations of reset and update gates might occur when processing speech sequences. For instance, when it is necessary to give more importance to the current information, the GRU model can set small values of rt. A similar effect can also be achieved with the update gate only, by setting small values of zt. The latter so- lution tends to weight more the candidate state ht, which, as de- sired, depends more heavily on the current input and on its more recent history. Similarly, a high value can be assigned either to rt or to zt, in order to place more importance on past states. This redundancy is also highlighted in Fig. 1, where a temporal correlation in the average activations of update and reset gates can be readily appreciated for a GRU trained on TIMIT.The first modification to standard GRUs proposed in this work thus concerns the removal of the reset gate rt, which helps in limiting the redundancy in the gating mechanism. The main benefits of this intervention are related to the improved com- putational efficiency, which is achieved thanks to the reduced number of parameters necessary to reach the performance of a standard GRU.The reset gate can be useful when significant discontinuities occur in the sequence. For language modeling, this may hap- pen when moving from one text to another one which is not- semantically related. In such situations, it is convenient to reset the stored memory in order to avoid taking a decision biased by an uncorrelated history. However, for some specific tasks such a functionality might not be useful. In [31], for instance, removing rt from the GRU model has led to a single-gate archi- tecture called Minimal Gated Recurrent Unit (M-GRU), which achieves a performance comparable to that obtained by standard GRUs in handwritten digit recognition as well as in a sentiment classification task.We argue that the role played by the reset gate should be re- considered also for acoustic modeling in speech recognition. In fact, a speech signal is a sequence that evolves rather slowly (the features are typically computed every 10 ms), in which the past history can virtually always be helpful. Even in the presence of strong discontinuities, for instance observable at the bound- ary between a vowel and a fricative, completely resetting the past memory can be harmful. On the other hand, it is helpful to memorize phonotactic features, since some phone transitions are more likely than others.The second modification consists in replacing the standard hy- perbolic tangent with ReLU activations in the state update equa- tions (Eq. 1c -1d). Tanh activations are, indeed, rather criti- cal since their saturation slows down the training process and causes vanishing gradient issues. The adoption of ReLU-based neurons, which have shown effective in improving such limita- tions, was not so common in the past for RNNs, due to numer- ical instabilities originated by the unbounded ReLU functions applied over long time series. Nevertheless, some recent works have shown that ReLU RNNs can be effectively trained with a proper orthogonal initialization [32].Formally, removing the reset gate and replacing the hyper- bolic tangent function with the ReLU activation now leads to the following update equations:We called this architecture M-reluGRU, to emphasize the mod- ifications carried out on a standard GRU.Batch normalization [4] has been recently proposed in the ma- chine learning community and addresses the so-called internal covariate shift problem by normalizing, for each training mini- batch, the mean and the variance of each layer pre-activations. Such a technique has shown to be crucial both to improve the system performance and to speed-up the training procedure. Batch normalization can be applied to RNNs in different ways.In [33], authors suggest to apply it to feed-forward connections only, while in [34] the normalization step is extended to recur- rent connections, using separate statistics for each time-step. In this work, we tried both approaches and we observed a comparable performance between them. We also noticed that coupling the proposed model with batch-normalization [4] helps in avoiding the numerical issues that often occur when dealing with ReLU RNNs applied to long time se- quences. Batch normalization, indeed, rescales the neuron pre- activations, inherently bounding the values of the ReLU neu- rons. This allows the network to take advantage of the well- known benefits of such activations.To provide an accurate evaluation of the proposed architecture, the experimental validation was conducted using different train- ing datasets, tasks and environmental conditions. A first set of experiments with TIMIT was performed to test the proposed model in a close-talking scenario. The experiments with TIMIT are based on the standard phoneme recognition task, which is aligned with that proposed in the Kaldi s5 recipe [35].A set of experiments was also conducted in a distant-talking scenario with a Wall Street Journal (WSJ) task, to validate our model in a more realistic situation. The reference context was a domestic environment characterized by the presence of non- stationary noise (with an average SNR of about 10dB) and acoustic reverberation (with an average reverberation time T60 of about 0.7 seconds). The original WSJ training dataset was contaminated with a set of impulse responses measured in a real apartment. The test phase was carried out with the DIRHA- English corpus 1 (with both real and simulated datasets), con- sisting of 409 WSJ sentences uttered by six native American speakers in the same apartment. A development set of 310 WSJ sentences uttered by six different speakers was also used for hyperparameter tuning. More details on this dataset and on the impulse responses can be found in [36,37]. over, batch normalization was adopted exploiting the method suggested in [33], as discussed in Sec. 2. The feed-forward connections of the architecture were initialized according to the Glorot initialization [39], while recurrent weights were initial- ized with orthogonal initialization [32]. Similarly to [40], the gain factor γ of batch normalization was initialized to γ = 0.1 and the shift parameter β was initialized to 0.Before training, the sentences were sorted in ascending or- der according to their lengths and, starting from the shortest utterances, minibatches of 8 sentences were progressively pro- cessed by the training algorithm. This sorting approach, besides minimizing zero-paddings when forming mini-batches, exploits a curriculum learning strategy [41] which has been shown to im- prove the performance and to ensure numerical stability of gra- dients. The optimization was done using the Adam algorithm [5], which ran for 22 epochs. The performance on the develop- ment set was monitored after each epoch, while the learning rate was halved when the performance improvement went below a certain threshold. Gradient truncation was not applied, allow- ing the system to learn arbitrarily long time dependences. The speech recognition labels were derived by performing a forced alignment procedure on the original training datasets. See the standard s5 recipe of Kaldi for more details [35].To evaluate the proposed architecture on different input features, three sets of experiments were performed with 39 MFCCs, with 40 mel-filterbank coefficients, as well as with 40 fMLLR features derived by a Speaker Adaptive Training (SAT) procedure [35]. All of these feature vectors were computed ev- ery 10 ms with a frame length of 25 ms.The main hyperparameters of the model (i.e., learning rate, number of hidden layers, hidden neurons per layer, dropout fac- tor) were optimized on the development data. In particular, we guessed some initial values according to our experience, and starting from them we performed a grid search to progressively explore better configurations. A total of 20-25 experiments were conducted for the RNN models. As a result, an initial learning rate of 0.0013 and a dropout factor of 0.2 were cho- sen for all the experiments. The optimal numbers of hidden layers and hidden neurons, instead, depend on the considered dataset/model, and range from 4 to 5 hidden layers with 375- 607 neurons.The proposed system was implemented with Theano [42] and coupled with the Kaldi decoder [35] to form a context- dependent DNN/HMM speech recognizer 2 .The architecture adopted for the experiments consisted of mul- tiple recurrent layers, which were stacked together prior to the final softmax context-dependent classifier. These recurrent lay- ers were bidirectional RNNs [11], which were obtained by con- catenating the forward hidden states (collected by processing the sequence from the beginning to the end) with backward hidden states (gathered by scanning the speech in the reverse time order). Recurrent dropout was used as regularization tech- nique. Since extending standard dropout to recurrent connec- tions hinders learning long-term dependencies, we followed the approach introduced in [38], which tackles this issue by shar- ing the same dropout mask across all the time steps. More- In the following sub-sections, a comparison of the proposed ar- chitecture with other popular RNNs is presented. The results will be reported for the standard close-talking TIMIT dataset and for the distant-talking WSJ task based on the DIRHA En- glish dataset. Table 1 presents the results obtained with the TIMIT dataset.To perform a more accurate comparison of the various architec- tures, at least five experiments varying the initialization seeds were conducted for each RNN model. The results in Table 1 are reported as the average Phone Error Rates (PER%) with their corresponding standard deviations.1 This dataset is being distributed by the Linguistic Data Consortium (LDC). 2 The code used for the experiments can be found at https:// github.com/mravanelli/theano-kaldi-rnn/.  [32], our speech recogni- tion performance results confirm that gated recurrent networks (rows 2-5) still outperform traditional RNNs. We also observed that GRUs tend to slightly outperform the LSTM, in the tasks addressed in these experiments.The M-GRU architecture is the version of GRU without the reset gate. Table 1 highlights that the M-GRU achieves a performance very similar to that obtained with standard GRUs, further confirming our speculation on the negligible role played by the reset gate in a speech recognition application. The last row of Table 1 reports the performance achieved with the pro- posed model, in which, besides removing the reset gate, ReLU activations are used. Results indicate that M-reluGRU consis- tently achieves the best performance across the various input features. A remarkable achievement is the average PER(%) of 14.9% obtained with the proposed architecture using fMLLR features. To the best of our knowledge, such a result yields the best published performance on the TIMIT test-set. Table 2 reports a comparison of the per-epoch training time for all the considered RNNs. Results confirm that the main ad- vantage of the proposed model is its ability to significantly re- duce the training time. In our Theano implementation, running each GRU epoch lasts 580 seconds on an NVIDIA K40 GPU against just 390 seconds taken by the M-reluGRU, with a train- ing time reduction of more than 32%. Table 2 also reports the best architectures obtained after optimizing the hyperparame- ters on the TIMIT development set. Results show that for GRU models the best performance is achieved with 5 hidden layers of 465 neurons. Table 3 and 4 exhibit a trend comparable to that observed for the TIMIT dataset, confirming the effectiveness of the pro- posed architecture also in a more realistic and challenging sce- nario. The results are consistent over both real and simulated data as well as across the different features considered in this study. The reduction of the training time is about 36% (25 min- utes for per-epoch training with the proposed model against 40 minutes spent by the standard GRU).Moreover, the reset gate removal seems to play a more cru- cial role in the addressed distant-talking scenario. If the close- talking results reported in Table 1 highlight comparable error rates between standard GRU and M-GRU, in the distant-talking case we even observe a performance gain when removing the reset gate. We suppose that this behaviour is due to reverbera- tion, which implicitly introduces redundancy in the signal, due to the multiple delayed replicas of each sample. This results in a forward memory effect that smooths energy envelopes as well as sub-band energy contours. Due to this effect, a reset gate mechanism might become ineffective to forget the past.In this paper, we revised standard GRUs for speech recognition purposes. The proposed architecture is a simplified version of a GRU, in which the reset gate is removed and ReLU activa- tions are used. The experiments, conducted on different tasks, features and environmental conditions, have confirmed the ef- fectiveness of the proposed model not only to reduce the com- putational complexity of our implementation (with a reduction of more than 30% of the training time over a standard GRU), but also to slightly improve the recognition performance. Tables 3 and 4 summarize the results obtained with the simu- lated and real parts of the DIRHA English WSJ dataset. For the sake of compactness, only the average performance (obtained by running five experiments with different initialization seeds) is reported, while standard deviations (which range from 0.2% to 0.4%) are omitted.Future efforts will be focused on extending this work to different tasks and larger datasets (e.g., switchboard or Lib- riSpeech) and to test the revised architecture on modern end- to-end systems (such as CTC and attention-based models). To ensure the practical value of the computational gains of our models, these experiments should be replicated using more op- timized implementations.
