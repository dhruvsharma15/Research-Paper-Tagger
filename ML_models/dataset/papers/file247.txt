There has been a strong interest toward obtaining highly efficient deep neural network architectures that maintain strong modeling power for different applications such as self-driving cars and smart- phone applications where the available computing resources are practically limited to a combination of low-power, embedded GPUs and CPUs with limited memory and computing power. The optimal brain damage method [1] was one of the first approaches in this area, where synapses were pruned based on their strengths. Gong et al. [2] proposed a network compression framework where vec- tor quantization was leveraged to shrink the storage requirements of deep neural networks. Han et al. [3] utilized pruning, quantization and Huffman coding to further reduce the storage requirements of deep neural networks. Hashing is another trick utilized by Chen et al. [4] to compress the network into a smaller amount of storage space. Low rank approximation [5,6] and sparsity learning [7][8][9] are other strategies used to sparsify deep neural networks.Recently, Shafiee et al. [10] tackled this problem in a very different manner by proposing a novel framework for synthesizing highly efficient deep neural networks via the idea of evolutionary synthe- sis. Differing significantly from past attempts at leveraging evolutionary computing methods such as genetic algorithms for creating neural networks [11,12], which attempted to create neural net- works with high modeling capabilities in a direct but highly computationally expensive manner, the proposed novel evolutionary deep intelligence approach mimics biological evolution mechanisms such as random mutation, natural selection, and heredity to synthesize successive generations of deep neural networks with progressively more efficient network architectures. The architectural traits of ancestor deep neural networks are encoded via probabilistic 'DNA' sequences, with new offspring networks possessing diverse network architectures synthesized stochastically based on the 'DNA' from the ancestor networks and computational environmental factor models, thus mimicking random mutation, heredity, and natural selection. These offspring networks are then trained, much like one would train a newborn, and have more efficient, more diverse network architectures while achieving powerful modeling capabilities.An important aspect of evolutionary deep intelligence that is particular interesting and worth deeper investigation is the genetic encoding scheme used to mimic heredity, which can have a significant impact on the way architectural traits are passed down from generation to generation and thus impact the quality of descendant deep neural networks. A more effective genetic encoding scheme can facilitate for better transfer of important genetic information from ancestor networks to allow for the synthesis of even more efficient and powerful deep neural networks in the next generation. As such, a deeper investigation and exploration into the incorporation of synaptic clustering into the genetic encoding scheme can be potentially fruitful for synthesizing highly efficient deep neural networks that are more geared for improving not only memory and storage requirements, but also be tailored for devices designed for highly parallel computations such as embedded GPUs.In this study, we introduce a new synaptic cluster-driven genetic encoding scheme for synthesizing highly efficient deep neural networks over successive generations. This is achieved through the in- troduction of a multi-factor synapse probability model where the synaptic probability is a product of both the probability of synthesis of a particular cluster of synapses and the probability of synthesis of a particular synapse within the synapse cluster. This genetic encoding scheme effectively promotes the formation of synaptic clusters over successive generations while also promoting the formation of highly efficient deep neural networks.The proposed genetic encoding scheme decomposes synaptic probability into a multi-factor proba- bility model, where the architectural traits of a deep neural network are encoded probabilistically as a product of the probability of synthesis of a particular cluster of synapses and the probability of synthesis of a particular synapse within the synapse cluster.Cluster-driven Genetic Encoding. Let the network architecture of a deep neural network be ex- pressed by H(N, S), with N denoting the set of possible neurons and S denoting the set of possible synapses in the network. Each neuron n i ∈ N is connected via a set of synapses ¯ s ⊂ S to neuron n j ∈ N such that the synaptic connectivity s i ∈ S is associated with a w i ∈ W denoting its strength. The architectural traits of a deep neural network in generation g can be encoded by a conditional probability given its architecture at the previous generation g − 1, denoted by P (H g |H g−1 ), which can be treated as the probabilistic 'DNA' sequence of a deep neural network.Without loss of generality, based on the assumption that synaptic connectivity characteristics in an ancestor network are desirable traits to be inherited by descendant networks, one can instead en- code the genetic information of a deep neural network by synaptic probability P (S g |W g−1 ), where w k,g−1 ∈ W g−1 encodes the synaptic strength of each synapse s k,g ∈ S g . In the proposed ge- netic encoding scheme, we wish to take into consideration and incorporate the neurobiological phe- nomenon of synaptic clustering [13][14][15][16][17], where the probability of synaptic co-activation increases for correlated synapses encoding similar information that are close together on the same dendrite.To explore the idea of promoting the formation of synaptic clusters over successive generations while also promoting the formation of highly efficient deep neural networks, the following multi- factor synaptic probability model is introduced:where the first factor (first conditional probability) models the probability of the synthesis of a partic- ular cluster of synapses, ¯ s g,c , while the second factor models the probability of a particular synapse, s g,i , within synaptic cluster c. More specifically, the probability P (¯ s g,c |W g−1 ) represents the like- lihood that a particular synaptic cluster, ¯ s g,c , be synthesized as a part of the network architecture in generation g given the synaptic strength in generation g − 1. For example, in a deep convolutional neural network, the synaptic cluster c can be any subset of synapses such as a kernel or a set of kernels within the deep neural network. The probability P (s g,i |w g−1,i ) represents the likelihood of existence of synapse i within the cluster c in generation g given its synaptic strength in generation g − 1. As such, the proposed synaptic probability model not only promotes the persistence of strong synaptic connectivity in offspring deep neural networks over successive generations, but also pro-motes the persistence of strong synaptic clusters in offspring deep neural networks over successive generations.Cluster-driven Evolutionary Synthesis. In the seminal paper on evolutionary deep intelligence by Shafiee et al. [10], the synthesis probability P (H g ) is composed of the synaptic probability P (S g |W g−1 ), which mimic heredity, and environmental factor model F (E) which mimic natural selection by introducing quantitative environmental conditions that offspring networks must adapt to:In this study, (2) is reformulated in a more general way to enable the incorporation of different quan- titative environmental factors over both the synthesis of synaptic clusters as well as each synapse:where F c (·) and F s (·) represents environmental factors enforced at the cluster and synapse levels, respectively.Realization of Cluster-driven Genetic Encoding. In this study, a simple realization of the proposed cluster-driven genetic encoding scheme is presented to demonstrate the benefits of the proposed scheme. Here, since we wish to promote the persistence of strong synaptic clusters in offspring deep neural networks over successive generations, the probability of the synthesis of a particular cluster of synapses, ¯ s g,c is modeled aswhere ⌊·⌋ encodes the truncation of a synaptic weight and Z is a normalization factor to make (4) a probability distribution,The truncation of synaptic weights in the model reduces the influence of very weak synapses within a synaptic cluster on the genetic en- coding process. The probability of a particular synapse, s g,i , within synaptic cluster c, denoted by P (s g,i = 1|w g−1,i ) can be expressed as:where z is a layer-wise normalization constant. By incorporating both of the aforementioned prob- abilities in the proposed scheme, the relationships amongst synapses as well as their individual synaptic strengths are taken into consideration in the genetic encoding process.Evolutionary synthesis of deep neural networks across several generations is performed using the proposed genetic encoding scheme, and their network architectures and accuracies are investigated using three benchmark datasets: MNIST [18], STL-10 [19] and CIFAR10 [20]. The LeNet-5 archi- tecture [18] is selected as the network architecture of the original, first generation ancestor network for MNIST and STL-10, while the AlexNet architecture [21] is utilized for the ancestor network for CIFAR10, with the first layer modified to utilize 5 × 5 × 3 kernels instead of 11 × 11 × 3 kernels given the smaller image size in CIFAR10. The environmental factor model being imposed at different generations in this study is designed to form deep neural networks with progressively more efficient network architectures than its ancestor networks while maintaining modeling accu- racy. More specifically, F c (·) and F s (·) is formulated in this study such that an offspring deep neural network should not have more than 80% of the total number of synapses in its direct ances- tor network. Furthermore, in this study, each kernel in the deep neural network is considered as a synaptic cluster in the synapse probability model. In other words, the probability of the synthesis of a particular synaptic cluster (i.e, P (¯ s g,c |W g−1 )) is modeled as the truncated summation of the weights within a kernel.Results &amp; Discussion. In this study, offspring deep neural networks were synthesized in successive generations until the accuracy of the offspring network exceeded 3%, so that we can better study the changes in architectural efficiency in the descendant networks over multiple generations. Ta- ble 1 shows the architectural efficiency (defined in this study as the total number of synapses of the original, first-generation ancestor network divided by that of the current synthesized network) versus the modeling accuracy at several generations for three datasets. As observed in Table 1, the descendant network at the 13th generation for MNIST was a staggering ∼125-fold more efficient than the original, first-generation ancestor network without exhibiting a significant drop in the test Table 1: Architectural efficiency vs. test accuracy for different generations of synthesized networks. "Gen.", "A-E" and "ACC." denote generation, architectural efficiency, and accuracy, respectively.A-E ACC.Gen.A-E ACC.Gen.A   Table 3: Cluster efficiency of the convolutional layers (layers 1-5) and fully connected layers (layers 6-7) at first and the last reported generations of deep neural networks for CIFAR10.Gen accuracy (∼1.7% drop). This trend was consistent with that observed with the STL-10 results, where the descendant network at the 10th generation was ∼56-fold more efficient than the original, first- generation ancestor network without a significant drop in test accuracy (∼1.2% drop). It also worth noting that since the training dataset of the STL-10 dataset is relatively small, the descendant net- works at generations 2 to 8 actually achieved higher test accuracies when compared to the original, first-generation ancestor network, which illustrates the generalizability of the descendant networks compared to the original ancestor network as the descendant networks had fewer parameters to train. Finally, for the case of CIFAR10 where a different network architecture was used (AlexNet), the descendant network at the 6th generation network was ∼14.4-fold more efficient than the original ancestor network with ∼2% drop in test accuracy, thus demonstrating the applicability of the pro- posed scheme for different network architectures. Embedded GPU Ramifications. Table 2 and 3 shows the cluster efficiency per layer of the synthe- sized deep neural networks in the last generations, where cluster efficiency is defined in this study as the total number of kernels in a layer of the original, first-generation ancestor network divided by that of the current synthesized network. It can be observed that for MNIST, the cluster efficiency of last-generation descendant network is ∼9.7X, which may result in a near 9.7-fold potential speed-up in running time on embedded GPUs by reducing the number of arithmetic operations by ∼9.7-fold compared to the first-generation ancestor network, though computational overhead in other layers such as ReLU may lead to a reduction in actual speed-up. The potential speed-up from the last- generation descendant network for STL-10 is lower compared to MNIST dataset, with the reported cluster efficiency in last-generation descendant network ∼6X. Finally, the cluster efficiency for the last generation descendant network for CIFAR10 is ∼2.8X, as shown in Table 3. These results demonstrate that not only can the proposed genetic encoding scheme promotes the synthesis of deep neural networks that are highly efficient yet maintains modeling accuracy, but also promotes the formation of highly sparse synaptic clusters that make them highly tailored for devices designed for highly parallel computations such as embedded GPUs.
