Recursive neural network models [1] constitute one type of neural structure for obtaining higher- level representations beyond word-level such as phrases or sentences. It works in a bottom-up fashion on tree structures (e.g., parse trees) in which long-term dependency can be to some extent captured. Figure 1 gives a brief illustration about how recursive neural models work to obtain the distributed representation for the short sentence "The movie is wonderful". Suppose h is and h wonderful are the embeddings for tokens is and wonderful. The representation for their parent node VP at second layer is given by:where W and b denote parameters involved in the convolutional function. f(·) is the activation function, usually tanh or sigmod or the rectifier linear function.For NLP tasks, the obtained embeddings could be further fed into task-specific machine learning models 1 , through which parameters are to be optimized. Take sentiment analysis as an example, we could feed the aforementioned sentence embedding into a logistic regression model to classify it as either positive or negative. Embeddings are sometimes more capable of capturing latent semantic meanings or syntactic rules within the text than manually developed features, from which many NLP tasks would benefit (e.g., [2,3]).Such a type of structure suffers some sorts of intrinsic drawbacks. Revisit Figure 1, common sense tells us that tokens like "the", "movie" and "is" do not contribute much to the sentiment decision but word "wonderful" is the key part (and a good machine learning model should have the ability of learning these rules). Unfortunately, the intrinsic structure of recursive neural networks makes it less flexible to get rid of the influence from less sentiment-related tokens. If the keyword "wonderful" hides too deep in the parse tree, for example, as in the sentence "I studied Russia in Moscow, where  Table 1: A brief comparison between SVM and standard neural network models for sentence-level sentiment classification using date set from [4]. Neural network models are trained with L2 reg- ularization, using AdaGrad [5] with minibatches (for details about implementations of recursive networks, please see Section 2). Parameters are trained based on 5-fold cross validation on the train- ing data. We report the best performance searching optimum regularization parameter, optimum batch size for mini-batches and convolutional function. Word embeddings are borrowed from Glove [6] with dimensionality of 300, which generates better performance than word2vect, SENNA [7] and RNNLM [8].all my family think the winter is wonderful", it will takes quite a few convolution steps before the keyword 'wonderful" comes up to the surface, with the consequence that its influence on the final sentence representation could be very trivial. Such an issue, usually referred to as gradient vanishing [9]. is not specific for recursive models, but for most deep learning architectures.When we compare neural models with SVM, one notable weakness of big-of-word based SVM is its inability of considering how words are combined to form meanings (or order information in other words) [10]. But interestingly, such downside of SVM comes with the advantage of resilience in feature managing as the optimization is "flat-expanded". Low weights will be assigned to less- informative evidence, which could further be pushed to zero by regularization. Table 1 gives a brief comparison between unigram based SVM and neural network models for sentence-level sentiment prediction on Pang et al.'s dataset [4], and as can be seen, in this task, standard neural network models underperform SVM 2 .Revisit the form of Equ.1, there are two straws we can grasp at to deal with the aforementioned problem: (1) expecting the learned feature embeddings for less useful words such as the 3 exert very little influence (for example, a zero vector for the best) (2) expecting the compositional parameters W and b are extremely powerful. For the former, it is sometimes hard, as mostly we borrow (or initialize) word embeddings from those trained from large corpus (e.g., word2vec, RNNLM [8,11], SENNA [7]), rather than training embeddings from task-specific objective functions as neural models can be easily over fitted given the small amount of training data 4 .Regarding the latter issue, several alternative compositional functions have been proposed to enable more varieties in composition to cater. Recent proposed approaches include, for example, Matrix- Vector RNN [12], which represents every word as both a vector and a matrix, RNTN [2] which allows greater interactions between the input vectors, and the algorithm presented in [13] which associates different labels (e.g., POS tags, relation tags) with different sets of compositional param- eters. These approaches to some extent enlarge the power of compositional functions.In this paper, we borrow the idea of "weight tuning" from feature based SVM and try to incorporate such idea into neural architectures. To achieve this goal, we propose two recursive neural archi- tectures, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN). The major idea involved in the proposed approaches is to associate each node in the recursive network with additional parameters, indicating how important it is for final decision. For example, we would expect such type of a structure would dilute the influence of tokens like "the" and "movie" but mag- nifies the impact of tokens like "wonderful" and "great" in sentiment analysis tasks. Parameters associated with proposed models are automatically optimized through the objective function man- ifested by the data. The proposed model combines the capability of neural models to capture the local compositional meanings with weight tuning approach to reduce the influence of undesirable information at the same time, and yield better performances in a range of different NLP tasks when compared with standard neural models.The rest of this paper is organized as follows: Section 2 briefly describes the related work. The details of WNN and BENN are illustrated in Section 4 and experimental results are presented in Section 5, followed by a brief conclusion.Distributed representations, calculated based on neural frameworks, are extended beyond token- level, to represent N-grams [14], phrases [2], sentences (e.g., [3,15]), discourse [16,13], para- graphs [17] or documents [18]. Recursive and recurrent [19,20] models constitute two types of commonly used frameworks for sentence-level embedding acquisition. Different variations of recurrent/recursive models are proposed to cater for different scenarios (e.g., [3,2]). Other re- cently proposed approaches included sentence compositional approach proposed in [21], or para- graph/sentence vector [17] where representations are optimized through predicting words within the sentence.Neural network architecture sometimes requires a vector representation of each input token. Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [22,23,24,25], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.Both of the proposed architectures are in this work inspired by the long short-term memory (LSTM) model, first proposed by Hochreiter and Schmidhuber back in 1990s [26,27] to process time se- quence data where there are very long time lags of unknown size between important events 5 . LSTM associates each time with a series of "gates" to determine whether the information from early time- sequence should be forgotten [27] and when current information should be allowed to flow into or out of the memory. LSTM could partially address gradient vanishing problem in recurrent neural models and have been widely used in machine translation [28,29] 3 "Weight Tuning" for Neural Network Let s denote a sequence of token s = {w 1 , w 2 , ..., w n s }. It could be phrases, sentences etc. Each word w is associated with a specific vector embedding e w = {e For any node C in the parse tree, it is associated with representation h C . The basic idea of WNN is to associate each node C with an additional weight variable M C , which is in range (0,1), to denote the importance of current node. Technically, M C is used to pushing the output representation of not-useful node towards the direction of 0 and retain relatively important information.We expect that information regarding the importance of current node (e.g., whether it is relevant to positive/negative sentiment) is embedded in its representation h C . So we use a convolution func- tion to enable this type of information to emerge to the surface from the following compositional functions:where Let output(C) denote the output from node C to its parent. In WNN, output(C) would consider both current information, which is embedded in the embedding h C and its related importance M C . output(C) is therefore given byRecall the example in Figure 1, we have:If the model thinks not too much relevant information embedded in h C , the value of M C would be small, pushing the output vector towards 0. The representations for parents, for example VP and NP in Figure 1, are therefore computed as follows:where W B denotes a K × 2K dimensional matrix and [output(is), output(wonderful)] denotes the concatenation of the two vectors. In an optimum situation, M the and M movie will take the values around 0, leading to the representation of node NP to an around-zero vector.Training WNN For illustration purpose, we use a binary classification task to show how to train WNN. To note, the described training approach applies to other situations (e.g., multi-class classifi- cation, regression) with minor adjustments.In a binary classification task, each sequence is associated with a gold-standard label y s . y s takes value of 1 if positive and 0 otherwise. Standardly, to determine the value of y s , we feed the repre- sentation h s into a logistic regression model:where U T is a 1 * K vector and b denotes the bias. Then by adding the regularization part parame- terized by Q, the loss function J(Θ) for the training dataset is given by:Revisit the example in Figure 1, for any parameter θ to optimize, the calculation for gradient ∂J/∂θ is trivial once ∂[M VP · h VP ]/∂θ and ∂[M NP · h NP ]/∂θ are obtained, which are given by:To note, h VP is embraced in M VP . As all components in Equ 9 are continuous, the gradient can be efficiently obtained from standard backpropagation [31,32]. BENN associates each node with a binary variable B C , which is sampled from a binary distribution parameterized by L C . L C is a scalar fixed to the range of [0,1], indicating the possibility that current node should pass information to its ancestors. L C is obtained in the similar ways as in WNN by using a convolution to project the current representation h C to a scalar lying within [0,1].For smoothing purpose, in BENN, current node C outputs the expectation of embedding h C to its parent, as given by:Take the case in Figure 1 as an example again, vector h NP will therefore follow the following distribution:E[h NP ] can be further obtained based on such distributionh To note, for leaf nodes,Training BENN For training, we again use binary sentiment classification for illustration. For any sentence s with label y s , we haveWith respect to any given parameter θ, the derivative of E[h s ] is further given by:With all components being continuous, the gradient can be efficiently obtained from standard back- propagation.  We perform experiments to better understand the behavior of the proposed models compared with standard neural models (and other variations). To achieve this, we implement our model on problems that require fixed-length vector representations for phrases or sentences.Sentence-level Labels We first perform experiments on dateset from [4]. In this setting, binary labels at the top of sentence constitute the only resource of supervision (to note, it is different from settings described in [2]). All neural models adopt the same settings for fair comparison: L2 reg- ularization, gradient decent based on AdaGrad with mini batch size of 25, tuned parameters for regularization on 5-fold cross validation.For standard neural models, we implement two settings: standard (GLOVE) where word embed- dings are directly fixed to GLOVE and standard (learned) where word embeddings are treated as parameters to optimize in the framework. Additionally, we implemented some recent popular vari- ations of recursive models with more sophisticatedly designed compositional functions, including:• MV-RNN (Matrix-Vector RNN): which was proposed in [12] which represents every node in a parse tree as both a vector and a matrix. Given the vector representation h C 1 , matrix representation V C 1 for child node C 1 , h C 2 and V C 2 for child node C 2 , the vector represen- tation h p and matrix representation V p for parent p are given by:We fix word vector embeddings using SENNA and treat matrix representations as parame- ters to optimize.• RNTN (Recursive Neural Tensor Network): proposed in [2]. Given h C 1 and h C 2 for chil- dren nodes, RNTN computes parent vector h p in the following way:• Label-specific: associate each of the sentence roles (i.e., VP, NP or NN) with a specific composition matrix.We report results in Table 2. As discussed earlier, standard neural models underperform the bag of word models. To note, for derivations of standard neural models such as Standard (learned) and MV- RNN with many more parameters to learn, the performance is even worse due to over-fitting. WNN and BENN , although not significantly output bag of words SVM, generates better results, yielding significant improvement over standard neural models and existing revised versions. Figure 3 illus- trates the automatic learned muted factor M C regarding different nodes in the parse tree based on recursive network. As we can observe , the model is capable of learning the proper weight of vocab- ularies, assigning larger weight values to important sentiment indicators (e.g., wonderful, silly and tedious) and suppressing the influence of less important ones. We attribute the better performance of proposed models over standard neural models to such automatic weight-tuning ability. To note, in this scenario, we are not claiming that we generate state-of-art results using the proposed model. More sophisticated bag-of-word models, for example, (e.g., [33]) can generate better per- formance that what the proposed models achieve. The point we wish to illustrate here is that the proposed models provide a promising perspective over standard neural models due to the "weight tuning" property. And in the cases where more detailed data is available to capture the composition- ally, the proposed models hold promise to generate more compelling results, as we will illustrate in Socher et al's setting for sentiment analysis.Socher et al's settings We now consider Socher et al's dataset [2] for sentiment analysis, where contains gold-standard labels at every phrase node in the parse tree. The task could be considered either as a 5-way fine-grained classification task where the labels are very- negative/negative/neutral/positive/very-positive or a 2-way coarse-way as positive/negative based on labeled dataset. We follow the experimental protocols described in [2] (word embeddings are treated as parameters to learn rather than fixed to externally borrowed embeddings). In this work we only consider labeling the full sentences.In addition to varieties of neural models mentioned in Socher et al's work, we also report the perfor- mance of recently proposed paragraph vector model [17], which first obtains sentence embeddings in an unsupervised manner by predicting words within the context and then feeds the pre-obtained embeddings into a logistic regression model. paragraph vector achieves current the state-of-art performance regarding Socher et al's dataset.Performances are reported in Table 3. As can be seen, the proposed approach slightly underperforms current state-of-art performance achieved by paragraph vector but outperforms all the other versions of recursive neural models, indicating the adding "weight tuning" parameters indeed leads to better compositionally.To note, when there is more comprehensive dataset which we can rely on to obtain the favorable task-specific word embeddings, compositionally plays an important role in deciding whether the review is positive or negative by harnessing local word order information. In that case, neural models exhibit its power in capturing local evidence from the composition, leading to significantly better performance than all bag-of-words based models (i.e., SVM and Bigram Naives Bayes).We move on to sentiment analysis at document level. We use the IMDB dataset proposed by Maas et al. [33]. The dataset consists of 100,000 movie reviews taken from IMDB and each movie review contains several sentences. We follow the experimental protocols described in [33].We first train word vectors from word2vect using the 75,000 training documents. Next we train the compositional functions using the 25,000 labeled documents by keeping the word embedding fixed. We first obtain sentence-level representations using WNN/BENN (recursive   The results of our approach and other baselines are reported in Table 5. As can be seen, for long documents, bag-of-words (both unigram and diagram) perform quite well and it is difficult to beat. Standard neural models again do not generate competent results compared with bag of word models in this task. But by incorporating weighted tuning mechanism, we got much better performance, roughly 5% when compared against standard neural models. Although WNN and BENN still un- derperform current state-of-art model Paragraph Vector [17], they produces better performance than bag-of-word models.Sentiment analysis forces more on the semantic perspective of meaning. Next we turn to a more syntactic oriented task, where we obtain sentence-level representations based on the proposed model to decide the coherence of a given sequence of sentences.We use corpora widely employed for coherence prediction [34,35]. One contains reports on air- plane accidents from the National Transportation Safety Board and the other contains reports about earthquakes from the Associated Press. Standardly, we use pairs of articles, one containing the orig- inal document order which is assumed to be coherent and used as positive examples, and the other a random permutation of the sentences from the same document, which are treated as not-coherent examples. We follow the protocols introduced in [34,36,37] by considering a window approach and feeding the concatenation of representations of adjacent sentences into a logistic regression model, to be classified as either coherent or non-coherent. In test time, we assume that the model makes a right decision if the original document gets a score higher than the one with random permutations. Current state-of-art performance regarding this task is obtained by using standard recursive network as described in [37]. Table 5 illustrates the performance of different models. Entity-grid model [34] generates state-of-art performance among all non-neural network models. As can be seen, neural models perform pretty well in this task when compared against existing feature based algorithm. From the reported results, better sentence representations are obtained by incorporating "weighted tuning" properties, pushing the state of art of this task to the accuracy of 0.936.  In this paper, we propose two revised versions of neural models, WNN and BENN for obtaining higher level feature representations for a sequence of tokens. The proposed framework automat- ically incorporates the concept of "weight tuning" of SVM into the DL architectures which lead to better higher-level representations and generate better performance against standard neural mod- els in multiple tasks. While it still underperforms bag-of-word models in some cases, and the newly proposed paragraph vector approach, it provides as an alternative to existing recursive neural models for representation learning.To note, while we limit our attentions to recursive models in this work, the idea of weight tuning in WNN and BENN, that associates nodes in neural models with additional weighed variables is a general one and can be extended to many other deep learning models with minor adjustment.
