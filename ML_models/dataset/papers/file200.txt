Human activity recognition (HAR) plays an important role in people's daily life for its competence in learning profound high-level knowledge about human activity from raw sensor inputs. Successful HAR applications include home behavior analysis ( Vepakomma et al., 2015), video surveillance ( Qin et al., 2016), gait analysis ( Hammerla et al., 2016), and gesture recognition (Kim and Toomajian, 2016). There are mainly two types of HAR: video-based HAR and sensor-based HAR ( Cook et al., 2013). Video-based HAR analyzes videos or images con- taining human motions from the camera, while sensor-based HAR focuses on the motion data from smart sensors such as an accelerometer, gyroscope, Bluetooth, sound sensors and so on. Due to the thriving development of sensor technology and per- vasive computing, sensor-based HAR is becoming more popu- lar and widely used with privacy well protected. Therefore, in this paper, our main focus is on sensor-based HAR.HAR can be treated as a typical pattern recognition (PR) problem. Conventional PR approaches have made tremendous progress on HAR by adopting machine learning algorithms such as decision tree, support vector machine, naive Bayes, and hidden Markov models ( Lara and Labrador, 2013). It is no won- der that in some controlled environments where there are only a few labeled data or certain domain knowledge is required (e.g. some disease issues), conventional PR methods are fully ca- pable of achieving satisfying results. However, in most daily HAR tasks, those methods may heavily rely on heuristic hand- crafted feature extraction, which is usually limited by human domain knowledge (Bengio, 2013). Furthermore, only shallow features can be learned by those approaches ( Yang et al., 2015), leading to undermined performance for unsupervised and in- cremental tasks. Due to those limitations, the performances of conventional PR methods are restricted regarding classification accuracy and model generalization.Recent years have witnessed the fast development and ad- vancement of deep learning, which achieves unparalleled per- formance in many areas such as visual object recognition, nat- ural language processing, and logic reasoning ( LeCun et al., 2015). Different from traditional PR methods, deep learning can largely relieve the effort on designing features and can learn much more high-level and meaningful features by training an end-to-end neural network. In addition, the deep network structure is more feasible to perform unsupervised and incre- mental learning. Therefore, deep learning is an ideal approach for HAR and has been widely explored in existing work ( Alsheikh et al., 2016;Plötz et al., 2011).Although some surveys have been conducted in deep learn- ing ( LeCun et al., 2015;Schmidhuber, 2015;Bengio, 2013) and HAR ( Lara and Labrador, 2013;Bulling et al., 2014), respec- tively, there has been no specific survey focusing on the inter- sections of these two areas. To our best knowledge, this is the first article to present the recent advance on deep learning based HAR. We hope this survey can provide a helpful summary of existing work, and present potential future research directions.The rest of this paper is organized as follows. In Section 2, we briefly introduce sensor-based activity recognition and ex- plain why deep learning can improve its performance. In Sec- tion 3, 4 and 5, we review recent advance of deep learning based HAR from three aspects: sensor modality, deep model, and ap- plication, respectively. We also introduce several benchmark datasets. Section 6 presents summary and insights on existing work. In Section 7, we discuss some grand challenges and fea- sible solutions. Finally, this paper is concluded in Section 8.where n denotes the length of sequence and n ≥ m.The goal of HAR is to learn the model F by minimizing the discrepancy between predicted activityˆAactivityˆ activityˆA and the ground truth activity A * . Typically, a positive loss function L(F (s), A * ) is constructed to reflect their discrepancy. F usually does not directly take s as input, and it usually assumes that there is a projection function Φ that projects the sensor reading dataTo that end, the goal turns into minimizing the loss function L(F (Φ(d i )), A * ). Fig. 1 presents a typical flowchart of HAR using conven- tional PR approaches. First, raw signal inputs are obtained from several types of sensors (smartphones, watches, Wi-Fi, Bluetooth, sound etc.). Second, features are manually extracted from those readings based on human knowledge ( Bao and Intille, 2004), such as the mean, variance, DC, and amplitude in traditional machine learning approaches ( Hu et al., 2016). Fi- nally, those features serve as inputs to train a PR model to make activity inference in real HAR tasks.HAR aims to understand human behaviors which enable the computing systems to proactively assist users based on their requirement ( Bulling et al., 2014). Formally speaking, suppose a user is performing some kinds of activities belonging to a predefined activity set A:(1) where m denotes the number of activity types. There is a se- quence of sensor reading that captures the activity informationwhere d t denotes the sensor reading at time t.We need to build a model F to predict the activity sequence based on sensor reading swhile the true activity sequence (ground truth) is denoted asConventional PR approaches have made tremendous progress in HAR ( Bulling et al., 2014). However, there are sev- eral drawbacks to conventional PR methods.Firstly, the features are always extracted via a heuristic and hand-crafted way, which heavily relies on human experience or domain knowledge. This human knowledge may help in certain task-specific settings, but for more general environments and tasks, this will result in a lower chance and longer time to build a successful activity recognition system.Secondly, only shallow features can be learned according to human expertise ( Yang et al., 2015). Those shallow features often refer to some statistical information including mean, vari- ance, frequency and amplitude etc. They can only be used to recognize low-level activities like walking or running, and hard to infer high-level or context-aware activities (Yang, 2009). For instance, having coffee is more complex and nearly impossible to be recognized by using only shallow features.Thirdly, conventional PR approaches often require a large amount of well-labeled data to train the model. However, most of the activity data are remaining unlabeled in real applications. Thus, these models' performance is undermined in unsuper- vised learning tasks (Bengio, 2013). In contrast, existing deep generative networks ( Hinton et al., 2006) are able to exploit the unlabeled samples for model training.Moreover, most existing PR models mainly focus on learning from static data; while activity data in real life are coming in stream, requiring robust online and incremental learning.Deep learning tends to overcome those limitations. Fig. 2 shows how deep learning works for HAR with different types of networks. Compared to Fig. 1, the feature extraction and model building procedures are often performed simultaneously in the deep learning models. The features can be learned au- tomatically through the network instead of being manually de- signed. Besides, the deep neural network can also extract high- level representation in deep layer, which makes it more suit- able for complex activity recognition tasks. When faced with a large amount of unlabeled data, deep generative models (Hinton et al., 2006) are able to exploit the unlabeled data for modelet al., 2016a). The RFID can provide more fine-grained infor- mation for more complex activity recognition. It should be noted that object sensors are less used than body- worn sensors due to the difficulty in its deployment. Besides, the combination of object sensors with other types is emerging in order to recognize more high-level activities (Yang, 2009). training. What's more, deep learning models trained on a large- scale labeled dataset can usually be transferred to new tasks where there are few or none labels.In the following sections, we mainly summarize the exist- ing work based on the pipeline of HAR: (a) sensor modality, (b) deep model, and (c) application.Although some HAR approaches can be generalized to all sensor modalities, most of them are only specific to certain types. According to (Chavarriaga et al., 2013), we mainly clas- sify those modalities into three aspects: body-worn sensors, ob- ject sensors, and ambient sensors. Table 1 briefly outlines all the modalities.Ambient sensors are used to capture the interaction between humans and the environment. They are usually embedded in users' smart environment. There are many kinds of ambient sensors such as radar, sound sensors, pressure sensors, and tem- perature sensors. Different from object sensors which measure the object movements, ambient sensors are used to capture the change of the environment.Several literature used ambient sensors to recognize daily ac- tivities and hand gesture ( Wang et al., 2016a;Kim and Toomajian, 2016). Most of the work was tested in the smart home environment. Same as object sensors, the deploy- ment of ambient sensors is also difficult. In addition, ambient sensors are easily affected by the environment, and only certain types of activities can be robustly inferred.Body-worn sensors are one of the most common modalities in HAR. Those sensors are often worn by the users, such as an accelerometer, magnetometer, and gyroscope. The accel- eration and angular velocity are changed according to human body movements; thus they can infer human activities. Those sensors can often be found on smart phones, watches, bands, glasses, and helmets.Body-worn sensors were widely used in deep learning based HAR ( Chen and Xue, 2015;Plötz et al., 2011;Zeng et al., 2014;Jiang and Yin, 2015;Yang et al., 2015). Among those work, the accelerometer is mostly adopted. Gyroscope and magnetometer are also frequently used together with the accelerometer. Those sensors are often exploited to recognize activities of daily liv- ing (ADL) and sports. Instead of extracting statistical and fre- quency features from the movement data, the original signal is directly used as inputs for the network.Some work combined different types of sensors for HAR. As shown in (Hayashi et al., 2015), combining acceleration with acoustic information could improve the accuracy of HAR. Am- bient sensors are also used together with object sensors; hence they can record both the object movements and environment state. ( Vepakomma et al., 2015) designed a smart home envi- ronment called A-Wristocracy, where a large number of fine- grained and complex activities of multiple occupants can be recognized through body-worn, object, and ambient sensors. It is obvious that the combination of sensors is capable of captur- ing rich information of human activities, which is also possible for a real smart home system in the future.In this section, we investigate the deep learning models used in HAR tasks. Table 2 lists all the models.Object sensors are usually placed on objects to detect the movement of a specific object ( Chavarriaga et al., 2013). Differ- ent from body-worn sensors which capture human movements, object sensors are mainly used to detect the movement of cer- tain objects in order to infer human activities. For instance, the accelerometer attached to a cup can be used to detect the drink- ing water activity. Radio frequency identifier (RFID) tags are typically used as object sensors and deployed in smart home environment ( Vepakomma et al., 2015;Yang et al., 2015;Fang and Hu, 2014) and medical activities ( Li et al., 2016b; WangDeep neural network (DNN) is developed from artificial neu- ral network (ANN). Traditional ANN often contains very few hidden layers (shallow) while DNN contains more (deep). With more layers, DNN is more capable of learning from large data. DNN usually serves as the dense layer of other deep models. For example, in a convolution neural network, several dense layers are often added after the convolution layers. In this part, we mainly focus on DNN as a single model, while in other sec- tions we will discuss the dense layer.( Vepakomma et al., 2015) first extracted hand-engineered features from the sensors, then those features are fed into a DNN model. Similarly, ( Walse et al., 2016) performed PCA before using DNN. In those work, DNN only served as a classi- fication model after hand-crafted feature extraction, hence they Crossing sensor boundary Combination of types, often deployed in smart environments may not generalize well. And the network was rather shallow.( Hammerla et al., 2016) used a 5-hidden-layer DNN to perform automatic feature learning and classification with improved per- formance. Those work indicated that, when the HAR data is multi-dimensional and activities are more complex, more hid- den layers can help the model train well since their representa- tion capability is stronger (Bengio, 2013). However, more de- tails should be considered in certain situations to help the model fine-tune better.Convolutional Neural Network (ConvNets, or CNN) lever- ages three important ideas: sparse interactions, parameter shar- ing, and equivariant representations ( LeCun et al., 2015). After convolution, there are usually pooling and fully-connected lay- ers, which perform classification or regression tasks.CNN is competent to extract features from signals and it has achieved promising results in image classification, speech recognition, and text analysis. When applied to time series clas- sification like HAR, CNN has two advantages over other mod- els: local dependency and scale invariance. Local dependency means the nearby signals in HAR are likely to be correlated, while scale invariance refers to the scale-invariant for different paces or frequencies. Due to the effectiveness of CNN, most of the surveyed work focused on this area.When applying CNN to HAR, there are several aspects to be considered: input adaptation, pooling, and weight-sharing. 1) Input adaptation. Unlike images, most HAR sensors pro- duce time series readings such as acceleration signal, which is temporal multi-dimensional 1D readings. Input adaptation is necessary before applying CNN to those inputs. The main idea is to adapt the inputs in order to form a virtual image. There are mainly two types of adaptation: model-driven and data-driven.• Data-driven approach treats each dimension as a channel, then performs 1D convolution on them. After convolution and pooling, the outputs of each channel are flattened to unified DNN layers. A very early work is ( Zeng et al., 2014), where each dimension of the accelerometer was treated as one channel like RGB of an image, then the con- volution and pooling were performed separately.  , 2016b). This model-driven approach can make use of the temporal correlation of sensor. But the map of time series to image is non-trivial task and needs domain knowledge. 2) Pooling. The convolution-pooling combination is com- mon in CNN, and most approaches performed max or average pooling after convolution ( Ha et al., 2015;Kim and Toomajian, 2016;Pourbabaee et al., 2017). Apart from avoiding overfit- ting, pooling can also speed up the training process on large data (Bengio, 2013).3) Weight-sharing. Weight sharing ( Zebin et al., 2016;Sathyanarayana et al., 2016) is an efficient method to speed up the training process on a new task. ( Zeng et al., 2014) utilized a relaxed partial weight sharing technique since the signal ap- peared in different units may behave differently. (Ha and Choi, 2016) adopted a CNN-pf and CNN-pff structure to investigate the performance of different weight-sharing techniques. It is shown in those literature that partial weight-sharing could im- prove the performance of CNN.Autoencoder learns a latent representation of the input val- ues through the hidden layers, which can be considered as an encoding-decoding procedure. The purpose of autoencoder is to learn more advanced feature representation via an unsuper- vised learning schema. Stacked autoencoder (SAE) is the stack of some autoencoders. SAE treats every layer as the basic model of autoencoder. After several rounds of training, the learned features are stacked with labels to form a classifier.( Almaslukh et al., 2017;Wang et al., 2016a) used SAE for HAR, where they first adopted the greedy layer-wise pre- training ( Hinton et al., 2006), then performed fine-tuning. Com- pared to those works, ( Li et al., 2014) investigated the sparse au- toencoder by adding KL divergence and noise to the cost func- tion, which indicates that adding sparse constraints could im- prove the performance of HAR. The advantage of SAE is that it can perform unsupervised feature learning for HAR, which could be a powerful tool for feature extraction. But SAE de- pends too much on its layers and activation functions which may be hard to search the optimal solutions. Restricted Boltzmann machine (RBM) is a bipartite, fully- connected, undirected graph consisting of a visible layer and a hidden layer ( Hinton et al., 2006). The stacked RBM is called deep belief network (DBN) by treating every two con- secutive layers as an RBM. DBN/RBM is often followed by fully-connected layers.In pre-training, most work applied Gaussian RBM in the first layer while binary RBM for the rest layers ( Similar results are also shown in (Singh et al., 2017). The rea- son is that CNN is able to capture the spatial relationship, while RNN can make use of the temporal relationship. Combining CNN and RNN could enhance the ability to recognize different activities that have varied time span and signal distributions. Other work combined CNN with models such as SAE ( Zheng et al., 2016) and RBM ( Liu et al., 2016). In those work, CNN performs feature extraction, and the generative models can help in speeding up the training process. In the future, we expect there will be more research in this area.HAR is always not the final goal of an application, but it serves as an important step in many applications such as skill assessment and smart home assistant. In this section, we survey deep learning based HAR from the application perspective.Recurrent neural network (RNN) is widely used in speech recognition and natural language processing by utilizing the temporal correlations between neurons. LSTM (long-short term memory) cells are often combined with RNN where LSTM is serving as the memory units through gradient descent.Few work used RNN for the HAR tasks ( Hammerla et al., 2016;Inoue et al., 2016;Edel and Köppe, 2016;Guan and Ploetz, 2017), where the learning speed and resource consump- tion are the main concerns for HAR. (Inoue et al., 2016) inves- tigated several model parameters first and then proposed a rela- tively good model which can perform HAR with high through- put. ( Edel and Köppe, 2016) proposed a binarized-BLSTM- RNN model, in which the weight parameters, input, and output of all hidden layers are all binary values. The main line of RNN based HAR models is dealing with resource-constrained envi- ronments while still achieve good performance.Hybrid model is the combination of some deep models. One emerging hybrid model is the combination of CNN and RNN. (Ordóñez and Roggen, 2016;Yao et al., 2017) provided good examples for how to combine CNN and RNN. It is shown in (Ordóñez and Roggen, 2016) that the performance of 'CNN + recurrent dense layers' is better than 'CNN + dense layers'.Most of the surveyed work focused on recognizing activities of daily living (ADL) and sports ( Zeng et al., 2014;Chen and Xue, 2015;Ronao and Cho, 2016;Rav`Rav`ı et al., 2017). Those activities of simple movements are easily captured by body- worn sensors. Some research studied people's lifestyle such as sleep ( Sathyanarayana et al., 2016) and respiration ( Khan et al., 2017;Hannink et al., 2017). The detection of such activities often requires some object and ambient sensors such as WiFi and sound, which are rather different from ADL.It is a developing trend to apply HAR to health and dis- ease issues. Some pioneering work has been done with re- gard to Parkinson's disease (Hammerla et al., 2015), trauma resuscitation (Li et al., 2016a,b) and paroxysmal atrial fibrilla- tion (PAF) ( Pourbabaee et al., 2017). Disease issues are always related to the change of certain body movements or functions, so they can be detected using corresponding sensors.Under those circumstances, the association between disease and activity should be given more consideration. It is important to use the appropriate sensors. For instance, Parkinson's disease is often related to the frozen of gait, which can be reflected by some inertial sensors attached to shoes (Hammerla et al., 2015).Other than health and disease, the recognition of high-level activities is helpful to learn more resourceful information for HAR. The movement, behavior, environment, emotion, and thought are critical parts in recognizing high-level activities. However, most work only focused on body movements in smart homes ( Vepakomma et al., 2015;Fang and Hu, 2014), which is not enough to recognize high-level activities. For instance, ( Vepakomma et al., 2015) combined activity and environment signal to recognize activities in a smart home, but the activities are constrained to body movements without more information on user emotion and state, which are also important. In the fu- ture, we expect there will be more research in this area.We extensively explore the benchmark datasets for deep learning based HAR. Basically, there are two types of data ac- quisition schemes: self data collection and public datasets.• Self data collection: Some work performed their own data collection (e.g. ( Chen and Xue, 2015;Zhang et al., 2015b;Bhattacharya and Lane, 2016;Zhang et al., 2015a)). Very detailed efforts are required for self data collection, and it is rather tedious to process the collected data.• Public datasets: There are already many public HAR datasets that are adopted by most researchers (e.g. (Plötz et al., 2011;Ravi et al., 2016;Hammerla et al., 2016)). By summarizing existing literature, we present several widely used public datasets in Table 3.Table 4 presents all the surveyed work in this article. We can make several observations based on the table.1) Sensor deployment and preprocessing. Choosing the suitable sensors is critical for successful HAR. In surveyed lit- erature, body-worn sensors serve as the most common modal- ities and accelerometer is mostly used. The reasons are two folds. Firstly, a lot of wearable devices such as smartphones or watches are equipped with an accelerometer, which is easy to access. Secondly, the accelerometer is competent to recog- nize many types of daily activities since most of them are sim- ple body movements. Compared to body-worn sensors, object and ambient sensors are better at recognizing activities related to context and environment such as having coffee. Therefore, it is suggested to use body-worn sensors (mostly accelerome- ter+gyroscope) for ADL and sports activities. If the activities are pertaining to some semantic meaning but more than simple body movements, it is better to combine the object and ambient sensors. In addition, there are few public datasets for object and ambient sensors probably because of privacy issues and deploy- ment difficulty of the data collecting system. We expect there will be more open datasets regarding those sensors.Sensor placement is also important. Most body-worn sen- sors are placed on the dominant wrist, waist, and the domi- nant hip pocket. This placement strategy can help to recog- nize most common daily activities. However, when it comes to object and ambient sensors, it is critical to deploy them in a non-invasive way. Those sensors are not usually interacting with users directly, so it is critical to collect the data naturally and non-invasively.Before using deep models, the raw sensor data need to be pre- processed accordingly. There are two important aspects. The first aspect is sliding window. The inputs should be cut into in- dividual inputs according to the sampling rate. This procedure is similar to conventional PR approaches. The second one is channels. Different sensor modalities can be treated as separate channels, and each axis of a sensor can also be a channel. Us- ing multi-channel could enhance the representation capability of the deep model since it can reflect the hidden knowledge of the sensor inputs.2) Model selection. There are several deep models surveyed in this article. Then, a natural question arises: which model is the best for HAR? (Hammerla et al., 2016) did an early work by investigating the performance of DNN, CNN and RNN through 4,000 experiments on some public HAR datasets. We com- bine their work and our explorations to draw some conclusions: RNN and LSTM are recommended to recognize short activities that have natural order while CNN is better at inferring long- term repetitive activities ( Hammerla et al., 2016). The reason is that RNN could make use of the time-order relationship be- tween sensor readings, and CNN is more capable of learning deep features contained in recursive patterns. For multi-modal signals, it is better to use CNN since the features can be in- tegrated through multi-channel convolutions (Zeng et al., 2014;  Zheng et al., Ha et al., 2015). While adapting CNN, data- driven approaches are better than model-driven approaches as the inner properties of the activity signal can be exploited bet- ter when the input data are transformed into the virtual image. Multiple convolutions and poolings also help CNN perform bet- ter. RBM and autoencoders are usually pre-trained before being fine-tuned. Multi-layer RBM or SAE is preferred for more ac- curate recognition.Technically there is no model which outperforms all the oth- ers in all situations, so it is recommended to choose models based on the scenarios. To better illustrate the performance of some deep models, Table 5 offers some results comparison of existing work on public datasets in Table 3 1 . In Skoda and UCI Smartphone protocols, CNN achieves the best performance. In two OPPORTUNITY protocols, DBN and RNN outperform the others. This confirms that no models can achieve the best in all tasks. Moreover, the hybrid models tend to perform better than single models (DeepConvLSTM in OPPORTUNITY 1 and Skoda). For a single model, CNN with shifted inputs (Fourier transform) generates better results compared to shifted kernels.Despite the progress in previous work, there are still chal- lenges for deep learning based HAR. In this section, we present those challenges and propose some feasible solutions.A. Online and mobile deep activity recognition. Two criti- cal issues are related to deep HAR: online deployment and mo- bile application. Although some existing work adopted deep HAR on smartphone ( ) and watch , they are still far from online and mobile deployment. Because the model is often trained offline on some remote server and the mobile device only utilizes a trained model. This approach is neither real-time nor friendly to incremental learning. There are two approaches to tackle this problem: reducing the communication cost between mobile and server, and enhancing computing ability of the mobile devices.B. More accurate unsupervised activity recognition. The performance of deep learning still relies heavily on labeled sam- ples. Acquiring sufficient activity labels is expensive and time- consuming. Thus, unsupervised activity recognition is urgent.• Take advantage of the crowd. The latest research indicates that exploiting the knowledge from the crowd will facili- tate the task ( Prelec et al., 2017). Crowd-sourcing takes advantage of the crowd to annotate the unlabeled activ- ities. Other than acquiring labels passively, researchers could also develop more elaborate, privacy-concerned way to collect useful labels.• Deep transfer learning. Transfer learning performs data annotation by leveraging labeled data from other auxiliary domains (Pan and Yang, 2010;Cook et al., 2013;Wang et al., 2017). There are many factors related to human activity, which can be exploited as auxiliary information using deep transfer learning. Problems such as sharing weights between networks, exploiting knowledge between activity related domains, and how to find more relevant domains are to be resolved. C. Flexible models to recognize high-level activities. More complex high-level activities need to be recognized other than only simple daily activities. It is difficult to determine the hi- erarchical structure of high-level activities because they contain more semantic and context information. Existing methods often ignore the correlation between signals, thus they cannot obtain good results.• Hybrid sensor. Elaborate information provided by the hy- brid sensor is useful for recognizing fine-grained activi- ties ( Vepakomma et al., 2015). Special attention should be paid to the recognition of fine-grained activities by ex- ploiting the collaboration of hybrid sensors.• Exploit context information. Context is any information that can be used to characterize the situation of an entity ( Abowd et al., 1999). Context information such as Wi-Fi, Bluetooth, and GPS can be used to infer more environ- mental knowledge about the activity. The exploitation of resourceful context information will greatly help to recog- nize user state as well as more specific activities. D. Light-weight deep models. Deep models often require lots of computing resources, which is not available for wearable devices. In addition, the models are often trained off-line which cannot be executed in real-time. However, less complex models such as shallow NN and conventional PR methods could not achieve good performance. Therefore, it is necessary to develop light-weight deep models to perform HAR.• Combination of human-crafted and deep features. Re- cent work indicated that human-crafted and deep features together could achieve better performance (Plötz et al., 2011). Some pre-knowledge about the activity will greatly contribute to more robust feature learning in deep mod- els (Stewart and Ermon, 2017). Researchers should con- sider the possibility of applying two kinds of features to HAR with human experience and machine intelligence.• Collaboration of deep and shallow models. Deep mod-els have powerful learning abilities, while shallow models are more efficient. The collaboration of those two models has the potential to perform both accurate and light-weight HAR. Several issues such as how to share the parameters between deep and shallow models are to be addressed. E. Non-invasive activity sensing. Traditional activity col- lection strategies need to be updated with more non-invasive approaches. Non-invasive approaches tend to collect informa- tion and infer activity without disturbing the subjects and re- quires more flexible computing resources.• Opportunistic activity sensing with deep learning. Op- portunistic sensing could dynamically harness the non- continuous activity signal to accomplish activity infer- ence ( Chen et al., 2016a). In this scenario, back propa- gation of deep models should be well-designed. F. Beyond activity recognition: assessment and assistant. Recognizing activities is often the initial step in many applica- tions. For instance, some professional skill assessment is re- quired in fitness exercises and smart home assistant plays an important role in healthcare services. There is some early work on climbing assessment ( Khan et al., 2015). With the advance- ment of deep learning, more applications should be developed to be beyond just recognition.Human activity recognition is an important research topic in pattern recognition and pervasive computing. In this pa- per, we survey the recent advance in deep learning approaches for sensor-based activity recognition. Compared to traditional pattern recognition methods, deep learning reduces the depen- dency on human-crafted feature extraction and achieves better performance by automatically learning high-level representa- tions of the sensor data. We highlight the recent progress in three important categories: sensor modality, deep model, and application. Subsequently, we summarize and discuss the sur- veyed research in detail. Finally, several grand challenges and feasible solutions are presented for future research.
