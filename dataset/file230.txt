ISH images of mammalian brains reveal highly complex patterns of gene expres- sion varying on multiple scales. Our study follows [17], which we pursue using deep learning. In [17] the authors present FuncISH, a learning method of functional rep- resentations of ISH images, using a histogram of local descriptors on several scales.They first represent each image as a collection of local descriptors using SIFT features. Next, they construct a standard bag-of-words description of each image, giving a 2004-dimension representation vector for each gene. Finally, given a set of predefined GO annotations of each gene, they train a separate classifier for each known biological category, using the SIFT bag-of-words representation as an input vector. Specifically, they used a set of 2081 L 2 -regularized logistic regression classi- fiers for this training. A scheme representing the work flow is presented in Figure 2 (see Section 4).Applying their method to the genomic set of mouse neural ISH images available from the Allen Brain Atlas, they found that most neural biological processes could be inferred from spatial expression patterns with high accuracy. Despite ignoring important global location information, they successfully inferred ∼700 functional annotations, and used them to detect gene-gene similarities which were not captured by previous, global correlation-based methods. According to [17], combining local and global patterns of expression is an important topic for further research, e.g., the use of more sophisticated non-linear classifiers.Pursuing further the above classification problem poses a number of challenges. First, we cannot define a certain set of rules that an ISH image has to conform to in order to classify it to the correct GO category. Therefore, conventional computer vision techniques, capable of identifying shapes and objects in an image, are not likely to provide effective solutions to the problem. Thus, we use deep learning to achieve better results, as far as functional representations of the ISH images. This yields an interpretable measure of similarity between complex images that are difficult to analyze and interpret.Deep learning techniques that support this kind of problems use AE and CNN, as well as CAE, which are successful in preforming feature extraction and finding compact representations for the kind of large ISH images we have been dealing with. While traditional machine learning is useful for algorithms that learn iteratively from the data, our second issue concerns the type of data we possess. Our data consist of 16K images, representing about 15K different genes, i.e., an average of one image per gene. This prevents us from extracting features from each gene independently, but rather consider the data in their entirety. Moreover, not only is there only one image per gene, there are merely a few genes in every examined GO category, and the genes are not unique to one category, i.e., each gene may belong to more than one category. Despite these difficulties, machine learning is capable of capturing underlying "insights" without resorting to manual feature selection. This makes it possible to automatically produce models that can analyze larger and more complex data, achieving thereby more accurate results.In the next section we present our convolutional autoencoder approach, which operates solely on raw pixel data. This supports our main goal, i.e., learning rep- resentations of given brain images to extract useful information, more easily, when building classifiers or other predictors. The representations obtained are vectors which can be used to solve a variety of problems, e.g., the problem of GO classifica- tion. For this reason, a good representation is also one that is useful as input to a supervised predictor, as it allows us to build classifiers for the biological categories known.While convolutional neural networks (CNN) are effective in a supervised framework, provided a large training set is available, this is incompatible to our case. If only a small number of training samples is available, unsupervised pre-training methods, such as restricted Boltzmann machines (RBM) [21] or autoencoders [24], have proven highly effective.An AE is a neural network which sets the target values (of the output layer) to be equal to those of the input, using hidden layers of smaller and smaller size, which comprise a bottleneck. Thus, an AE can be trained in an unsupervised manner, forcing the network to learn a higher-level representation of the input. An improved approach, which outperforms basic autoencoders in many tasks is due to denoising autoencoders (DAEs) [24], [25]. These are built as regular AEs, where each input is corrupted by added noise, or by setting to zero some portion of the values. Although the input sample is corrupted, the network's objective is to produce the original (uncorrupted) values in the output layer. Forcing the network to recreate the un- corrupted values results in reduced network overfitting (also due to the fact that the network rarely receives the same input twice), and in extraction of more high- level features. For any autoencoder-based approach, once training is complete, the decoder layer(s) are removed, such that a given input passes through the network and yields a high-level representation of the data. In most implementations (such as ours), these representations can then be used for supervised classification.CNNs and AEs can be combined to produce CAEs. As with CNNs, the CAE weights are shared among all locations in the input, preserving spatial locality and reducing the number of parameters. In practice, to combine CNNs with AEs (or DAEs), it is necessary for each encoder layer to have a corresponding decoder layer. Deconvolution layers are essentially the same as convolutional layers, and similarly to standard autoencoders, they can either be learned or set equal to (the transpose of) the original convolution layers, as with tied weights in autoencoders (both work well). For the unpooling operation, more than one method exists [19], [6]. In the CAE we use, during unpooling all locations are set to the maximum value which is stored in that layer (Figure 1).Similarly to an AE, after training a CAE, the unpooling and deconvolution layers are removed. At this point, a neural net, composed from convolution and pooling layers, can be used to find a functional representation, as in our case, or initialize a supervised CNN. Similarly to a DAE, a CAE with input corrupted by added noise is called a convolutional denoising autoencoders (CDAE). Figure 2 depicts a framework for capturing the representation of FuncISH. A SIFT- based module was used in [17] for feature extraction. Alternatively, our scheme learns a CDAE-based representation, before applying a similar classification method as in [17], where two layers of 5-fold cross-validation were used, one for training the clas- sifier and the other for tuning the logistic regression regularization hyperparameter.For unsupervised training of our CDAE we use the genomic set of mouse neu- ral ISH images available from the Allen Brain Atlas, which includes 16,351 im- ages representing 15,612 genes. These JPEG images have an average resolution of 15, 000 × 7, 500 pixels. To get a representation vector of size ∼2,000, the images were downsampled to 300 × 140 pixels. The CDAE architecture for finding a compact representation for these down- sampled images is as follow:1 Input layer: Consists of the raw image, resampled to 300 × 140 pixels, and corrupted by setting to zero 20% of the values, 2 three sequential convolutional layers with 32 9 × 9 filters each, 3 max-pooling layer of size 2 × 2, 4 three sequential convolutional layers with 32 7 × 7 filters each, 5 max-pooling layer of size 2 × 2, 6 two sequential convolutional layers with 64 5 × 5 filters each, 7 convolutional layer with a single 5 × 5 filter, 8 unpooling layer of size 2 × 2, 9 three sequential deconvolution layers with 32 7 × 7 filters each, 10 unpooling layer of size 2 × 2, 11 three sequential deconvolution layers with 32 9 × 9 filters each, 12 deconvolution layer with a single 5 × 5 filter, and 13 output layer with the uncorrupted resampled image.After training the CDAE, all layers past item 8 are removed, so that item 7 (the convolutional layer of size 1 × 2, 625) becomes the output layer. Therefore, each image is mapped to a vector of 2,625 functional features. Given a set of predefined GO annotations for each gene (where each GO category consists of 15-500 genes), we trained a separate classifier for each biological category. Training requires careful consideration, in this case, due to the vastly imbalanced nature of the training sets.Similarly to [17], we performed a weighted SVM classification using 5-fold cross- validation.This network yields remarkable AUC results for every category of the top 15 GO categories reported in [17]. Figure 3 illustrates the AUC scores achieved for various representation vectors. While the average AUC score (of the top 15 categories) re- ported in [17] was 0.92, the average AUC using our CDAE scheme was 0.98, i.e., a 75% reduction in error.The above improvement was achieved with a vector size of 2,625, which is larger than the 2004-dimensional vector obtained by SIFT. In an attempt to maintain, as much as possible, the scheme's performance for a comparable vector size, we explored the use of smaller vectors, by resampling the images to different scales, and constructing CDAEs with various numbers of convolution and pooling layers. Downsampling to 240 × 120 images, we obtained a 1800-dimensional representa- tion vector, for which the AUC scores are still superior (relatively to [17]) for each of the top 15 GO categories (as shown in Figure 3(a)). The 10%-dimensionality reduction results only in a slightly lower AUC average of 0.97 (see Figure 3(b)).The CDAE network for the more compact representation is shown in Figure 4. output layer with the uncorrupted resampled image. We used the ReLU activation function for all convolution and deconvolution layers, except for the last deconvolution layer, which uses tanh.The learning rate starts from 0.05 and is multiplied by 0.9 after each epoch, and the denoising effect is obtained by randomly removing 20% of the pixels every image in the input layer. We used the AUC as a measure of classification accuracy.Many machine learning algorithms have been designed lately to predict GO anno- tations. For the task of learning functional representations of mammalian neural images, we used deep learning techniques, and found convolutional denoising au- toencoder to be very effective. Specifically, using the presented scheme for feature learning of functional GO categories improved the previous state-of-the-art classi- fication accuracy from an average AUC of 0.92 to 0.98, i.e., a 75% reduction in  error. We demonstrated how to reduce the vector dimensionality by 10% compared to the SIFT vectors, with very little degradation of this accuracy. Our results further attest to the advantages of deep convolutional autoencoders, as were applied here to extracting meaningful information from very high resolution images and highly complex anatomical structures. Until gene product functions of all species are dis- covered, the use of CDAEs may well continue to serve the field of Bioinformatics in designing novel biological experiments.We provide a brief explanation as to the choice of the main parameters of the CDAE architecture. Our objective was to obtain a more compact feature representation than the 2,004-dimensional vector used in FuncISH.Since a CNN is used, the representation along the grid should capture the two- dimensional structure of the input, i.e., the image dimensions should be determined according to the intended representation vector, while maintaining the aspect ratio of the original input image. Thus, we picked an 1,800-dimensional feature vector, corresponding to an (output) image of size 60 × 30. Taking into account the charac- teristic of max-pooling (i.e., that at each stage the dimension is reduced by 2), the desire to keep the number of layers as small as possible, and the fact that the encod- ing and decoding phases each contains the same number of layers (resulting in twice the number of layers in the network), we settled for two max-pooling layers, namely an input image of size 240 × 120. Between each two max-pooling layers, which elim- inate feature redundancy, there is an "array" of 16 convolution layers, each with the purpose of detecting locally connected features from its previous layer. The number of convolution layers (i.e., different filters used) was determined after experimenting with several different layers, all of which gave similar results. Choosing 16 layers (as shown in Figure 4) provided the best result.We experimented also with various filter sizes for each layer, ranging from 3 × 3 to 11 × 11; while increasing the filter size significantly increased the amount of network parameters learned, it did not contribute much to the feature extraction or the improvement of the results. Using a learning rate decay in the training of large networks (where there is a large number of randomly generated parameters) has proven helpful in the network's convergence. Specifically, the combination of a 0.05 learning rate parameter with a 0.9 learning rate decay resulted in an optimal change of the parameter value. In this case, too, small changes in the parameters did not result in significant changes in the results.
