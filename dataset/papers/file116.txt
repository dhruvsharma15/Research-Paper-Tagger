Entity Tracking The idea to keep information relating to specific concepts in dedicated cells was inspired in part by the work of Henaff et al. [2016] on recurrent entity networks. They propose to define one full RNN per entity of interest which is tasked with keeping track of all of the relevant information as a story is read, and their model is able to answer questions about the state of the world by consulting all of their final hidden states. Such an approach is unfortunately impractical for our case, which presents thousands of concepts of interest. Instead, we use one dimension in the recurrent hidden state for each of them to track one specific piece of information: the likelihood that they are present in the current example.Sparse Recurrent Units Subakan and Smaragdis [2017] use diagonal recurrent transition matrices for music modeling and show that their model outperforms the the one with full transition matrices. The upper-left block of the transition matrix in our model is diagonal. Narang et al. [2017] explored sparsity in recurrent models, and showed that sparse weight matrices result in much smaller models without significant loss in accuracy. The weight sparsity introduced in our model through a semi diagonal weight matrix additionally limits over-fitting, as discussed in Section 3.Interpretable RNNs Lei et al. [2016] propose a method for encouraging interpretability in the learning objective by selecting predictive phrases as the first step. Additionally, Lample et al. [2016] discuss recurrent models followed by a pairwise conditional random field (CRF) for named entity recognition. Their LSTM+CRF model is used to predict word tags that determine the span a word belongs to and the entity the span represents. They also present a transition-based chunking model which uses a stacked LSTM where words are pushed onto a stack and popped with an entity label.Medical Concept Extraction Joshi et al. [2016] use non-negative matrix factorization for simulta- neous phenotyping of co-occurring medical conditions from clinical text. They obtain identifiable sparse latent factors by grounding them to have a one-to-one mapping with a fixed set of chronic conditions. Our model grounds its recurrent hidden state dimensions to have a one-to-one mapping with the labels for a task. Automatic ICD9 coding has been explored in clinical machine learning literature, such as Perotte et al. [2014]. Their work presents a tree structured Support Vector Machine (SVM) which takes advantage of the hierarchical nature of ICD9 codes to outperform a flat baseline.In this section, we show how to derive a Grounded Recurrent Neural Network (GRNN) architecture given a label set L. Note that while we decide to build our version of GRNN on top of a Gated Recurrent Unit, similar ideas can be applied to add grounding to other types of recurrence functions, such as the Elman or LSTM unit.  Sequence Labeling with GRUs Let L be a set of labels of interest. Consider a text sequence x = (x 1 , . . . , x T ) and a label assignment for the sequence denoted by y ∈ {0, 1} |L| . For ease of exposition, we identify the words (x 1 , . . . , x T ) with their embeddings of dimension D e . The task of sequence labeling consists in predicting y given x. To that end, we can use a Recurrent Neural Network to obtain a vector representation of the text of dimension D h , then compute the likelihood of each label being present given that representation. More specifically, the recurrent model starts with a representation h 0 , and updates it at each timestep by using a recurrence function f to obtain the global sequence representation h T :In the case of the Gated Recurrent Unit, which we build upon in this work, the recurrence function f is parameterized by the D h × (D e + D h ) matrices Z, R and W (and dimension D h bias vectors b z , b r and b w ), and computed as follows. Let [a, b] denote the concatenation of vectors a and b, and be the element-wise product. Then, h t = f (h t−1 , x t ) is given by:From the final text representation h T , we can obtain a prediction vector˜yvector˜ vector˜y ∈ [0, 1] |L| by applying an affine transformation followed by a sigmoid function, parameterized by a |L| × D h matrix P and bias vector b p . The model parameters Θ can then be learned by minimizing the expected sum of the binary cross-entropy loss for each (text, label set) pair, denoted as L:Grounded Dimensions At a high level, the above approach corresponds to having a model sum- marize all of the relevant information from a text sequence in the final recurrent state h T , such that each label corresponds to a different sub-space. This presents several challenges. On the one hand, if the dimension of the recurrent space is much smaller than the label space size, the model capacity might be too small to store the required information about all of the labels in the target set. On the other hand, too large a recurrent space, while making the model more expressive, can make it prone to over-fitting, rendering optimization difficult when training data is limited. In addition, even though GRUs and LSTMs are better than standard Elman units at modeling long term dependencies, it can still be challenging for them to maintain relevant information from the very beginning of longer sequences (up to a few thousand words in many applications). Our goal in this work is to alleviate the aforementioned problems by adding grounded dimensions to the model's recurrent space.We split the recurrent state h into |L| grounded dimensions g and D c control dimensions c. At each time step, the value stored in a grounded dimension g l corresponds to the model's current belief that y l = 1. Since by construction we have g ∈ [−1, 1] |L| , the label predictions˜ypredictions˜ predictions˜y can simply be obtained by scaling and shifting the final grounded state g T . However, we also found it useful to use a bias term to allow grounded dimensions to be centered on 0 regardless of the corresponding label frequency in the data. With this rescaling, and keeping with the notations introduced in the previous paragraph, the model predictions are then given by:Figure 1 illustrates this process as the model reads the end of a medical note: after reading the phrase "patient went into afib", the model increases its belief that this person was diagnosed with Atrial Fibrillation. This formulation already presents some advantages. For example, dedicated dimensions can make learning of long-term dynamics easier. However, simply applying a GRU update to the complete h = [g, c] recurrent state at each time step yields a model with very large capacity (D h = |L| + D c ), which, as stated previously, can make optimization difficult with limited data. We address this issue in the next paragraph.Semi Diagonal Updates When given liberty to use the |L| grounded dimensions without con- straints, the model can choose to use them to store other information than label-specific beliefs, especially in the case of dimensions corresponding to rarer concepts. To avoid this potential issue, we propose to restrict the model dynamics by making the Z, R and W matrices in Equations 2 to 4 semi-diagonal, as illustrated in Figure 2. . v corresponds to z, r or˜hor˜ or˜h from Equations 2, 3 and 4 respectively. M ∈ {Z, R, W} is a weight matrix and b M is the corresponding bias vector.With this architecture, the value of any g l is updated at each timestep based solely on its previous state, the current input and control state c, which is then made responsible for modeling correlations. Moreover, the number of parameters of the model and computational cost of each time step with semi diagonal transitions grows linearly in the size of the label space, which allows learning to remain tractable even when |L| is large.Bidirectional GRNN Finally, in many tasks, it is common for bidirectional recurrent models to outperform the unidirectional recurrent models, as they allow for context from the future to be taken into consideration at each timestep along with the history. We extend the GRNN to the bidirectional setting by running a standard GRU in the reverse direction on the document, and concatenating the outputs of this GRU to the inputs of the GRNN, allowing modifications of the grounded dimensions to be based on future context as well as past.In this Section, we presented the Grounded Recurrent Neural Network: a recurrent architecture designed to have significantly higher capacity than comparable models while making optimization easier and improving interpretability. Section 4 analyzes the model's behavior on real world data, and demonstrates these properties experimentally.Datasets We evaluate our model on the task of multi-label classification on three datasets: two versions of the MIMIC medical dataset and Stack Overflow questions.MIMIC-II [Saeed et al., 2011] is a dataset of Intensive Care Unit medical records. Each patient admission ends with a free text discharge summary describing the patient's stay, diagnoses, and procedures performed. Here, we consider the problem of predicting diagnosis codes from the text, learning from tags manually provided by humans after going through the admission records. We follow Perotte et al. [2014] in extending the label set to also consider parents of the gold label codes in the ICD9 hierarchy, which yields a total vocabulary of 7042 ICD9 codes that we use as labels, with 36.7 labels per note on average. We split the data into the same training and test sets as Perotte et al. [2014], and further split their training set into training and validation.  2 to evaluate the GRNN on the task of predicting tags from question text. We pre-process the data to remove all code blocks from the questions, and select questions which have more than 100 words left. This gave us 365, 192 training, 13, 390 validation and 27, 187 testing samples. We chose the 4000 most frequent tags as our label set, with 2.9 tags per sample on average. The average sentence length is much shorter at 190.5 words, and we truncate the maximum length to 600.Baselines We first compare the GRNN with a Bag-of-Words baseline, where independent binary classifiers are trained via L1-regularized logistic regression for each label, with early stopping based on validation loss. The regularization parameters are tuned independently for each label using the validation sets. Considering recent advances in attention-based models , we also devise a neural Bag-of-Words approach that uses soft attention over the words in a note and adds their embeddings. For all our neural approaches, we used a word embedding size of 192. The attention scores for words are based on the local neighborhoods of the words, and are shared by all labels. This method ensures that only the significant words are considered for the prediction of labels. The note representation h is then computed as the weighted sum of the embeddings, and we obtain predictions and learn the model parameters as described in Section 3 (replacing h T with h in Equation 6). The GRU baseline is as defined in Section 3, with a hidden size of either 128, or a larger dimension corresponding to a GRU with the same number of parameters as the corresponding GRNN (846 for 5000 labels, 793 for 4000). This allows us to test whether the difference in our model's performance is due to grounding or simply comes from an increased capacity. We also consider bidirectional GRUs with a dimension 64 hidden state. Furthermore, for the MIMIC-II dataset, we compare our results to those of Perotte et al. [2014], which uses a flat and a hierarchical SVM for the same task. For each example, the hierarchical SVM node decision for an ICD9 code is trained only if its parent code is positive, and a child code is evaluated only if its parent is classified as positive during testing. The flat SVM predicts all the leaf ICD9 codes independently and builds the extended predictions according to the hierarchy. Unlike Perotte et al. [2014], we learn and predict on the entire extended label set without considering the ICD9 hierarchy, letting our learning algorithm infer relevant label correlations. However, we also note that the authors of the baseline SVM models could have obtained better results by fine-tuning their regularization parameters.  Table 2: Results on MIMIC-3, 5000 labels.Other Comparisons We ran early experiments with grounded models without a semi diagonal constraint on the recurrent transition matrices, and found that such models failed to generalize for large label spaces due to over-fitting. We also tried grounded recurrent models without any control dimensions, which always performed worse, since we deny the model the capacity to track history beyond current beliefs in labels. In addition, we investigated a variant of the bidirectional GRU closer to our formulation of the bidirectional GRNN, wherein the outputs of a reverse GRU are concatenated to the inputs for a forward GRU (denoted as BiGRU-l in the supplementary material tables), but we found that it always performed worse than the standard bidirectional GRU described earlier.Finally, we ran the entity network of [Henaff et al., 2016] on MIMIC-III text with 6 entity RNNs. The keys for the entities are learned globally, enabling the network to learn clusters of related labels, with each entity tracking one such cluster. For predictions, each label performed attention over the entity network blocks to determine its value, enabling each label to focus on the cluster it is a part of. This network took several days to train while performing similar to or worse than the GRU baselines.Curriculum Learning To maintain the invariance of the grounded dimensions representing the likelihoods of labels at intermediate timesteps, we train the model on truncated documents. A convenient way to do this is to start with small sentence lengths and increase the maximum document length as training progresses. At smaller lengths, this helps learn the overall statistics of labels and any early evidence in text. As the maximum document length is increased, the model learns to attribute labels with evidence in text that appears later on. Since we never go back to a smaller length, this enables the model to fine-tune its predictions as more useful information becomes available. We can also view this strategy as a way of doing curriculum learning, enabling the model to perform well on long documents by initially learning on shorter documents. The initial document length was set to 50, and increased by a factor of 1.35 on every training epoch. Initial experiments without curriculum learning took much longer to train, and performed worse than models trained with curriculum learning. We found that this approach got better results and trained faster for all recurrent models, and thus we decided to use the same strategy for the GRU baselines as well.Quantitative Evaluation Tables 1, 2   Table 3: Results on StackOverflow, 4000 labels.true or false, and computing the statistics on all of those together. To obtain macro-averaged measures, the statistics are computed independently on each label, then averaged uniformly, regardless of the label frequency in the data. Compared to the micro-averaged versions, macro-averaging puts much more of a weight on the model's ability to accurately predict rare labels. We also consider our model's performance in actual health care applications. Given the specific requirements of the domain, one successful human-in-the-loop strategy consists in using the model scores to show a user the n highest scored labels in a highly multi-class application, or to use these scores to improve an auto-complete system as in Jernite et al. [2013] and Greenbaum et al. [2017]. In that case, it is important to know how many of the proposals are correct (precision at n: P@n in the tables). Indeed, low precision can significantly hurt a user's confidence in the system. Additionally, we want to know how many of the example's labels are covered by the proposed predictions (recall at n: R@n). We provide these measures for n = 8 and n = 40. Tables 1 and 2 show that the GRNN performs significantly better on medical data all measures combined. The gap is greater for MIMIC-II, which agrees with our intuition: MIMIC-II has less data, which makes having a data efficient model more important, and the target label space has a hierarchical structure, which makes being able to take advantage of correlations all the more useful.In particular, the advantage of the grounded architecture is most noticeable on the precision and recall at n measures, which correspond to our proposed use case. Finally, we give results on StackOverflow data in Table 3 to show that our model also performs well on a different domain and setting: the dataset is much larger, while the individual example text sequences are significantly shorter. The grounded architectures are on par with the best baselines, and still consistently out-perform them on the P@n and R@n measures. Figure 3 provides more insight into the properties of the model architecture. The left plot shows that the GRNN AUC(PR) outperforms most baselines regardless of label frequency. We note that the Logistic curve is close to the grounded architectures, which corresponds to the similar macro-averaged PR AUC, as shown in Table 1. Compared to the other models, the GRNN has most of an advantage on concepts in the middle of the frequency spectrum: labels which appear between a few dozen and a few hundred times in the training data. We also investigate our model's data efficiency by training both a GRU and GRNN on subsets of MIMIC-III of increasing size: using 20%, 40% and 60% of the data. While the GRNN always outperforms the standard GRU, the gap is larger the less training data the model has access to, which implies that grounding does indeed allow a recurrent neural network to learn from less data.Interpretable Predictions We also want our model to provide interpretable predictions. Indeed, better interpretability of the model decisions is as important as improved quantitative performances in a medical setting, where practitioners need to be able to trust the system they use, and to easily query the decision process for predictions that are more surprising to them. It should be noted that one can obtain some limited insight into the decision process of the attention-based baseline, for instance, by looking at the global scores. However, for both the GRU and GRNN, we can actually track the model's belief in the presence of a specific label as a note is read. As mentioned in Section 3, for the GRNN, one simply needs to look at the evolution of the corresponding grounded dimension between −1 and 1. It is possible to obtain similar information from the GRU, by applying the projection defined in Equation 6 at each time step, which gives a value between 0 and 1.  Figure 4 presents this visualization for extracts from two discharge summaries, outlining the time steps which either increase (red, orange, yellow) or decrease (green or blue) the model's belief in the presence of a concept. In both cases, the Grounded architecture provides a sharper, more interpretable signal, focusing on clinically meaningful passages ("herniated disc" or "lumbar spondylosis" for the patient with dorsopathies, "transfusions of [. . . ] ffp" and "become increasingly coagulopathic" for the coagulation defect diagnosis). On the other hand, while the GRU's belief does increase somewhat on those same phrases, this effect is similar to that of other more distantly related phrases ("fibrillation") as well as some that do not seem especially relevant ("patient").In this work, we introduce the Grounded Recurrent Neural Network, a recurrent network architecture which learns to perform multi-label text classification in a data efficient way by tying concepts of interest to specific dimensions of its hidden state. At the same time structural constraints on the recurrence matrices allow the model to remain tractable even in the presence of a large number of labels. Thus, the model is able to combine the data efficiency of simple Bag-of-Word text classification methods with an RNN's ability to model linguistic structures by tying labels of interest to specific dimensions of its hidden state.We show that our model is especially suited to a medical setting where its ability to learn from limited data, model concept correlations, and provide interpretable predictions lead to both better performance and improved trustworthiness for practitioners over several strong baselines. We also demonstrate our network's ability to match or outperform these baselines even in a case where data efficiency is less crucial. We hope to improve our model further in future work by introducing structured prediction objectives so as to take better advantage of our proposed architecture's ability to represent interactions between concepts.    Table 6: Results on StackOverflow, 4000 labels.
