At each stage of neural network acoustic model design and training there is a tremendous breadth and depth of prior work. Researchers often focus on improving one particular component of this pipeline while holding all other components fixed. Unfortunately, there is no well-established baseline for the acoustic model building pipeline, so performance improve- ments of, for example, a particular architectural variant are difficult to assess from examining the literature. Our examines the relative importance of several acoustic model design and training decisions. By systematically varying several critical design components we are able to test the limits of certain architectural choices, and uncover which variations among baseline systems are most relevant for LVCSR performance. We specifically address the following questions in this work:1) What aspects of neural network architecture are most important for acoustic modeling tasks? We investigate total network size and number of hidden layers using two corpora to avoid overfitting as a confounding factor. We build DNNs with five to ten times the total number of free parameters of DNNs used in most previous work. We also compare optimization algorithms to test whether more modern approaches to stochastic gradient descent are a driving factor in building large DNN acoustic models.We additionally compare a much broader architectural choice -locally-connected models versus the stan- dard densely-connected DNN models. Recent work has found improvements when using DCNNs combined with DNNs for acoustic modeling, or when applying DCNNs to audio features with sufficient pre-processing [29]. improve generalization performance. Many loss functions spe- cific to speech recognition tasks exist, and are a topic of active research. We choose to focus only on cross entropy because training with cross entropy is almost always the first step, or an additional loss function criterion, when experimenting with more task-specific loss functions. Additionally, the cross entropy loss function is a standard choice for classification tasks, and using it allows our experiments to serve as a case study for large scale DNN classification tasks more generally.The cross entropy loss function does not consider each utterance in its entirety. Instead it is defined over individual samples of acoustic input x and senone label y. The cross entropy objective function for a single training pair px, yq is,where K is the number of output classes, andˆyandˆ andˆy k is the probability that the model assigns to the input example taking on label k. Cross entropy is a convex approximation to the ideal 0-1 loss for classification. However, when training acoustic models perfect classification at the level of short acoustic spans is not our ultimate goal. Instead, we wish to minimize the word error rate (WER) of the final LVCSR system. WER measures mistakes at the word level, and it is possible to perfectly transcribe the words in an utterance without perfectly clas- sifying the HMM state present at each time step. Constraints present in the HMM and word sequence probabilities from the language model can correct minor errors in state-level HMM observation estimates. Conversely, not all acoustic spans are of equal importance in obtaining the correct word-level transcription. The relationship between classification accuracy rate at the frame level and overall system WER is complex and not well understood. In our experiments we always report both frame-level error metrics and system-level WER to elicit insights about the relationship between DNN loss function performance and overall system performance.To address the stated research questions we employ three different classes of neural network architecture. Each architec- ture amounts to a different set of equations to convert input features into a predicted distribution over output classes. We describe here the specifics of each architecture, along with the loss function and optimization algorithms we use.A DNN is a series of fully connected hidden layers which transform an input vector x into a probability distributionˆydistributionˆ distributionˆy to estimate the output class. The DNN thus acts as a function approximator for the conditional distribution ppy|xq. A DNN parametrizes this function using L layers, a series of hidden layers followed by an output layer. Figure 1 shows an example DNN.Each layer has a weight matrix W and bias vector b. We compute vector h 1 of first layer activations of a DNN using,All of our experiments utilize the cross entropy classifica- tion loss function. For some experiments we apply regulariza- tion techniques in addition to the cross entropy loss function to connectivity, we are effectively constraining certain entries in W to be 0. Subsequent hidden layers compute their hidden activation vector h piq using the hidden activations of the previous layer h pi´1q ,In all hidden layers we apply a point-wise nonlinearity function σpzq as part of the hidden layer computation. Tradi- tional approaches to neural networks typically use a sigmoidal function. However, in this work we use rectified linear units which were recently shown to lead to better performance in hybrid speech recognition as well as other DNN classification tasks [23], [24], [25]. The rectifier nonlinearity is defined as, σpzq " maxpz, 0q "The final layer of the DNN must output a properly formed probability distribution over the possible output categories. To do this, the final layer of the DNN uses the softmax nonlinearity, which is defined as,Using the softmax nonlinearity we obtain the output vectorˆy vectorˆ vectorˆy which is a well-formed probability distribution over the N output classes. This distribution can then be used in the loss function stated in Equation 3, or other loss functions.Having chosen a loss function and specified our DNN computation equations, we can now compute a sub-gradient of the loss function with respect to the network parameters. Note that because we are using rectifier nonlinearities this is not a true gradient, as the rectifier function is non-differentiable at 0. In practice we treat this sub-gradient as we would a true gradient and apply gradient-based optimization procedures to find settings for the DNN's parameters.This DNN formulation is fairly standard when compared to work in the speech recognition community. The choice of rectifier nonlinearities is a new one, but their benefit has been reproduced by several research groups. Fully connected neural networks have been widely used in acoustic modeling for over 20 years, but the issues of DNN total size and depth have not been thoroughly studied.The fully-connected DNN architecture presented thus far serves as the primary neural network acoustic modeling choice for modern speech recognition tasks. In contrast, neural net- works for computer vision tasks are often deep convolutional neural networks (DCNNs) which exploit spatial relationships in input data [56], [57]. When using spectrogram filter bank representations of speech data, analogous time-frequency rela- tionships may exist. The DCNN architecture allows for param- eter sharing and exploiting local time-frequency relationships for improved classification performance. DCNNs follow a con- volutional layer with a pooling layer to hard-code invariance to slight shifts in time and frequency. Like fully connected neural network acoustic models, the idea of using localized time-frequency regions for speech recognition was introduced over 20 years ago [30]. Along with the modern resurgence of interest in neural network acoustic models researchers have taken a modern approach to DCNN acoustic models. Our formulation is consistent with other recent work on DCNN acoustic models [29], but we do not evaluate specialized feature post-processing or combining DNNs with DCNNs to form an ensemble of acoustic models. Instead, we ask whether DCNNs should replace DNNs as a robust baseline recipe for building neural network acoustic models.Like a DNN, a DCNN is a feed-forward model which computes the conditional distribution ppy|xq. The initial layers in a DCNN use convolutional layers in place of the standard fully-connected layers present in DNNs. Convolutional layers were originally developed to enable neural networks to deal with large image inputs for computer vision tasks. In a convolutional model, we restrict the total number of network parameters by using hidden units which connect to only a small, localized region of the input. These localized hidden units are applied at many different spatial locations to obtain hidden layer representations for the entire input. In addition to controlling the number of free parameters, reusing localized hidden units at different locations leverages the stationary nature of many input domains. In the computer vision domain, this amounts to reusing the same edge-sensitive hidden units at each location of the image rather than forcing the model to learn the same type of hidden unit for each location separately. Figure 2 shows a convolutional hidden layer connected to input features with time and frequency axes. A single weight matrix W 1 connects to a 3x3 region of the input and we compute a hidden unit activation value using the same rectifier nonlinearity presented in Equation 6. We apply this same procedure at all possible locations of the input, moving one step at a time across the input in both dimensions. This process produces a feature map h p1,1q which is the hidden activation values for W 1 at each location of the input. The feature map itself has meaningful time and frequency axes because we preserve these dimensions as we convolve across the input to compute hidden unit activations.Our convolutional hidden layer has a feature map with redundancies because we apply the hidden units at each location as we slide across the input. Following the convo- lutional layer, we apply a pooling operation. Pooling acts as Note that the 5 ˆ 5 filters applied to each position in the convolution step are constrained to be the same. For max-pooling, the maximum value in each 3 ˆ 3 grid is extracted.a down-sampling step, and hard-codes invariance to slight translations in the input. Like the localized windows used in the convolutional layer, the pooling layer connects to a contiguous, localized region of its input -the feature map produced by a convolutional hidden layer. The pooling layer does not have overlapping regions. We apply this pooling function to local regions in each feature map. Recall that a feature map contains the hidden unit activations for only a single hidden unit. We are thus using pooling to select activation values for each hidden unit separately, and not forcing different hidden units to compete with one another. In our work, we use max pooling which applies a max function to the set of inputs in a single pooling region. Max pooling is a common choice of pooling function for neural networks in both computer vision and acoustic modeling tasks [58], [27], [29]. The most widely used alternative to max pooling replaces the max function with an averaging function. Results with max pooling and average pooling are often comparable. The overall architecture of a DCNN consists of one or more layers of convolution followed by pooling followed by densely connected hidden layers and a softmax classifier. Essentially we build convolution and pooling layers to act as input to a DNN rather than building a DNN from the original input features. It is not possible to interleave densely connected and convolutional hidden layers because a densely connected hidden layer does not preserve spatial or time-frequency relationships in their hidden layer representations. The DCNN architecture contains more hyper-parameters than a standard DNN because we must select the number of convolutional layers, input region size for all convolution and pooling layers, and pooling function. These are additional hyper-parameters to the choices of depth and hidden layer size common to all types of deep neural network architectures. multiple hidden units. We need not apply both of these ar- chitectural ideas simultaneously. In a deep local untied neural network (DLUNN) we again utilize locally-connected hidden units but do not share weights at different regions of the input. Figure 3 shows an example DLUNN architecture, which differs only from a DCNN architecture by using different weights at each location of the first hidden layer. When applying a local untied hidden layer to Mel-spectrum time- frequency input features the hidden units can process different frequency ranges using different hidden units. This allows the network to learn slight variations that may occur when a feature occurs at a lower frequency versus a higher frequency.In DLUNNs, the architecture is the same as in the con- volutional network, except that filters applied to different regions of the input are not constrained to be the same. Thus untied neural networks can be thought of as convolutional neural networks using locally connected computations and without weight-sharing. This results in a large increase in the number of parameters for the untied layers relative to DCNNs. Following each locally united layer we apply a max pooling layer which behaves identically to the pooling layers in our DCNN architecture. Grouping units together with a max pooling function often results in hidden weights being similar such that the post-pooling activations are an invariant feature which detects a similar time-frequency pattern at different regions of the input.DCNNs combine two architectural ideas simultaneously - locally-connected hidden units and sharing weights acrossHaving defined several neural network architectures and the loss function we wish to optimize, we must specify which gradient-based algorithm we use to find a local minimum of our loss function. We consider only stochastic gradient techniques in our work as batch optimization, which requires computing the gradient across the entire dataset at each step, is impractical for the datasets we use. There are several variants of stochastic gradient techniques, many with different convergence properties when applied to convex optimization problems. Because neural network training is a non-convex problem, it is difficult to make general statements about optimality of optimization methods. Instead, we consider the choice of optimization algorithm as a heuristic which may lead to better performance in practice. We consider two of the most popular stochastic gradient techniques for our neural network training.The first optimization algorithm we consider is stochastic gradient with classical momentum (CM) [59], [60]. This tech- nique is probably the most standard optimization algorithm choice in modern neural network research. To minimize a cost function f pθq classical momentum updates amount to, optimization by looking ahead to the gradient along the update direction. For a more detailed explanation of the intuition underlying NAG optimization for neural network tasks see Figure 7.1 in [62]. In our work, we treat optimization algo- rithm choice as an empirical question and compare CM with NAG on our acoustic modeling task to establish performance differences.where v t denotes the accumulated gradient update, or velocity, ą 0 is the learning rate, and the momentum constant µ P r0, 1s governs how we accumulate the velocity vector over time. By setting µ close to one, one can expect to accumulate the gradient information across a larger set of past updates. However, it can be shown that for extremely ill-conditioned problems, a high momentum for classical momentum method might actually cause fluctuations in the parameter updates. This in turn can result in slower convergence.Recently the Nesterov's accelerated gradient (NAG) [61] technique was found to address some of the issues encountered when training neural networks with CM. Both methods follow the intuition that accumulating the gradient updates along the course of optimization will help speed up convergence. NAG accumulates past gradients using an alternative update equation that finds a better objective function value with less sensitivity to optimization algorithm hyper-parameters on some neural network tasks. The NAG update rule is defined as, We first carry out LVCSR experiments on the 300 hour Switchboard conversational telephone speech corpus (LDC97S62). The baseline GMM system and forced align- ments are created using the Kaldi open-source toolkit 1 [63]. The baseline recognizer has 8,986 sub-phone states and 200k Gaussians. The DNN is trained to estimate state likelihoods which are then used in a standard hybrid HMM/DNN setup. Input features for the DNNs are MFCCs with a context of ˘10 frames. Per-speaker CMVN is applied and speaker adap- tation is done using fMLLR. The features are also globally normalized prior to training the DNN. Overall, the baseline GMM system setup largely follows the existing s5b Kaldi recipe and we defer to previous work for details [4]. For recognition evaluation, we report on a test set consisting of both the Switchboard and CallHome subsets of the HUB5 2000 data (LDC2002S09) as well as a subset of the training set consisting of 5,000 utterances.We first experiment with perhaps the most direct approach to improving performance with DNNs -making DNNs larger by adding hidden units. Increasing the number of parameters in a DNN directly increases the representational capacity of the model. Indeed, this representational scalability drives much of the modern interest in applying DNNs to large datasets which might easily saturate other types of models. Many existing experiments with DNN acoustic models focus on introducing architecture or loss function variants to further specialize DNNs for speech tasks. We instead ask the question of whether model size alone can drive significant improvements in overall system performance. We additionally experiment with using a larger context window of frames as a DNN input as this should also serve as a direct path to improving the frame classification performance of DNNs.1) Experiments: We explore three different model sizes by varying the total number of parameters in the network. The number of hidden layers is fixed to five, so altering the total number of parameters affects the number of hidden units in each layer. All hidden layers in a single network have the same number of hidden units. The hidden layer sizes are 2048, 3953 and 5984 which respectively yield models with approximately 36 million (M), 100M and 200M parameters. There are 8,986 output classes which results in the output layer being the largest single layer in any of our networks. In DNNs of the size typically studied in the literature this output layer often consumes a majority of the total parameters in the network. For example in our 36M parameter model the output layer comprises 51% of all parameters. In contrast, the output layer in our 200M model is only 6% of total parameters. Many output classes occur rarely so devoting a large fraction of network parameters to class-specific modeling may be wasteful. Previous work explores factoring the output layer to increase the relative number of shared parameters [64], [65], but this effect occurs naturally by substantially increasing network size. For our larger models we experiment with the standard input of ˘10 context frames and additionally models trained with ˘20 context frames.All models use hidden units with the rectified linear non- linearity. For optimization, we use Nesterov's accelerated gradient with a smooth initial momentum schedule which we clamp to a maximum of 0.95 [46]. The stochastic updates are on mini-batches of 512 examples. After each epoch, or full pass through the data, we anneal the learning rate by half. Training is stopped after improvement in the cross entropy objective evaluated on held out development set falls below a small tolerance threshold.In order to efficiently train models of the size mentioned above, we distribute the model and computation across sev- eral GPUs using the distributed neural network infrastructure proposed by [53]. Our GPU cluster and distributed training software is capable of training up to 10 billion parameter DNNs. We restrict our attention to models in the 30M -200M parameter range. In preliminary experiments we found that DNNs with 200M parameters are representative of DNNs with over one billion parameters for this task. We train models for this paper in a model-parallel fashion by distributing the parameters across four GPUs. A single pass through the training set for a 200M parameter DNN takes approximately 1.5 days. Table I shows frame-level and WER evaluations of acoustic models of varying size compared against our baseline GMM recognizer.2) Results: However, frame-level performance is not always a good proxy for WER performance of a final system. We evaluate WER on a subset of the training data as well as the final evaluation sets. Large DNN acoustic models substantially reduce WER on the training set. Indeed, our results suggest that further training set WER reductions are possible by continuing to increase DNN model size. However, the gains we observe on the training set in WER do not translate to large performance gains on the evaluation sets. While there is a small benefit of using models larger than the 36M DNN baseline size, building models larger than 100M parameters does not prove beneficial for this task.3) Discussion: To better understand the dynamics of train- ing large DNN acoustic models, we plot training and eval- uation WER performance during DNN training. Figure 4 shows WER performance for our 100M and 200M parameter DNNs after each epoch of cross entropy training. We find that training WER reduces fairly dramatically at first and then continues to decrease at a slower but still meaningful rate. In contrast, nearly all of our evaluation set performance is realized within the first few epochs of training. This has two important practical implications for large DNN training for speech recognition. First, large acoustic models are not beneficial but do not exhibit a strong over-fitting effect where evaluation set performance improves for awhile before be- coming increasingly worse. Second, it may be possible to utilize large DNNs without prohibitively long training times by utilizing our finding that most performance comes from the first few epochs, even with models at our scale. Finally, although increasing context window size improves all training set metrics, those gains do not translate to improved test set performance. It seems that increasing context window size provides an easy path to better fitting the training function, but does not result in the DNN learning a meaningful, gener- alizable function.Dropout is a recently-introduced technique to prevent over- fitting during DNN training [2]. The dropout technique ran- domly masks out hidden unit activations during training, which prevents co-adaptation of hidden units. For each example observed during training, each unit has its activation set to zero with probability p P r0, 0.5s. Several experiments demonstrate dropout as a good regularization technique for tasks in computer vision and natural language processing [57], [66]. [23] found a reduction in WER when using dropout on a 10M parameter DNN acoustic model for a 50 hour broadcast news LVCSR task. Dropout additionally yielded performance gains for convolutional neural networks with less than 10M parameters on both 50 and 400 hour broadcast news LVCSR tasks [28]. While networks which employ dropout during training were found effective in these studies, the authors did not perform control experiments to measure the impact of dropout alone. We directly compare a baseline DNN to a DNN of the same architecture trained with dropout. This experiment tests whether dropout regularization can mitigate the poor generalization performance of large DNNs observed in Section V-A. 1) Experiments: We train DNN acoustic models with dropout to compare generalization WER performance against that of the DNNs presented in Section V. The probability of dropout p is a hyper-parameter of DNN training. In prelimi- nary experiments we found setting p " 0.1 to yield the best generalization performance after evaluating several possible values, p P t0.01, 0.1, 0.25, 0.5u. The DNNs presented with dropout training otherwise follow our same training and eval- uation protocol used thus far, and are built using the same forced alignments from our baseline HMM-GMM system.2) Results: Table II shows the test set performance of DNN acoustic models of varying size trained with dropout. DNNs trained with dropout improve over the baseline model for all acoustic model sizes we evaluate. The improvement is a consistent 0.2% to 0.4% reduction in absolute WER on the test set. While beneficial, dropout seems insufficient to fully harness the representational capacity of our largest models. Additionally, we note that hyper-parameter selection was criti- cal to finding any gain when using dropout. With a poor setting of the dropout probability p preliminary experiments found no gain and often worse results from training with dropout.Early stopping is a regularization technique for neural networks which halts loss function optimization before com- pletely converging to the lowest possible function value. We evaluate early stopping as another standard DNN reg- ularization technique which may improve the generalization performance of large DNN acoustic models. Previous work by [67] found that early stopping training of networks with large capacity produces generalization performance on par with or better than the generalization of a smaller network. Further, this work found that, when using back-propagation for opti- mization, early in training a large capacity network behaves similarly to a smaller capacity network. Finally, early stopping as a regularization technique is similar to an 2 weight norm penalty, another standard approach to regularization of neural network training.1) Results: By analyzing the training and test WER curves in Figure 4 we can observe the best-case performance of an early stopping approach to improving generalization. If we select the lowest test set WER the system achieves during DNN optimization, the 200M parameter DNN achieves 20.7% WER on the EV subset -only 0.1% better than the 100M parameter baseline DNN system. This early stopped 200M model achieves only a 0.5% absolute WER reduction over the much smaller 36M parameter DNN. This suggests that early stopping is beneficial, but perhaps insufficient to yield the full possible benefits of large DNN acoustic models.We next introduce a potential regularization technique which leverages the process by which training labels are created for DNN acoustic model training. Acoustic model training data is labeled via a forced alignment of the word- level transcriptions. We test whether re-labeling the training data during training using the partially-trained DNN leads to improved generalization performance. Each short acoustic span x i has an associated HMM state label y i to form a supervised learning problem for DNN training. Recall that the labels y are generated by a forced alignment of the word-level ground truth labels w to the acoustic signal x. This forced alignment uses an existing LVCSR system to generate a labeling y consistent with the word-level transcription w. The system used to generate the forced alignment is, of course, imperfect, as is the overall speech recognition framework's ability to account for vari- ations in pronunciation. This leads to a dataset D where supervised training pairs px i , y i q P D contain labels y which are imperfect. We can consider a label y i as a corrupted version of the true label y ˚ i . The corruption function which maps y ˚ i to y i is difficult to specify and certainly not independent nor identically distributed at the level of individual samples. Such a complex corruption function is difficult to analyze or address with standard machine learning techniques for label noise. We hypothesize, however, that the noisy labels y are sufficiently correct as to make significantly corrupted labels appear as outliers with respect to the true labels y ˚ . Under this assumption we outline an approach to improving generalization based on the dynamics of DNN performance during training optimization.Neural networks exhibit interesting dynamics during opti- mization. Work on early stopping found that networks with high capacity exhibit behavior similar to smaller, limited capacity networks in early phases of optimization [67]. Com- bining this finding with the generally smooth functional form of DNN hidden and output units suggests that early in training a large capacity DNN may fit a smooth output function which ignores some of the label noise in y. Of course, a large enough DNN should completely fit the corruptions present in y as optimization converges. Studies on the learning dynamics of DNNs for hierarchical categorization tasks additionally suggest that coarse, high-level output classes are fit first during training optimization [68].Realignment, or generating a new forced alignment using an improved acoustic model, is a standard tool for LVCSR system training. Baseline LVCSR systems using GMM acous- tic models realign several times during training to iteratively improve. While iterative realignments have been helpful in improving system performance in single-layer ANN-HMM hybrid models [5], realignment is typically not used with large DNN acoustic models because of the long training times of DNNs. However, realignment using a fully trained DNN acoustic model often can produce a small reduction in final system WER [2].We evaluate early realignment which generates a new forced alignment early in DNN optimization and then continues training on the new set of labels. Because large capacity DNNs begin accurately predicting labels much earlier in training, early realignment may save days of training time. Further, we hypothesize that a less fully converged network can remove some label distortions while a more completely trained DNN may already be fitting to the corrupt labels given by an imperfect alignment.1) Experiments: We begin by training an initial DNN using the same HMM-GMM forced alignments and non-regularized training procedures presented thus far. After training the DNN using the initial HMM-GMM alignments for a fixed number of epochs, we use our new HMM-DNN system to generate a new forced alignment for the entire training set. DNN training then proceeds using the same DNN weights but the newly- generated training set labels. As in our other regularization experiments, we hold the rest of our DNN training and evaluation procedures fixed to directly measure the impact of early realignment training. We train 100M parameter five hidden layer DNNs and build models by realigning after either two or five epochs.In preliminary experiments we found that realignment after each epoch was too disruptive to DNN training and resulted in low quality DNN models. Similarly, we found that starting from a fresh, randomly initialized DNN after realignment per- formed worse than continuing training from the DNN weights used to generate the realignment. We found it important to reset the stochastic gradient learning rate to its initial value after realignment occurs. Without doing so, our annealing schedule sets the learning rate too low for the optimization procedure to fully adjust to the newly-introduced labels. In a control experiment, we found that resetting the learning rate alone, without realignment, does not improve system performance.2) Results: Epoch with realignment after epoch five must train an additional three epochs, for a total of eight, before it can match the performance of a DNN trained with early realignment. For DNNs of the scale we use, this translates to several days of compute time. The training time and WER reduction of DNNs with early realignment comes with a cost of implementing and performing realignment, which is of course not a standard DNN training technique. Realignment requires specializing DNN training to the speech recognition domain, but any modern LVCSR system should already contain infrastructure to generate a forced alignment from an HMM-DNN system. Overall, we conclude that early realignment is an effective technique to improve performance of DNN acoustic models with minimal additional training time. A DNN which re-generates its training labels with a forced alignment early during optimization generalizes much better to test data than a DNN which converges to the original labels.DNN system, but slightly worse than a system which realigns after two epochs of training. Early realignment leads to better WER performance than all models we evaluated trained with dropout and early stopping. This makes early realignment the overall best regularization technique we evaluated on the Switchboard corpus. We note that only early realignment outperforms dropout regularization -a DNN trained with realignment after five epochs performs comparably to a DNN of the same size trained with dropout.3) Discussion: Figure 5 shows training and test WER curves for 100M parameter DNN acoustic models trained with early realignment and a baseline DNN with no realignment. We note that just after realignment both train and test WER increase briefly. This is not surprising as realignment substan- tially changes the distribution of training examples. The DNN trained with realignment trains for three epochs following realignment before it begins to outperform the baseline DNN system.We can quantify how much the labeling from realignment differs from the original labeling by computing the fraction of labels changed. In early realignment 16.4% of labels are changed by realignment while only 10% of labels are changed when we realign with the DNN trained for five epochs. This finding matches our intuition that as a large capacity DNN trains it converges to fit the corrupted training samples extremely well. Thus when we realign the training data with a fully trained large capacity DNN the previously observed labels are reproduced nearly perfectly. Realigning with a DNN earlier in optimization mimics realigning with a higher bias model which relabels the training set with a smoother approximate function. Taken together, our results suggest early realignment leverages the high bias characteristics of the initial phases of DNN training to reduce WER while requiring minimal additional training time.Early realignment also shows a huge benefit to training time compared to traditional realignment. The DNN trainedThe experiments thus far modify DNN training by adding various forms of regularization. We now experiment with alternative neural network architectures -deep convolutional neural networks (DCNNs) and deep local untied neural net- works (DLUNNs).We trained DCNN and DLUNN acoustic models using the same Switchboard training data as used for our DNN acoustic model experiments to facilitate direct comparisons across architectures. We evaluate filter bank features in addition to the fMLLR features used in DNN training because filter bank features have meaningful spectro-temporal dimensions for local receptive field computations. All models have five hidden layers and were trained using Nesterov's accelerated gradient with a smoothly increasing momentum schedule capped at 0.95 and a step size of 0.01, halving the step size after each epoch.For our DCNN and DLUNN acoustic models we chose a receptive field of 9 ˆ 9 and non-overlapping pooling regions of dimension 1 ˆ 3 (time by frequency). Our models with two convolutional layers have the same first layer filter and pooling sizes. The second layer uses a filter size of 3 ˆ 3 and does not use pooling. These parameters were selected using results from preliminary experiments as well as results from previous work [28].In the DCNNs one convolutional layer was used followed by four densely connected layers with equal number of hidden units, and similarly for the DLUNNs. Map depth and number of hidden units were selected such at all models have approx- imately 36M parameters. For DCNNs, the convolutional first layer has a map depth of 128 applied to an input with ˘10 frame context. The following dense hidden layers each have 1,240 hidden units. Our 2 convolutional layer DCNN uses 128 feature maps in both convolutional layers and 3 dense layers with 1,240 hidden units each. All DLUNNs use 108 filters at each location in the first layer, and 4 hidden layers each with 1,240 hidden units.The filter bank and fMLLR features are both 40- dimensional. We ran initial experiments convolving filters along frequency only, pooling along both frequency and time, and overlapping pooling regions, but did not find that these settings gave better performance. We ran experiments with a context window of ˘20 frames but found results to be worse than results obtained with a context window of ˘10 frames, so we report only the ˘10 frame context results.Table III shows the frame-level and final system perfor- mance results for acoustic models built from DNNs, DCNNs, and DLUNNs. When using filter bank features, DCNNs and DLUNNs both achieve improvements over DNNs. DCNN models narrowly outperform DLUNN models. For locally connected acoustic models it appears that the constraint of tied weights in convolutional models is advantageous as compared to allowing a different set of localized receptive fields to be learned at different time-frequency regions of the input.DLUNNs outperform DCNNs in experiments with fMLLR features. Indeed, the DLUNN performs about as well as the DNN. The DCNN is harmed by the lack of meaningful rela- tionships along the frequency dimension of the input features, whereas the more flexible architecture of the DLUNN is able to learn useful first layer parameters. We also note that our fMLLR features yield much better performance for all models as compared with models trained on filter bank features.In order to examine how much benefit using DCNNs to leverage local correlations in the acoustic signal yields, we ran control experiments with filter bank features randomly permuted along both the frequency and time axes. The results show that while this harms performance the convolutional architecture can still obtain fairly competitive word error rates. This control experiment confirms that locally connected models do indeed leverage localized properties of the input features to achieve improved performance.While DCNN and DLUNN models are promising as com- pared to DNN models on filter bank features, our results with filter bank features are overall worse than results from models utilizing fMLLR features.Note that the filter bank features we used are fairly simple as compared to our fMLLR features as the filter bank features do not contain significant post-processing for speaker adapta- tion. While performing such feature transformations may give improved performance, they call into question the initial moti- vation for using DCNNs to automatically discover invariance to gender, speaker and time-frequency distortions. The fMLLR features we compare against include much higher amounts of specialized post-processing, which appears beneficial for all neural network architectures we evaluated. This confirms recent results from previous work, which found that DCNNs alone are not typically superior to DNNs but can complement a DNN acoustic model when both are used together, or achieve competitive results when increased amounts of post-processing are applied to filter bank features [29]. In summary, we con- clude that DCNNs and DLUNNs are not sufficient to replace DNNs as a default, reliable choice for acoustic modeling network architecture. We additionally conclude that DLUNNs warrant further investigation as alternatives to DCNNs for acoustic modeling tasks.On the Switchboard 300 hour corpus we observed limited benefits from increasing DNN model size for acoustic model- ing, even with a variety of techniques to improve generaliza- tion performance. We next explore DNN performance using a substantially larger training corpus. This set of experiments explores how we expect DNN acoustic models to behave when training set size is not a limiting factor. In this setting, over- fitting with large DNNs should be less of a problem and we can more thoroughly explore architecture choices in large DNNs rather than regularization techniques to reduce over-fitting and improve generalization with a small training corpus.To maximize the amount of training data for a conversa- tional speech transcription task, we combine the Switchboard corpus with the larger Fisher corpus [69]. The Fisher corpus contains approximately 2,000 hours of training data, but has transcriptions which are slightly less accurate than those of the Switchboard corpus.Our baseline GMM acoustic model was trained on features that are obtained by splicing together 7 frames (3 on each side of the current frame) of 13-dimensional MFCCs (C0-C12) and projecting down to 40 dimensions using linear discriminant analysis (LDA). The MFCCs are normalized to have zero mean per speaker 2 . After obtaining the features with LDA, we also use a single semi-tied covariance (STC) transform on the fea- tures. Moreover, speaker adaptive training (SAT) is done using a single feature-space maximum likelihood linear regression (fMLLR) transform estimated per speaker. The models trained on the full combined Fisher+Switchboard training set contain 8725 tied triphone states and 3.2M Gaussians.The language model in our baseline system is trained on the combination of the Fisher transcripts and the Switchboard Mississippi State transcripts. Kneser-Ney smoothing was ap- plied to fine-tune the back-off probabilities to minimize the perplexity on a held out set of 10K transcript sentences from Fisher transcripts. In preliminary experiments we interpolated the transcript-derived language model with a language model built from a large collection of web page text, but found no gains as compared with using the transcript-derived language model alone.We use two evaluation sets for all experiments on this corpus. First, we use the same Hub5'00 (Eval2000) corpus used to evaluate systems on the Switchboard 300hr task. This evaluation set serves as a reference point to compare systems built on our combined corpus to those trained on Switchboard alone. Second, we use the RT-03 evaluation set which is more frequently used in the literature to evaluate Fisher-trained systems. Performance of the baseline HMM-GMM system is shown in Table IV and Table V. 3 1) Experiments: We train several DNNs with five hidden layers, where each layer has 2,048 hidden units. This results in DNNs with roughly 36M total free parameters, which is a typical size for acoustic models used for conversational speech transcription in the research literature. For both the classical momentum and Nesterov's accelerated gradient op- timization techniques the two key hyper-parameters are the initial learning rate and the maximum momentum µ max . In all cases we decrease the learning rate by a factor of 2 every 200,000 iterations. This learning rate annealing was chosen after preliminary experiments, and overall performance does not appear to be significantly affected by annealing schedule. It is more common to anneal the learning rate after each pass through the dataset. Because our dataset is quite large we found that annealing only after each epoch leads to much slower convergence to a good optimization solution.2) Results: Table IV shows both WER performance and classification accuracy of DNN-based ASR systems with vari- ous optimization algorithm settings. We first evaluate the effect of optimization algorithm choice. We evaluated DNNs with µ max P 0.9, 0.95, 0.99 and P t0.1, 0.01, 0.001u. For both optimization algorithms DNNs achieve the best performance by setting µ max " 0.99 and " 0.01.In terms of frame level accuracy the NAG optimizer nar- rowly outperforms the CM optimizer, but WER performance across all evaluation sets are nearly identical. For both op- timization algorithms a high value of µ max is important for good performance. Note most previous work in hybrid acoustic models use CM with µ max " 0.90, which does not appear to be optimal in our experiments. We also found that a larger initial learning rate was beneficial. We ran experiments using ě 0.05 but do not report results because the DNNs diverged during the optimization process. Similarly, all models trained with " 0.001 had WER more than 1% absolute higher on the EV test set as compared to the same architecture trained with " 0.01. We thus omit the results for models trained with " 0.001 from our results table.For the remainder of our experiments we use the NAG optimizer with µ max " 0.99 and " 0.01. These settings achieve the best performance overall in our initial experiments, and generally we have found the NAG optimizer to be some- what more robust than the CM optimizer in producing good parameter solutions.To avoid exhaustively searching over all DNN architecture and training parameters simultaneously, we first establish the impact of optimization algorithm choice while holding the DNN architecture fixed. We train networks with the two optimization algorithms described in Section IV-E to deter- mine which optimization algorithm to use in the rest of the experiments on this corpus.We next evaluate the performance of DNNs as a function of the total number of model parameters while keeping network depth and optimization parameters fixed. This approach di- rectly assesses the hypothesis of improving performance as a function of model size when there is sufficient training data available. We train DNNs with 5 hidden layers, and keep the number of hidden units constant across each hid- den layer. Varying total free parameters thus corresponds to adding hidden units to each hidden layer.  Overall, the 400M parameter model performs best in terms of both frame classification and WER across all evaluation sets. Unlike with our smaller Switchboard training corpus experiments, increasing DNN model size does not lead to significant over-fitting problems in WER. However, the gain from increasing model size from 36M to 400M, more than a 10x increase, is somewhat limited. On the Eval2000 evaluation set we observe a 3.8% relative gain in WER from the 100M DNN as compared to the 36M DNN. When moving from the 100M DNN to the 200M DNN there is relative WER gain of 2.5%. Finally the model size increase from 200M to 400M total parameters yields a relative WER gain of 1%. There are clearly diminishing returns as we increase model size. The trend of diminishing relative gains in WER also occurs on the RT03 evaluation set, although relative gains on this evaluation set are somewhat smaller overall.Frame classification rates on this corpus are much lower overall as compared with our Switchboard corpus DNNs. We believe this corpus is more challenging due to more overall acoustic variation, and errors induced by quick transcriptions. Even our largest DNN leaves room for improvement in terms of frame classification. In Section VIII we explore more thoroughly the frame classification performance of the DNNs presented here. performance for DNNs with 1, 3, 5, and 7 hidden layers for DNNs of at multiple total parameter counts.The most striking distinction in terms of both frame clas- sification and WER is the performance gain of deep models versus those with a single hidden layer. Single hidden layer models perform much worse than DNNs with 3 hidden layers or more. Among deep models there are much smaller gains as a function of depth. Models with 5 hidden layers show a clear gain over those with 3 hidden layers, but there is little to no gain from a 7 hidden layer model when compared with a 5 hidden layer model. These results suggest that for this task 5 hidden layers may be deep enough to achieve good performance, but that DNN depth taken further does not increase performance. It's also interesting to note that DNN depth has a much larger impact on performance than total DNN size. For this task, it is much more important to select an appropriate number of hidden layers than it is to choose an appropriate total model size.For each total model size there is a slight decrease in frame classification in 7 layer DNNs as compared with 5 hidden layer DNNs. This trend of decreasing frame-level performance is also present in the training set, which suggests that as networks become very deep it is more difficult to minimize the training objective function. This is evidence for a potential confounding factor when building DNNs. In theory deeper DNNs should be able to model more complex functions than their shallower counterparts, but in practice we found that depth can act as a regularizer due to the difficulties in optimizing very deep models.We next compare performance of DNN systems while keeping total model size fixed and varying the number of hidden layers in the DNN. The optimal architecture for a neural network may change as the total number of model parameters changes. There is no a priori reason to believe that 5 hidden layers is optimal for all model sizes. Furthermore, there are no good general heuristics to select the number of hidden layers for a particular task. Table V shows DNN system VIII. WER AND FRAME CLASSIFICATION ERROR ANALYSIS We now decompose our task performance metrics of frame classification accuracy and WER into their constituent compo- nents to gain a deeper understanding of how models compare to one another. This analysis attempts to uncover differences in models which achieve similar aggregate performance. For example, two systems which have the same final WER may have different rates of substitutions, deletions, and insertions -the constituent components of the WER metric.  Figure 6 shows decomposed WER performance of HMM- DNN systems of varying DNN size. Each HMM-DNN system uses a DNN with 5 hidden layers, these are the same HMM- DNN systems reported in Table V. We see that decreases in overall WER as a function of DNN model size are largely driven by lower substitution rates. Insertions and deletions remain relatively constant across systems, and are generally the smaller components of overall WER. Decreased substi- tution rates should be a fairly direct result of improving acoustic model quality as the system becomes more confident in matching audio features to senones. While the three WER sub-components are linked, it is possible that insertions and deletions are more an artifact of other system shortcomings such as out of vocabulary words (OOVs) or a pronunciation dictionary which does not adequately capture pronunciation variations.We next analyze performance in terms of frame-level clas- sification grouped by phoneme. When understanding senone classification we can think of the possible senone labels as leaves from a set of trees. Each phoneme acts as the root of a different tree, and the leaves of a tree correspond to the senones associated with a the tree's base phoneme. Figure 7 shows classification percentages of senones grouped by their base phoneme. The DNNs analyzed are the same 5 hidden layer models presented in our WER analysis of Figure 6 and Table V. The total height of each bar reflects its percentage of occurrence in our data. Each bar is then broken into three components -correct classifications, errors within the same base phoneme, and errors outside the base phoneme. Errors within the base phoneme correspond the examples where the true label is a senone from a particular base phone, e.g. ah, but the network predicts an incorrect senone label also rooted in ah. The other type of error possible is predicting a senone from a different base phoneme. Together these three categories, correct, same base phone, and different base phone, Eval2000 WER of 5 hidden layer DNN systems of varying total parameter count. WER is broken into its sub-components -insertions, substitutions, and deletions. additively combine to form the total set of senone examples for a given base phone.The rate of correct classifications is non-decreasing as a function of DNN model size for each base phoneme. The overall increasing accuracy of larger DNNs comes from small correctness increases spread across many base phonemes. Across phonemes we see substantial differences in within- base-phoneme versus out-of-base-phoneme error rates. For example, the vowel iy has a higher rate of within-base- phoneme errors as compared to the fairly similar vowel ih. Similarly, the consonants m, k, and d have varying rates of within-base versus out-of-base errors despite having similar total rates of base phoneme occurrence in the data. We note that our DNNs generally exhibit similar error patterns  to those observed with DNN acoustic models on smaller corpora [70]. However, due to the challenging nature of our corpus we observe overall lower phone accuracies than those found in previous work. Performance as a function of model size appears to change gradually and fairly uniformly across phonemes, rather than larger models improving upon only specific phonemes, perhaps at the expense of performance on others.Our experiments so far focus on task performance at varying levels of granularity. These metrics address the question of what DNNs are capable of doing as classifiers and when in- tegrated with HMM speech decoding infrastructure. However, we have not yet completely addressed the question of how various DNN architectures achieve their various levels of task performance. While the DNN computation equations presented in Section IV describe the algorithmic steps necessary to compute predictions, there are many possible settings of the free parameters in a model. In this section we offer a descrip- tive analysis of how our trained DNNs encode information. This analysis aims to uncover quantifiable differences in how models of various sizes and depths encode input data and transform it to make a final prediction.Our first analysis focuses on the sparsity patterns of units with each hidden layer of a DNN. We compute the empirical lifetime sparsity of each hidden unit by forward propagating a set of 512,000 examples through the DNN. We consider a unit as a active when its output is non-zero, and compute the fraction of examples for which a unit is active as its lifetime activation probability. This value gives the empirical probability that a particular unit will activate given a random input drawn from our sample distribution. For each hidden layer of a network, we can plot all hidden units' lifetime activation probabilities sorted in decreasing order to get a sense for the distribution of activation probabilities within a layer. This plotting technique, sometimes called a scree plot, helps us understand how information coding is distributed across units in a hidden layer. Figure 8 shows a set of scree plots for 5 hidden layer DNNs of varying total model size.From a coding theory perspective, researchers often discuss DNNs as learning efficient codes which are both sparse and dispersed. Sparsity generally refers to relatively few hidden units in a hidden layer being active in response to an input. Sparsity is efficient and seems natural given modern DNN structures in which hidden layer size is often much larger than input vector dimensionality. Dispersion refers to units within a hidden layer equally sharing responsibility for coding inputs. A representation with perfect dispersion would appear flat in a scree plot. A scree plot also visualizes sparsity as the average height of representation units on the y axis.Generally we see that in all model sizes sparsity increases in deeper layers of the DNN. The first hidden layer is noticeably more active on average as compared with every other layer in the DNN, in most cases by almost a factor of two. Beyond the first layer, activation probability per layer decreases slightly as we look at deeper layers of the DNN. The changes in activation probability per layer within deeper hidden layers are fairly minor, which suggest that a representation is transformed but not continually compressed.Dispersion is similar within layers of a particular DNN size. Generally the representations appear fairly disperse, with a mostly flat curve for each hidden layer and only a few units which are on or off for a large percentage of inputs at each tail. There does appear to be a slight trend of increasing dispersion in deeper layers of the DNN, especially in larger models.Most importantly, we do not observe a significant set of permanently inactive units as DNNs grow in total number of parameters. In larger DNNs the representation remains fairly disperse, with only a small set of units which are active for less than 1% of inputs. This is an important metric because adding more parameters to a DNN is only useful in so far as those parameters are actually used in encoding and transforming inputs.Given the task performance differences observed as a function of DNN depth for a fixed number of total DNN parameters, we also compare scree plots as a function of DNN depth to better understand their coding properties. Figure 9 shows scree plots for DNNs with 1, 3, 5, and 7 hidden layers for DNNs of total size 36M, 100M, and 200M. We observe a general trend of average activation probability decreasing in subsequent hidden layers of DNNs at each size. This is not true, however, for models with 7 hidden layers, which have slightly less sparse activations on average in layers 6 and 7 as compared to layer 5. As we compare models across total model size we find that larger models are more sparse than smaller models. Larger models also tend to be slightly more dispersed on average compared with smaller models. for each hidden layer over a large sample of inputs from our dataset. Figure 10 shows average code length for each hidden layer of DNNs of varying depth and total size.As we compare code length across models of varying total parameter size, we see that larger DNNs use more hidden units per layer to encode information at each hidden layer. This trend is especially evident in the first hidden layer, where 100M parameter models use nearly twice the code length as compared to 36M models. In deeper layers, we again observe that models with more parameters have greater code length. It is unclear to what extend the longer codes are capturing more information about an input, which in turn should en- able greater classification accuracy, versus redundancy where multiple hidden units encode overlapping information.Code length in deeper versus more shallow models of the same total size exhibit an interesting trend. DNNs of increasing depth show a generally decreasing or constant code length per layer, except in the case of our 7 hidden layer DNNs. In 7 hidden layer DNNs, the deepest models we trained, code length decreases until it reaches a minimum at layer 5, but then increases in layers 6 and 7. This trend is evident in models of 36M, 100M, and 200M total parameters. We note that this trend of decreasing code length followed by increasing code length is correlated with the lack of improvement of 7 hidden layer models as compared to 5 hidden layer models. More experiments are needed to establish whether code length in deeper models is more generally correlated to diminishing task performance.Our sparsity and dispersion metrics serve as indicators for how hidden units within each layer behave. We now focus on code length, which analyzes each hidden layer as a transformed representation of the input rather than focusing on individual units with each hidden layer. For a given input we compute the number of non-zero hidden unit activations in a hidden layer. We can then compute the average code lengthThe multi-step process of building neural network acoustic models comprises a large design space with a broad range of previous work. Our work sought to address which of the most fundamental DNN design decisions are most relevant for final ASR system performance. We found that increasing model size and depth are simple but effective ways to improve WER performance, but only up to a certain point. For the Switchboard corpus, we found that regularization can im- prove the performance of large DNNs which otherwise suffer from overfitting problems. However, a much larger gain was achieved by utilizing the combined 2,100hr training corpus as opposed to applying regularization with less training data.Our experiments suggest that the DNN architecture is quite competitive with specialized architectures such as DCNNs and DLUNNs. The DNN architecture outperformed other archi- tecture variants in both frame classification and final system WER. While previous work has used more specialized features with locally connected models, we note that DNNs enjoy the benefit of making no assumptions about input features having meaningful time or frequency properties. This enables us to build DNNs on whatever features we choose, rather than ensuring our features match the assumptions of our neural net- work. We found that DLUNNs performed slightly better and DCNNs, and may be an interesting approach for specialized acoustic modeling tasks. For example, locally untied models may work well for robust or reverberant recognition tasks where particular frequency ranges experience interference or distortion.We trained DNN acoustic models with up to 400M pa- rameters and 7 hidden layers, comprising some of the largest models evaluated to date for acoustic modeling. When trained with the simple NAG optimization procedure, these large DNNs achieved clear gains on both frame classification and WER when the training corpus was large. An analysis of performance and coding properties revealed a fairly gradual change in DNN properties as we move from smaller to larger models, rather than finding some phase transition where large models begin to encode information differently from smaller models. Overall, total network size, not depth, was the most critical factor we found in our experiments. Depth is certainly important with regards to having more than one hidden layer, but differences among DNNs with multiple hidden layers were fairly small with regards to all metrics we evaluated. At a certain point it appears that increasing DNN depth yields no performance gains, and may indeed start to harm performance. When applying DNN acoustic models to new tasks it appears sufficient to use a fixed optimization algorithm, we suggest NAG, and cross-validate over total network size using a DNN of at least three hidden layers, but no more than five. Based on our results, this procedure should instantiate a reasonably strong baseline system for further experiments, by modifying whatever components of the acoustic model building procedure researchers choose to explore.Finally, we note that a driving factor in the uncertainty around DNN acoustic model research stems from training the acoustic model in isolation from the rest of the larger ASR system. All models trained in this paper used the cross entropy criterion, and did not perform as well as DNNs trained with discriminative loss functions in previous work. We hypothesize that large DNNs will become increasingly useful as researchers invent loss functions which entrust larger components of the ASR task to the neural network. This allows the DNN to utilize its function fitting capacity to do more than simply map acoustic inputs to HMM states.We believe a better understanding of task performance and coding properties can guide research on new, improved DNN architectures and loss functions. We trained DNNs using approximately 300 lines of Python code, demonstrating the feasibility of fairly simple architectures and optimization procedures to achieve good system performance. We hope that this serves as a reference point to improve communication and reproducibility in the now highly active research area of neural networks for speech and language understanding.  figure shows a DNN with 1, 3, 5, and 7 hidden layers. Hidden units (x axis) are sorted by their probability of activation. We consider any positive value as active (hpxq ą 0).  Fig. 10. Effective code length for each hidden layer in DNNs with varying total size and depth. We compute the number on non-zero hidden unit activations for a given input, and then average over a large sample of inputs. Plots show the average number of units active in each hidden layer of DNNs of varying depth and total size. Within each sub-plot layers are ordered left to right from first to final hidden layer.
