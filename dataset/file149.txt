Boltzmann machines have seen successful applications in recognition of images and other tasks of machine learning [38,32,5,4], particularly with recent development of deep learning [14]. The standard approaches to training a Boltzmann machine iteratively apply a Hebbian rule [10] either exactly or approximately, where the values of the parameters are updated in the directions of in- creasing the likelihood of given training data with respect to the equilibrium distribution of the Boltzmann machine [15]. This Hebbian rule for the Boltzmann machine is limited in the sense that the concept of time is missing. For biological neural networks, spike-timing dependent plasticity (STDP) has been postulated and supported empirically [21,2,31]. For example, the synaptic weight is strengthened when a post-synaptic neuron fires shortly after a pre-synaptic neuron fires (i.e., long term potentiation) but is weakened if this order of firing is reversed (i.e., long term depression).In this paper, we study the dynamics of a Boltzmann machine, or a dynamic Boltzmann machine (DyBM) 1 , and derive a learning rule for the DyBM that can be interpreted as STDP. While the conventional Boltzmann machine is trained with a collection of static patterns (such as images), the DyBM is trained with a time-series of patterns. In particular, the DyBM gives the conditional probability of the next values (patterns) of a time-series given its historical values. This conditional probability can depend on the whole history of the time-series, and the DyBM can thus be used iteratively as a generative model of a time-series.Specifically, we define the DyBM as a Boltzmann machine having multiple layers of units, where one layer represents the most recent values of a time-series, and the remaining layers represent the historical values of the time-series. We assume that the most recent values are conditionally independent of each other given the historical values. The DyBM allows an infinite number of layers, so that the most recent values can depend on the whole history of the time series. We train the DyBM in such a way that the likelihood of given time-series is maximized with respect to the conditional distribution of the next values given the historical values. This definition of the DyBM and the general approach to training the DyBM constitute the first contribution of this paper.We show that the learning rule for the DyBM is significantly simplified and exhibits various char- acteristics of STDP that have been observed in biological neural networks [1], when the DyBM has an infinite number of layers and particularly structured parameters. Specifically, we assume that the weight between a unit representing a most recent value (time 0) and a unit representing the value in the past (time −t) is the sum of geometric functions with respect to t. We show that updating pa- rameters associated with a pair of units requires only the information that is available at those units (i.e., local in space), and the required information can be maintained by keeping a first-in-first-out (FIFO) queue of the last values of a unit (i.e., local in time). The convergence of the learning rule is guaranteed with sufficiently low learning rate, because the parameters are always updated such that the likelihood of given training data is increased. The learning rule that is formally derived for the DyBM and its interpretation as STDP constitute the second contribution of this paper.The prior work has extended the Boltzmann machine to incorporate the timing of spikes in various ways [13,33,34,36,24]. However, the existing learning rules for those extended Boltzmann ma- chines involve approximation with contrastive divergence and do not have some of the characteristics of the STDP (e.g., long term depression) that we show for the DyBM.We will see that the DyBM can be considered as a recurrent neural network (RNN) equipped with memory units. The DyBM is thus related to Long Short Term Memory [17,7,8,6] and other RNNs [11,26,28,22,35]. What distinguishes the DyBM from existing RNNs is that training a DyBM does not require "backpropagation through time," or a chain rule of derivatives. This distinguishing feature of the DyBM follows from the fact the DyBM can be equivalently interpreted both as an RNN and as a non-recurrent Boltzmann machine. The learning rule derived from the interpretation as a non-recurrent Boltzmann machine clearly does not involve backpropagation through time but is a proper learning rule for the equivalent RNN. As a result, training a DyBM is free from the "vanishing gradient problem" [16,27].The learning rule for some of the existing recurrent neural networks involves STDP but in a more limited form than our learning rule. For example, the learning rule of [26] depends on the timing of spikes but only on whether a post-synaptic neuron fires immediately after a pre-synaptic neuron fires. In our learning rule, the magnitude of the changes in the weight can depend on the difference between the timings of the two spikes, as has been observed for biological neural networks [1].An extended version of this paper has appeared in [25]. This paper, however, contains some of the details and perspectives that are omitted from [25]. Also, note that the notations and terminologies in this paper are not necessarily consistent with those in [25].After reviewing the Boltzmann machine and the restricted Boltzmann machine in Section 2.1, we define the DyBM in Section 2.2. A learning rule for the DyBM and its interpretation as STDP will be provided in Section 3.A Boltzmann machine is a network of units that are mutually connected with weight (see Fig- ure 1 (a)). Let N be the number of units. For i ∈ [1, N ], let x i be the value of the i-th unit, where x i ∈ {0, 1}. Let w i,j be the weight between the i-th unit and the j-th unit for i, j ∈ [1, N ]. It is standard to assume that w i,i = 0 and w i,j = w j,i for i, j ∈ [1, N ]. For i ∈ [1, N ], the bias, b i , is associated with the i-th unit. We use the following notations of column vectors and matrices: [1,N ] , and W ≡ (w i,j ) i,j∈ [1,N ] 2 . Let θ ≡ (b, W) denote the parameters of the Boltzmann machine.The energy of the values, x, for the N units of the Boltzmann machine having θ is given by The (equilibrium) probability of generating particular values, x, is given bywhere the summation over˜xover˜ over˜x denotes the summation over all of the possible configurations of a binary vector of length N , and τ is the parameter called temperature.The Boltzmann machine can be trained in the direction of maximizing the log likelihood of a given set, D, of the vectors of the values:x∈D where P θ (D) ≡ x∈D P θ (x), and˜ x A Hebbian rule can be derived from (4). For example, to increase the likelihood of a data point, x, we should update W i,j bywhere i X j θ denotes the expected value of the product of the values of the i-th unit and the j-th unit with respect to P θ in (2), and η is a learning rate [15].A particularly interesting case is a restricted Boltzmann machine (RBM), where the units are divided into two layers, and there is no weight between the units in each layer (see Figure 1 (b)). In this case, P θ (x) can be evaluated approximately with contrastive divergence [12,30] or other methods [37,3] when the exact computation of (4) is intractable (e.g., when N is large).A key property of the RBM that allows contrastive divergence is the following conditional indepen- dence. For t ∈ [1,2], let x [t] be the values of the units in the t-th layer and b [t] be the corresponding bias. Let W [1] be the matrix whose (i, j) element is the weight between the i-th unit in the first layer and the j-th unit in the second layer. Then the conditional probability of x [2] given x [1] is given byx [2] j ∈{0,1}where P θ,j (x [2] j |x [1] ) denotes the conditional probability that the j-th unit of the second layer has value x [2] j given that the first layer has values x [1] , and we definewhere W [1] :,j denotes the j-th column of W [1] . Namely, the value of the units in the second layer,x [2] , is conditionally independent of each other given x [1] .We propose the dynamic Boltzmann machine (DyBM), which can have infinitely many layers of units (see Figure 1 (c)). Similar to the RBM, the DyBM has no weight between the units in the right-most layer of Figure 1 (c). Unlike the RBM, each layer of the DyBM has a common number, N , of units, and the bias and the weight in the DyBM can be shared among different units in a particular manner.Formally, we define the DyBM-T as the Boltzmann machine having T layers from −T + 1 to 0, where T is a positive integer or infinity. Letis the values of the units in the t-th layer, which we consider as the values at time t. Let b be the bias for the N units at the 0-th layer (the right-most layer of Figure 1 i,j , denotes the weight between the i-th unit at time −δ and the j-th unit at time 0 for any t. A unit in the s-th layer for s ≤ −1 can have arbitrary bias and arbitrary weight with a unit in the t-th layer for t ≤ −1, but such bias and weight have no effect in the DyBM, which we will see in the following. The DyBM with infinitely many layers is defined with a formal limit of T → ∞., where we use x I for an interval I such as (−T, −1] to denote (x [t] ) t∈I . Because the units in the 0-th layer have no weight with each other, this conditional probability has the property of conditional independence analogous to (7). Note thatis independent of the bias and the weight that are not associated with the units in the 0-th layer.We propose the DyBM as a model of a time-series in the following sense. Specifically, given a history x (−T,−1] of a time-series, the DyBM-T gives the probability of the next values, x [0] of the time-series with P θ (x [0] |x (−T,−1] ). A DyBM-2 can be interpreted as a Markov model, and DyBM-T for T &gt; 2 as a (T − 1)-st order Markov model, where the next values are conditionally independent of the history given the values of the last (T − 1) steps. With a DyBM-∞, the next values can depend on the whole history of the time-series. In principle, the DyBM-∞ can thus model any time-series possibly with long-term dependency, as long as the values of the time-series at a moment is conditionally independent of each other given its values preceding that moment. Using the conditional probability given by a DyBM-T , the probability of a sequence,where we arbitrarily defineNamely, the values are set zero if there are no corresponding history.We derive a learning rule for a DyBM-T in such a way that the log likelihood of a given (set of) time-series is maximized. We will see that the learning rule is particularly simplified in the limit of T → ∞ when the parameters of the DyBM-∞ have particular structures. We will show that this learning rule exhibits various characteristics of spike-timing dependent plasticity (STDP).The log likelihood of a given set, D, of time-series can be maximized by maximizing the sum of the log likelihood of x ∈ D. By (9), the log likelihood of x = x (−L,0] has the following gradient:The figure illustrates Equation (12) with particular forms of Equation (13). The horizontal axis represents δ, and the vertical axis represents the value of Wi,j is defined for δ &gt; 0 and is discontinuous atare defined for −∞ &lt; δ &lt; ∞ and discontinuous at δ = d i,j and δ = −d j,i , respectively.We can thus exploit the conditional independence of (7) to derive the learning rule:One could, for example, update θ in the direction of (11) every time new x [0] is observed, using the latest history, x (−T, −1] . This is an approach of stochastic gradient. In practice, however, the computation of (11) can be intractable for a large T , because there are Θ(M T ) parameters to learn, where M is the number of the pairs of connected units (M = Θ(N 2 ) when all of the units are densely connected).We thus propose a particular form of weight sharing, which is motivated by observations from biological neural networks [1] but leads to particularly simple, exact, and efficient learning rule. In biological neural networks, STDP has been postulated and supported experimentally. In particular, the synaptic weight from a pre-synaptic neuron to a post-synaptic neuron is strengthened, if the post-synaptic neuron fires (generates a spike) shortly after the pre-synaptic neuron fires (i.e., long term potentiation or LTP). This weight is weakened, if the post-synaptic neuron fires shortly before the pre-synaptic neuron fires (i.e., long term depression or LTD). These dependency on the timing of spikes is missing in the Hebbian rule for the Boltzmann machine (5).To derive a learning rule that has the characteristics of STDP with LTP and LTD, we consider the weight of the form illustrated in Figure 2. For δ &gt; 0, we define the weight, W In Figure 2, the value ofˆWofˆ ofˆW [δ] i,j is high when δ = d i,j , the (synaptic) delay from i-th (pre-synaptic) unit to the j-th (post-synaptic) unit. Namely, the post-synaptic neuron is likely to fire (i.e., x[0] j = 1) immediately after the spike from the pre-synaptic unit arrives with the delay of. This likelihood is controlled by the magnitude ofˆWofˆ ofˆW, which we will learn from training data.The value ofˆWofˆ ofˆW [δ] i,j gradually decreases, as δ increases from d i,j . That is, the effect of the stimulus of the spike arrived from the i-th unit diminishes with time [1].The value ofˆWofˆ ofˆWis low, suggesting that the post-synaptic unit is unlikely to fire (i.e., x[0] j = 1) immediately before the spike from the i-th (pre-synaptic) unit arrives. This unlikelihood is controlled by the magnitude ofˆWofˆ ofˆW, which we will learn. As δ decreases from d i,j − 1, the magnitude ofˆWofˆ ofˆW [δ] [δ]i,j gradually decreases [1]. Here, δ can get smaller than 0, andˆWandˆ andˆW i,j with δ &lt; 0 represents the weight between the spike of the pre-synaptic neuron that is generated after the spike of the post-synaptic neuron.The assumption of W [0] i,j = 0 is convenient for computational purposes but can be justified in the limit of infinitesimal time steps. Specifically, consider a scaled DyBM where both the step size of the time and the probability of firing are made 1/n-th of the original DyBM. In the limit of n → ∞, the scaled DyBM has continuous time, and the probability of having simultaneous spikes from two units tends to zero.For tractable learning and inference, we assume the following form of weight:where λ k , µ ∈ (0, 1) for k ∈ K and ∈ L. We will learn the values of u i,j,k and v i,j,, based on training dataset. We assume that λ k for k ∈ K, µ for ∈ L, and d i,j for i, j ∈ [1, N ] are given (or need to be learned as hyper-parameters). With an analogy to biological neural networks, these given parameters (λ k , µ , and d i,j ) are determined based on physical constraints or chemical properties, while the weight (u i,j,k and v i,j,, ) and the bias (b) are learned based on the neural spikes (x). The sum of geometric functions with varying decay rates [19] in (13) is motivated by long-term memory (or dependency) [23,34]. See Figure 3 for the flexibility of the sum of geometric functions. In particular, the sum of geometric functions can well approximate a hyperbolic function, whose value decays more slowly than any geometric functions. This slow decay is considered to be essential for long-term memory. However, our results also hold for the simple cases where |K| = |L| = 1.With the above specifications and letting T → ∞, we can now represent the E θ,j (x [0] j |x (−T,−1] ) appearing in (11) as follows:t=−∞ where Wdenotes a column vector. By (12) and (13), the second term of (14) is given by where α i,j,k , β i,j,, , and γ i,, are the quantities, which we refer to as eligibility traces, that depend on x, the history of the i-th unit:t=−∞ We now derive the derivatives of (14), which we need for (11), as follows:By plugging the above derivatives into (11), we have, for i, j ∈ [1, N ], k ∈ K, and ∈ L, that where [0] j θ denotes the expected value of the j-th unit in the 0-th layer of the DyBM-∞ given the history x (−∞, −1] . Because the value is binary, this expected value is given bywhich can be calculated, using the eligibility traces in (16)- (18). Here, the first term of the denomi- nator of (27) To maximize the likelihood of a given set, D, of sequences, the parameters θ can be updated withx∈D Typically, a single time-series, y [1,L] , is available for training the DyBM-∞. In this case, we form D ≡ {y [1,t] | t ∈ [1, L]}, where y [1,t] is used as, where recall that we arbitrarily set zeros when there are no history (i.e.,When D is made from a single time-series, the eligibility traces (16)- (18) needed for training with y [1,t] can be computed recursively from the ones used for y [1,t−1] . In particular, we haveThis recursive calculation requires keeping the following FIFO queue of length d i,j − 1:for each i, j ∈ [1, N ]. After training with y [1,t−1] , the q i,j is updated by adding y and delet- ing y. The remaining eligibility trace β i,j,, can be calculated non-recursively by the use of q i,j . In fact, our experience suggests that a recursive calculation of β i,j,k is amenable to numerical instability, so that β i,j,k should be calculated non-recursively.Here, we will show that the DyBM-∞ can indeed be understood as a generative model of a time series. For this purpose, we will specify the bias for the units in the s-th layer for s ≤ −1 and the weight between the units in the s-th layer and the units in the t-th layer, for s, t ≤ −1. Recall thata i,j,k t t Figure 5: Spikes traveling from a pre-synaptic neuron (i) to a post-synaptic neuron (j) and eligibility traces.these bias and weight have not been specified in the discussion so far, because they do not affect the conditional probability:This model of a single time step with (32) is used iteratively in (9) to define the distribution of a time series. Strictly speaking, however, this iterative use of (32) is not a generative model defined solely with a Boltzmann machine.We now consider the following homogeneous DyBM (see Figure 4), which is a special case of a DyBM-∞. Let each layer of the units has a common vector of bias. That is, for any s, the units in the s-th layer have the bias, b. Let the matrix of the weight between two layers, s and t, depend only on the distance, t − s, between the two layers. That is, for any pair of s and t, the i-th unit in the s-th layer and the j-th unit in the t-th layer is connected with the weight,The learning rule derived for the DyBM-∞ in Section 3.1 -Section 3.2 holds for the homogeneous DyBM.The key property of the homogeneous DyBM is that the homogeneous DyBM consisting of the layers up to the t-th layer, for t &lt; 0, is equivalent to the homogeneous DyBM consisting of the layers up to the 0-th layer. Therefore, the iterative use of the model of the single time step (32) is now equivalent to the generative model of a time-series defined with a single homogeneous DyBM. Specifically, the values of the time-series at time t are generated based on the conditional probability,, that is given by the homogeneous DyBM consisting of the layers up to the t- th layer. The values, x [t] , generated at time t are then used as a part of the history x (−∞,t] for the homogeneous DyBM consisting of the layers up to the (t + 1)-st layer, which in turn defines the conditional probability of the values at time t + 1. The homogeneous DyBM can also have the layers for positive time steps (t &gt; 0). The homogeneous DyBM can then be interpreted as a recurrent neural network, which we will discuss in the following with reference to an artificial neural network. Figure 5 illustrates the learning rule derived in Section 3.1-Section 3.2 from a point of artificial neural networks. Consider a pre-synaptic neuron, i, and a post-synaptic neuron, j. The FIFO queue, q i,j , can be considered as an axon that stores the spikes traveling from i to j. The conduction delay of this axon is d i,j , and the spikes generated in the last d i,j − 1 steps are stored. The spikes in the axon determine the value of β i,j,, for each ∈ L. Another eligibility trace, γ i,, , records the aggregated information about the spikes generated at the neuron i, where the spikes generated in the past are discounted with the rate that depends on ∈ L. The remaining eligibility trace, α i,j,k , records the aggregated information about the spikes that have reached j from i, where the spikes arrived in the past are discounted with the rate that depends on k ∈ K.The DyBM-∞ can then be considered as a recurrent neural network, taking binary values, equipped with memory units that store eligibility traces and the FIFO queue (see Figure 6). i,j = 0 for a δ ≥ 1 in the DyBM-∞), and D is the maximum delay such that d i,j ≤ D. Specifically, the binary bits correspond to the N bits of x [0] and M FIFO queues. The floating-point numbers correspond to eligibility traces (α i,j,k and γ i,k for i, j ∈ [1, N ], k ∈ K, and ∈ L), the coefficients of the weight (u i,j,k and v i,j,, for i, j ∈ [1, N ], k ∈ K, and ∈ L), and the bias (b j for j ∈ [1, N ]).Each of the parameters of the DyBM-∞ can be updated in a distributed manner by the use of the learning rules from (23)-(25). Observe that this distributed update can be performed in constant time that is independent of N , D, |K|, and |L|.According to (27) and (14), the neuron j is more likely to fire (xand α i,j,k are high, or (iii) both conditions are met. The learning rule (23) suggests that b j increases over time, if the neuron j fires more often than it is expected from the latest values of the parameters of the DyBM-∞. The learning rule (24) suggests that u i,j,k increases over time, if the neuron j fires more often than it is expected, and the magnitude of the changes in u i,j,k is proportional to the magnitude of α i,j,k . These implement long term potentiation.According to (27) and (14), the neuron j is less likely to fire when (i) v i,j,, and β i,j,, are high, (ii) v j,i,, and γ i,, are high, (iii) or both conditions are met. The learning rule (25) suggests that v i,j,k increases over time, if the neuron j fires less often than it is expected, and the magnitude of the changes in v i,j,k is proportional to the magnitude of β i,j,, . When i and j are exchanged, the learning rule (25) suggests that v i,j,k increases over time, if the neuron j fires less often than it is expected, and the magnitude of the changes in v i,j,k is proportional to the magnitude of γ i,, . These implement long term depression.Here, the terms in (23)-(25) that involve expected values (27) can be considered as a mechanism of homeostatic plasticity [20] that keeps the firing probability relatively constant. This particular mechanism of homeostatic plasticity does not appear to have been discussed with STDP in the liter- ature [20,39]. We expect, however, that this formally derived mechanism of homeostatic plasticity plays an essential role in stabilizing the learning of artificial neural networks. Without this homeo- static plasticity, the values of the parameters can indeed diverge or fluctuate during training.Our work provides theoretical underpinnings on the postulates about STDP. Recall that the Hebb rule was first postulated in the middle of the last century [10] but had seen limited success in engineering applications until more than 30 years later when the Hopfield network [18] and the Boltzmann machine [15] are used to provide theoretical underpinnings. In particular, a Hebbian rule was shown to increase the likelihood of data with respect to the distribution associated with the Boltzmann machine [15]. STDP has been postulated for biological neural networks and has been used for artificial neural networks but in rather ad hoc ways. Our work establishes the relation between STDP and the Boltzmann machine for the first time in a formal manner.Specifically, we propose the DyBM as a stochastic model of time-series. The DyBM gives the conditional probability of the next values of a multi-dimensional time-series given its historical values. This conditional probability can depend on the whole history of arbitrary length, so that the DyBM (specifically, DyBM-∞) does not have the limitation of a Markov model or a higher order Markov model with a finite order. The conditional probability given by the DyBM-∞ can thus be applied recursively to obtain the probability of generating a particular time-series of arbitrary length.The DyBM-∞ can be trained in a distributed manner (i.e., local in space) with limited memory (i.e., local in time) when its parameters have a proposed structure. The learning rule is local in space in that the parameters associated with a pair of the units in the DyBM-∞ can be updated by using only the information that is available locally in those units. The learning rule is local in time in that it requires only a limited length of the history of a time-series. This training is guaranteed to converge as long as the learning rate is set sufficiently small.The DyBM-∞ having the proposed structure (i.e., the homogeneous DyBM) can be considered as a recurrent neural network, taking binary values, with memory units (or a recurrent Boltzmann machine with memory). Specifically, each neuron stores eligibility traces and updates their values based on the spikes that it generates and the spikes received from other neurons. An axon stores spikes that travel from a pre-synaptic neuron to a post-synaptic neuron. The synaptic weight is updated every moment, depending on the spikes that are generated at that moment and the values of these eligibility traces and the spikes stored in the axon. This learning rule exhibits various characteristics of STDP, including long term potentiation and long term depression, which have been postulated and observed empirically in biological neural networks. The learning rule also exhibits a form of homeostatic plasticity that is similar to those studied for Bayesian spiking networks (e.g., [9]). However, the Bayesian spiking network is a mixture-of-expert model, which is a particular type of a directed graphical model, while we study a product-of-expert model, which is a particular type of an undirected graphical model. We expect that the theoretical underpinnings on STDP provided in this paper will accelerate engi- neering applications of STDP. In particular, the prior work [13,33,34,36] has proposed various extensions of the Boltzmann machine to deal with time-series data, but existing learning algorithms for these extended Boltzmann machines involve approximations. On the other hand, the homoge- neous DyBM can be considered as a recurrent Boltzmann machine with memory, which naturally extends the Boltzmann machine (at the equilibrium state) by taking into account the dynamics and by incorporating the memory. STDP is to the DyBM what the Hebb rule is to the Boltzmann machine.
