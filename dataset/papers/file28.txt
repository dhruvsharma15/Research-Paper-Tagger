There has recently been a lot of work on adaptive gradient algorithms such as Adagrad ( Duchi et al., 2011), RMSProp ( Hinton et al., 2012), ADADELTA (Zeiler, 2012), and Adam (Kingma &amp; Bai, 2015). The original idea of Ada- grad to have a parameter specific learning rate by analyz- ing the gradients observed during the optimization turned out to be useful not only in online convex optimization but also for training deep neural networks. The original analysis of Adagrad (Duchi et al., 2011) was limited to the case of all convex functions for which it obtained a data- dependent regret bound of order O( √ T ) which is known to be optimal (Hazan, 2016) for this class. However, a lot of learning problems have more structure in the sense that one optimizes over the restricted class of strongly convex functions. It has been shown in (Hazan et al., 2007) that one can achieve much better logarithmic regret bounds for the class of strongly convex functions.The goal of this paper is twofold. First, we propose SC- Adagrad which is a variant of Adagrad adapted to the strongly convex case. We show that SC-Adagrad achieves a logarithmic regret bound for the case of strongly convex functions, which is data-dependent. It is known that such bounds can be much better in practice than data indepen- dent bounds (Hazan et al., 2007), (McMahan, 2014). Sec- ond, we analyze RMSProp which has become one of the standard methods to train neural networks beyond stochas- tic gradient descent. We show that under some condi- tions on the weighting scheme of RMSProp, this algorithm achieves a data-dependent O( √ T ) regret bound. In fact, it turns out that RMSProp contains Adagrad as a special case for a particular choice of the weighting scheme. Up to our knowledge this is the first theoretical result justifying the usage of RMSProp in online convex optimization and thus can at least be seen as theoretical support for its usage in deep learning. Similarly, we then propose the variant SC- RMSProp for which we also show a data-dependent loga- rithmic regret bound similar to SC-Adagrad for the class of strongly convex functions. Interestingly, SC-Adagrad has been discussed in (Ruder, 2016), where it is said that "it does not to work". The reason for this is that SC-Adagrad comes along with a damping factor which prevents poten- tially large steps in the beginning of the iterations. How- ever, as our analysis shows this damping factor has to be rather large initially to prevent large steps and should be then monotonically decreasing as a function of the itera- tions in order to stay adaptive. Finally, we show in experi- ments on three datasets that the new methods are compet- itive or outperform other adaptive gradient techniques as well as stochastic gradient descent for strongly convex op- timization problem in terms of regret and training objective but also perform very well in the training of deep neural networks, where we show results for different networks and datasets.We denote by [T ] the set {1, . . . , T }. Let A ∈ R d×d be a symmetric, positive definite matrix. We denote as Lemma 2.2 For any symmetric, positive semi-definite ma- trix A ∈ R d×d we haveAx ≤ λ max (A) x ≤ tr(A) xwhere λ max (A) is the maximum eigenvalue of matrix A and tr(A) denotes the trace of matrix A .Note that the standard Euclidean inner product becomes y = i x i y i = y I While we use here the gen- eral notation for matrices for comparison to the literature. All positive definite matrices A in this paper will be diago- nal matrices, so that the computational effort for computing inner products and norms is still linear in d. The Cauchy- Schwarz inequality becomes, y A ≤ A A . We further introduce the element-wise product a of two vec- tors. Let a, b ∈ R d , then (a b) i = a i b i for i = 1, . . . , d.In this paper we analyze the online convex optimization setting, that is we have a convex set C and at each round we get access to a (sub)-gradient of some continuous convex function f t : C → R. At the t-th iterate we predict θ t ∈ C and suffer a loss f t (θ t ). The goal is to perform well with respect to the optimal decision in hindsight defined asThe adversarial regret at time T ∈ N is then given asIt is well-known that the weighted projection is unique and non-expansive.Lemma 2.1 Let A ∈ R d×d be a symmetric, positive defi- nite matrix and C ⊂ R d be a convex set. ThenWe assume that the adversarial can choose from the class of convex functions on C, for some parts we will specialize this to the set of strongly convex functions.Definition 2.1 Let C be a convex set. We say that a func- tion f : C → R is µ-strongly convex, if there exists µ ∈ R d with µ i &gt; 0 for i = 1, . . . , d such that for all x, y ∈ C,The first order optimality condition for the weighted projection in (1) is given aswhere N C (x) denotes the normal cone of C at x. This can be rewritten asLet ζ = min i=1,...,d µ i , then this function is ζ-strongly con- vex (in the usual sense), that isThis yieldsAdding these two inequalities yields Note that the difference between our notion of component- wise strong convexity and the usual definition of strong convexity is indicated by the bold font versus normal font. We have two assumptions:• A1: It holds sup t≥1 t 2 ≤ G which implies the existence of a constant G ∞ such that sup t≥1 t ∞ ≤ G ∞ .The result follows from the application of the weighted Cauchy-Schwarz inequality.• A2: It holdsAlgorithm 1 Adagrad (4)Thus an alternative point of view of Adagrad, is that it has a decaying stepsizeOne of the first methods which achieves the optimal regret bound of O( √ T ) for convex problems is online projected gradient descent (Zinkevich, 2003), defined as but now the correction term becomes the running average of the squared derivatives plus a van- ishing damping term. However, the effective stepsize has to decay faster to get a logarithmic regret bound for the strongly convex case. This is what we analyze in the next section, where we propose SC-Adagrad for strongly convex functions.whereis the step-size scheme and g t is a (sub)- gradient of f t at θ t . With α t = α t , online projected gradi- ent descent method achieves the optimal O(log(T )) regret bound for strongly-convex problems (Hazan et al., 2007). We consider Adagrad in the next subsection which is one of the popular adaptive alternative to online projected gradient descent.The modification SC-Adagrad of Adagrad which we pro- pose in the following can be motivated by the observation that the online projected gradient descent ( Hazan et al., 2007) uses stepsizes of order α = O( 1 T ) in order to achieve the logarithmic regret bound for strongly convex functions. In analogy with the derivation in the previous section, we still havet,i . But now we modify (A t )and set it as a diagonal matrix with entries In this section we briefly recall the main result for the Ada- grad. The algorithm for Adagrad is given in Algorithm 1. If the adversarial is allowed to choose from the set of all pos- sible convex functions on C ⊂ R d , then Adagrad achieves the regret bound of order O( √ T ) as shown in (Duchi et al., 2011). This regret bound is known to be optimal for this class, see e.g. (Hazan, 2016). For better comparison to our results for RMSProp, we recall the result from (Duchi et al., 2011) in our notation. For this purpose, we introduce the notation, g 1:T,i = (g 1,i , g 2,i , .., g T,i ) T , where g t,i is the i-th component of the gradient g t ∈ R d of the function f t evaluated at θ t .Again, we have in the denominator a running average of the observed gradients and a decaying damping factor. In this way, we get an effective stepsize of order O( 1 T ) in SC- Adagrad. The formal method is presented in Algorithm 2. As just derived the only difference of Adagrad and SC- Adagrad is the definition of the diagonal matrix A t . Note Algorithm 2 SC-Adagradhold and let θ t be the sequence generated by Adagrad in Algorithm 1, where g t ∈ ∂f t (θ t ) and f t : C → R is an arbitrary convex function, then for stepsize α &gt; 0 the regret is upper bounded asThe effective step-length of Adagrad is on the order of also that we have defined the damping factor δ t as a func- tion of t which is also different from standard Adagrad. The constant δ in Adagrad is mainly introduced due to nu- merical reasons in order to avoid problems when g t,i is very small for some components in the first iterations and is typically chosen quite small e.g. δ = 10 −8 . For SC- Adagrad the situation is different. If the first components g 1,i , g 2,i , . . . are very small, say of order then the update is to be small. This would make the method very unstable and would lead to huge constants in the bounds. This is probably why in (Ruder, 2016), the modification of Ada- grad where one "drops the square-root" did not work. A good choice of δ t should be initially roughly on the order of 1 and it should decay asProof: Consider the following summation, 2 t,i starts to grow. This is why we propose to usefor ξ 1 &gt; 0, ξ 2 &gt; 0 as a potential decay scheme as it satis- fies both properties for sufficiently large ξ 1 and ξ 2 chosen on the order of 1. Also, one can achieve a constant de- cay scheme for ξ 1 = 0 , ξ 2 &gt; 0. We will come back to this choice after the proof. In the following we provide the regret analysis of SC-Adagrad and show that the optimal logarithmic regret bound can be achieved. However, as it is data-dependent it is typically significantly better in practice than data-independent bounds.Note: There was initial work in ( Duchi et al., 2010) where SC-Adagrad using a constant decay scheme was already proposed and also shown to have O(log T ) regret bounds for strongly convex problems. Unfortunately, we were un- aware of this work when we created SC-Adagrad, as it wasn't mentioned in journal version (Duchi et al., 2011).Recently, there was a related work done for constant de- cay scheme in ( Gupta et al., 2017) which gives a unified analysis of all the adaptive algorithms and some of the re- sults regarding logarithmic regret bounds (see for Section 4 in ( Gupta et al., 2017)) closely match to that of SC- Adagrad (Algorithm 2). Our new contribution is that the decay scheme is vectorized so one need not restrict to a constant scheme. We only require a mild condition, that it is non-increasing element-wise in order to achieve loga- rithmic regret bounds.In the first step we usewhere A is a diagonal matrix and subsequently we use ∀t, and for t = 1 we have diag(g 1 g TFor any two matrices A, B ∈ R d×d , we use the notation • to denote the inner product i.e A • B = 1 ) = A 1 − diag(δ 1 ). In the first inequality we use Lemma 3.1 also see for Lemma 12 of ( Hazan et al., 2007). Note that for T = 1, the upper bound results in 0.Lemma 3.1 [Lemma 12 (Hazan et al., 2007)] Let A, B be positive definite matrices, let A B 0 then Theorem 3.1 Let Assumptions A1, A2 hold and let θ t be the sequence generated by the SC-Adagrad in Algorithm 2, where g t ∈ ∂f t (θ t ) andwhere |A| denotes the determinant of the matrix A 2µi . Furthermore, let δ t &gt; 0 and, then the regret of SC- Adagrad can be upper bounded for T ≥ 1 as Lemma 3.2 Let Assumptions A1, A2 hold, then for T ≥ 1 and A t , δ t as defined in the SC-Adagrad algorithm we have,For constant δ t i.e δ t,i = δ &gt; 0 ∀t ∈ [T ] and ∀i ∈ [d] then the regret of SC-Adagrad is upper bounded asIn the last step we use the equality ∀x ∈ R nwe ob- tain the above mentioned regret bounds.A−B where A, B ∈ R n x n and both are diagonal matrices. Now, we choose α such thatProof: We rewrite the regret bound with the definition of µ-strongly convex functions as ∞ I + diag(δ 1 ) because at any round the difference between subsequent squares of sub-gradients is bounded byUsing the non-expansiveness we haveHence we can upper bound the regret as followsIn the second inequality we boundedIn the second last step we use the Lemma 3.2. So under a constantwe obtain the the same results as µ-strongly convex functions. This can be seen by settingNote that the first and the last term in the regret bound can be upper bounded by constants. Only the second term depends on T . Note that 1:T,i 2 ≤ T G 2 and as δ t is monotonically decreasing, the second term is on the order of O(log(T )) and thus we have a logarithmic regret bound. As the bound is data-dependent, in the sense that it depends on the observed sequence of gradients, it is much tighter than a data-independent bound.an interesting question for future work, if there exists an optimal decay scheme which provably works better than any constant one.The bound includes also the case of a non-decaying damp- ing factor δ t = δ = ξ 2 (ξ 1 = 0). While a rather large constant damping factor can work well, we have noticed that the best results are obtained with the decay schemewhere, which is what we use in the ex- periments. Note that this decay scheme for ξ 1 , ξ 2 &gt; 0 is adaptive to the specific dimension and thus increases the adaptivity of the overall algorithm. For completeness we also give the bound specialized for this decay scheme.Corollary 3.1 In the setting of Theorem 3.1 choose δ t,i = ξ 2 e −ξ1vt,i for i = 1, . . . , d for some ξ 1 &gt; 0, ξ 2 &gt; 0 . Then the regret of SC-Adagrad can be upper bounded for T ≥ 1 as RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks ( Schaul et al., 2014;Dauphin et al., 2015;Daniel et al., 2016;Schmidhuber, 2015). It has been used frequently in computer vision (Karpathy &amp; Fei-Fei, 2016) e.g. to train the latest InceptionV4 network ( Szegedy et al., 2016a;b). Note that RMSProp outperformed other adaptive methods like Adagrad order Adadelta as well as SGD with momen- tum in a large number of tests in ( Schaul et al., 2014). It has been argued that if the changes in the parameter update are approximately Gaussian distributed, then the matrix A t can be seen as a preconditioner which approximates the di- agonal of the Hessian ( Daniel et al., 2016). However, it is fair to say that despite its huge empirical success in prac- tice and some first analysis in the literature, there is so far no rigorous theoretical analysis of RMSProp. We will an- alyze RMSProp given in Algorithm 3 in the framework of of online convex optimization.Plugging this into Theorem 3.1 for ξ 1 , ξ 2 &gt; 0 yields the results for the first three terms. UsingFirst, we will show that RMSProp reduces to Adagrad for a certain choice of its parameters. Second, we will prove for the general convex case a regret bound of O( √ T ) sim- ilar to the bound given in Theorem 2.1. It turns out that the convergence analysis requires that in the update of the weighted cumulative squared gradients (v t ) , it has to holdNote that 1:j,i 2 + δ j,i = v j,i + ξ 2 e −ξ1vj,i , in order to find the minimum of this term we thus analyze the func- tion f : R + → R, f (x) = x + ξ 2 e −ξ1x . and a straight- forward calculation shows that the minimum is attained at(log(ξ 1 ξ 2 ) + 1). This yields the fourth term.Unfortunately, it is not obvious that the regret bound for our decaying damping factor is better than the one of a constant damping factor. Note, however that the third term in the re- gret bound of Theorem 3.1 can be negative. It thus remains for some 0 &lt; γ ≤ 1. This is in contrast to the original suggestion of ( Hinton et al., 2012) to choose β t = 0.9. It will turn out later in the experiments that the constant choice of β t leads sometimes to divergence of the sequence, whereas the choice derived from our theoretical analysis always leads to a convergent scheme even when applied to deep neural networks. Thus we think that the analysis in the following is not only interesting for the convex case but can give valuable hints how the parameters of RMSProp should be chosen in deep learning.Before we start the regret analysis we want to discuss the sequence v t in more detail. Using the recursive definition of v t , we get the closed form expression Note that in the last step we have used thatgiven that x − c ≥ 0, along with, then we recover the update scheme of Adagrad, see (4), as a particular case of RMSProp. We are not aware of that this correspondence of Adagrad and RMSProp has been observed before.for v t,i = 0 we haveThe proof of the regret bound for RMSProp relies on the following lemma.Lemma 4.1 Let Assumptions A1 and A2 and suppose that 1 −Using the above bound we have the following 1 γ t ≤ β t ≤ 1 − t for some 0 &lt; γ ≤ 1, and t ≥ 1. Also for t &gt; 1 supposeProof: The lemma is proven via induction. For T = 1 we have v 0 = 0 and thus1,i and thussince 2(2 − γ) &gt; 1 for γ ≤ 1 hence the bound holds for T = 1. For T &gt; 1 we suppose that the bound is true for T − 1 and getIn the last step we use (8) and since for T &gt; 1 the term 1 −Corollary 4.1 Let Assumptions A1, A2 hold and supposet for some 0 &lt; γ ≤ 1, and t ≥ 1. Also for t &gt; 1 suppose (t − 1) t−1 ≤ √ tt t , and set α t = α √ t , thenT,i and withProof: Using the definition of A t = diag( √ v t ) + t I,along with Lemma 4.1 we getWith the help of Lemma 4.1 and Corollary 4.1 we can now state the regret bound for RMSProp.Theorem 4.1 Let Assumptions A1, A2 hold and let θ t be the sequence generated by RMSProp in Algorithm 3, where g t ∈ ∂f t (θ t ) and f t : C → R is an arbitrary convex func- tion and α t = g t , A t=2 t=1 α t 2In the last step we used. We show that for some 0 &lt; γ ≤ 1. Also for t &gt; 1 let (t − 1) t−1 ≤ √ tt t , then the regret of RMSProp can be upper bounded for T ≥ 1 asNote that A t are diagonal matrices for all t ≥ 1. We note thatProof: Note that for every convex function f t : C → R it holds for all x, y ∈ C and g t ∈ ∂f t (x), which implies tβ t ≥ t − 1. We getWe use this to upper bound the regret aswhere we used in the last inequality thatUsing the non-expansiveness of the weighted projection, we haveThis yieldsHence we can upper bound the regret as follows where the inequality could be done as we showed before that the difference of the terms in v t,i is non-negative forVariants of RMSProp and Adagrad with Logarithmic Regret Bounds Algorithm 4 SC-RMSPropand A t as defined in SC-RMSProp, then it holds for all T ≥ 1,and using that A t is diagonal with (A t ) ii = v t,i + t,i , we get Thus in total we haveFinally, with Corollary 4.1 we get the result.Note that for, that is γ = 1, and t = T where RMSProp corresponds to Adagrad we recover the regret bound of Adagrad in the convex case, see Theorem 2.1, up to the damping factor. Note that in this caseSimilar to the extension of Adagrad to SC-Adagrad, we present in this section SC-RMSProp which achieves a log- arithmic regret bound. Note that again there exist choices for the parameters of SC-RMSProp such that it reduces to SC-Adagrad. The cor- respondence is given by the choicefor which again it follows v t,i = 1 t t j=1 g 2 T j,i with the same argument as for RMSProp. Please see Equation (5) for the correspondence. Moreover, with the same argument as for SC-Adagrad we use a decay scheme for the damping factorIn the first inequality we useγ . In the last step we use, that ∀t &gt; 1, The analysis of SC-RMSProp is along the lines of SC- Adagrad with some overhead due to the structure of v t .We note thatTheorem 4.2 Let Assumptions A1, A2 hold and let θ t be the sequence generated by SC-RMSProp in Algorithm 4, where g t ∈ ∂f t (θ t ) and f t : C → R is an arbitrary µ- strongly convex function (µ ∈ R d α + ) with α t = and similar, log(|diagNote that for γ = 1 and the choice t = δt δt t this reduces to the result of Lemma 3.2.t for some 0 &lt; γ ≤ 1. Furthermore, set t = t and assume 1 ≥ δ t,i &gt; 0 and, then the regret of SC- RMSProp can be upper bounded for T ≥ 1 asProof: We rewrite the regret bound with the definition of µ-strongly convex functions asUsing the non-expansiveness of the weighted projection, we getThis yieldsHence we can upper bound the regret as followswhere we have used Lemma 4.2 in the last inequality andNote that the regret bound reduces for γ = 1 to that of SC- Adagrad. For 0 &lt; γ &lt; 1 a comparison between the bounds is not straightforward as the v t,i terms cannot be compared. It is an interesting future research question whether it is possible to show that one scheme is better than the other one potentially dependent on the problem characteristics.   The idea of the experiments is to show that the proposed algorithms are useful for standard learning problems in both online and batch settings. We are aware of the fact that in the strongly convex case online to batch conver- sion is not tight (Hazan &amp; Kale, 2014), however that does not necessarily imply that the algorithms behave gener- ally suboptimal. We compare all algorithms for a strongly convex problem and present relative suboptimality plots, log 10 training objective as well as for best test performance after a fixed number of epochs (typically 200 epochs).Datasets: We use three datasets where it is easy, diffi- cult and very difficult to achieve good test performance, just in order to see if this influences the performance. For this purpose we use MNIST (60000 training samples, 10 classes), CIFAR10 (50000 training samples, 10 classes) and CIFAR100 (50000 training samples, 100 classes). We refer to (Krizhevsky, 2009) for more details on the CIFAR datasets., where p * is the global optimum, as well as separate regret plots, where we compare to the best op- timal parameter in hindsight for the fraction of training points seen so far. On the other hand RMSProp was origi- nally developed by (Hinton et al., 2012) for usage in deep learning. As discussed before the fixed choice of β t is not allowed if one wants to get the optimal O( √ T ) regret bound in the convex case. Thus we think it is of interest to the deep learning community, if the insights from the con- vex optimization case transfer to deep learning. Moreover, Adagrad and RMSProp are heavily used in deep learning and thus it is interesting to compare their counterparts SC- Adagrad and SC-RMSProp developed for the strongly con- vex case also in deep learning. For the deep learning ex- periments we optimize the learning rate once for smallest Algorithms: We compare 1) Stochastic Gradient Descent (SGD) (Bottou, 2010) with O(1/t) decaying step-size for the strongly convex problems and for non-convex problems we use a constant learning rate, 2) Adam ( Kingma &amp; Bai, 2015) , is used with step size decay of α t = α √ t for strongly convex problems and for non-convex problems we use a constant step-size. 3) Adagrad, see Algorithm 1, remains the same for strongly convex problems and non-convex problems. 4) RMSProp as proposed in (Hinton et al., 2012) is used for both strongly convex problems and non-convex problems with β t = 0.9 ∀t ≥ 1. 5) RMSProp (Ours) is used with step-size decay of α t =    + λ k t and γ = 0.9 as RMSProp (Ours) 7) SC-Adagrad is used with a constant stepsize α. The decaying damp- ing factor for both SC-Adagrad and SC-RMSProp is used with ξ 1 = 0.1, ξ 2 = 1 for convex problems and we use ξ 1 = 0.1, ξ 2 = 0.1 for non-convex deep learning prob- lems. Finally, the numerical stability parameter δ used in Adagrad, Adam, RMSProp is set to 10 −8 as it is typically recommended for these algorithms.Setup: Note that all methods have only one varying pa- rameter: the stepsize α which we choose from the set of {1, 0.1, 0.01, 0.001, 0.0001} for all experiments. By this setup no method has an advantage just because it has more hyperparameters over which it can optimize. The optimal rate is always chosen for each algorithm separately so that one achieves either best training objective or best test per- formance after a fixed number of epochs.All methods are initialized with zero weights. The regu- larization parameter was chosen so that one achieves the best prediction performance on the test set. The results are shown in Figure 1. We also conduct experiments in an on- line setting, where we restrict the number of iterations to the number of training samples. Here for all the algorithms, we choose the stepsize resulting in best regret value at the end. We plot the Regret ( in log scale ) vs dataset propor- tion seen, and as expected SC-Adagrad and SC-RMSProp outperform all the other methods across all the considered datasets. Also, RMSProp (Ours) has a lower regret values than the original RMSProp as shown in Figure 2.Strongly Convex Case -Softmax Regression: Given the training data (x i , y i ) i∈ [m] and let y i ∈ [K]. we fit a lin- ear model with cross entropy loss and use as regularization the squared Euclidean norm of the weight parameters. TheConvolutional Neural Networks: Here we test a 4-layer CNN with two convolutional (32 filters of size 3 × 3) and one fully connected layer (128 hidden units followed by 0.5 dropout). The activation function is ReLU and after the last convolutional layer we use max-pooling over a 2 × 2 win- dow and 0.25 dropout. The final layer is a softmax layer and the final objective is cross-entropy loss. This is a pretty  The results are shown in Figures 3, 6. SC-RMSProp is competitive in terms of training objective on all datasets though SGD achieves the best performance. SC-Adagrad is not very competitive and the reason seems to be that the nu- merical stability parameter is too small. RMSProp diverges on CIFAR10 dataset whereas RMSProp (Ours) converges on all datasets and has similar performance as Adagrad in terms of training objective. Both RMSProp (Ours) and SC- Adagrad perform better than all the other methods in terms of test accuracy for CIFAR10 dataset. On both CIFAR100 and MNIST datasets SC-RMSProp is very competitive.ally performs as good as Adagrad. Here, the performance is not as competitive as Adagrad, because the numerical sta- bility decay parameter of SC-Adagrad and SC-RMSProp are too prohibitive.Residual Network: We also conduct experiments for ResNet-18 network proposed in ( He et al., 2016a) where the residual blocks are used with modifications proposed in ( He et al., 2016b) on CIFAR10 dataset. We report the results in Figures 4. SC-Adagrad, SC-RMSProp and RM- SProp (Ours) have the best performance in terms of test Accuracy and RMSProp (Ours) has the best performance in terms of training objective along with Adagrad.Multi-Layer Perceptron: We also conduct experiments for a 3-layer Multi-Layer perceptron with 2 fully connected hidden layers and a softmax layer according to the number of classes in each dataset. For the first two hidden layers we have 512 units in each layer with ReLU activation function and 0.2 dropout. The final layer is a softmax layer. We report the results in Figures 7, 8. On all the datasets, SC- Adagrad and SC-RMSProp perform better in terms of Test accuracy and also have the best training objective perfor- mance on CIFAR10 dataset. On MNIST dataset, Adagrad and RMSProp(Ours) achieves best training objective per- formance however SC-Adagrad and SC-RMSProp eventu- Given these experiments, we think that SC-Adagrad, SC- RMSProp and RMSProp (Ours) are valuable new adaptive gradient techniques for deep learning.We have analyzed RMSProp originally proposed in the deep learning community in the framework of online con- vex optimization. We show that the conditions for conver- gence of RMSProp for the convex case are different than what is used by (Hinton et al., 2012) and that this leads to better performance in practice. We also propose vari-   ants SC-Adagrad and SC-RMSProp which achieve loga- rithmic regret bounds for the strongly convex case. More- over, they perform very well for different network models and datasets and thus they are an interesting alternative to existing adaptive gradient schemes. In the future we want to explore why these algorithms perform so well in deep learning tasks even though they have been designed for the strongly convex case.Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization. COLT 2010COLT , pp. 257, 2010 
