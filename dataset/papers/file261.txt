There is a growing interest in incorporating external memory into neural networks. For example, memory networks ( Weston et al., 2014;Sukhbaatar et al., 2015) are equipped with static memory slots that are content or location addressable. Neural Turing machines ( Graves et al., 2014) imple- ment memory slots that can be read and written as in Turing machines (Turing, 1938) but through differentiable attention mechanism.Each memory slot in these models stores a vector corresponding to a continuous representation of the memory content. In order to recall a piece of information stored in memory, attention is typically employed. Attention mechanism introduced by Bahdanau et al. (2014) uses a network that outputs a discrete probability mass over memory items. A memory read can be implemented as a weighted sum of the memory vectors in which the weights are given by the attention network. Reading out a single item can be realized as a special case in which the output of the attention network is peaked at the desired item. The attention network may depend on the current context as well as the memory item itself. The attention model is called location-based and content-based, if it depends on the location in the memory and the stored memory vector, respectively.Knowledge bases, such as WordNet and Freebase, can also be stored in memory either through an explicit knowledge base embedding (Bordes et al., 2011;Nickel et al., 2011;Socher et al., 2013) or through a feedforward network ( Bordes et al., 2015).When we embed entities from a knowledge base in a continuous vector space, if the capacity of the embedding model is appropriately controlled, we expect semantically similar entities to be close to each other, which will allow the model to generalize to unseen facts. However the notion of proximity may strongly depend on the type of a relation. For example, Benjamin Franklin was an engineer but also a politician. We would need different metrics to capture his proximity to other engineers and politicians of his time.Figure 1: Comparison of the conventional content-based attention model using inner product and the proposed Gaussian attention model with the same mean but two different covariances.In this paper, we propose a new attention model for content-based addressing. Our model scores each item v item in the memory by the (logarithm of) multivariate Gaussian likelihood as follows:score(v item ) = log φ(v item |µ context , Σ context )where context denotes all the variables that the attention depends on. For example, "American engineers in the 18th century" or "American politicians in the 18th century" would be two contexts that include Benjamin Franklin but the two attentions would have very different shapes.Compared to the (normalized) inner product used in previous work (Sukhbaatar et al., 2015;Graves et al., 2014) for content-based addressing, the Gaussian model has the additional control of the spread of the attention over items in the memory. As we show in Figure 1, we can view the conven- tional inner-product-based attention and the proposed Gaussian attention as addressing by an affine energy function and a quadratic energy function, respectively. By making the addressing mechanism more complex, we may represent many entities in a relatively low dimensional embedding space. Since knowledge bases are typically extremely sparse, it is more likely that we can afford to have a more complex attention model than a large embedding dimension.We apply the proposed Gaussian attention model to question answering based on knowledge bases. At the high-level, the goal of the task is to learn the mapping from a question about objects in the knowledge base in natural language to a probability distribution over the entities. We use the scoring function (1) for both embedding the entities as vectors, and extracting the conditions mentioned in the question and taking a conjunction of them to score each candidate answer to the question.The ability to compactly represent a set of objects makes the Gaussian attention model well suited for representing the uncertainty in a multiple-answer question (e.g., "who are the children of Abraham Lincoln?"). Moreover, traversal over the knowledge graph (see Guu et al., 2015) can be naturally handled by a series of Gaussian convolutions, which generalizes the addition of vectors. In fact, we model each relation as a Gaussian with mean and variance parameters. Thus a traversal on a relation corresponds to a translation in the mean and addition of the variances.The proposed question answering model is able to handle not only the case where the answer to a question is associated with an atomic fact, which is called simple Q&amp;A ( Bordes et al., 2015), but also questions that require composition of relations (path queries in Guu et al. (2015)) and conjunction of queries. An example flow of how our model deals with a question "Who plays forward for Borussia Dortmund?" is shown in Figure 2 in Section 3.This paper is structured as follows. In Section 2, we describe how the Gaussian scoring function (1) can be used to embed the entities in a knowledge base into a continuous vector space. We call our model TransGaussian because of its similarity to the TransE model proposed by Bordes et al. (2013). Then in Section 3, we describe our question answering model. In Section 4, we carry out experiments on WorldCup2014 dataset we collected. The dataset is relatively small but it allows us to evaluate not only simple questions but also path queries and conjunction of queries. The proposed TransGaussian embedding with the question answering model achieves significantly higher accuracy than the vanilla TransE embedding or TransE trained with compositional relations Guu et al. (2015) combined with the same question answering model.In this section, we describe the proposed TransGaussian model based on the Gaussian attention model (1). While it is possible to train a network that computes the embedding in a single pass (Bordes et al., 2015) or over multiple passes ( Li et al., 2015), it is more efficient to offload the embedding as a separate step for question answering based on a large static knowledge base.Let E be the set of entities and R be the set of relations. A knowledge base is a collec- tion of triplets (s, r, o), where we call s ∈ E, r ∈ R, and o ∈ E, the subject, the re- lation, and the object of the triplet, respectively. Each triplet encodes a fact. For example, (Albert Einstein, has profession, theoretical physicist). All the triplets given in a knowledge base are assumed to be true. However generally speaking a triplet may be true or false. Thus knowledge base embedding aims at training a model that predict if a triplet is true or not given some parameterization of the entities and relations (Bordes et al., 2011;Nickel et al., 2011;Socher et al., 2013;Wang et al., 2014).In this paper, we associate a vector v s ∈ R d with each entity s ∈ E, and we associate each relation r ∈ R with two parameters, δ r ∈ R d and a positive definite symmetric matrix Σ r ∈ R d×d ++ . Given subject s and relation r, we can compute the score of an object o to be in triplet (s, r, o) using the Gaussian attention model as (1) withwhereNote that if Σ r is fixed to the identity matrix, we are modeling the relation of subject v s and object v o as a translation δ r , which is equivalent to the TransE model ( Bordes et al., 2013). We allow the covariance Σ r to depend on the relation to handle one-to-many relations (e.g., profession has person relation) and capture the shape of the distribution of the set of objects that can be in the triplet. We call our model TransGaussian because of its similarity to TransE (Bordes et al., 2013).Parameterization For computational efficiency, we will restrict the covariance matrix Σ r to be diagonal in this paper. Furthermore, in order to ensure that Σ r is strictly positive definite, we employ the exponential linear unit (ELU, Clevert et al., 2015) and parameterize Σ r as follows:where m r,j (j = 1, . . . , d) are the unconstrained parameters that are optimized during training and is a small positive value that ensure the positivity of the variance during numerical computation. The ELU is defined asRanking loss Suppose we have a set of tripletsfrom the knowledge base. Let N (s, r) be the set of incorrect objects to be in the triplet (s, r, ·).Our objective function uses the ranking loss to measure the margin between the scores of true an- swers and those of false answers and it can be written as follows: Here, we treat an inverse relation as a separate relation and denote by ¯ R = R ∪ R −1 the set of all the relations including both relations in R and their inverse relations; a relatioñ r is the inverse relation of r if (s, ˜ r, o) implies (o, r, s) and vice versa. Moreover, E t ∼N (s,r) denotes the expectation with respect to the uniform distribution over the set of incorrect objects, which we approximate with 10 random samples in the experiments. Finally, the last terms are 2 regularization terms for the embedding parameters.2.2 COMPOSITIONAL RELATIONS Guu et al. (2015) has recently shown that training TransE with compositional relations can make it competitive to more complex models, although TransE is much simpler compared to for exam- ple, neural tensor networks (NTN, Socher et al. (2013)) and TransH Wang et al. (2014). Here, a compositional relation is a relation that is composed as a series of relations in R, for exam- ple, grand father of can be composed as first applying the parent of relation and then the father of relation, which can be seen as a traversal over a path on the knowledge graph.TransGaussian model can naturally handle and propagate the uncertainty over such a chain of relations by convolving the Gaussian distributions along the path. That is, the score of an en- tity o to be in the τ -step relation r 1 /r 2 / · · · /r τ with subject s, which we denote by the tripletwithwhere the covariance associated with each relation is parameterized in the same way as in the previous subsection.be a set of randomly sampled paths from the knowledge graph. Here relation r i k in a path can be a relation in R or an inverse relation in R −1 . With the scoring function (4), the generalized training objective for compositional relations can be written identically to (3) except for replacing T with T ∪ P and replacing N with N = |T ∪ P|.Given a set of question-answer pairs, in which the question is phrased in natural language and the an- swer is an entity in the knowledge base, our goal is to train a model that learns the mapping from the question to the correct entity. Our question answering model consists of three steps, entity recog- nition, relation composition, and conjunction. We first identify a list of entities mentioned in the question (which is assumed to be provided by an oracle in this paper). If the question is "Who plays Forward for Borussia Dortmund?" then the list would be [Forward, Borussia Dortmund]. The next step is to predict the path of relations on the knowledgegraph starting from each en- tity in the list extracted in the first step. In the above example, this will be (smooth versions of) /Forward/position played by/ and /Borussia Dortmund/has player/ predicted as series of Gaussian convolutions. In general, we can have multiple relations appearing in each path. Finally, we take a product of all the Gaussian attentions and renormalize it, which is equivalent to Bayes' rule with independent observations (paths) and a noninformative prior.We assume that there is an oracle that provides a list containing all the entities mentioned in the question, because (1) a domain specific entity recognizer can be developed efficiently ( Williams et al., 2015) and (2) generally entity recognition is a challenging task and it is beyond the scope of this paper to show whether there is any benefit in training our question answering model jointly with a entity recognizer. We assume that the number of extracted entities can be different for each question.We train a long short-term memory (LSTM, Hochreiter &amp; Schmidhuber, 1997) network that emits an output h t for each token in the input sequence. Then we compute the attention over the hidden í µí² Marco_Reus score using Eq. (7 Figure 2: The input to the system is a question in natural language. Two entities Forward and Borussia Dortmund are identified in the question and associated with point mass distributions centered at the corresponding entity vectors. An LSTM encodes the input into a sequence of output vectors of the same length. Then we take average of the output vectors weighted by attention p t,e for each recognized entity e to predict the weight α r,e for relation r associated with entity e. We form a Gaussian attention over the entities for each entity e by convolving the corresponding point mass with the (pre-trained) Gaussian embeddings of the relations weighted by α r,e according to Eq. (6). The final prediction is produced by taking the product and normalizing the Gaussian attentions.states for each recognized entity e aswhere v e is the vector associated with the entity e. We use a two-layer perceptron for f in our experiments, which can be written as follows:is the rectified linear unit. Finally, softmax denotes softmax over the T tokens.Next, we use the weights p t,e to compute the weighted sum over the hidden states h t asThen we compute the weights α r,e over all the relations as α r,e = ReLU wHere the rectified linear unit is used to ensure the positivity of the weights. Note however that the weights should not be normalized, because we may want to use the same relation more than once in the same path. Making the weights positive also has the effect of making the attention sparse and interpretable because there is no cancellation.For each extracted entity e, we view the extracted entity and the answer of the question to be the subject and the object in some triplet (e, p, o), respectively, where the path p is inferred from the question as the weights α r,e as we described above. Accordingly, the score for each candidate answer o can be expressed using (1) as:withr,e Σ r , where v e is the vector associated with entity e and ¯ R = R ∪ R −1 denotes the set of relations including the inverse relations.Let E(q) be the set of entities recognized in the question q. The final step of our model is to take the conjunction of the Gaussian attentions derived in the previous step. This step is simply carried out by multiplying the Gaussian attentions as follows:which is again a (logarithm of) Gaussian scoring function, where µ e,α,KB and Σ e,α,KB are the mean and the covariance of the Gaussian attention given in (6). Here Θ denotes all the parameters of the question-answering model.Suppose we have a knowledge base (E, R, T ) and a trained TransGaussian model, where ¯ R is the set of all relations including the inverse rela- tions. During training time, we assume the training set is a supervised question-answer pairs {(q i , E(q i ), a i ) : i = 1, 2, . . . , m}. Here, q i is a question formulated in natural language, E(q i ) ⊂ E is a set of knowledge base entities that appears in the question, and a i ∈ E is the answer to the question. For example, on a knowledge base of soccer players, a valid training sample could be ("Who plays forward for Borussia Dortmund?",[Forward, Borussia Dortmund], Marco Reus).Note that the answer to a question is not necessarily unique and we allow a i to be any of the true answers in the knowledge base. During test time, our model is shown (q i , E(q i )) and the task is to find a i . We denote the set of answers to q i by A(q i ).To train our question-answering model, we minimize the objective functionwhere E t ∼N (qi) is expectation with respect to a uniform distribution over of all incorrect answers to q i , which we approximate with 10 random samples. We assume that the number of relations implied in a question is small compared to the total number of relations in the knowledge base. Hence the coefficients α r,e computed for each question q i are regularized by their 1 norms.As a demonstration of the proposed framework, we perform question and answering on a dataset of soccer players. In this work, we consider two types of questions. A path query is a question that contains only one named entity from the knowledge base and its answer can be found from the knowledge graph by walking down a path consisting of a few relations. A conjunctive query is a question that contains more than one entities and the answer is given as the conjunction of all path queries starting from each entity. Furthermore, we experimented on a knowledge base completion task with TransGaussian embeddings to test its capability of generalization to unseen fact. Since knowledge base completion is not the main focus of this work, we include the results in the Appendix.We build a knowledge base of football players that participated in FIFA World Cup 2014 1 . The original dataset consists of players' information such as nationality, posi- tions on the field and ages etc. We picked a few attributes and constructed 1127 en- tities and 6 atomic relations.The entities include 736 players, 297 professional soc- cer clubs, 51 countries, 39 numbers and 4 positions. And the six atomic relations are plays in club: PLAYER → CLUB, plays position: PLAYER → POSITION, is aged: PLAYER → NUMBER, wears number 2 : PLAYER → NUMBER, plays for country: PLAYER → COUNTRY, is in country: CLUB → COUNTRY, where PLAYER, CLUB, NUMBER, etc, denote the type of entities that can appear as the left or right argument for each relation. Some relations share the same type as the right argument, e.g., plays for country and is in country.Given the entities and relations, we transformed the dataset into a set of 3977 triplets. A list of sample triplets can be found in the Appendix. Based on these triplets, we created two sets of question answering tasks which we call path query and conjunctive query respectively. The answer of every question is always an entity in the knowledge base and a question can involve one or two triplets. The questions are generated as follows.Path queries. Among the paths on the knowledge graph, there are some natural composition of relations, e.g., plays in country (PLAYER → COUNTRY) can be decomposed as the com- position of plays in club (PLAYER→ CLUB) and is in country (CLUB → COUNTRY). In addition to the atomic relations, we manually picked a few meaningful compositions of relations and formed query templates, which takes the form "find e ∈ E, such that (s, p, e) is true", where s is the subject and p can be an atomic relation or a path of relations. To formulate a set of path-based question-answer pairs, we manually created one or more question templates for every query tem- plate (see Table 5) Then, for a particular instantiation of a query template with subject and object entities, we randomly select a question template to generate a question given the subject; the object entity becomes the answer of the question. See Table 6 for the list of composed relations, sample questions, and answers. Note that all atomic relations in this dataset are many-to-one while these composed relations can be one-to-many or many-to-many as well.Conjunctive queries. To generate question-and-answer pairs of conjunctive queries, we first picked three pairs of relations and used them to create query templates of the form "Find e ∈ E, such that both (s 1 , r 1 , e) and (s 2 , r 2 , e) are true." (see Table 5). For a pair of relations r 1 and r 2 , we enumerated all pairs of entities s 1 , s 2 that can be their subjects and formulated the corresponding query in natural language using question templates as in the same way as path queries. See Table 7 for a list of sample questions and answers.As a result, we created 8003 question-and-answer pairs of path queries and 2208 pairs of conjunctive queries which are partitioned into train / validation / test subsets. We refer to Table 1 for more statistics about the dataset. Templates for generating the questions are list in Table 5.To perform question and answering under our proposed framework, we first train the TransGaussian model on WorldCup2014 dataset. In addition to the atomic triplets, we randomly sampled 50000 paths with length 1 or 2 from the knowledge graph and trained a TransGaussian model composi- tionally as described in Set 2.2. An inverse relation is treated as a separate relation. Following the naming convention from Guu et al. (2015), we denote this trained embedding by TransGaus- sian (COMP). We found that the learned embedding possess some interesting properties. Some dimensions of the embedding space dedicate to represent a particular relation. Players are clustered by their attributes when entities' embeddings are projected to the corresponding lower dimensional subspaces. We elaborate and illustrate such properties in the Appendix.Baseline methods We also trained a TransGaussian model only on the atomic triplets and denote such a model by TransGaussian (SINGLE). Since no inverse relation was involved when Trans- Gaussian (SINGLE) was trained, to use this embedding in question answering tasks, we represent the inverse relations as follows: for each relation r with mean δ r and variance Σ r , we model its inverse r −1 as a Gaussian attention with mean −δ r and variance equal to Σ r .We also trained TransE models on WorldCup2014 dataset by using the code released by the authors of Guu et al. (2015). Likewise, we use TransE (SINGLE) to denote the model trained with atomic triplets only and use TransE (COMP) to denote the model trained with the union of triplets and paths. Note that TransE can be considered as a special case of TransGaussian where the variance matrix is the identity and hence, the scoring formula Eq. (7) is applicable to TransE as well.Training configurations For all models, dimension of entity embeddings was set to 30. The hid- den size of LSTM was set to 80. Word embeddings were trained jointly with the question answering model and dimension of word embedding was set to 40. We employed Adam (Kingma &amp; Ba, 2014) as the optimizer. All parameters were tuned on the validation set. Under the same setting, we exper- imented with two cases: first, we trained models for path queries and conjunctive queries separately; Furthermore, we trained a single model that addresses both types queries. We present the results of the latter case in the next subsection while the results of the former are included in the Appendix.Evaluation metrics During test time, our model receives a question in natural language and a list of knowledge base entities contained in the question. Then it predicts the mean and variance of a Gaussian attention formulated in Eq. (7) which is expected to capture the distribution of all positive answers. We rank all entities in the knowledge base by their scores under this Gaussian attention. Next, for each entity which is a correct answer, we check its rank relative to all incorrect answers and call this rank the filtered rank. For example, if a correct entity is ranked above all negative answers except for one, it has filtered rank two. We compute this rank for all true answers and report mean filtered rank and H@1 which is the percentage of true answers that have filtered rank 1.We present the results of joint learning in Table 2. These results show that TransGaussian works better than TransE in general. In fact, TransGaussian (COMP) achieved the best performance in almost all aspects. Most notably, it achieved the highest H@1 rates on challenging questions such as "where is the club that edin dzeko plays for?" (#11, composition of two relations) and "who are the defenders on german national team?" (#14, conjunction of two queries).The same table shows that TransGaussian benefits remarkably from compositional training. For example, compositional training improved TransGaussian's H@1 rate by near 60% in queries on players from a given countries (#8) and queries on players who play a particular position (#9). It also boosted TransGaussian's performance on all conjunctive quries (#13-#15) significantly.To understand TransGaussian (COMP)'s weak performance on answering queries on the profes- sional football club located in a given country (#10) and queries on professional football club that has players from a particular country (#12), we tested its capability of modeling the composed re- lation by feeding the correct relations and subjects during test time. It turns out that these two relations were not modeled well by TransGaussian (COMP) embedding, which limits its perfor- mance in question answering. (See Table 8 in the Appendix for quantitative evaluations.) The same limit was found in the other three embeddings as well.Note that all the models compared in Table 2 uses the proposed Gaussian attention model because TransE is the special case of TransGaussian where the variance is fixed to one. Thus the main differ- ences are whether the variance is learned and whether the embedding was trained compositionally. Finally, we refer to Table 9 and 10 in the Appendix for experimental results of models trained on path and conjunctive queries separately. The work of Vilnis &amp; McCallum (2014) is similar to our Gaussian attention model. They discuss many advantages of the Gaussian embedding; for example, it is arguably a better way of handling asymmetric relations and entailment. However the work was presented in the word2vec ( Mikolov et al., 2013)-style word embedding setting and the Gaussian embedding was used to capture the diversity in the meaning of a word. Our Gaussian attention model extends their work to a more general setting in which any memory item can be addressed through a concept represented as a Gaussian distribution over the memory items.  Bordes et al. (2014; proposed a question-answering model that embeds both questions and their answers to a common continuous vector space. Their method in Bordes et al. (2015) can combine multiple knowledge bases and even generalize to a knowledge base that was not used during training. However their method is limited to the simple question answering setting in which the answer of each question associated with a triplet in the knowledge base. In contrast, our method can handle both composition of relations and conjunction of conditions, which are both naturally enabled by the proposed Gaussian attention model. Neelakantan et al. (2015a) proposed a method that combines relations to deal with compositional relations for knowledge base completion. Their key technical contribution is to use recurrent neural networks (RNNs) to encode a chain of relations. When we restrict ourselves to path queries, question answering can be seen as a sequence transduction task (Graves, 2012;Sutskever et al., 2014) in which the input is text and the output is a series of relations. If we use RNNs as a decoder, our model would be able to handle non-commutative composition of relations, which the current weighted convolution cannot handle well. Another interesting connection to our work is that they take the maximum of the inner-product scores (see also Weston et al., 2013;Neelakantan et al., 2015b), which are computed along multiple paths connecting a pair of entities. Representing a set as a collection of vectors and taking the maximum over the inner-product scores is a natural way to represent a set of memory items. The Gaussian attention model we propose in this paper, however, has the advantage of differentiability and composability.In this paper, we have proposed the Gaussian attention model which can be used in a variety of contexts where we can assume that the distance between the memory items in the latent space is compatible with some notion of semantics. We have shown that the proposed Gaussian scoring function can be used for knowledge base embedding achieving competitive accuracy. We have also shown that our embedding model can naturally propagate uncertainty when we compose relations together. Our embedding model also benefits from compositional training proposed by Guu et al. (2015). Furthermore, we have demonstrated the power of the Gaussian attention model in a chal- lenging question answering problem which involves both composition of relations and conjunction of queries. Future work includes experiments on natural question answering datasets and end-to-end training including the entity extractor.    what is the nationality of (player) ? which national team does (player) play for ? which country is (player) from ? 6Find e ∈ E: ((club), is in country, e) is true which country is the soccer team (club) based in ?7 Find e ∈ E: ((club), plays in club −1 , e) is true name a player from (club) ? who plays at the soccer club (club) ? who is from the professional football team (club) ? who plays professionally at (club) ?8 Find e ∈ E: ((country 1), plays for countrywhich player is from (country 1) ? name a player from (country 1) ? who is from (country 1) ? who plays for the (country 1) national football team ?9 Find e ∈ E: ((position), plays position −1 , e) is true name a player who plays (position) ? who plays (position) ?Find e ∈ E: ((country 1), is in country −1 , e) is true which soccer club is based in (country 1) ? name a soccer club in (country 1) ?Find e ∈ E: ((player), plays in club / is in country, e) is true which country does (player) play professionally in ? where is the football club that (player) plays for ?Find e ∈ E: ((country 1), plays for country −1 / plays in club, e) is true which professional football team do players from (country 1) play for ? name a soccer club that has a player from (country 1) ? which professional football team has a player from (country 1) ?     figure, we can see that every relation has a small variance in two or more dimensions. This implies that the coordinates of the embedding space are partitioned into semantically coherent clusters each of which represent a particular attribute of a player (or a football club). To verify this further, we picked the two coordinates in which a relation (e.g. plays position) has the least variance and projected the embedding of all valid subjects and objects (e.g. players and positions) of the relation to this 2 dimensional subspace. See Fig. 4. The relation between the subjects and the objects are simply translation in the projection when the corresponding subspace is two dimensional (e.g., plays position relation in Fig. 4 (a)). The same is true for other relations that requires larger dimension but it is more challenging to visualize in two dimensions. For relations that have a large number of unique objects, we only plotted for the eight objects with the most subjects for clarity of illustration.Furthermore, in order to elucidate whether we are limited by the capacity of the TransGaussian embedding or the ability to decode question expressed in natural language, we evaluated the test question-answer pairs using the TransGaussian embedding composed according to the ground-truth relations and entities. The results were evaluated with the same metrics as in Sec. 4.3. This es- timation is conducted for TransE embeddings as well. See Table 8 for the results. Compared to Table 2, the accuracy of TransGaussian (COMP) is higher on the atomic relations and path queries but lower on conjunctive queries. This is natural because when the query is simple there is not much room for the question-answering network to improve upon just combining the relations ac- cording to the ground truth relations, whereas when the query is complex the network could com- bine the embedding in a more creative way to overcome its limitation. In fact, the two queries (#10 and #12) that TransGaussian (COMP) did not perform well in Table 2 pertain to a single re- lation is in country −1 (#10) and a composition of two relations plays for country −1 / plays in club (#12). The performance of the two queries were low even when the ground truth  relations were given, which indicates that the TransGaussian embedding rather than the question- answering network is the limiting factor.Knowledge base completion has been a common task for testing knowledge base models on their ability of generalizing to unseen facts. Here, we apply our TransGaussian model to a knowledge completion task and show that it has competitive performance.We tested on the subset of WordNet released by Guu et al. (2015). The atomic triplets in this dataset was originally created by Socher et al. (2013) and Guu et al. (2015) added path queries that were randomly sampled from the knowledge graph. We build our TransGaussian model by training on these triplets and paths and tested our model on the same link prediction task as done by Socher et al. (2013); Guu et al. (2015).As done by Guu et al. (2015), we trained TransGaussian (SINGLE) with atomic triplets only and trained TransGaussian (COMP) with the union of atomic triplets and paths. We did not incorporate  word embedding in this task and each entity is assigned its individual vector. Without getting param- eters tuned too much, TransGaussian (COMP) obtained accuracy comparable to TransE (COMP). See Table 11.   
