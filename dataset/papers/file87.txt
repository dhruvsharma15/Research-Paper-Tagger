Different types of semantic relations exist be- tween words such as HYPERNYMY between ostrich and bird, or ANTONYMY between hot and cold. If we consider entities 1 , we can ob- serve even a richer diversity of relations such as FOUNDER-OF between Bill Gates and Mi- crosoft, or CAPITAL-OF between Tokyo and Japan. Identifying the relations between words and entities is important for various Natural Lan- guage Processing (NLP) tasks such as automatic knowledge base completion (Socher et al., 2013), analogical reasoning ( Turney and Littman, 2005;Bollegala et al., 2009) and relational information retrieval ( Duc et al., 2010). For example, to solve a word analogy problem of the form "a is to b as c is to ?", the relationship between the two words in the pair (a, b) must be correctly identified in or- der to find candidates d that have similar relations with c. For example, given the query "Bill Gates is to Microsoft as Steve Jobs is to ?", a relational search engine must retrieve Apple Inc. because the FOUNDER-OF relation exists between the first and the second entity pairs.Two main approaches for creating relation em- beddings can be identified in the literature. In the first approach, from given corpora or knowledge bases, word and relation embeddings are jointly learnt such that some objective is optimised ( Guo et al., 2016;Yang et al., 2015;Nickel et al., 2016;Bordes et al., 2013;Rocktäschel et al., 2016;Minervini et al., 2017;Trouillon et al., 2016). In this approach, word and relation embeddings are con- sidered to be independent parameters that must be learnt by the embedding method. For exam- ple, TransE (Bordes et al., 2013) learns the word and relation embeddings such that we can accu- rately predict relations (links) in a given knowl- edge base using the learnt word and relation em- beddings. Because relations are learnt indepen- dently from the words, we refer to methods that are based on this approach as independent rela- tional embedding methods.A second approach for creating relational em- beddings is to apply some operator on two word embeddings to compose the embedding for the re- lation that exits between those two words, if any. In contrast to the first approach, we do not have to learn relational embeddings and hence this can be considered as an unsupervised setting, where the compositional operator is predefined. A popu- lar operator for composing a relational embedding from two word embeddings is PairDiff, which is the vector difference (offset) of the word embed- dings ( Mikolov et al., 2013b;Levy and Goldberg, 2014;Vylomova et al., 2016;Bollegala et al., 2015b;Blacoe and Lapata, 2012). Specifically, given two words a and b represented by their word embeddings respectively a and b, the rela- tion between a and b is given by a − b under the PairDiff operator. Mikolov et al. (2013b) showed that PairDiff can accurately solve analogy equa- tions such aswhere we have used the top arrows to denote the embeddings of the corresponding words. Bollegala et al. (2015a) showed that PairDiff can be used as a proxy for learning better word embed- dings and Vylomova et al. (2016) conducted an extensive empirical comparison of PairDiff using a dataset containing 16 different relation types. Be- sides PairDiff, concatenation (Hakami and Bollegala, 2017;Yin and Schütze, 2016), circular cor- relation and convolution (Nickel et al., 2016) have been used in prior work for representing the rela- tions between words. Because the relation embed- ding is composed using word embeddings instead of learning as a separate parameter, we refer to methods that are based on this approach as compo- sitional relational embedding methods. Note that in this approach it is implicitly assumed that there exist only a single relation between two words.In this paper, we focus on the operators that are used in compositional relational embedding meth- ods. If we assume that the words and relations are represented by vectors embedded in some com- mon space, then the operator we are seeking must be able to produce a vector representing the rela- tion between two words, given their word embed- dings as the only input. Although there have been different proposals for computing relational em- beddings from word embeddings, it remains un- clear as to what is the best operator for this task. The space of operators that can be used to com- pose relational embeddings is open and vast. A space of particular interest from a computational point-of-view is the bilinear operators that can be parametrised using tensors and matrices. Specif- ically, we consider operators that consider pair- wise interactions between two word embeddings (second-order terms) and contributions from indi- vidual word embeddings towards their relational embedding (first-order terms). The optimality of a relational compositional operator can be eval- uated, for example, using the expected relational distance/similarity such as 2 between analogous (positive) vs. nonanalogous (negative) word-pairs.If we assume that word embeddings are stan- dardised, uncorrelated and word-pairs are i.i.d, then we prove in §3 that bilinear relational compo- sitional operators are independent of bilinear pair- wise interactions between the two input word em- beddings. Moreover, under regularised settings ( §3.1), the bilinear operator further simplifies to a linear combination of the input embeddings, and the expected loss over positive and negative in- stances becomes zero. In §4.1, we empirically validate the uncorrelation assumption for differ- ent pre-trained word embeddings such as the Con- tinuous Bag-of-Words Model (CBOW) ( Mikolov et al., 2013a), Skip-Gram with negative sam- pling (SG) ( Mikolov et al., 2013a), Global Vec- tors (GloVe) ( Pennington et al., 2014), word em- beddings created using Latent Semantic Anal- ysis (LSA) (Deerwester et al., 1990), Sparse Coding (HSC) (Faruqui et al., 2015;Yogatama et al., 2015), and Latent Dirichlet Allocation (LDA) ( Blei et al., 2003a). This empirical evi- dence implies that our theoretical analysis is appli- cable to relational representations composed from a wide-range of word embedding learning meth- ods. Moreover, our experimental results show that a bilinear operator reaches its optimal per- formance in two different word-analogy bench- mark datasets, when it satisfies the requirements of the PairDiff operator. We hope that our theo- retical analysis will expand the understanding of relational embedding methods, and inspire future research on accurate relational embedding meth- ods using word embeddings as the input.As already mentioned in §1, methods for rep- resenting a relation between two words can be broadly categorised into two groups depending on whether the relational embeddings are learnt in- dependently of the word embeddings, or they are composed from the word embeddings, in which case the relational embeddings fully depend on the input word embeddings. Next, we briefly overview the different methods that fall under each category. For a detailed survey of relation embed- ding methods see ( Nickel et al., 2015).Given a knowledge base where an entity h is linked to an entity t by a relation r, the TransE model ( Bordes et al., 2013) scores the tuple (h, t, r) by the 1 or 2 norm of the vec- tor (h + r − t). (Nickel et al., 2011) proposed RESCAL, which uses h M r t as the scoring func- tion, where M r is a matrix embedding of the re- lation r. Similar to RESCAL, Neural Tensor Net- work ( Socher et al., 2013) also models a relation by a matrix. However, compared to vector em- beddings of relations, matrix embeddings increase the number of parameters to be estimated, result- ing in an increase in computational time/space and likely to overfit. To overcome these limitations, DistMult (Yang et al., 2015) models relations by vectors and use elementwise multilinear dot prod- uct r h t. Unfortunately, DistMult cannot capture directionality of a relation. Complex Em- beddings ( Trouillon et al., 2016) overcome this limitation of DistMult by using complex embed- dings and defining the score to be the real part of r h ¯ t, where ¯ t denotes the complex conjugate of t.complex embeddings (Hayashi and Shinbo, 2017).Although PairDiff operator has been widely used in prior work for computing relation embed- dings from word embeddings, to the best of our knowledge, no theoretical analysis has been con- ducted so far explaining why and under what con- ditions PairDiff is optimal, which is the focus of this paper.The observation made by Mikolov et al. (2013b) that the relation between two words can be rep- resented by the difference between their word embeddings sparked a renewed interest in meth- ods that compose relational embeddings using word embeddings. Word analogy datasets such as Google dataset (Mikolov et al., 2013b), Se- mEval 2012 Task2 dataset ( Jurgens et al., 2012), BATS ( Drozd et al., 2016) etc. have established as benchmarks for evaluating word embedding learn- ing methods.Let us consider the problem of representing the se- mantic relation r(h, t) between two given words h and t. We assume that h and t are already rep- resented in some d-dimensional space respectively by their word embeddings h, t ∈ R d . The relation between two words can be represented using dif- ferent linear algebraic structures. Two popular al- ternatives are vectors ( Nickel et al., 2016;Bordes et al., 2013;Minervini et al., 2017;Trouillon et al., 2016) and matrices (Socher et al., 2013;Bollegala et al., 2015b). Vector representations are preferred over matrix representations because of the smaller number of parameters to be learnt ( Nickel et al., 2015).Let us assume that the relation r is represented by a vector r ∈ R δ in some δ-dimensional space. Therefore, we can write r(h, t) as a function that takes two vectors (corresponding to the embed- dings of the two words) as the input and returns a single vector (representing the relation between the two words) as given in (1).Different methods have been proposed to mea- sure the similarity between the relations that exist between two given word pairs such as CosMult, CosAdd and PairDiff (Levy and Goldberg, 2014;Bollegala et al., 2015a). Vylomova et al. (2016) studied as to what extent the vectors generated us- ing simple PairDiff encode different relation types. Under supervised classification settings, they con- clude that PairDiff can cover a wide range of se- mantic relation types. Holographic embeddings proposed by Nickel et al. (2016) use circular con- volution to mix the embeddings of two words to create an embedding for the relation that exist be- tween those words. It can be showed that circular correlation is indeed an elementwise product in the Fourier space and is mathematically equivalent to Having both words and relations represented in the same δ = d dimensional space is useful for performing linear algebraic operations using those representations in that space. For example, in TransE ( Bordes et al., 2013), the strength of a re- lation r that exists between two words h and t is computed as the 1,2 norm of the vector (h+r−t) using the word and relation embeddings. Such di- rect comparisons between word and relation em- beddings would not be possible if words and re- lations were not embedded in the same vector space. If δ &lt; d, we can first project word embed- dings to a lower δ-dimensional space using some dimensionality reduction method such as SVD, whereas if δ &gt;d we can learn higher δ-dimensional overcomplete word representations (Faruqui et al., 2015) from the original d-dimensional word em- beddings. Therefore, we will limit our theoretical analysis to the δ = d case for ease of description.Different functions can be used as r(h, t) that satisfy the domain and range requirements speci- fied by (1). If we limit ourselves to bilinear func- tions, the most general functional form is given by (2).Assuming that the training word-pairs are ran- domly sampled from D + and D − according to two distributions respectively p + and p − , we can com- pute the total expected loss, E p [J], as follows:Here, A ∈ R d×d×d is a 3-way tensor in which each slice is a d × d real matrix. Let us denote the k-th slice of A by A (k) and its (i, j) element byWe make the following assumptions to further analyse the properties of relational embeddings.ij . The first term in (2) corresponds to the pair- wise interactions between h and t. P, Q ∈ R d×d are the nonsingular 2 projection matrices involving first-order contributions respectively of h and t to- wards r.Let us consider the problem of learning the simplest bilinear functional form according to (2) from a given dataset of analogous word-pairs D + = {((h, t), (h , t ))}. Specifically, we would like to learn the parameters A, P and Q such that some distance (loss) between analogous word- pairs is minimised. As a concrete example of a dis- tance function, let us consider the popularly used Euclidean distance 3 ( 2 loss) for two word pairs given by (3).Uncorrelation: The correlation between any two distinct dimensions of a word embedding is zero. One might think that the uncorrelation of word embedding dimensions to be a strong assumption, but we later show its validity em- pirically in §4.1 for a wide range of word em- beddings.Standerdisation: Word embeddings are standerdised to zero mean and unit vari- ance. This is a linear transformation in the word embedding space and does not affect the topology of the embedding space. In particular, translating word embeddings such that they have a zero mean has shown to improve performance in similarity tasks ( Mu et al., 2017).If we were provided only analogous word-pairs (i.e. positive examples), then this task could be trivially achieved by setting all parameters to zero. However, such a trivial solution would not gen- eralise to unseen test data. Therefore, in ad- dition to D + we would require a set of non- analogous word-pairs D − as negative examples. Such negative examples are often generated in prior work by randomly corrupting positive rela- tional tuples ( Nickel et al., 2016;Bordes et al., 2013;Trouillon et al., 2016) or by training an ad- versarial generator ( Minervini et al., 2017).The total loss J over both positive and negative training data can be written as follows:Relational Independence Word pairs in the training data are assumed to be i.i.d. For example, whether a particular semantic relation r exists between h and t, is assumed to be independent of any other relation r that exists between h and t in a different pair.For relation representations given by (2), Theo- rem 1 holds: Theorem 1. Consider the bilinear relational em- bedding defined by (2) computed using uncorre- lated word embeddings. If the word embeddings are standerdised, then the expected loss given by (5) over a relationally independent set of word pairs is independent of A.((h,t),(h ,t ))∈D − 2 If the projection matrix is nonsingular, then the inverse projection exists, which preserves the dimensionality of the embedding space. 3 For normalised vectors, their Euclidean distance is a monotonously decreasing function of their cosine similarity.Proof. Let us consider the bilinear term in (2), because i and j( = i) dimensions of word em- beddings are uncorrelated by the assumption (i.e. corr(u i , u j ) = 0), from the definition of correla- tion we have,Moreover, from the standerdisation assumption we have, E[u i ] = 0, ∀ i=1...n . From (7) it follows that:Taking the expectation of (13) w.r.t. p + we get,for i = j dimensions. We will next show that (5) is independent of A. For this purpose, let us consider the E p + term first and write the k-th dimension of r(h, t) using A (k) , P and Q as follows:Likewise, from the uncorrelation assumption and relational independence it follows that all the ex- pectations in (14) are zero. A similar argument can be used to show that terms that involve Ai,j n n Plugging (9) in (5) and computing the loss over all positive training instances we get, ij Q kn disappear from (10). Therefore, A does not play any part in the expected loss over positive exam- ples. Similarly, we can show that A is indepen- dent of the expected loss over negative examples. Therefore, from (5) we see that the expected loss over the entire training dataset is independent of A.n n Terms that involve only elements in A (k) take the form:As a special case, if we attempt to minimise the expected loss under some regularisation on A such as the Frobenius norm regularisation, then this can be achieved by sending A to zero tensor because according to Theorem 1 (2) is independent from A.With A = 0, the relation between h and t can be simplified to:Then the expected loss over the positive in- stances is given by (16).In cases where i = j and l = m, each of the four expectations in (11) contains the product of different dimensionalities, which is zero from (8). For i = j = l = m case we have,The second expectation term in RHS of (16) can be computed as follows:From the relational independence we haveMoreover, because the word embeddings are assumed to be standerdised to unit variance we have Therefore, (12) evaluates to zero and none of the terms arising purely from A will remain in the ex- pected loss over positive examples.Next, lets consider the A i,j(k) ij P kn terms in the ex- pansion of (10) given by,When i = j, each of the four expectations in the RHS of (19) are zero from the uncorrelation as- sumption. When i = j, each term will be equal to one from the standeridisation assumption (unit variance) and cancel each other out. A similar ar- gument can be used to show that the third expec- tation term in the RHS of (16) vanishes.i,j n Now lets consider the first expectation term in the RHS of (16), which can be computed as fol- lows:When i = j, it follows from the uncorrelation as- sumption that each of the four expectation terms in the RHS of (18) will be zero. For i = j case we have,Note that from the relational independence between h and h we haveFrom the standerdidation (zero mean) assumption this term is zero. On the other handfrom the standerdi- dation (unit variance) assumption, which gives the result in (19). Similarly, the fourth expectation term in the RHS of (16) evaluates to 2 i,j (Q Q) ii , which shows that (16) evaluates to 2Note that this is independent of the positive instances and will be equal to the expected loss over negative in- stances, which gives E p [J] = 0 for the relational embedding given by (15).It is interesting to note that PairDiff is a spe- cial case of (15), where P = I and Q = −I. In the general case where word embeddings are non- standerdised to unit variance, we can set P to be the diagonal matrix where P ii = 1/σ i , where σ i is the variance of the i-th dimension of the word em- bedding space, to enforce standerdisation. Con- sidering that P, Q are parameters of the relational embedding, this is analogous to batch normalisa- tion ( Ioffe and Szegedy, 2015), where the appro- priate parameters for the normalisation are learnt during training. in word embeddings. Here, we empirically ver- ify the uncorrelation assumption for different in- put word embeddings. For this purpose, we cre- ate SG, CBOW and GloVe embeddings from the ukWaC corpus 4 . We use a context window of 5 tokens and select words that occur at least 6 times in the corpus. We use the publicly available implementations for those methods by the origi- nal authors and set the parameters to the recom- mended values in ( Levy et al., 2015) to create 50- dimensional word embeddings. As a representa- tive of counting-based word embeddings, we cre- ate a word co-occurrence matrix weighted by the positive pointwise mutual information (PPMI) and apply singular value decomposition (SVD) to ob- tain 50-dimensional embeddings, which we refer to as the Latent Semantic Analysis (LSA) embed- dings.We use Latent Dirichlet Allocation (LDA) ( Blei et al., 2003b) to create a topic model, and represent each word by its distribution over the set of top- ics. Ideally, each topic will capture some semantic category and the topic distribution provides a se- mantic representation for a word. We use gensim 5 to extract 50 topics from a 2017 January dump of English Wikipedia. In contrast to the above- mentioned word embeddings, which are dense and flat structured, we used Hierarchical Sparse Cod- ing 6 (HSC) ( Yogatama et al., 2015) to produce sparse and hierarchical word embeddings.Given a word embedding matrix W ∈ R m×d , where each row correspond to the d-dimensional embedding of a word in a vocabulary contain- ing m words, we compute a correlation matrix C ∈ R d×d , where the (i, j) element, C ij , denotes the Pearson correlation coefficient between the i- th and j-th dimensions in the word embeddings over the m words. By construction C ii = 1 and the histograms of the cross-dimensional correla- tions (i = j) are shown in Figure 1 for 50 di- mensional word embeddings obtained from the six methods described above. The mean of the abso- lute pairwise correlations for each embedding type and the standard deviation (sd) are indicated in the figure.From Figure 1, irrespective of the word em-A key assumption in our theoretical analysis is the uncorrelations between different dimensions bedding learning method used, we see that cross- dimensional correlations are distributed in a nar- row range with an almost zero mean. This result empirically validates the uncorrelation assumption we used in our theoretical analysis. Moreover, this result indicates that Theorem 1 can be applied to a wide-range of existing word embeddings. Our theoretical analysis in §3 claims that the per- formance of the bilinear relational embedding is independent of the tensor operator A. To em- pirically verify this claim, we conduct the fol- lowing experiment. For this purpose, we use the BATS dataset ( Gladkova et al., 2016) that contains of 40 semantic and syntactic relation types 7 , and generate positive examples by pairing word-pairs that have the same relation types. Approximately each relation type has 1,225 word-pairs, which en- ables us to generate a total of 48k positive train- ing instances (analogous word-pairs) of the form ((h, t), (h , t )). For each pair (h, t) related by a relation r, we randomly select pairs (h , t ) with a different relation type r , according to the 2 distance between the two pairs to create negative (nonanalogous) instances. 8 We collectively refer both positive and negative training instances as the training dataset.Using the d = 50 dimensional word embed- dings from CBOW, SG, GloVe, LSA, LDA, and HSC methods created in §4.1, we learn relational embeddings according to (2) by minimising the 2 loss, (4). To avoid overfitting, we perform 2 reg- ularisation on A, P and Q are regularised to di- agonal matrices pI and qI, for p, q ∈ R. We ini- tialise all parameters by uniformly sampling from [−1, +1] and use AdaGrad (Duchi et al., 2011) with initial learning rate set to 0.01. Figure 2 shows the Frobenius norm of the ten- sor A (on the left vertical axis) and the values of p and q (on the right vertical axis) for the six word embeddings. In all cases, we see that as the training progresses, A goes to zero as pre- dicted by Theorem 1 under regularisation. More- over, we see that approximately p ≈ −q = c is reached for some c ∈ R in all cases, which im- plies that P ≈ −Q = cI, which is the PairDiff operator. Among the six input word embeddings compared in Figure 1, HSC has the highest mean correlation (0.082), which implies that its dimen- sions are correlated more than in the other word embeddings. This is to be expected by design be- cause a hierarchical structure is imposed on the di- mensions of the word embedding during training. However, HSC embeddings also satisfy the A ≈ 0 and p ≈ −q = c requirements, as expected by the PairDiff. This result shows that the claim of Theorem 1 is empirically true even when the un- correlation assumption is mildly violated. between the corresponding relational embeddings r(a, b) and r(c, d). The candidate word-pair that has the highest relational similarity with the stem word-pair is selected as the correct answer to a word analogy question. The reported accuracy is the ratio of the correctly answered questions to the total number of questions. On the other hand, Se- mEval dataset has 79 semantic relations, with each relation having ca. 41 word-pairs and four proto- typical examples. The task is to assign a score for each word pair which is the average of the rela- tional similarity between the given word-pair and prototypical word-pairs in a relation. Maximum difference scaling (MaxDiff) is used as the evalu- ation measure in this task.So far we have seen that the bilinear relational representation given by (2) does indeed converge to the form predicted by our theoretical analysis for different types of word embeddings. How- ever, it remains unclear whether the parameters learnt from the training instances generated from the BATS dataset accurately generalise to other benchmark datasets for analogy detection. To em- phasise, our focus here is not to outperform rela- tional representation methods proposed in previ- ous works, but rather to empirically show that the learnt operator converges to the popular PairDiff for the analogy detection task. To measure the generalisation capability of the learnt relational embeddings from BATS, we mea- sure their performance on two other benchmark datasets: SAT ( Turney and Bigham, 2003) and Se- mEval 2012-Task2 9 . Note that we do not retrain A, P and Q in (2) on SAT nor SemEval, but sim- ply to use their values learnt from BATS because the purpose here to evaluate the generalisation of the learnt operator.In SAT analogical questions, given a stem word- pair (a, b) with five candidate word-pairs (c, d), the task is to select the word-pair that is relation- ally similar to the the stem word-pair. The re- lational similarity between two word-pairs (a, b) and (c, d) is computed by the cosine similarity Figure 3 shows the performance of the relational embeddings composed from 50- dimensional CBOW embeddings. 10 The level of performance reported by PairDiff on SAT and SemEval datasets are respectively 35.16% and 41.94%, and are shown by horizontal dashed lines. From Figure 3, we see that the training loss gradually decreases with the number of training epochs and the performance of the relational embeddings on SAT and SemEval datasets reach that of the PairDiff operator. This result indicates that the relational embeddings learnt not only converge to PairDiff operator on training data but also generalise to unseen relation types in SAT and SemEval test datasets.We showed that, if the word embeddings are stan- dardised and uncorrelated, then the expected 2 distance between analogous and non-analogous word-pairs is independent of bilinear terms, and the relation embedding further simplifies to the popular PairDiff operator under regularised set- tings. Moreover, we provided empirical evidence showing the uncorrelation in word embedding di- mensions, where their cross-dimensional correla- tions are narrowly distributed around a mean close to zero. An interesting future research direction of this work is to extend the theoretical analysis to nonlinear relation composition operators, such as for nonlinear neural networks.
