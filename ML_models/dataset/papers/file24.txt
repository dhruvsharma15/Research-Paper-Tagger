Consider designing a vision system that solves many tasks. Ideally, any such system should be able to reuse representations for different applications. As the system is trained to solve more problems, its core representations should become more complete and accurate, facilitating the learning of additional tasks. Vision research often focuses on designing good representations for a given task, but what are good core representations to facilitate learning the next?The application of knowledge learned while solving one task to solve another task is known as transfer learning or inductive transfer. Inductive transfer has been demonstrated in recent vision-language tasks in [34,37,26,16,49,24], where the hidden or output layers of deep networks learned from pre-training (e.g. on ImageNet [14]) or multitask learning serve as the foundation for learning new tasks. However, the relations of features to each new task needs to be re-learned using the new task's data. The goal of our work is to transfer knowledge between related tasks without the need to re-learn this mapping. Further, as we are work- ing with vision-language tasks, we aim to transfer knowl- edge of both vision and language across tasks.In this work we propose a Shared Vision-Language Rep- resentation (SVLR) module that improves inductive transfer between related vision-language tasks (see Fig. 1). We ap- ply our approach to visual recognition (VR) and attention- based visual question answering (VQA). We formulate VR in terms of a joint embedding of textual and visual repre- sentations computed by the SVLR module. Each region is mapped closest to its correct (textual) class label. For ex- ample, the embedding of "dog" should be closer to an em- bedded region showing a dog than any other object label. We formulate VQA as predicting an answer from a relevant region, where relevance and answer scores are computed from embedded word-region similarities. For example, a region will be considered relevant to "Is the elephant wear- ing a pink blanket?" if the embedded "pink" and either "elephant" or "blanket" are close to the embedded region. Similarly, the answer score considers embedded similari- ties, but in a more comprehensive manner. We emphasize that the same word-region embedding is learned for both VR and VQA. Our experiments show that formulating both tasks in terms of the SVLR module leads to better cross-task transfer than if features are shared through multitask learn-ing but without exploiting the alignment between words and regions.In summary, our main contribution is to show that the proposed SVLR module leads to better inductive transfer than unaligned feature sharing through multitask learning. As an added benefit, attention in our VQA model is highly interpretable: we can show what words cause the system to score a particular region as relevant. We take a small step towards lifelong-learning vision systems by showing the benefit of an interpretable, flexible, and trainable core representation. caption-generation model on the same dataset. Our work goes further by allowing the supervision to come from sepa- rate datasets, thereby increasing the amount of training data available for the shared parameters. Additionally, we look at how each task has benefited from jointly training with the other.Never-ending learning: NEL [43,9,53,50,11] aims to continuously learn from multiple tasks such that learning to solve newer problems becomes easier. Representation learning [7], multitask learning [10], and curriculum learn- ing [45] are different aspects of this larger paradigm. Induc- tive transfer through shared representations is a necessary first step for NEL. Most works focus on building transfer- able representations within a single modality such as lan- guage or vision only. We extend this framework to learn a joint vision-language representation which enables a much larger class of new vision-language tasks to easily build on and contribute to the shared representation.VR using Vision-language embeddings: Traditionally, vi- sual recognition has been posed as multiclass classification over discrete labels [21,51,32]. Using these recogniz- ers for tasks like VQA and image captioning is challeng- ing because of the open-vocabulary nature of these prob- lems. However, availability of continuous word embed- dings (e.g. word2vec [40]) has allowed reformulation of visual recognition as a nearest neighbor search in a learned image-language embedding space [55]. Such embeddings have been successfully applied to a variety of tasks that re- quire recognition such as image captioning [35,23], phrase localization [46,31], referring expressions [29,39], and VQA [5,47,60].Our recognition model is related to previous open- vocabulary recognition/localization models [55,48,18], which learn to map visual CNN features to continuous word vector representations. However, we specifically fo- cus on the multitask setting where VR forms a part of a higher-level vision-language task such as VQA. Since the SVLR module is reused in both tasks with inner products in the embedding space forming the basis for both mod- els, during joint training VQA provides a weak supervi- sion for recognition as well. Fang et al. [15] also learn object and attribute classifiers from weak supervision in the form of image-caption pairs using a multiple instance learning (MIL) framework, but do not use a vision-language embedding. Liu et al. [36] similarly use VR annotation from Flickr30K entities [46] to co-supervise attention in a VQA: Visual Question Answering (VQA) involves re- sponding to a natural language query about an image. Our VQA model is closely related to attention-based VQA mod- els [16,24,37,58,49,59,4,3,33,54] which attempt to compute a distribution (region relevance or attention) over the regions/pixels in an image using inner product of image- region and the full query embedding [58,49,24,37]. Re- gion relevance is used as a weight to pool relevant visual information which is usually combined with the language representation to create a multimodal representation. Vari- ous methods of pooling such as elementwise-addition, mul- tiplication, and outer-products have been explored [59,16]. Attention models are themselves an active area of re- search with applications in visual recognition [44,27], ob- ject localization, caption generation [28], question answer- ing [56,52,33], machine comprehension [22] and transla- tion [6,57], and neural turing machines [20].Our model explicitly formulates attention in VQA as im- age localization of nouns and adjectives mentioned in a can- didate QA pair. Ilievski et al. [24] use a related approach for attention. They use word2vec to map individual words in the question to the class labels of a pre-trained object de- tector which then generates the attention map by identifying regions for those labels. Tommasi et al. [54] similarly use a pre-trainined CCA [18] vision-language embedding model to localize noun phrases, then extracts scene, attribute, and object features to answer VQA questions. Our model dif- fers from these methods in two ways: (i) vision-language embeddings for VR allow for end-to-end trainability, and (ii) jointly training on VR provides additional supervision of attention through a different (non-VQA) dataset.Andreas et al. [4,3] rely heavily on the syntactic parse to dynamically arrange a set of parametrized neural mod- ules. Each module performs a specific function such as lo- calizing a specific word or verifying relative locations. In contrast, our approach uses a static model but relies on lan- guage parse to make it interpretable and modular.We propose an SVLR module to facilitate greater in- ductive transfer across vision-language tasks. As shown in Fig. 2, the word and region representations required for ob- ject recognition, attribute recognition, and VQA are com- puted through the SVLR module. By specifically formulat- ing each task in terms of inner products of word and region representations and training on all tasks jointly, we ensure each task provides a consistent, non-conflicting training sig-  nal for aligning words and region representations. Dur- ing training, the joint-task model is fed batches containing training examples from each task's dataset.The SVLR module converts words and image-regions into feature representations that are aligned to each other and shared across tasks.The visual recognition task is to classify image regions into one or more object and attribute categories. The classification score for region R and object category w is f T o (R)g(w). The classification score for an attribute cate- gory v is f T Word Representations: The representation g(w) for a word w is constructed by applying two fully connected layers (with 300 output units each) to pretrained word2vec representation [41] of w with ReLU after the first layer.Region Representations: A region R is represented using two 300 dimensional feature vectors f o (R) and f a (R) that separately encode the objects and attributes contained. We used two representations instead of one to encourage disen- tangling of these two factors of variation. For example, we do not expect "red" to be similar to "apple", but we expect f o (R) and f a (R) to be similar to g("red") and g("apple") if R depicts a red apple. The features are constructed by ex- tracting the average pooled features from Resnet [21] pre- trained on ImageNet and then passing through separate ob- ject and attribute networks. Both networks consist of two fully connected layers (with 2048 and 300 output units) with batch normalization [25] and ReLU activations. a (R)g(v). Attributes may include adjectives and adverbs (e.g., "standing"). Though our recognition dataset has a limited set of object categories O and attribute cate- gories T , our model can produce classification scores for any object or attribute label given its word2vec representa- tion. In experiments, the O and T consist of 1000 most fre- quent object and attribute categories in the Visual Genome dataset [31].Our VR model is trained using the Visual Genome dataset which provides image regions annotated with object and attribute labels. VR uses only the parameters for the embedding functions f o , f a and g that are part of the SVLR module. The parameters of f o receive gradients from the object loss while those of f a receive gradients from the attribute loss. The parameters of word embedding model g receive gradients from both losses.Object loss: We use a multi-label loss as object classes may not be mutually exclusive (e.g., "man" is a "person"). For a region R j , we denote the set of annotated object categories and their hypernyms extracted from WordNet [42] by H j . The object loss forces the true labels and their hypernyms to score higher than all other object labels by a margin η obj . For a batch of M samples {(R j , Hj)} M j=1 the object loss is:Thus, a region's attention score is the sum of maximum adjective and noun scores for words mentioned in the ques- tion or answer (which need not be in sets O and T ). Image Representation: To score an answer, the content of region R is encoded using the VR scores for all objects and attributes in O and T , as presence of unmentioned objects or attributes may help answer the question. The image rep- resentation is an attention-weighted average of these scores across all regions:The attribute loss is a multi-label classifi- cation loss with two differences from object classification. Attribute labels are even less likely to be mutually exclu- sive than object labels. As such, we predict each attribute with independent cross entropy losses. We also weigh the samples based on fraction of positive labels in the batch to balance the positive and negative labels in the dataset. For a batch with M sampleswhere T j is the set of attributes annotated for region R j , the attribute loss is:where σ is a sigmoid activation function and Γ(t) is the frac- tion of positive samples for attribute t in the batch.Our VQA model is illustrated in Fig. 3. The input to our VQA model is an image, a question, and a candi- date answer. Regions are extracted from the image using Edge Boxes [61]. The same SVLR module used by VR (Sec. 3.2) is explicitly applied to VQA for attention and an- swer scoring. Our system assigns attention scores to each region according to how well it matches words in the ques- tion/answer, then scores each answer based on the question, answer, and attention-weighted scores for all objects (O) and attributes (T ). Attention Scoring: Unlike other attention models [59,37] that are free to learn any correlation between regions and question/answers, our attention model encodes an explicit notion of vision-language grounding. Let R be the set of region proposals extracted from the image, and N and J denote the set of nouns and adjectives in the (Q, A) pair. Each region R ∈ R(I) is assigned an attention score a(R) as follows:where I is the image, s o (R) are the scores for 1000 objects in O for each image region R, s a (R) are the scores for 1000 attributes in T , and a(R) is the attention score. Question/Answer Representation: To construct repre- sentations q(Q) and a(A) for the question and answer, we follow Shih et al. [49], dividing question words into 4 bins, averaging word representations in each bin, and concatenating the bin representations resulting in a 1200 (= 300 × 4) dimensional vector q(Q). The answer rep- resentation a(A) ∈ R 300 is obtained by averaging the word representations of all answer words. The word representa- tions used here are produced by the SVLR module. Answer Scoring: We combine the image and Q/A repre- sentations to jointly score the (Q, I, A) triplet.To ensure equal contribution of language and visual fea- tures, we apply batch normalization [25] on linear transfor- mations of these features before adding them together to get a bimodal representation β(Q, I, A) ∈ R 2500 :Here, B 1 , B 2 denote batch normalization and W 1 ∈ R 2500×2000 and W 2 ∈ R 2500×1500 define the linear trans- formations. The bimodal representation is:with , the loss is written as  The representations produced by SVLR module should be directly usable in related vision-language tasks with- out any additional learning. To demonstrate this zero- shot cross-task transfer, we train the SVLR module us- ing Genome VR data only and apply to VQA. Since bi- modal pooling and scoring layers cannot be learned without VQA data, we use a proxy scoring function constructed us- ing region-word scores only. For each region, we compute p q (R) as the sum of its scores for the maximally aligned question nouns and question adjectives (Eq. 3 with only question words). A score p a (R) is similarly computed us- ing answer nouns and adjectives. The final score for the answer is defined by We use 100 region proposals resized to 224 × 224 for all experiments. Resnet-50 was used for image feature extrac- tion in all experiments except those in Tab. 3 which used Resnet-152. The nouns and adjectives are extracted from the (Q, A) and lemmatized using the part-of-speech tagger and WordNet lemmatizer in NLTK [8]. We use the Stan- ford Dependency Parser [13] to parse the question into bins as detailed in [49]. All models are implemented and trained using TensorFlow [1]. We train the model jointly for the recognition and VQA tasks by minimizing the following loss function using Adam [30]:R∈R where a is the attention score computed using Eq. 4. There- fore, the highest score is given to QA pairs where question as well as answer nouns and adjectives can be localized in the image. Note that the since the model is not trained on even a single question from VQA, the zero-shot VQA task also shows that our model does use the image to answer questions instead of solely relying on the language prior which is a common concern with most VQA models [2,19].We observe that values of α obj and α atr relative to α ans can be used to trade-off performance between visual recog- nition and VQA tasks. For experiments that analyze the effect of transfer from VR to VQA (Sec. 5.2), we set α ans = 1, α obj = 0.1, and α atr = 0.1. For VQA only and Genome only baselines, we set the corresponding α to 1 and others to 0. For experiments dealing with transfer in the other direction (Sec. 5.3), we set α ans = 0.1, α obj = 1, and α atr = 1. The margins used for object and answer losses are η ans = η obj = 1. The object and attribute losses are computed for the same set of Visual Genome regions with a batch size of M = 200. The answer loss is computed for a batch size of P = 50 questions sampled from VQA. We use an exponentially decaying learning rate schedule with an initial learning rate of 10 −3 and decay rate of 0.5 every Figure 4: Interpretable inference in VQA: Our model produces interpretable intermediate computation for region relevance and ob- ject/attribute predictions for the most relevant regions. Our region relevance explicitly grounds nouns and adjectives from the Q/A input in the image. We also show object and attribute predictions for the most relevant region identified for a few correctly answered questions. The relevance masks are generated from relevance scores projected back to their source pixels locations.24000 iterations. Weight decay is used on all trainable vari- ables with a coefficient of 10 −5 . All the variables are Xavier initialized [17].Our experiments investigate the extent to which using SVLR as a core representation improves transfer in mul- titask learning. We first analyze how including the VR task improves VQA (Sec. 5.2, Tab. 1). We find that using SVLR doubles the improvement compared to standard mul- titask learning, and demonstrate performance well above chance in a zero-shot setup (trained only on VR, applied to VQA). We then analyze improvement to VR due to train- ing with (weakly supervised) VQA (Sec. 5.3, Fig. 5). We find moderate overall improvements (1.2%), with the largest improvements for classes that have few VR training exam- ples. We also quantitatively evaluate how well our atten- tion maps correlate with that of humans using data provided by [12] in Table 2. We include results of our VQA system trained with ResNet-152 architecture on val, test-dev, test- std, along with state-of-the-art (Tab. 3).held-out and use the latter for model selection. The train- subset consists of 236,277 (Q, I, A) samples whereas train- held-out contains 12,072 samples. The val and test set con- tain 121,512 and 244,302 samples respectively. There are exactly 3 questions per image. We use VQA val for evalu- ating on specific question types. Visual Genome: We use only images from Visual Genome not in VQA (overlaps identified using md5 hashes). The se- lected images were divided into train-val-test using an 85- 5-10 split, yielding 1,565,280, 90,212 and 181,141 anno- tated regions in each. We use val for selecting the model for evaluating recognition performance.Our model is trained on two separate datasets: one for VQA supervision, one for visual recognition (attributes and object classification). We use the image-question-answer annotation triplets from Antol et al. [5] and bounding box annotations for object and attribute categories from Visual Genome [31]. The train-val-test splits for the datasets are as follows. VQA: We split the train set into train-subset and train- Table 1, we analyze the role of SVLR module for in- ductive transfer in both joint training and zero-shot settings. Joint Training: During joint training, the VR models and VQA model are simultaneously trained using object and attribute annotations from Genome, and Q/A annotations from the VQA dataset. The common approach to joint train- ing is to use a common network for extracting image fea- tures (e.g. class logits from ResNet), which feeds into the task-specific networks as input. We refer to this approach in Table 1 as Joint Multitask. This baseline is implemented by replacing g(y) (see Fig. 2), with a fixed set of vectors h y for each of the predetermined 1000 object and 1000 at- tribute categories in the VR models. The embedding g(y) is still in the VQA model, but is no longer shared across tasks. Our proposed Joint SVLR outperforms VQA-only by 2.4%, doubling the 1.2% improvement achieved by Joint Multi- task. Our formulation of VR and VQA tasks in terms of shared word-region representations more effectively trans-  Table 1: Inductive transfer from VR to VQA through SVLR in joint training and zero-shot settings: We evaluate the performance of our model with SVLR module trained jointly with VR and VQA supervision (provided by Genome and VQA datasets respectively) on the VQA task. We compare this jointly-trained model to a model trained on only VQA data. We also compare to a traditional multitask learning setup that is jointly trained on VQA and VR (i.e. uses same amount of data as Joint SVLR) and shares visual features but does not use the object and attribute word embeddings for recognition. While multitask learning outperforms VQA-only model, using the SVLR module doubles the improvement. Our model is most suited for the question types in bold that require visual recognition without specialized skills like counting or reading. Formulation of VR and attention in VQA in terms of inner products between word and region representations enables Zero-Shot VQA. In this setting we train on Genome VR data and apply to VQA val (Sec 5.2).fers recognition knowledge from VR than shared features. The gain is often larger on questions that involve recogni- tion (in bold in Table 1 Zero-Shot VQA: We evaluate Zero-shot VQA to fur- ther highlight transfer from VR to VQA. We train on only Genome VR annotations but test on VQA val. The model has not seen any Q/A training data, but achieves an overall accuracy of 16.4% where random guessing yields 5.6% (18 choices). Our zero-shot system does not exploit language priors, which alone can score as high as 54.0% [49]. This shows that some knowledge can be directly applied to re- lated tasks using SVLR without additional training.  Occurences in Genome Train Occurences in VQA TrainWe compare the performance of our SVLR based model trained jointly on VQA and VR data with a model trained only on Genome data to analyze transfer from VQA to VR. Genome test is used for evaluation. We observe an increase in the overall object recognition accuracy from 43.3% to 44.5%, whereas average attribute accuracy remained un- changed at 36.9%. In Fig. 5, we show that nouns that are rare in Genome (left columns) but have 20 or more exam- ples in VQA (upper rows) benefit the most from weak su- pervision provided by VQA. On average, we measure im- provement from 21% to 32% for the 8 classes that have fewer than 125 examples in Genome train but occur more than 160 times in VQA questions. We conducted the same analysis on Genome attributes, but did not observe any no- table pattern, possibly due to the inherent difficult in eval- uating the multi-label attribute classification problem (the absence of attributes is not annotated in Genome). Most gains are in nouns rare in Genome but common in VQA (top left), suggesting that the weak supervision provided by training VQA attention augments recognition performance via the SVLR. The numbers in each cell show the Genome-only mean accuracy +/-the change due to SVLR multitask training, followed by the number of classes in the cell in parentheses.As shown in Fig. 4, our VQA model produces inter- pretable intermediate outputs such as region relevance and visual category predictions, similar to [54]. The answer choice is explained by the object and attribute predictions associated with the most relevant regions. Because rele- vance is posed as the explicit localization of words in the question and answer, we can qualitatively evaluate the rel- evance prediction by verifying that the predicted regions match said words. This also provides greater insight into the failure modes as shown in Fig. 6.We also quantitatively evaluate our attention using col- lected human attention maps from Das et al. [12] in Table 2 Figure 6: Failure modes: Our model cannot count or read, though it will still identify the relevant regions. It is blind to relations and thus fails to recognize that birds, while present, are not drinking water. The model may give a low score to the correct answer despite accurate visual recognition. For instance, the model observes asphalt but predicts concrete, likely due to language bias. A clear example of an error due to language bias is in the top-left image as it believes the lady is holding a baby rather than a dog, even though visual recognition confirms evidence for dog. Finally, our model fails to answer questions that require complex reasoning comparing multiple regions.  Comparison was done by resizing attention maps to 14×14 and computing the Spearman rank correlation as in [12]. We include a strong baseline using a synthetic center-focused heatmap (also used by [12]) under the Center column. The Human column rep- resents the inter-annotator agreement. Scores for HiCo and SAN2 were recomputed using released data from [12], and differ slightly from originally reported. Our model leads to significantly higher correlation with human annotations than existing models. and in Figure 7. Table 2 includes the correlation scores between the attention maps from various VQA attention models and human attention on a subset of question-answer pairs on the validation set. Our proposed SVLR model significantly outperforms other models we compare with. However, we note that a strong center-focused heatmap baseline still outperforms all models, signifying that the main topic of a question is very often located in the cen- ter of the image. As such, we also evaluate correlations on multiple subsets of the human attention maps is Fig- ure 7, thresholding them based on correlation with the cen- ter heatmap. We note that learned attention models appear to have better correlation with human attention at lower thresholds where the human attention correlates poorly with the center-focused heatmap -a result also demonstrated in [12].  [38]. For test accuracy, it is unclear whether FDA uses val to train. The original MLP implementation [26] using Resnet-101 yields 64.9 and 65.2 on test-dev and test-std respectively. MCB reports only test-dev accuracy for the directly comparable model (final without ensemble). Note that the overall performance of our model is slightly worse than MLP and MCB because only about 10% of the VQA dataset benefits from visual attention. Our model achieves 62.1% on color questions using attention, outperforming WTL's 54% and MLP's 51.9%.In Table 4, we compare the word representations of the SVLR model to that of Word2Vec [41] by showing several nearest neighbors from both embeddings. We observe a shift from non-visual neighborhoods and meanings (mon- itor, control) to visual ones (monitor, keyboard). Neighbors were computed using cosine distance after mean centering.Humans learn new skills by building upon existing knowledge and experiences. We attempt to apply this be- havior to AI models by demonstrating cross-task learning Figure 7: Mean Spearman rank-correlation between model predicted and human attention at various thresholds. Each threshold point defines a subset of the dataset for which the human attention correlation with the synthetic center heatmap is below that threshold value. For example: the first sample point of each curve is the mean correlation of each model with human attention, measured on a subset in which the human attention's correlation with the center heatmap is less than or equal to 0. As can be seen, the attention maps produced by the proposed SVLR model corre- late with human attention significantly more than other models. As the threshold approaches 1, the synthetic center heatmap baseline outperforms all proposed models, confirming that the majority of the questions are about something in the center of the image. Note that due to slight differences in implementations, the subsets at ≤ 0 differ slightly from those used in [12] Word Word2Vec SVLR column newspaper, magazine, book, letter pillar, post, pole, tower, chimney counter curb, stem, foil, stop, dispenser shelf, stove, countertop, burner horn piano, guitar, brass, pedal tail, harness, tag, paw meat chicken, lamb, food, uncooked rice, scrambled, piled, slice monitor watch, control, checked, alert keyboard, computer, portable Table 4: Word Representations from SVLR vs Word2Vec: We compare nearest neighbors (cosine distance) for a set of words us- ing word2vec embeddings as well as SVLR.for the class of vision-language problems using VQA and VR. To enhance inductive transfer, we propose sharing core vision and language representations across all tasks in a way that exploits the word-region alignment. We plan to extend our method to larger sets of vision-language tasks.
