The statistical complexity, or capacity, of unregularized feed-forward neural networks, as a function of the network size and depth, is fairly well understood. With hard-threshold activations, the VC-dimension, and hence sample complexity, of the class of functions realizable with a feed-forward network is equal, up to logarithmic factors, to the number of edges in the network ( Anthony and Bartlett, 2009;Shalev-Shwartz and Ben-David, 2014), corresponding to the number of parameters. With continuous activation functions the VC- dimension could be higher, but is fairly well understood and is still controlled by the size and depth of the network.But feedforward networks are often trained with some kind of explicit or implicit reg- ularization, such as weight decay, early stopping, "max regularization", or more exotic regularization such as drop-outs. What is the effect of such regularization on the induced hypothesis class and its capacity?For linear prediction (a one-layer feed-forward network) we know that using regular- ization the capacity of the class can be bounded only in terms of the norms, with no (or a very weak) dependence on the number of edges (i.e. the input dimensionality or number of linear coefficients). E.g., we understand very well how the capacity of ℓ 2 -regularized linear predictors can be bounded in terms of the norm alone (when the norm of the data is also bounded), even in infinite dimension.A central question we ask is: can we bound the capacity of feed-forward network in terms of norm-based regularization alone, without relying on network size and even if the network size (number of nodes or edges) is unbounded or infinite? What type of regular- izers admit such capacity control? And how does the capacity behave as a function of the norm, and perhaps other network parameters such as depth?Beyond the central question of capacity control, we also analyze the convexity of the resulting hypothesis class-unlike unregularized size-controlled feed-forward networks, infinite magnitude-controlled networks have the potential of yielding convex hypothesis classes (this is the case, e.g., when we move from rank-based control on matrices, which limits the number of parameters to magnitude based control with the trace-norm or max- norm). A convex class might be easier to optimize over and might be convenient in other ways.In this paper we focus on networks with rectified linear units and two natural types of norm regularization: bounding the norm of the incoming weights of each unit (per-unit reg- ularization) and bounding the overall norm of all the weights in the system jointly (overall regularization, e.g. limiting the overall sum of the magnitudes, or square magnitudes, in the system). We generalize both of these with a single notion of group-norm regulariza- tion: we take the ℓ p norm over the weights in each unit and then the ℓ q norm over units. In Section 3 we present this regularizer and obtain a tight understanding of when it provides for size-independent capacity control and a characterization of when it induces convex- ity. We then apply these generic results to per-unit regularization (Section 4) and overall regularization (Section 5), noting also other forms of regularization that are equivalent to these two. In particular, we show how per-unit regularization is equivalent to a novel path- based regularizer and how overall ℓ 2 regularization for two-layer networks is equivalent to so-called "convex neural networks" ( Bengio et al., 2005). In terms of capacity control, we show that per-unit regularization allows size-independent capacity-control only with a per-unit ℓ 1 -norm, and that overall ℓ p regularization allows for size-independent capacity control only when p ≤ 2, even if the depth is bounded. In any case, even if we bound the sum of all magnitudes in the system, we show that an exponential dependence on the depth is unavoidable.As far as we are aware, prior work on size-independent capacity control for feed- forward networks considered only per-unit ℓ 1 regularization, and per-unit ℓ 2 regularization for two-layered networks (see discussion and references at the beginning of Section 4). Here, we extend the scope significantly, and provide a broad characterization of the types of regularization possible and their properties. In particular, we consider overall norm regu- larization, which is perhaps the most natural form of regularization used in practice (e.g. in the form of weight decay). We hope our study will be useful in thinking about, analyzing and designing learning methods using feed-forward networks. Another motivation for us is that complexity of large-scale optimization is often related to scale-based, not dimension- based complexity. Understanding when the scale-based complexity depends exponentially on the depth of a network might help shed light on understanding the difficulties in opti- mizing deep networks.A feedforward neural network that computes a function f : R D → R is specified by a directed acyclic graph (DAG) G(V, E) with D special "input nodes" v in [1], . . . , v in [D] ∈ V with no incoming edges and a special "output node" v out ∈ V with no outgoing edges, weights w : E → R on the edges, and an activation function σ : R → R.Given an input x ∈ R D , the output values of the input units are set to the coordinates of x, o(v in [i]) = x[i] (we might want to also add a special "bias" node with o(v in [0]) = 1, or just rely on the inputs having a fixed "bias coordinate"), the output value of internal nodes (all nodes except the input and output nodes) are defined according to the forward propagation equation:and the output value of the output unit is defined asThe network is then said to compute the function f G,w,σ (x) = o(v out ). Given a graphs G and activation function σ, we can consider the hypothesis class of functions N G,σ = {f G,w,σ : R D → R | w : E → R} computable using some setting of the weights. We will refer to the size of the network, which is the overall number of edges |E|, the depth d of the network, which is the length of the longest directed path in G, and the in-degree (or width) H of a network, which is the maximum in-degree of a vertex in G.A special case of feedforward neural networks are layered fully connected networks where vertices are partitioned into layers and there is a directed edge from every vertex in layer i to every vertex in layer i + 1. We index the layers from the first layer, i = 1 whose inputs are the input nodes, up to the last layer i = d which contains the single output node-the number of layers is thus equal to the depth and the in-degree is the maximal layer size. We denote by layer(d, H) the layered fully connected network with d layers and H nodes per layer (except the output layer that has a single node), and also allow H = ∞. We will also use the shorthandLayered networks can be parametrized by a sequence of matriceswhere the row W i [j, :] contains the input weights to unit j in layer i, andwhere σ is applied element-wise. We will focus mostly on the hinge, or RELU (REctified Linear Unit) activation, which is currently in popular use (Nair and Hinton, 2010;Bordes and Bengio, 2011;Zeiler et al., 2013), σ RELU (z) = [z] + = max(z, 0). When the activation will not be specified, we will implicitly be referring to the RELU. The RELU has several convenient properties which we will exploit, some of them shared with other activation functions:Lipshitz The hinge is Lipschitz continuous with Lipshitz constant one. This property is also shared by the sigmoid and the ramp activation σ(z) = min(max(0, z), 1). Idempotency The hinge is idempotent, i.e. σ RELU (σ RELU (z)) = σ RELU (z). This property is also shared by the ramp and hard threshold activations. Non-Negative Homogeneity For a non-negative scalar c ≥ 0 and any input z ∈ R we have σ RELU (c · z) = c · σ RELU (z). This property is important as it allows us to scale the incoming weights to a unit by c &gt; 0 and scale the outgoing edges by 1/c without changing the the function computed by the network. For layered graphs, this means we can scale W i by c and compensate by scaling W i+1 by 1/c.We will consider various measures α(w) of the magnitude of the weights w(·). Such a measure induces a complexity measure on functions f ∈ N G,σ defined by α G,σ (f ) = inf f G,w,σ =f α(w). The sublevel sets of the complexity measure α G,σ form a family of hy-Again we will use the shorthand α d,H,σ and α d,σ when referring to layered graphs layer(d, H) and layer(d, ∞) respectively, and frequently drop σ when RELU is implicitly meant.For binary function g : {±1} D → ±1 we say that g is realized by f with unit margin if ∀ x f (x)g(x) ≥ 1. A set of points S is shattered with unit margin by a hypothesis class N if all g : S → ±1 can be realized with unit margin by some f ∈ N .Considering the grouping of weights going into each edge of the network, we will consider the following generic group-norm type regularizer, parametrized by 1 ≤ p, q ≤ ∞:Here and elsewhere we allow q = ∞ with the usual conventions that ( z q i ) 1/q = sup z i and 1/q = 0 when it appears in other contexts. When q = ∞ the group regularizer (3) imposes a per-unit regularization, where we constrain the norm of the incoming weights of each unit separately, and when q = p the regularizer (3) is an "overall" weight regularizer, constraining the overall norm of all weights in the system. E.g., when q = p = 1 we are paying for the sum of all magnitudes of weights in the network, and q = p = 2 corresponds to overall weight-decay where we pay for the sum of square magnitudes of all weights (i.e. the overall Euclidean norm of the weights).For a layered graph, we have:  (4) holds regardless of the activation function, and so for any σ we have:But due to the homogeneity of the RELU activation, when this activation is used we can always balance the norm between the different layers without changing the computed func- tion so as to achieve equality in (4):Proof Let W be weights that realizes f and are optimal with respect to γ p,g ; i.e., and observe that they also realize f . We now have:which together with (4) completes the proof.The two measures are therefore equivalent when we use RELUs, and define the same level sets, or family of hypothesis classes, which we refer to simply as N d,H p,q . In the remainder of this Section, we investigate convexity and generalization properties of these hypothesis classes.In order to understand the effect of the norm on the sample complexity, we bound the Rademacher complexity of the classes N d,H p,q . Recall that the Rademacher Complexity is a measure of the capacity of a hypothesis class on a specific sample, which can be used to bound the difference between empirical and expected error, and thus the excess gener- alization error of empirical risk minimization (see, e.g., Bartlett and Mendelson (2003) for a complete treatment, and Appendix A for the exact definitions we use). In particular, the Rademacher complexity typically scales as C/m, which corresponds to a sample com- plexity of O(C/ǫ 2 ), where m is the sample size and C is the effective measure of capacity of the hypothesis class.and so:where the second inequalities hold only if 1 ≤ p ≤ 2, R linear m,p,D is the Rademacher complexity of D-dimensional linear predictors with unit ℓ p norm with respect to a set of m samples and p * is such thatProof sketch We prove the bound by induction, showing that for any q, d &gt; 1 and).The intuition is that when p * &lt; q, the Rademacher complexity increases by simply dis- tributing the weights among neurons and if p * ≥ q then the supremum is attained when the output neuron is connected to a neuron with highest Rademacher complexity in the lower layer and all other weights in the top layer are set to zero. For a complete proof, see Appendix A.Note that for 2 ≤ p &lt; ∞, the bound on the Rademacher complexity scales with m 1 p (see section A.1 in appendix) because:The bound in Theorem 1 depends on both the magnitude of the weights, as captured by µ p,q (W ) or γ p,q (W ), and also on the width H of the network (the number of nodes in each layer). However, the dependence on the width H disappears, and the bound depends only on the magnitude, as long as q ≤ p * (i.e. 1/p + 1/q ≥ 1). This happens, e.g., for overall ℓ 1 and ℓ 2 regularization, for per-unit ℓ 1 regularization, and whenever 1/p + 1/q = 1. In such cases, we can omit the size constraint and state the theorem for an infinite-width layered network (i.e. a network with an infinitely countable number of units, when the number of units is allowed to be as large as needed):, and any setand so:where the second inequalities hold only if 1 ≤ p ≤ 2 and R linear m,p,D is the Rademacher complexity of D-dimensional linear predictors with unit ℓ p norm with respect to a set of m samples.We next investigate the tightness of the complexity bound in Theorem 1, and show that when 1/p + 1/q &lt; 1 the dependence on the width H is indeed unavoidable. We show not only that the bound on the Rademacher complexity is tight, but that the implied bound on the sample complexity is tight, even for binary classification with a margin over binary inputs. To do this, we show how we can shatter the m = 2 D points {±1} D using a network with small group-norm: We construct the first layer using m units. Each unit has a unique weight vector consisting of +1 and −1's and will output a positive value if and only if the sign pattern of the input x ∈ S m matches that of the weight vector. The second layer has a single unit and connects to all m units in the first layer. For any m dimensional sign pattern b ∈ {−1, +1} m , we can choose the weights of the second layer to be b, and the network will output the desired sign for each x ∈ S m with unit margin. The norm of the network is at most. This establishes the claim for d = 2. For d &gt; 2 and 1/p + 1/q ≥ 1, we obtain the same norm and unit margin by adding d − 2 layers with one unit in each layer connected to the previous layer by a unit weight. For d &gt; 2 and 1/p + 1/q &lt; 1, we show the dependence on H by recur- sively replacing the top unit with H copies of it and adding an averaging unit on top of that. More specifically, given the above d = 2 layer network, we make H copies of the output unit with rectified linear activation and add a 3rd layer with one output unit with uniform weight 1/H to all the copies in the 2nd layer. Since this operation does not change the output of the network, we have the same margin and now the norm of the network isThat is, we have reduced the norm by factor H 1/q−1/p * . By repeating this process, we get the geometric reduction in the norm, which concludes the proof.To understand this lower bound, first consider the bound without the dependence on the width H. We have that for any depth d ≥ 2, γ ≤ m r D = m r log m (since 1/p ≤ 1 always) where r = 1/p + 1/q ≤ 2. This means that for any depth d ≥ 2 and any p, q the sample complexity of learning the class scales as m = Ω(γ 1/r / log γ) ≥ ˜ Ω( √ γ). This shows a polynomial dependence on γ, though with a lower exponent than the γ 2 (or higher for p &gt; 2) dependence in Theorem 1. Still, if we now consider the complexity control as a function of µ p,q we get a sample complexity of at least Ω(µ d/2 / log µ), establishing that if we control the group-norm as in (3), we cannot avoid a sample complexity which depends exponentially on the depth. Note that in our construction, all other factors in Theorem 1, namely max i i and log D, are logarithmic (or double-logarithmic) in m.Next we consider the dependence on the width H when 1/p + 1/q &lt; 1. Here we have to use depth d ≥ 3, and we see that indeed as the width H and depth d increase, the magnitude control γ can decrease as H(1/p * −1/q)(d−2) without decreasing the capacity, matching Theorem 1 up to an offset of 2 on the depth. In particular, we see that in this regime we can shatter an arbitrarily large number of points with arbitrarily low γ by using enough hidden units, and so the capacity of N d p,q is indeed infinite and it cannot ensure any generalization.Finally we establish a sufficient condition for the hypothesis classes N d p,q to be convex. We are referring to convexity of the functions in the N d p,q independent of a specific represen- tation. If we consider a, possibly regularized, empirical risk minimization problem on the weights, the objective (the empirical risk) would never be a convex function of the weights (for depth d ≥ 2), even if the regularizer is convex in w (which it always is for p, q ≥ 1). But if we do not bound the width of the network, and instead rely on magnitude-control alone, we will see that the resulting hypothesis class, and indeed the complexity measure, may be convex (with respect to taking convex combinations of functions, not of weights).In particular, under the condition of the Theorem, γ Proof sketch To show convexity, consider two functions f, g ∈ N d γp,q≤γ and 0 &lt; α &lt; 1, and let U and V be the weights realizing f and g respectively with γ p,q (U) ≤ γ and γ p,q (V ) ≤ γ. We will construct weights W realizing αf + (1 − α)g with γ p,q (W ) ≤ γ. This is done by first balancing U and V s.t. at each layer i p,q = d γ p,q (U) and i p,q = d γ p,q, (V ) and then placing U and V side by side, with no interaction between the units calculating f and g until the output layer. The output unit has weights αU d coming in from the f -side and weights (1 − α)V d coming in from the g-side. In Appendix B we show that under the condition in the theorem, γ p,q (W ) ≤ γ. To complete the proof, we also show γ d p,q is homogeneous and that this is sufficient for convexity.In this Section we will focus on the special case of q = ∞, i.e. when we constrain the norm of the incoming weights of each unit separately.Per-unit ℓ 1 -regularization was studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003) who showed generalization guarantees. A two-layer network of this form with RELU activation was also considered by Bach (2014), who stud- ied its approximation ability and suggested heuristics for learning it. Per-unit ℓ 2 regular- ization in a two-layer network was considered by Cho and Saul (2009), who showed it is equivalent to using a specific kernel. We now introduce Path regularization and discuss its equivalence to Per-Unit regularization.Consider a regularizer which looks at the sum over all paths from input nodes to the output node, of the product of the weights along the path:where p ≥ 1 controls the norm used to aggregate the paths. We can motivate this regularizer as follows: if a node does not have any high-weight paths going out of it, we really don't care much about what comes into it, as it won't have much effect on the output. The path-regularizer thus looks at the aggregated influence of all the weights. Referring to the induced regularizer φ(with the usual short- hands for layered graphs), we now observe that for layered graphs, path regularization and per-unit regularization are equivalent:It is important to emphasize that even for layered graphs, it is not the case that for all weights φ p (w) = γ p,∞ (w). E.g., a high-magnitude edge going into a unit with no non-zero outgoing edges will affect γ p,∞ (w) but not φ p (w), as will having high-magnitude edges on different layers in different paths. In a sense path regularization is as more careful regularizer less fooled by imbalance. Nevertheless, in the proof of Theorem 5 in Appendix C.1, we show we can always balance the weights such that the two measures are equal. The equivalence does not extend to non-layered graphs, since the lengths of different paths might be different. Again, we can think of path regularizer as more refined regularizer taking into account the local structure. However, if we consider all DAGs of depth at most d (i.e. with paths of length at most d), the notions are again equivalent (see proof in Appendix C.2):Theorem 6 For any p ≥ 1 and any d: γIn particular, for any graph G of depth d, we have that φ Note that in order to apply Corollary 2 and obtain a width-independent bound, we had to limit ourselves to p = 1. We further explore this issue next.Capacity As was previously noted, size-independent generalization bounds for bounded depth networks with bounded per-unit ℓ 1 norm have long been known (and make for a popular homework problem). These correspond to a specialization of Corollary 2 for the case p = 1, q = ∞. Furthermore, the kernel view of Cho and Saul (2009) allows obtaining size-independent generalization bound for two-layer networks with bounded per-unit ℓ 2 norm (i.e. a single infinite hidden layer of all possible unit-norm units, and a bounded ℓ 2 - norm output unit). However, the lower bound of Theorem 3 establishes that for any p &gt; 1, once we go beyond two layers, we cannot ensure generalization without also controlling the size (or width) of the network.Convexity An immediately consequence of Theorem 4 is that per-unit regularization, if we do not constrain the network width, is convex for any p ≥ 1. In fact, γare convex, we might hope that this might make learn- ing computationally easier. Indeed, one can consider functional-gradient or boosting-type strategies for learning a predictor in the class ( Lee et al., 1996). However, as Bach (2014) points out, this is not so easy as it requires finding the best fit for a target with a RELU unit, which is not easy. Indeed, applying results on hardness of learning intersections of half- spaces, which can be represented with small per-unit norm using two-layer networks, we can conclude that, subject to certain complexity assumptions, it is not possible to efficiently PAC learn N Sharing We conclude this Section with an observation on the type of networks obtained by per-unit, or equivalently path, regularization., and the out- degree of every internal (non-input) node in G is one. That is, the subgraph of G induced by the non-input vertices is a tree directed toward the output vertex.What the Theorem tells us is that we can realize every function as a tree with optimal per- unit norm. If we think of learning with an infinite fully-connected layered network, we can always restrict ourselves to models in which the non-zero-weight edges form a tree. This means that when using per-unit regularization we have no incentive to "share" lower- level units-each unit will only have a single outgoing edge and will only be used by a single down-stream unit. This seems to defy much of the intuition and power of using deep networks, where we expect lower layers to represent generic feature useful in many higher- level features. In effect, we are not encouraging any transfer between learning different aspects of the function (or between different tasks or classes, if we do have multiple output units). Per-unit regularization therefore misses out on much of the inductive bias that we might like to impose when using deep learning (namely, promoting sharing). Proof [of Theorem 9] For any f G,w ∈ N DAG(d) , we show how to construct such G and w. We first sort the vertices of G based on topological ordering such that the out-degree of the first vertex is zero. Let G 0 = G and w 0 = w. At each step i, we first set G i = G i−1 and w i = w i−1 and then pick the vertex u that is the ith vector in the topological ordering. If the out-degree of u is at most 1. Otherwise, for any edge (u → v) we create a copy of vertex u that we call it u v , add the edge (u v → v) to G i and connect all incoming edges of u with the same weights to every such u v and finally we delete the vertex u from G i together with all incoming and outgoing edges of u. It is easy to indicate that f G i ,w i = f G i−1 ,w i−1 . After at most |V | such steps, all internal nodes have out-degree one and hence the subgraph induced by non-input vertices will be a tree.In this Section, we will focus on "overall" ℓ p regularization, corresponding to the choice q = p, i.e. when we bound the overall (vectorized) norm of all weights in the system:Capacity For p ≤ 2, Corollary 2 provides a generalization guarantee that is independence of the width-we can conclude that if we use weight decay (overall ℓ 2 regularization), or any tighter ℓ p regularization, there is no need to limit ourselves to networks of finite size (as long as the corresponding dual-norm of the inputs are bounded). However, in Section 3.2 we saw that with d ≥ 3 layers, the regularizer degenerates and leads to infinite capacity classes if p &gt; 2. In any case, even if we bound the overall ℓ 1 -norm, the complexity increases exponentially with the depth.Convexity The conditions of Theorem 4 for convexity of N d 2,2 are ensured when p ≥ d. For depth d = 1, i.e. a single unit, this just confirms that ℓ p -regularized linear prediction is convex for p ≥ 1. For depth d = 2, we get convexity with ℓ 2 regularization, but not ℓ 1 . For depth d &gt; 2 we would need p &gt; d ≥ 3, however for such values of p we know from Theorem 3 that N d p,p degenerates to an infinite capacity class if we do not control the width (if we do control the width, we do not get convexity). This leaves us with N 2 2,2 as the interesting convex class. Below we show an explicit convex characterization of N 2 2,2 by showing it is equivalent to so-called "convex neural nets".Convex Neural Nets ( Bengio et al., 2005) over inputs in R D are two-layer networks with a fixed infinite hidden layer consisting of all units with weights w ∈ G for some base class G ∈ R D , and a second ℓ 1 -regularized layer. Since over finite data the weights in the second layer can always be taken to have finite support (i.e. be non-zero for only a finite number of first-layer units), and we can approach any function with countable support, we can instead think of a network in N 2 where the bottom layer is constraint to G and the top layer is ℓ 1 regularized. Focusing on G = {w | p ≤ 1}, this corresponds to imposing an ℓ p constraint on the bottom layer, and ℓ 1 regularization on the top layer and yields the following complexity measure over N 2 :This is similar to per-unit regularization, except we impose different norms at different layers (if p = 1). We can see that N 2 νp≤ν = ν · conv(σ(G)), and is thus convex for any p. Focusing on RELU activation we have the equivalence:That is, overall ℓ 2 regularization with two layers is equivalent to a convex neural net with ℓ 2 -constrained units on the bottom layer and ℓ 1 (not ℓ 2 !) regularization on the output. Proof We can calculate:Here (9) is the arithmetic-geometric mean inequality for which we can achieve equality by balancing the weights (as in Claim 1) and (10) again follows from the homogeneity of the RELU which allows us to rebalance the weights.Hardness As with N d 1,∞ , we might hope that the convexity of N 2 2,2 might make it com- putationally easy to learn. However, by the same reduction from learning intersection of halfspaces (Theorem 22  Up until now we discussed relying on magnitude-based regularization instead of directly controlling network size, thus allowing unbounded and even infinite width. But we still relied on a finite bound on the depth in all our derivations. Can the explicit dependence on the depth be avoided, and replaced with only a measure of scale of the weights? We already know we cannot rely only on a bound on the group-norm µ p,q when the depth is unbounded, as we know from Theorem 3 that in terms of µ p,q the sample complex- ity necessarily increases exponentially with the depth: if we allow arbitrarily deep graphs we can shrink µ p,q toward zero without changing the scale of the computed function. How- ever, controlling the γ-measure, or equivalently the path-regularizer φ, in arbitrarily-deep graphs is sensible, and we can define:where the minimization is over any DAG. From Theorem 6 we can conclude that φ p (f ) = γ p,∞ (f ). In any case, γ p,q (f ) is a sensible complexity measure, that does not collapse despite the unbounded depth. Can we obtain generalization guarantees for the class N γp,q≤γ ? Unfortunately, even when 1/p + 1/q ≥ 1 and we can obtain width-independent bounds, the bound in Corollary 2 still has a dependence on 4 d , even if γ p,q is bounded. Can such a dependence be avoided?For anti-symmetric Lipschitz-continuous activation functions (i.e. such that σ(−z) = −σ(z)), such as the ramp, and for per-unit ℓ p -regularization µ The proof is again based on an inductive argument similar to Theorem 1 and you can find it in appendix A.4. However, the ramp is not homogeneous and so the equivalent between µ, γ and φ breaks down. Can we obtain such a bound also for the RELU? At the very least, what we can say is that an inductive argument such that used in the proofs of Theorems 1 and 12 cannot be used to avoid an exponential dependence on the depth. To see this, consider γ 1,∞ ≤ 1 (this choice is arbitrary if we are considering the Rademacher complexity), for which we havewhere conv(·) is the symmetric convex hull, and [·] + = max(z, 0) is applied to each func- tion in the class. In order to apply the inductive argument without increasing the complex- ity exponentially with the depth, we would need the operation [conv(H)] + to preserve the Rademacher complexity, at least for non-negative convex cones H. However we show a simple example of a non-negative convex cone H for which R m ([conv(H)] + ) &gt; R m (H).We will specify H as a set of vectors in R m , corresponding to the evaluation of h(x i ) of different functions in the class on the m points x i in the sample. In our construction, we will have only m = 3 points. Consider H = conv({ (1, 0, 1 = R m (H).We presented a general framework for norm-based capacity control for feed-forward net- works, and analyzed when the norm-based control is sufficient and to what extent capacity still depends on other parameters. In particular, we showed that in depth d &gt; 2 networks, per-unit control with p &gt; 1 and overall regularization with p &gt; 2 is not sufficient for capac- ity control without also controlling the network size. This is in contrast with linear models, where with any p &lt; ∞ we have only a weak dependence on dimensionality, and two-layer networks where per-unit p = 2 is also sufficient for capacity control. We also obtained generalization guarantees for perhaps the most natural form of regularization, namely ℓ 2 regularization, and showed that even with such control we still necessarily have an expo- nential dependence on the depth. Although the additive µ-measure and multiplication γ-measure are equivalent at the optimum, they behave rather differently in terms of optimization dynamics (based on anec- dotal empirical experience) and understanding the relationship between them, as well as the novel path-based regularizer can be helpful in practical regularization of neural networks.Although we obtained a tight characterization of when size-independent capacity con- trol is possible, the precise polynomial dependence of margin-based classification (and other tasks) on the norm in might not be tight and can likely be improved, though this would require going beyond bounding the Rademacher complexity of the real-valued class. In particular, Theorem 1 gives the same bound for per-unit ℓ 1 regularization and overall ℓ 1 regularization, although we would expect the later to have lower capacity.Beyond the open issue regarding depth-independent γ-based capacity control, another interesting open question is understanding the expressive power of N Peter L. Bartlett. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE transactions on information theory, 44 (2) The sample based Rademacher complexity of a class F of function mapping from X to R with respect to a set S = {x 1 , . . . , x m } is defined as:In this section, we prove an upper bound for the Rademacher complexity of the class N d,H,σ RELU γp,q≤γ , i.e., the class of functions that can be represented as depth d, width H network with rectified linear activations, and the layer-wise group norm complexity γ p,q bounded by γ. As mentioned in the main text, our proof is an induction with respect to the depth d. We start with d = 1 layer neural networks, which is essentially the class of linear separators.For completeness, we prove the upper bounds on the Rademacher complexity of class of linear separators with bounded ℓ p norm. The upper bounds presented here are particularly similar to generalization bounds in Kakade et al. (2009) and Balcan and Berlind (2014). We first mention two already established lemmas that we use in the proofs.Theorem 13 (Khintchine-Kahane Inequality) For any 0 &lt; p &lt; ∞ and S = {z 1 , . . . , z m }, if the random variable ξ is uniform over {±1} m , thenwhere C p is a constant depending only on p.The sharp value of the constant C p was found by Haagerup (1981) but for our analysis, it is enough to note that if p ≥ 1 we have C p ≤ √ p.where |A| is the cardinality of A.We are now ready to show upper bounds on Rademacher complexity of linear separators with bounded ℓ p norm.Proof First, note that N 1 is the class of linear functions and hence for any function f w ∈ N 1 , we have that γ p,q (w) = p . Therefore, we can write the Rademacher complexity for a set S = {x 1 , . . . , x m } as:2 log(2D) 2 log(2D)−1 (and therefore 2 log(2D) ≤ p * ), we have2 log(2D) 2 log(2D)−1 &lt; p &lt; ∞, by Khintchine-Kahane inequality we have We define the hypothesis class N d,H,H to be the class of functions from X to R H computed by a layered network of depth d, layer size H and H outputs.For the proof of theorem 1, we need the following two technical lemmas. The first is the well-known contraction lemma:Lemma 16 (Contraction Lemma) Let function φ : R → R be Lipschitz with constant L φ such that φ satisfies φ(0) = 0. Then for any class F of functions mapping from X to R and any set S = {x 1 , . . . , x m }:Next, the following lemma reduces the maximization over a matrix W ∈ R H×H that ap- pears in the computation of Rademacher complexity to H independent maximizations over a vector w ∈ R H (the proof is deferred to Subsection A.3):where p * is such thatand so:m where p * is such thatProof By the definition of Rademacher complexity if ξ is uniform over {±1} m , we have:where the equality (13) is obtained by lemma 17 and inequality (14) is by Contraction Lemma. This will give us the bound on Rademacher complexity of N d,H γp,q≤γ based on the Rademacher complexity of N d−1,H γp,q≤γ . Applying the same argument on all layers and using lemma 15 to bound the complexity of the first layer completes the proof.Proof It is immediate that the right hand side of the equality in the statement is always less than or equal to the left hand side because given any vector w in the right hand side, by setting each row of matrix W in the left hand side we get the equality. Therefore, it is enough to prove that the left hand side is less than or equal to the right hand side. For the convenience of notations, let g(w)If q ≤ p * , then the right hand side of equality in the lemma statement will reduce to g( w)/ w p and therefore we need to show that for any matrix V ,Since q ≤ p * , we have p,p * ≤ p,q and hence it is enough to prove the following inequality:On the other hand, if q &gt; p * , then we need to prove the following inequality holds:We can rewrite the above inequality in the following form:By the definition of w, we know that the above inequality holds for each term in the sum and hence the inequality is true.The proof is similar to the proof of theorem 1 but here bounding µ 1,∞ by µ means the ℓ 1 norm of input weights to each neuron is bounded by µ. We use a different version of Contraction Lemma in the proof that is without the absolute value:Lemma 18 (Contraction Lemma (without the absolute value)) Let function φ : R → R be Lipschitz with constant L φ . Then for any class F of functions mapping from X to R and any set S = {x 1 , . . . , x m }:   where inequalities 20 to 24 are due to the fact that the ℓ p norm of input weights to each in- ternal neuron is exactly 1 and the last equality is again because ℓ p,∞ of all layers is exactly 1 except the layer d.In this section, without loss of generality, we assume that all the internal nodes in a DAG have incoming edges and outgoing edges because otherwise we can just discard them. Let d out (v) be the longest directed path from vertex v to v out and d in (v) be the longest directed path from any input vertex v in [i] to v. We say graph G is a sublayered graph if G is a subgraph of a layered graph. We first show the necessary and sufficient conditions under which a DAG is a sublay- ered graph.The graph G(E, V ) is a sublayered graph if and only if any path from input nodes to the output nodes has length d where d is the length of the longest path in G Proof Since the internal nodes have incoming edges and outgoing edges; hence if G is a sublayered graph it is straightforward by induction on the layers that for every vertex v in layer i, there is a vertex u in layer i + 1 such that (v → u) ∈ E and this proves the necessary condition for being sublayered graph.To show the sufficient condition, for any internal node u, u has d in (v) distance from the input node in every path that includes u (otherwise we can build a path that is longer than d). Therefore, for each vertex v ∈ V , we can place vertex v in layer d in (v) and all the outgoing edges from v will be to layer d in (v) + 1. 
